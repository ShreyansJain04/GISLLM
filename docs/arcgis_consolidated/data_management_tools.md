# Data Management Tools

*Consolidated from 433 individual documentation files*

---

## Add 3D Formats To Multipatch (Data Management)

## Summary

Converts a multipatch to a 3D object feature layer by linking the feature class with one or more 3D model formats.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input geodatabase multipatch feature that will be converted to a 3D object feature layer. | Table View |
| Use multipatch materials (Optional) | Note:This parameter is no longer supported. The option to control whether multipatch materials were used was removed to improve this tool's usability. Materials will always be used when they are available through the 3D object feature layer, and they will be automatically removed when the 3D object feature layer capabilities are removed from the multipatch. Additional parameters are available using the dedicated API for multipatches. | Boolean |
| 3D Formats to Add (Optional) | Specifies the 3D formats that will be associated with the multipatch features. Each input feature will be duplicated for each selected format. The available options depend on the codecs installed on the computer.Collada (.dae)—The COLLADA format will be added.Autodesk Drawing (.dwg)—The DWG format will be added.Autodesk (.fbx)—The Autodesk FilmBox format will be added.Khronos Group glTF binary (.glb)—The binary Graphics Library Transmission format will be added.Khronos Group glTF json (.gltf)—The JSON Graphics Library Transmission format will be added.Industry Foundation Classes (.ifc)—The Industry Foundation Classes format will be added.Wavefront (.obj)—The Wavefront format will be added.Universal Scene Description (.usdc)— The Universal Scene Description format will be added. Compressed Universal Scene Description (.usdz)— The compressed version of the Universal Scene Description format will be added. | String |
| in_features | The input geodatabase multipatch feature that will be converted to a 3D object feature layer. | Table View |
| multipatch_materials(Optional) | Note:This parameter is no longer supported. The option to control whether multipatch materials were used was removed to improve this tool's usability. Materials will always be used when they are available through the 3D object feature layer, and they will be automatically removed when the 3D object feature layer capabilities are removed from the multipatch. Additional parameters are available using the dedicated API for multipatches. | Boolean |
| formats[formats,...](Optional) | Specifies the 3D formats that will be associated with the multipatch features. Each input feature will be duplicated for each selected format. The available options depend on the codecs installed on the computer.FMT3D_DAE—The COLLADA format will be added.FMT3D_DWG—The DWG format will be added.FMT3D_FBX—The Autodesk FilmBox format will be added.FMT3D_GLB—The binary Graphics Library Transmission format will be added.FMT3D_GLTF—The JSON Graphics Library Transmission format will be added.FMT3D_IFC—The Industry Foundation Classes format will be added.FMT3D_OBJ—The Wavefront format will be added.FMT3D_USDC— The Universal Scene Description format will be added. FMT3D_USDZ— The compressed version of the Universal Scene Description format will be added. | String |

## Code Samples

### Example 1

```python
arcpy.management.Add3DFormats(in_features, {multipatch_materials}, {formats})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/data/city_models.gdb'
arcpy.management.Add3DFormats('Downtown_Buildings', 'MULTIPATCH_WITH_MATERIALS', 
                              ['FMT3D_DAE', 'FMT3D_OBJ'])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/data/city_models.gdb'
arcpy.management.Add3DFormats('Downtown_Buildings', 'MULTIPATCH_WITH_MATERIALS', 
                              ['FMT3D_DAE', 'FMT3D_OBJ'])
```

---

## Add Attachments (Data Management)

## Summary

Adds file attachments to the records of a geodatabase feature class or table. The attachments are stored in the geodatabase in a separate attachment table that maintains linkage to the target dataset. Attachments are added to the target dataset using a match table that indicates for each input record (or an attribute group of records) the path to a file to add as an attachment to that record.

## Usage

- This tool does not honor selections.
- Before attachments can be added using this tool, they must be enabled using the Enable Attachments tool.
- Attachments that are added using this tool will be copied to the geodatabase. The original attachment files will not be affected. If the original files are modified, the changes will not be automatically made to the geodatabase attachment. To synchronize changes to the geodatabase, remove the affected attachments using the Remove Attachments tool. Then add the modified files back as new attachments.
- If the Input Dataset parameter value contains an existing field that is the path to the attachments to add, and you do not want to use a separate value for the Match Table parameter, specify the same dataset for both the Input Dataset and Match Table parameters. The tool will automatically select the Object ID field for both join fields, and you can specify which field from the input contains the paths to the attachment files.
- Multiple files can be attached to a single feature class or table record. To accomplish this, ensure that the Match Table parameter value contains multiple records for that input ID. For example, record 1 has an input ID of 1 and a path name of pic1a.jpg, and record 2 has an input ID of 1 and a path name of pic1b.jpg.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The geodatabase table or feature class where attachments will be added. Attachments are not added directly to this table, but to a related attachment table that maintains linkage to the input dataset. The dataset must be stored in a version 10.0 or later geodatabase, and the table must have attachments enabled. | Table View |
| Input Join Field | A field from the Input Dataset parameter value that has matching values in the Match Join Field parameter value. Records with matching values will have attachments added. This field can be an Object ID field or any other identifying attribute. | Field |
| Match Table | A table that identifies which input records will have attachments added and the paths to those attachments. | Table View |
| Match Join Field | A field from the Match Table parameter value that indicates which records in the Input Dataset parameter value will have specified attachments added. | Field |
| Match Path Field | A field from the Match Table parameter value that contains paths to the attachments to add to the records of the Input Dataset parameter value. A field from the in_match_table parameter value that contains paths to the attachments to add to the records of the in_dataset parameter value. | Field |
| Working Folder (Optional) | A folder or workspace where attachment files are centralized. By specifying a working folder, the paths in the Match Path Field parameter value can be the short names of files relative to the working folder. For example, if loading attachments with paths such as C:\MyPictures\image1.jpg and C:\MyPictures\image2.jpg, use a parameter value of C:\MyPictures. Then the paths in the Match Path Field parameter value can be the short names such as image1.jpg and image2.jpg instead of the full paths. | Folder |
| in_dataset | The geodatabase table or feature class where attachments will be added. Attachments are not added directly to this table, but to a related attachment table that maintains linkage to the input dataset. The dataset must be stored in a version 10.0 or later geodatabase, and the table must have attachments enabled. | Table View |
| in_join_field | A field from the in_dataset parameter value that has matching values in the in_match_join_field parameter value. Records with matching values will have attachments added. This field can be an Object ID field or any other identifying attribute. | Field |
| in_match_table | A table that identifies which input records will have attachments added and the paths to those attachments. | Table View |
| in_match_join_field | A field from the in_match_table parameter value that indicates which records in the in_dataset parameter value will have specified attachments added. | Field |
| in_match_path_field | A field from the Match Table parameter value that contains paths to the attachments to add to the records of the Input Dataset parameter value. A field from the in_match_table parameter value that contains paths to the attachments to add to the records of the in_dataset parameter value. | Field |
| in_working_folder(Optional) | A folder or workspace where attachment files are centralized. By specifying a working folder, the paths in the in_match_path_field parameter value can be the short names of files relative to the working folder. For example, if loading attachments with paths such as C:\MyPictures\image1.jpg and C:\MyPictures\image2.jpg, use a parameter value of C:\MyPictures. Then the paths in the in_match_path_field parameter value can be the short names such as image1.jpg and image2.jpg, instead of the full paths. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.AddAttachments(in_dataset, in_join_field, in_match_table, in_match_join_field, in_match_path_field, {in_working_folder})
```

### Example 2

```python
import arcpy
arcpy.management.AddAttachments(
    r"C:\Data\City.gdb\Parcels", "ParcelID", r"C:\Data\matchtable.csv", 
    "ParcelID", "Picture", r"C:\Pictures")
```

### Example 3

```python
import arcpy
arcpy.management.AddAttachments(
    r"C:\Data\City.gdb\Parcels", "ParcelID", r"C:\Data\matchtable.csv", 
    "ParcelID", "Picture", r"C:\Pictures")
```

### Example 4

```python
"""
Example: We have a folder of .jpg photographs and .pdf files;
We want to add these files as attachments to our input dataset.
Create a match table, enable input dataset if needed, add attachments.
"""
import arcpy

input_dataset = r"C:\Data\City.gdb\Parcels"
input_path = arcpy.Describe(input_dataset).path

# GenerateAttachmentsMatchtable parameters
# Required
match_table = r"C:\Data\City.gdb\parcelsMatchtable"
key_field = 'PARCELS'
# Optional
data_filter ='*.jpg; *.pdf'  # Remove *.pdf to load only the .jpg images as another option.
rel_path = "RELATIVE"
match_pattern = 'ANY'
# pic_folder required for match table, optional for add attachments if rel_path is ABSOLUTE
pic_folder = r"C:\Pictures\Parcels"

arcpy.management.GenerateAttachmentMatchTable(input_dataset, pic_folder, match_table, key_field,
                                              data_filter, rel_path, match_pattern)

# Check if input dataset is enabled for attachments
rel_class = arcpy.Describe(input_dataset).relationshipClassNames
rel_class_path = (f"{input_path}\\{relclass[0]}")
rel_class_att = arcpy.Describe(relclassPath).isAttachmentRelationship

if rel_class_att:
    pass
else:
    arcpy.management.EnableAttachments(input_dataset)

# AddAttachments parameters
add_match_table = match_table
input_join_fld = 'ObjectID'
match_join_fld = 'MatchID'
match_path_fld = 'Filename'

# Add attachments from generated match table to input data ATTACH table
arcpy.management.AddAttachments(input_dataset, input_join_fld, add_match_table,
                                match_join_fld, match_path_fld, pic_folder)
```

### Example 5

```python
"""
Example: We have a folder of .jpg photographs and .pdf files;
We want to add these files as attachments to our input dataset.
Create a match table, enable input dataset if needed, add attachments.
"""
import arcpy

input_dataset = r"C:\Data\City.gdb\Parcels"
input_path = arcpy.Describe(input_dataset).path

# GenerateAttachmentsMatchtable parameters
# Required
match_table = r"C:\Data\City.gdb\parcelsMatchtable"
key_field = 'PARCELS'
# Optional
data_filter ='*.jpg; *.pdf'  # Remove *.pdf to load only the .jpg images as another option.
rel_path = "RELATIVE"
match_pattern = 'ANY'
# pic_folder required for match table, optional for add attachments if rel_path is ABSOLUTE
pic_folder = r"C:\Pictures\Parcels"

arcpy.management.GenerateAttachmentMatchTable(input_dataset, pic_folder, match_table, key_field,
                                              data_filter, rel_path, match_pattern)

# Check if input dataset is enabled for attachments
rel_class = arcpy.Describe(input_dataset).relationshipClassNames
rel_class_path = (f"{input_path}\\{relclass[0]}")
rel_class_att = arcpy.Describe(relclassPath).isAttachmentRelationship

if rel_class_att:
    pass
else:
    arcpy.management.EnableAttachments(input_dataset)

# AddAttachments parameters
add_match_table = match_table
input_join_fld = 'ObjectID'
match_join_fld = 'MatchID'
match_path_fld = 'Filename'

# Add attachments from generated match table to input data ATTACH table
arcpy.management.AddAttachments(input_dataset, input_join_fld, add_match_table,
                                match_join_fld, match_path_fld, pic_folder)
```

---

## Add Attribute Index (Data Management)

## Summary

Adds an attribute index to an existing table, feature class, shapefile, or attributed relationship class.

## Usage

- Shapefiles and file geodatabases do not support multiple indexes. Additional fields will become part of a composite index (that is, an index created on multiple fields in a table).
- A new index is added for each unique index name in a geodatabase. If an index name already exists, it must be dropped before it can be updated.
- For enterprise geodatabase data that is not registered as versioned, you can add both unique or nonunique indexes to GlobalID fields.
- Unique and ascending indexes are not supported for shapefiles or file geodatabases. These parameters are ignored when the tool is run on a shapefile or on file geodatabase data.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the fields to be indexed. | Mosaic Layer; Raster Layer; Table View |
| Fields to Index | The list of fields that will be indexed. Any number of fields can be specified. | Field |
| Index Name(Optional) | The name of the new index. An index name is necessary when adding an index to geodatabase feature classes and tables. For other types of input, the name is ignored. | String |
| Unique(Optional) | Specifies whether the values in the index are unique.Unchecked—No values in the index are unique. This is the default. Checked—All values in the index are unique. | Boolean |
| Ascending(Optional) | Specifies whether values will be indexed in ascending order.Unchecked—Values will not be indexed in ascending order. This is the default. Checked—Values will be indexed in ascending order. | Boolean |
| in_table | The table containing the fields to be indexed. | Mosaic Layer; Raster Layer; Table View |
| fields[fields,...] | The list of fields that will be indexed. Any number of fields can be specified. | Field |
| index_name(Optional) | The name of the new index. An index name is necessary when adding an index to geodatabase feature classes and tables. For other types of input, the name is ignored. | String |
| unique(Optional) | Specifies whether the values in the index are unique.NON_UNIQUE—No values in the index are unique. This is the default.UNIQUE—All values in the index are unique. | Boolean |
| ascending(Optional) | Specifies whether values will be indexed in ascending order.NON_ASCENDING—Values will not be indexed in ascending order. This is the default. ASCENDING—Values will be indexed in ascending order. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AddIndex(in_table, fields, {index_name}, {unique}, {ascending})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/input/indices.sde"
arcpy.management.AddIndex("gdb.USER1.lakes", ["NAME", "geocompID"], "NGIndex", 
                          "UNIQUE", "ASCENDING")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/input/indices.sde"
arcpy.management.AddIndex("gdb.USER1.lakes", ["NAME", "geocompID"], "NGIndex", 
                          "UNIQUE", "ASCENDING")
```

### Example 4

```python
# Name: AddAttIndex.py
# Description: Create an attribute Index for specified fields

# Import system modules
import arcpy

# Set a default workspace
arcpy.env.workspace = "c:/data"

# Create an attribute index for the few fields listed in command.
arcpy.management.AddIndex("counties.shp", ["NAME", "STATE_FIPS", "CNTY_FIPS"], 
                          "#", "NON_UNIQUE", "NON_ASCENDING")
arcpy.management.AddIndex("mexico.gdb/land/lakes", ["NAME", "geocompID"], 
                          "NGIndex", "NON_UNIQUE", "NON_ASCENDING")
```

### Example 5

```python
# Name: AddAttIndex.py
# Description: Create an attribute Index for specified fields

# Import system modules
import arcpy

# Set a default workspace
arcpy.env.workspace = "c:/data"

# Create an attribute index for the few fields listed in command.
arcpy.management.AddIndex("counties.shp", ["NAME", "STATE_FIPS", "CNTY_FIPS"], 
                          "#", "NON_UNIQUE", "NON_ASCENDING")
arcpy.management.AddIndex("mexico.gdb/land/lakes", ["NAME", "geocompID"], 
                          "NGIndex", "NON_UNIQUE", "NON_ASCENDING")
```

---

## Add Attribute Rule (Data Management)

## Summary

Adds an attribute rule to a dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class that will have the new rule applied. | Table View |
| Name | A unique name for the new rule. | String |
| Type | Specifies the type of attribute rule that will be added. Calculation—Attribute values will be automatically populated for features when another attribute is set on a feature. These rules are applied based on the triggering events specified. Long running calculations can be set to run in batch mode and will be evaluated at a user-defined time.When adding multiple calculation rules, the order in which the rules are added is important if there are circular dependencies. For example, Rule A calculates Field1 is equal to the value of $feature.Field2 + $feature.Field3, and Rule B calculates Field4 is equal to $feature.Field1 + $feature.Field5; the results of the calculation may be different depending on the order the rules are added. Constraint—Permissible attribute configurations will be specified on a feature. When the constraint rule is violated, an error is generated and the feature is not stored. For example, if the value of Field A must be less than the sum of Field B and Field C, an error will be generated when that constraint is violated.Validation—Existing features will be identified with a batch validation process. Rules are evaluated at a user-defined time. When a rule is violated, an error feature is created. | String |
| Script Expression | The Arcade expression that defines the rule. | Calculator Expression |
| Is Editable (Optional) | Specifies whether the attribute value can be edited. Attribute rules can be configured to either block or allow editors to edit the attribute values of the field being calculated. This parameter is only applicable for the calculation attribute rule type.Checked—The attribute value can be edited. This is the default.Unchecked—The attribute value cannot be edited. | Boolean |
| Triggering Events (Optional) | Specifies the editing events that will trigger the attribute rule to take effect. This parameter is valid for calculation and constraint rule types only. At least one triggering event must be provided for calculation rules in which the Batch parameter is unchecked. Triggering events are not applicable for calculation rules that have the Batch parameter checked.Insert—The rule will be applied when a new feature is added.Update—The rule will be applied when a feature is updated.Delete—The rule will be applied when a feature is deleted. | String |
| Error Number (Optional) | An error number that will be returned when this rule is violated. This value is not required to be unique, so the same custom error number may be returned for multiple rules. This parameter is required for the constraint and validation rule types. It is optional for the calculation rule type. | String |
| Error Message (Optional) | An error message that will be returned when this rule is violated. It is recommended that you use a descriptive message to help the editor understand the violation when it occurs. The message is limited to 256 characters. This parameter is required for the constraint and validation rule types. It is optional for the calculation rule type. | String |
| Description (Optional) | The description of the new attribute rule. The description is limited to 256 characters. | String |
| Subtype (Optional) | The subtype to which the rule will be applied if the dataset has subtypes. | String |
| Field (Optional) | The name of an existing field to which the rule will be applied. This parameter is only applicable for the calculation attribute rule type. | String |
| Exclude from application evaluation(Optional) | Specifies whether the application will evaluate the rule locally before applying the edits to the workspace. Not all clients have the capability to run all of the available rules, so authors can exclude certain rules from client evaluation. For example, some rules may refer to data that has not been made available to all clients (reasons can include the data is offline, size, or security), or some rules may depend on the user or context (that is, a lightweight field update in a data collection app may not run a rule that requires additional user input or knowledge; however, a client such as ArcGIS Pro may support it). This parameter is not applicable for validation rule or calculation rule types if the Batch parameter is checked. Checked—The rule will be excluded from client evaluation.Unchecked—The rule will not be excluded from client evaluation. This is the default.Note:For versions earlier than ArcGIS Pro 2.4, this parameter was labeled Server only. | Boolean |
| Batch (Optional) | Specifies whether the rule evaluation will be run in batch mode. Checked—The rule evaluation will be run in batch mode at a later time by running validate.Unchecked—The rule evaluation will not be run in batch mode. Triggering events will be used to determine when the rule is evaluated for insert, update, or delete edit operations. This is the default.For calculation rules, this parameter can be checked or unchecked. For validation rules, this parameter must be unchecked. For constraint rules, this parameter must be unchecked. | Boolean |
| Severity (Optional) | The severity of the error. A value within the range of 1 through 5 can be chosen to define the severity of the rule. A value of 1 is high, being the most severe, and a value of 5 is low, being the least severe. For example, you can provide a low severity for a specific attribute rule and ignore the error during data production workflows, or set a high severity in which the error must be fixed for accuracy of the data collected.This parameter is only applicable to validation rules. | Long |
| Tags (Optional) | A set of tags that identify the rule (for searching and indexing) as a way to map to a functional requirement in a data model. | String |
| Triggering Fields (Optional) | A list of fields that will trigger an attribute rule to run when an editing event occurs during an update trigger for calculation and constraint attribute rules. If no fields are specified, the tool will use all fields. This is default.To enter multiple triggering fields, use a semicolon delimiter, for example, Field1;Field2;Field3. | String |
| in_table | The table or feature class that will have the new rule applied. | Table View |
| name | A unique name for the new rule. | String |
| type | Specifies the type of attribute rule that will be added. CALCULATION—Attribute values will be automatically populated for features when another attribute is set on a feature. These rules are applied based on the triggering events specified. Long running calculations can be set to run in batch mode and will be evaluated at a user-defined time.When adding multiple calculation rules, the order in which the rules are added is important if there are circular dependencies. For example, Rule A calculates Field1 is equal to the value of $feature.Field2 + $feature.Field3, and Rule B calculates Field4 is equal to $feature.Field1 + $feature.Field5; the results of the calculation may be different depending on the order the rules are added. CONSTRAINT—Permissible attribute configurations will be specified on a feature. When the constraint rule is violated, an error is generated and the feature is not stored. For example, if the value of Field A must be less than the sum of Field B and Field C, an error will be generated when that constraint is violated.VALIDATION—Existing features will be identified with a batch validation process. Rules are evaluated at a user-defined time. When a rule is violated, an error feature is created. | String |
| script_expression | The Arcade expression that defines the rule. | Calculator Expression |
| is_editable(Optional) | Specifies whether the attribute value can be edited. Attribute rules can be configured to either block or allow editors to edit the attribute values of the field being calculated. This parameter is only applicable for the calculation attribute rule type.EDITABLE—The attribute value can be edited. This is the default.NONEDITABLE—The attribute value cannot be edited. | Boolean |
| triggering_events[triggering_events,...](Optional) | Specifies the editing events that will trigger the attribute rule to take effect. This parameter is valid for calculation and constraint rule types only. At least one triggering event must be provided for calculation rules in which the batch parameter is set to NOT_BATCH. Triggering events are not applicable for calculation rules that have the batch parameter set to BATCH.INSERT—The rule will be applied when a new feature is added.UPDATE—The rule will be applied when a feature is updated.DELETE—The rule will be applied when a feature is deleted. | String |
| error_number(Optional) | An error number that will be returned when this rule is violated. This value is not required to be unique, so the same custom error number may be returned for multiple rules. This parameter is required for the constraint and validation rule types. It is optional for the calculation rule type. | String |
| error_message(Optional) | An error message that will be returned when this rule is violated. It is recommended that you use a descriptive message to help the editor understand the violation when it occurs. The message is limited to 256 characters. This parameter is required for the constraint and validation rule types. It is optional for the calculation rule type. | String |
| description(Optional) | The description of the new attribute rule. The description is limited to 256 characters. | String |
| subtype(Optional) | The subtype to which the rule will be applied if the dataset has subtypes. | String |
| field(Optional) | The name of an existing field to which the rule will be applied. This parameter is only applicable for the calculation attribute rule type. | String |
| exclude_from_client_evaluation(Optional) | Specifies whether the application will evaluate the rule locally before applying the edits to the workspace. Not all clients have the capability to run all of the available rules, so authors can exclude certain rules from client evaluation. For example, some rules may refer to data that has not been made available to all clients (reasons can include the data is offline, size, or security), or some rules may depend on the user or context (that is, a lightweight field update in a data collection app may not run a rule that requires additional user input or knowledge; however, a client such as ArcGIS Pro may support it). This parameter is not applicable for validation rule or calculation rule types if the batch parameter is set to BATCH.EXCLUDE—The rule will be excluded from client evaluation.INCLUDE—The rule will not be excluded from client evaluation. This is the default. | Boolean |
| batch(Optional) | Specifies whether the rule evaluation will be run in batch mode.BATCH—The rule evaluation will be run in batch mode at a later time by running validate.NOT_BATCH—The rule evaluation will not be run in batch mode. Triggering events will be used to determine when the rule is evaluated for insert, update, or delete operations. This is the default.For calculation rules, this parameter value can be BATCH or NOT_BATCH. For validation rules, this parameter value must be BATCH. For constraint rules, this parameter value must be NOT_BATCH. | Boolean |
| severity(Optional) | The severity of the error. A value within the range of 1 through 5 can be chosen to define the severity of the rule. A value of 1 is high, being the most severe, and a value of 5 is low, being the least severe. For example, you can provide a low severity for a specific attribute rule and ignore the error during data production workflows, or set a high severity in which the error must be fixed for accuracy of the data collected.This parameter is only applicable to validation rules. | Long |
| tags[tags,...](Optional) | A set of tags that identify the rule (for searching and indexing) as a way to map to a functional requirement in a data model. To enter multiple tags, use a semicolon delimiter, for example, Tag1;Tag2;Tag3. | String |
| triggering_fields[triggering_fields,...](Optional) | A list of fields that will trigger an attribute rule to run when an editing event occurs during an update trigger for calculation and constraint attribute rules. If no fields are specified, the tool will use all fields. This is default.To enter multiple triggering fields, use a semicolon delimiter, for example, Field1;Field2;Field3. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddAttributeRule(in_table, name, type, script_expression, {is_editable}, {triggering_events}, {error_number}, {error_message}, {description}, {subtype}, {field}, {exclude_from_client_evaluation}, {batch}, {severity}, {tags}, {triggering_fields})
```

### Example 2

```python
'''****************************************************************************
Name:        AddAttributeRule_example1.py
Description: This script adds a calcualtion rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "calculateRuleLabel"
script_expression = 'if ($feature.material == 5) {return "Plastic"} else {return "Other"}'
triggering_events = "INSERT;UPDATE"
description = "Populate label text"
subtype = "Coated Steel"
field = "labeltext"

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, "CALCULATION", script_expression, "EDITABLE", triggering_events, "", "", description, subtype, field)
```

### Example 3

```python
'''****************************************************************************
Name:        AddAttributeRule_example1.py
Description: This script adds a calcualtion rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "calculateRuleLabel"
script_expression = 'if ($feature.material == 5) {return "Plastic"} else {return "Other"}'
triggering_events = "INSERT;UPDATE"
description = "Populate label text"
subtype = "Coated Steel"
field = "labeltext"

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, "CALCULATION", script_expression, "EDITABLE", triggering_events, "", "", description, subtype, field)
```

### Example 4

```python
'''****************************************************************************
Name:        AddAttributeRule_example2.py
Description: This script adds a constraint rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "constraintRuleOP"
script_expression = '$feature.OPERATINGPRESSURE < 300'
triggering_events = "INSERT;UPDATE"
description = "Operating pressure must be less than 300"
subtype = "ALL"
error_number = 2001
error_message = "Invalid operating pressure. Must be less than 300."

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, "CONSTRAINT", script_expression, "EDITABLE", triggering_events, error_number, error_message, description, subtype)
```

### Example 5

```python
'''****************************************************************************
Name:        AddAttributeRule_example2.py
Description: This script adds a constraint rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "constraintRuleOP"
script_expression = '$feature.OPERATINGPRESSURE < 300'
triggering_events = "INSERT;UPDATE"
description = "Operating pressure must be less than 300"
subtype = "ALL"
error_number = 2001
error_message = "Invalid operating pressure. Must be less than 300."

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, "CONSTRAINT", script_expression, "EDITABLE", triggering_events, error_number, error_message, description, subtype)
```

### Example 6

```python
'''****************************************************************************
Name:        AddAttributeRule_example3.py
Description: This script adds a validation rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "validationRuleMaxOP"
rule_type = "VALIDATION"
script_expression = "$feature.OPERATINGPRESSURE < 300"
error_number = 3001
error_message = "Maximum operating pressure exceeded"
description = "Validation rule: max operating pressure value"
subtype = "Coated Steel"
batch = "BATCH"
severity = 3
tags = "OP;MAXOP"

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, rule_type, 
                                  script_expression, "", "", 
                                  error_number, error_message, 
                                  description, subtype, "", "", 
                                  batch, severity, tags)
```

### Example 7

```python
'''****************************************************************************
Name:        AddAttributeRule_example3.py
Description: This script adds a validation rule to a feature class
Created by:  Esri
****************************************************************************'''

# Import required modules        
import arcpy

# Set local variables
in_table = "C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes"
name = "validationRuleMaxOP"
rule_type = "VALIDATION"
script_expression = "$feature.OPERATINGPRESSURE < 300"
error_number = 3001
error_message = "Maximum operating pressure exceeded"
description = "Validation rule: max operating pressure value"
subtype = "Coated Steel"
batch = "BATCH"
severity = 3
tags = "OP;MAXOP"

# Run the AddAttributeRule tool
arcpy.management.AddAttributeRule(in_table, name, rule_type, 
                                  script_expression, "", "", 
                                  error_number, error_message, 
                                  description, subtype, "", "", 
                                  batch, severity, tags)
```

---

## Add Coded Value To Domain (Data Management)

## Summary

Adds a value to a domain's coded value list.

## Usage

- Domain management involves the following steps:Create the domain using the Create Domain tool.Add values to or set the range of values for the domain using this tool or the Set Value For Range Domain tool.Associate the domain with a feature class using the Assign Domain To Field tool.
- Create the domain using the Create Domain tool.
- Add values to or set the range of values for the domain using this tool or the Set Value For Range Domain tool.
- Associate the domain with a feature class using the Assign Domain To Field tool.
- The coded value domain includes both the actual value that is stored in the database (for example, 1 for pavement) and a description of what the code value means (for example, pavement).
- A coded value domain which specifies a valid set of values for an attribute can apply to any type of attribute—text, numeric, date, and so on. For example, a coded value list for a text attribute might include valid pipe material values: CL - cast iron pipe; DL - ductile iron pipe; ACP - asbestos concrete pipe, or a coded value list might include the numeric values representing valid pipe diameters: .75–3/4"; 2–2"; 24–24"; and 30–30".

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase containing the domain to be updated. | Workspace |
| Domain Name | The name of the attribute domain that will have a value added to its coded value list. | String |
| Code Value | The value to be added to the specified domain's coded value list. | String |
| Code Description | A description of what the coded value represents. | String |
| in_workspace | The geodatabase containing the domain to be updated. | Workspace |
| domain_name | The name of the attribute domain that will have a value added to its coded value list. | String |
| code | The value to be added to the specified domain's coded value list. | String |
| code_description | A description of what the coded value represents. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddCodedValueToDomain(in_workspace, domain_name, code, code_description)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.AddCodedValueToDomain_management("montgomery.gdb", "material", "1", "PVC")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.AddCodedValueToDomain_management("montgomery.gdb", "material", "1", "PVC")
```

### Example 4

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"
 
# Process: Create the coded value domain
arcpy.CreateDomain_management("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")

# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", \
           "ACP": "Asbestos concrete", "COP": "Copper"}
    
# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.AddCodedValueToDomain_management(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.AssignDomainToField_management(inFeatures, inField, domName)
```

### Example 5

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"
 
# Process: Create the coded value domain
arcpy.CreateDomain_management("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")

# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", \
           "ACP": "Asbestos concrete", "COP": "Copper"}
    
# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.AddCodedValueToDomain_management(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.AssignDomainToField_management(inFeatures, inField, domName)
```

---

## Add Colormap (Data Management)

## Summary

Adds a new color map or replaces an existing color map on a raster dataset.

## Usage

- The color map applied to the input raster dataset can originate from a raster dataset that already has a color map, a .clr file, or an .act file.
- This tool will not work when the color map is internally stored in the attribute table of an IMG or a TIFF dataset. If the attribute table contains the fields Red, Green, and Blue, this tool cannot be used.
- The input raster dataset must be a single band raster dataset with integer values. Color maps can only be created for single-band raster datasets with a pixel depth of 16-bit unsigned or fewer. Certain formats cannot have a color map associated with them; please consult Supported raster dataset file formats.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset to add or replace a color map. | Raster Layer |
| Input Template Raster(Optional) | A raster dataset that has a color map that you want to apply to the input raster dataset. | Raster Layer |
| Input .clr or .act File (Optional) | Specify a .clr or .act file to use as the color map. | File |
| in_raster | The raster dataset to add or replace a color map. | Raster Layer |
| in_template_raster(Optional) | A raster dataset that has a color map that you want to apply to the input raster dataset. If this is entered the input_CLR_file parameter is ignored. | Raster Layer |
| input_CLR_file(Optional) | Specify a .clr or .act file to use as the color map. | File |

## Code Samples

### Example 1

```python
arcpy.management.AddColormap(in_raster, {in_template_raster}, {input_CLR_file})
```

### Example 2

```python
import arcpy
arcpy.AddColormap_management("c:/data/nocolormap.img", "",
                             "colormap_file.clr")
```

### Example 3

```python
import arcpy
arcpy.AddColormap_management("c:/data/nocolormap.img", "",
                             "colormap_file.clr")
```

### Example 4

```python
##====================================
##Add Colormap
##Usage: AddColormap_management in_raster {in_template_raster} {input_CLR_file}

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Assign colormap using template image
arcpy.AddColormap_management("nocolormap.img", "colormap.tif")

##Assign colormap using clr file
arcpy.AddColormap_management("nocolormap.img", "", "colormap_file.clr")
```

### Example 5

```python
##====================================
##Add Colormap
##Usage: AddColormap_management in_raster {in_template_raster} {input_CLR_file}

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Assign colormap using template image
arcpy.AddColormap_management("nocolormap.img", "colormap.tif")

##Assign colormap using clr file
arcpy.AddColormap_management("nocolormap.img", "", "colormap_file.clr")
```

---

## Add Contingent Value (Data Management)

## Summary

Adds a contingent value to a field group on a feature class or table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase feature class or table to which the contingent value will be added. | Table View |
| Field Group Name | The field group to which the contingent value will be added. | String |
| Values | The field name, field value type, and associated field values that will be used for the new contingent value.Field Name—The name of the field that participates in the field group.Field Value Type—The type of contingent value. The Any and Null types will ignore any value specified in the Field Value field.Any—The value can be any field value.Null—The value is null.Coded Value—The value is from a coded value domain.Range—The value is a minimum/maximum subset of a range domain.Field Value—The specific field value. If Field Value Type is Coded Value, specify the code. If Field Value Type is Range, specify the minimum and maximum values in the format min;max (for example, 10;100). | Value Table |
| Subtype (Optional) | The input table subtype to which the contingent value will be added. | String |
| Retire Value (Optional) | Specifies whether the contingent value will be retired. The contingent value is considered retired when it is no longer created but can still be used in an existing field. When a contingent value is retired, it will still be shown in the list of valid values for a field, such as in the Attribute pane. An example is using asbestos as a building material. New construction cannot use asbestos as a building material, but existing structures may still have this attribute. For more information about retired values, see Create and manage contingent values.Checked—The contingent value will be retired.Unchecked—The contingent value will not be retired. This is the default. | Boolean |
| target_table | The input geodatabase feature class or table to which the contingent value will be added. | Table View |
| field_group_name | The field group to which the contingent value will be added. | String |
| values[[Field Name, Field Value Type, Field Value],...] | The field name, field value type, and associated field values that will be used for the new contingent value.Field name—The name of the field that participates in the field group.Field value type—The type of contingent value. The ANY and NULL types will ignore any value specified in the field value field.ANY—The value can be any field value.NULL—The value is null.CODED_VALUE—The value is from a coded value domain.RANGE—The value is a minimum/maximum subset of a range domain.Field value—The specific field value. If the field value type is CODED_VALUE, specify the code. If the field value type is RANGE, specify the minimum and maximum values in the format min;max (for example, 10;100). | Value Table |
| subtype(Optional) | The input table subtype to which the contingent value will be added. | String |
| retire_value(Optional) | Specifies whether the contingent value will be retired. The contingent value is considered retired when it is no longer created but can still be used in an existing field. When a contingent value is retired, it will still be shown in the list of valid values for a field, such as in the Attribute pane. An example is using asbestos as a building material. New construction cannot use asbestos as a building material, but existing structures may still have this attribute. For more information about retired values, see Create and manage contingent values. RETIRE—The contingent value will be retired.DO_NOT_RETIRE—The contingent value will not be retired. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AddContingentValue(target_table, field_group_name, values, {subtype}, {retire_value})
```

### Example 2

```python
import arcpy
CV = [["FieldName1", "CODED_VALUE", "DomainValue1"], 
      ["FieldName2", "CODED_VALUE", "DomainValue2"]]
arcpy.management.AddContingentValue("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC", 
                                    "MyFieldGroup", CV, "My Subtype", 
                                    "DO_NOT_RETIRE")
```

### Example 3

```python
import arcpy
CV = [["FieldName1", "CODED_VALUE", "DomainValue1"], 
      ["FieldName2", "CODED_VALUE", "DomainValue2"]]
arcpy.management.AddContingentValue("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC", 
                                    "MyFieldGroup", CV, "My Subtype", 
                                    "DO_NOT_RETIRE")
```

---

## Add Data To Trajectory Dataset (Data Management)

## Summary

Adds trajectory data to an existing trajectory dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Trajectory Dataset | The trajectory dataset to which the data will be added. | Trajectory Layer |
| Trajectory Type | Specifies the data type that will be added.Cryosat-2—Cryosat-2 data will be added.ICESat-2—ICESat-2 data will be added.Sentinel-3 SRAL—Sentinel-3 SRAL data will be added.Sentinel-6—Sentinel-6 data will be added. This is the default. | Raster Type |
| Input Data | The input files or folder. The inputs can be netCDF or HDF (.nc or .hdf files). | Workspace; File; WCS Coverage; Image Service; Map Server; WMS Map; Table View; Raster Layer; Mosaic Layer; Terrain Layer; LAS Dataset Layer; Layer File; WMTS Layer |
| Input Data Filter (Optional) | The filter for the input data. The default will be determined by the Trajectory Type parameter value. Custom filter criteria can also be provided. For example, a value of STD_ will filter files that start with STD_ in the file name. | String |
| Include Sub Folders (Optional) | Specifies whether data in the Input Data subfolders will be searched and added.Checked—All subfolders will be searched and the data added. This is the default.Unchecked—Only the top-level folder will be searched and the data added. | Boolean |
| Auxiliary Inputs (Optional) | The properties that are determined by the Trajectory Type parameter value. Supported property names are ProductFilter, Frequency, PredefinedVariables, and Variables. For a list of supported values associated with each property name, see Trajectory type properties. | Value Table |
| in_trajectory_dataset | The trajectory dataset to which the data will be added. | Trajectory Layer |
| trajectory_type | Specifies the data type that will be added.Cryosat-2—Cryosat-2 data will be added.ICESat-2—ICESat-2 data will be added.Sentinel-3 SRAL—Sentinel-3 SRAL data will be added.Sentinel-6—Sentinel-6 data will be added. | Raster Type |
| input_path[input_path,...] | The input files or folder. The inputs can be netCDF or HDF (.nc or .hdf files). | Workspace; File; WCS Coverage; Image Service; Map Server; WMS Map; Table View; Raster Layer; Mosaic Layer; Terrain Layer; LAS Dataset Layer; Layer File; WMTS Layer |
| filter(Optional) | The filter for the input data. The default will be determined by the trajectory_type parameter value. Custom filter criteria can also be provided. For example, a value of STD_ will filter files that start with STD_ in the file name. | String |
| sub_folder(Optional) | Specifies whether data in the input_path subfolders will be searched and added.SUBFOLDERS—All subfolders will be searched and the data added. This is the default.NO_SUBFOLDERS—Only the top-level folder will be searched and the data added. | Boolean |
| aux_inputs[aux_inputs,...](Optional) | The properties that are determined by the trajectory_type parameter value. Supported property names are ProductFilter, Frequency, PredefinedVariables, and Variables. For a list of supported values associated with each property name, see Trajectory type properties. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.AddDataToTrajectoryDataset(in_trajectory_dataset, trajectory_type, input_path, {filter}, {sub_folder}, {aux_inputs})
```

### Example 2

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"

input_path = r"C:\data\Cryosat\CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001.nc"

# Execute
trajectory_output = arcpy.management.AddDataToTrajectoryDataset(in_trajectory_dataset, 
		   "Cryosat-2", input_path, "*CS_*.nc", "SUBFOLDERS", None)
```

### Example 3

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"

input_path = r"C:\data\Cryosat\CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001.nc"

# Execute
trajectory_output = arcpy.management.AddDataToTrajectoryDataset(in_trajectory_dataset, 
		   "Cryosat-2", input_path, "*CS_*.nc", "SUBFOLDERS", None)
```

---

## Add Feature Class To Topology (Data Management)

## Summary

Adds a feature class to a topology.

## Usage

- The new feature class must be in the same feature dataset as the topology.
- Adding a new feature class to a topology automatically makes the entire topology dirty, so when you finish adding feature classes, you need to revalidate the topology. The new features may create errors where previously there were none depending on the topology rules associated with the feature class.
- Feature classes can only be added to topologies with the same versioned status. For example, a versioned feature class can be added to a versioned topology, but a nonversioned feature class cannot be added to a versioned topology.
- If the feature class you are adding is z-aware, you can rank the relative accuracy of the feature class by elevation by setting the z rank for the feature class.
- When adding a feature class to a topology, you must specify the rank of the vertices in this feature class relative to those in other feature classes. When validation of the topology cracks and clusters feature vertices, vertices from feature classes assigned a higher rank will not be moved when snapping with vertices with lower-ranked feature classes. The highest rank is 1, and you can assign up to 50 different rank values.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology to which the feature class will be added. | Topology Layer |
| Input Feature class | The feature class that will be added to the topology. The feature class must be in the same feature dataset as the topology. | Feature Layer |
| XY Rank | The relative degree of positional accuracy associated with vertices of features in the feature class versus those in other feature classes in the topology. The feature class with the highest accuracy receives a higher rank (lower number, for example, 1) than a feature class that is known to be less accurate. | Long |
| Z Rank | Feature classes that are z-aware have elevation values embedded in their geometry for each vertex. By setting a z rank, you can influence how vertices with accurate z-values are snapped or clustered with vertices that contain less accurate z measurements. | Long |
| in_topology | The topology to which the feature class will be added. | Topology Layer |
| in_featureclass | The feature class that will be added to the topology. The feature class must be in the same feature dataset as the topology. | Feature Layer |
| xy_rank | The relative degree of positional accuracy associated with vertices of features in the feature class versus those in other feature classes in the topology. The feature class with the highest accuracy receives a higher rank (lower number, for example, 1) than a feature class that is known to be less accurate. | Long |
| z_rank | Feature classes that are z-aware have elevation values embedded in their geometry for each vertex. By setting a z rank, you can influence how vertices with accurate z-values are snapped or clustered with vertices that contain less accurate z measurements. | Long |

## Code Samples

### Example 1

```python
arcpy.management.AddFeatureClassToTopology(in_topology, in_featureclass, xy_rank, z_rank)
```

### Example 2

```python
# Name: AddFeatureClassToTopology_Example.py
# Description: Adds a feature class to participate in a topology

# Import system modules
import arcpy

arcpy.management.AddFeatureClassToTopology("D:/Calgary/Trans.gdb/Streets/Street_Topo", "D:/Calgary/Trans.gdb/Streets/StreetNetwork", 1, 1)
```

### Example 3

```python
# Name: AddFeatureClassToTopology_Example.py
# Description: Adds a feature class to participate in a topology

# Import system modules
import arcpy

arcpy.management.AddFeatureClassToTopology("D:/Calgary/Trans.gdb/Streets/Street_Topo", "D:/Calgary/Trans.gdb/Streets/StreetNetwork", 1, 1)
```

---

## Add Field Conflict Filter (Data Management)

## Summary

Adds a field conflict filter for a given field in a geodatabase table or feature class.

## Usage

- Field conflict filters are not supported on the following fields: ObjectID, BLOB, fields that store geometry such as Shape, the Subtype field, network-related fields such as the enabled ancillary role and weight fields, or Editor Tracking fields.
- When running from the tool dialog box, fields that already have filters applied and unsupported fields will not be displayed.
- From Python, the ArcPy function ListFieldConflictFilters can be used to identify the fields that have filters applied.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | Table or feature class containing the field or fields to which conflict filters will be applied. | Table View |
| Field Name | Field or list of fields that will have conflict filters applied. | Field |
| table | Table or feature class containing the field or fields to which conflict filters will be applied. | Table View |
| fields[fields,...] | Field or list of fields that will have conflict filters applied. | Field |

## Code Samples

### Example 1

```python
arcpy.management.AddFieldConflictFilter(table, fields)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "e:/Connections/airport.sde"
arcpy.AddFieldConflictFilter_management("Primary_UG", "phase")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "e:/Connections/airport.sde"
arcpy.AddFieldConflictFilter_management("Primary_UG", "phase")
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "f:/Connections/airport.sde"
arcpy.AddFieldConflictFilter_management("Primary_UG", ["phase", "material"])
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "f:/Connections/airport.sde"
arcpy.AddFieldConflictFilter_management("Primary_UG", ["phase", "material"])
```

---

## Add Field (Data Management)

## Summary

Adds a new field to a table or the table of a feature class or feature layer, as well as to rasters with attribute tables.

## Usage

- For shapefiles and dBase tables, if field type defines a character, blanks are inserted for each record. If field type defines a numeric item, zeros are inserted for each record.
- The Field Length parameter is only applicable to fields of type text. If no field length is provided, a length of 255 will be used.
- For geodatabases, if field type defines a character or number, <null> is inserted into each record if the Field supports null values parameter is checked.
- A nonnullable field cannot be added to a nonempty geodatabase feature class or table.
- A shapefile does not support aliases for fields, so you cannot add a field alias to a shapefile.
- You can use an existing domain from a feature class in a geodatabase for the Field Domain parameter value. The name of an existing domain must be provided. Providing an invalid domain name or value will not cause the tool to fail, but the invalid name or value will be ignored and no domain will be set for the field.
- The precision and scale of a field represent the maximum precision and size of data that can be stored in the field. The precision represents the number of digits that can be stored in the field, and the scale represents the number of decimal places for float and double fields. For example, for a value of 54.234, the scale is 3 and the precision is 5.Note:Field precision and scale are not supported in a file geodatabase feature class or table. If you provide a value for precision or scale, it will be ignored.For other database types, use the following guidelines to choose the correct field type for a given precision and scale: When you create a float or double field and specify a precision and scale, if the precision is greater than 6, use a double; otherwise, use a float.For a field with a scale of 0, use a long or short integer field type. When creating a long integer field, specify a precision of 10 or less, or the field may be created as a double.Learn more about numeric field types
- When you create a float or double field and specify a precision and scale, if the precision is greater than 6, use a double; otherwise, use a float.
- For a field with a scale of 0, use a long or short integer field type. When creating a long integer field, specify a precision of 10 or less, or the field may be created as a double.
- Required fields are permanent and cannot be deleted. To allow for deletion at a later time, set the field so it is not required (the default).
- A field of type raster allows you to include a raster image as an attribute. It is stored in or alongside the geodatabase. This is helpful when an image is the best way to describe a feature. Precision, scale, and length cannot be set for fields of type raster.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table where the specified field will be added. The field will be added to the existing input table and will not create a new output table.Fields can be added to feature classes in geodatabases, shapefiles, coverages, stand-alone tables, raster catalogs, rasters with attribute tables, and layers. | Mosaic Layer; Raster Layer; Table View |
| Field Name | The name of the field that will be added to the input table. | String |
| Field Type | Specifies the field type of the new field.Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.Text—The field type will be text. Text fields support a string of characters.Date—The field type will be date. Date fields support date and time values. Date (high precision)—The field type will be high precision date. High precision date fields support date and time values with millisecond time.Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.Raster— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | String |
| Field Precision(Optional) | The number of digits that can be stored in the field. All digits are counted regardless of which side of the decimal they are on.This parameter is only applicable to numeric field types.If the input table is in a file geodatabase, the field precision value will be ignored. | Long |
| Field Scale(Optional) | The number of decimal places stored in a field.This parameter is only applicable to fields of type float or double.If the input table is in a file geodatabase, the field scale value will be ignored. | Long |
| Field Length(Optional) | The length of the field. This sets the maximum number of allowable characters for each record of the field. If no field length is provided, a length of 255 will be used.This parameter is only applicable to fields of type text. | Long |
| Field Alias(Optional) | The alternate name for the field. This name is used to describe cryptic field names. This parameter only applies to geodatabases. | String |
| Field supports null values(Optional) | Specifies whether the field can contain null values. Null values are different from zero or empty fields and are only supported for fields in a geodatabase.Checked—The field can contain null values. This is the default.Unchecked—The field cannot contain null values. | Boolean |
| Field is required(Optional) | Specifies whether the field being created is a required field for the table. Required fields are only supported in a geodatabase.Checked—The field is a required field. Required fields are permanent and cannot be deleted. Unchecked—The field is not a required field. This is the default. | Boolean |
| Field Domain(Optional) | Constrains the values allowed in any particular attribute for a table, feature class, or subtype in a geodatabase. You must specify the name of an existing domain for it to be applied to the field. | String |
| in_table | The input table where the specified field will be added. The field will be added to the existing input table and will not create a new output table.Fields can be added to feature classes in geodatabases, shapefiles, coverages, stand-alone tables, raster catalogs, rasters with attribute tables, and layers. | Mosaic Layer; Raster Layer; Table View |
| field_name | The name of the field that will be added to the input table. | String |
| field_type | Specifies the field type of the new field.SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.TEXT—The field type will be text. Text fields support a string of characters.DATE—The field type will be date. Date fields support date and time values. DATEHIGHPRECISION—The field type will be high precision date. High precision date fields support date and time values with millisecond time.DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values.TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.RASTER— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used.Although the Field object's type property values are not an exact match for the keywords used by the Add Field tool's field_type parameter, all of the Field object's type values can be used as input to this parameter. The different field types are mapped as follows: Integer to LONG, String to TEXT, and SmallInteger to SHORT. | String |
| field_precision(Optional) | The number of digits that can be stored in the field. All digits are counted regardless of which side of the decimal they are on.This parameter is only applicable to numeric field types.If the input table is in a file geodatabase, the field precision value will be ignored. | Long |
| field_scale(Optional) | The number of decimal places stored in a field.This parameter is only applicable to fields of type float or double.If the input table is in a file geodatabase, the field scale value will be ignored. | Long |
| field_length(Optional) | The length of the field. This sets the maximum number of allowable characters for each record of the field. If no field length is provided, a length of 255 will be used.This parameter is only applicable to fields of type text. | Long |
| field_alias(Optional) | The alternate name for the field. This name is used to describe cryptic field names. This parameter only applies to geodatabases. | String |
| field_is_nullable(Optional) | Specifies whether the field can contain null values. Null values are different from zero or empty fields and are only supported for fields in a geodatabase.NULLABLE—The field can contain null values. This is the default. NON_NULLABLE—The field cannot contain null values. | Boolean |
| field_is_required(Optional) | Specifies whether the field being created is a required field for the table. Required fields are only supported in a geodatabase.NON_REQUIRED—The field is not a required field. This is the default. REQUIRED—The field is a required field. Required fields are permanent and cannot be deleted. | Boolean |
| field_domain(Optional) | Constrains the values allowed in any particular attribute for a table, feature class, or subtype in a geodatabase. You must specify the name of an existing domain for it to be applied to the field. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddField(in_table, field_name, field_type, {field_precision}, {field_scale}, {field_length}, {field_alias}, {field_is_nullable}, {field_is_required}, {field_domain})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.AddField("schools", "ref_ID", "LONG", 9, "", "", "refcode", 
                          "NULLABLE", "REQUIRED")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.AddField("schools", "ref_ID", "LONG", 9, "", "", "refcode", 
                          "NULLABLE", "REQUIRED")
```

### Example 4

```python
# Name: AddField_Example2.py
# Description: Add a pair of new fields to a table
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "schools"
fieldName1 = "ref_ID"
fieldPrecision = 9
fieldAlias = "refcode"
fieldName2 = "status"
fieldLength = 10

# Run AddField twice for two new fields
arcpy.management.AddField(inFeatures, fieldName1, "LONG", fieldPrecision,
                          field_alias=fieldAlias, field_is_nullable="NULLABLE")

arcpy.management.AddField(inFeatures, fieldName2, "TEXT", 
                          field_length=fieldLength)
```

### Example 5

```python
# Name: AddField_Example2.py
# Description: Add a pair of new fields to a table
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "schools"
fieldName1 = "ref_ID"
fieldPrecision = 9
fieldAlias = "refcode"
fieldName2 = "status"
fieldLength = 10

# Run AddField twice for two new fields
arcpy.management.AddField(inFeatures, fieldName1, "LONG", fieldPrecision,
                          field_alias=fieldAlias, field_is_nullable="NULLABLE")

arcpy.management.AddField(inFeatures, fieldName2, "TEXT", 
                          field_length=fieldLength)
```

---

## Add Fields (multiple) (Data Management)

## Summary

Adds new fields to a table, feature class, or raster.

## Usage

- For shapefiles and dBase tables, if field type defines a character, blanks are inserted for each record. If field type defines a numeric item, zeros are inserted for each record.
- Fields specified by the Field Properties parameter will have the following default properties: The Allow NULL property will be true. The Editable property will be true. The Required property will be false.The Precision and Scale properties are set by the field type and data source default values.
- The Allow NULL property will be true.
- The Editable property will be true.
- The Required property will be false.
- The Precision and Scale properties are set by the field type and data source default values.
- Fields can be added to the input using the Field Properties and Template Tables parameters. If values for both parameters are provided, fields will be added from both parameters.
- The Field Properties parameter's Field Length option is only applicable to fields of type text.
- A shapefile does not support aliases for fields, so you cannot add a field alias to a shapefile.
- A field of type raster allows you to include a raster image as an attribute. It is stored in or alongside the geodatabase. This is helpful when an image is the best way to describe a feature. Precision, scale, and length cannot be set for fields of type raster.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table where the fields will be added. The fields will be added to the existing input table and will not create a new output table. Fields can be added to feature classes in geodatabases, shapefiles, coverages, stand-alone tables, raster catalogs, rasters with attribute tables, and layers. | Table View; Raster Layer; Mosaic Layer |
| Field Properties(Optional) | The fields and their properties that will be added to the input table.Field Name—The name of the field that will be added to the input table.Field Type—The type of the new field. Field Alias—The alternate name for the field. This is used to describe cryptic field names. This value only applies to geodatabases.Field Length—The length of the field being added. This sets the maximum number of allowable characters for each record of the field. This option is only applicable to fields of type text. The default length is 255.Default Value—The default value of the field.Field Domain—The geodatabase domain that will be assigned to the field. Available field types are as follows: Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. Text—The field type will be text. Text fields support a string of characters. Date—The field type will be date. Date fields support date and time values.Date (high precision)—The field type will be high precision date. High precision date fields support date and time values with millisecond time.Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value. Blob (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. Raster imagery—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | Value Table |
| Template Tables(Optional) | The feature classes or tables that will be used as a template to define the attribute fields to add. Fields from the inputs specified by this parameter will be added to the Input Table value in addition to any fields specified by the Field Properties parameter. | Table View |
| in_table | The input table where the fields will be added. The fields will be added to the existing input table and will not create a new output table. Fields can be added to feature classes in geodatabases, shapefiles, coverages, stand-alone tables, raster catalogs, rasters with attribute tables, and layers. | Table View; Raster Layer; Mosaic Layer |
| field_description[[Field Name, Field Type, {Field Alias}, {Field Length}, {Default Value}, {Field Domain}],...](Optional) | The fields and their properties that will be added to the input table.Field Name—The name of the field that will be added to the input table.Field Type—The type of the new field. Field Alias—The alternate name for the field. This is used to describe cryptic field names. This value only applies to geodatabases.Field Length—The length of the field being added. This sets the maximum number of allowable characters for each record of the field. This option is only applicable to fields of type text. The default length is 255.Default Value—The default value of the field.Field Domain—The geodatabase domain that will be assigned to the field.Available field types are as follows: SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. TEXT—The field type will be text. Text fields support a string of characters. DATE—The field type will be date. Date fields support date and time values. DATEHIGHPRECISION—The field type will be high precision date. High precision date fields support date and time values with millisecond time.DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values. TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. RASTER—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used.When using this parameter with optional values, use None as an empty place holder. | Value Table |
| template[template,...](Optional) | The feature classes or tables that will be used as a template to define the attribute fields to add. Fields from the inputs specified by this parameter will be added to the in_table value in addition to any fields specified by the field_description parameter. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.AddFields(in_table, {field_description}, {template})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/district.gdb"
arcpy.management.AddFields(
    'school', 
    [['school_name', 'TEXT', 'Name', 255, 'Hello world', ''], 
     ['street_number', 'LONG', 'Street Number', None, 35, 'StreetNumDomain'],
     ['year_start', 'DATE', 'Year Start', None, '2017-08-09 16:05:07', '']])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/district.gdb"
arcpy.management.AddFields(
    'school', 
    [['school_name', 'TEXT', 'Name', 255, 'Hello world', ''], 
     ['street_number', 'LONG', 'Street Number', None, 35, 'StreetNumDomain'],
     ['year_start', 'DATE', 'Year Start', None, '2017-08-09 16:05:07', '']])
```

---

## Add Files To LAS Dataset (Data Management)

## Summary

Adds references for one or more LAS files and surface constraint features to a LAS dataset.

## Usage

- The LAS dataset is designed for use with point cloud data stored in the LAS format using file versions 1.0–1.4. LAS files stored in the compressed ZLAS format are also supported. ZLAS files can be generated by any tool that creates new LAS files, such as Convert LAS, Extract LAS, Thin LAS, or Tile LAS. Additionally, the LAS Optimizer 1.2 stand-alone application can also be used to compress .las files to .zlas or uncompress them back to .las files.
- Surface constraint features can be used to enforce feature-derived elevation values that represent surface characteristics in the LAS dataset.
- ArcGIS uses the LAS classification scheme defined by the American Society for Photogrammetry and Remote Sensing (ASPRS). Learn more about lidar point classification

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| LAS Files or Folders (Optional) | Inputs that can include any combination of .las files, .zlas files, LAS datasets, and folders containing .las or .zlas data. When a LAS dataset is specified as input, all .las and .zlas files that have a valid path reference will be added to the input LAS dataset. In the Geoprocessing pane, a folder can also be specified as an input by selecting the folder in File Explorer and dragging it onto the parameter's input box. | LAS Dataset Layer; Folder; File |
| Include subfolders(Optional) | Specifies whether .las files residing in the subdirectories of an input folder will be referenced by the LAS dataset. Unchecked—Only .las files residing in an input folder will be added to the LAS dataset. This is the default. Checked—All .las files residing in the subdirectories of an input folder will be added to the LAS dataset. | Boolean |
| Surface Constraints(Optional) | The features that will contribute to the definition of the triangulated surface generated from the LAS dataset. Input Features—The features with geometry that will be incorporated into the LAS dataset's triangulated surface. Height Field—The feature's elevation source can be derived from any numeric field in the feature's attribute table or the geometry by selecting Shape.Z. If no height is necessary, specify the keyword <None> to create z-less features with elevation that will be interpolated from the surface. Type—Defines the feature's role in the triangulated surface generated from the LAS dataset. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change.Surface Feature Type—The surface feature type that defines how the feature geometry will be incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints—Elevation points that will not be thinned away. This option is only available for single-point feature geometry. hardline or softline—Breaklines that enforce a height value. hardclip or softclip—Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase—Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace—Polygon dataset that defines areas of constant height. | Value Table |
| in_las_dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| in_files[in_files,...](Optional) | Inputs that can include any combination of .las files, .zlas files, LAS datasets, and folders containing .las or .zlas data. When a LAS dataset is specified as input, all .las and .zlas files that have a valid path reference will be added to the input LAS dataset. In the Geoprocessing pane, a folder can also be specified as an input by selecting the folder in File Explorer and dragging it onto the parameter's input box. | LAS Dataset Layer; Folder; File |
| folder_recursion(Optional) | Specifies whether lidar files residing in the subdirectories of an input folder will be added to the LAS dataset. NO_RECURSION—Only lidar files residing in an input folder will be added to the LAS dataset. This is the default.RECURSION—All lidar files residing in the subdirectories of an input folder will be added to the LAS dataset. | Boolean |
| in_surface_constraints[[in_feature_class, height_field, SF_type],...](Optional) | The features that will be referenced by the LAS dataset when generating a triangulated surface. Each feature must have the following properties defined: in_feature_class—The feature to be referenced by the LAS dataset. height_field—Any numeric field in the feature's attribute table can be used to define the height source. If the feature's geometry contains z-values, it can be selected by specifying Shape.Z. If no height is necessary, specify the keyword <None> to create z-less features with elevation that will be interpolated from the surface. SF_type—The surface feature type that defines how the feature geometry will be incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints—Elevation points that will not be thinned away. This option is only available for single-point feature geometry. hardline or softline—Breaklines that enforce a height value. hardclip or softclip—Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase—Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace—Polygon dataset that defines areas of constant height. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.AddFilesToLasDataset(in_las_dataset, {in_files}, {folder_recursion}, {in_surface_constraints})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.AddFilesToLasDataset_management("test.lasd", 
                                      ["LA_N", "LA_S/LA_5S4E.las"],
                                      "RECURSION", 
                                      ["boundary.shp <None> Soft_Clip",
                                       "breakline.shp Shape.Z Hard_Line"])
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.AddFilesToLasDataset_management("test.lasd", 
                                      ["LA_N", "LA_S/LA_5S4E.las"],
                                      "RECURSION", 
                                      ["boundary.shp <None> Soft_Clip",
                                       "breakline.shp Shape.Z Hard_Line"])
```

### Example 4

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

### Example 5

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

---

## Add Geometry Attributes (Data Management)

## Summary

Adds new attribute fields to the input features representing the spatial or geometric characteristics and location of each feature, such as length or area and x-, y-, z-, and m-coordinates.

## Usage

- One or many attribute fields will be added to the input features depending on the options specified in the Geometry Properties parameter. If the fields already exist in the input features, the values in those fields will be overwritten. Use the following table to determine which fields will be added for each geometry property:Geometry propertyAdded fieldAREAPOLY_AREA—The area of the polygon.AREA_GEODESICAREA_GEO—The shape-preserving geodesic area of the polygon.CENTROIDCENTROID_X—The x-coordinate of the centroid point.CENTROID_Y—The y-coordinate of the centroid point.CENTROID_Z—The z-coordinate of the centroid point. This field is only added if the input features are z-enabled.CENTROID_M—The m-coordinate of the centroid point. This field is only added if the input features are m-enabled.CENTROID_INSIDEINSIDE_X—The x-coordinate of a central point inside or on the input feature.INSIDE_Y—The y-coordinate of a central point inside or on the input feature.INSIDE_Z—The z-coordinate of a central point inside or on the input feature. This field is only added if the input features are z-enabled.INSIDE_M—The m-coordinate of a central point inside or on the input feature. This field is only added if the input features are m-enabled.EXTENTEXT_MIN_X—The minimum x-coordinate of the feature.EXT_MIN_Y—The minimum y-coordinate of the feature.EXT_MAX_X—The maximum x-coordinate of the feature.EXT_MAX_Y—The maximum y-coordinate of the feature.LENGTHLENGTH—The length of the line.LENGTH_GEODESICLENGTH_GEO—The shape-preserving geodesic length of the line.LENGTH_3DLENGTH_3D—The 3D length of the line.LINE_BEARINGBEARING—The start-to-end bearing of the line. Values range from 0 to 360, with 0 meaning north, 90 east, 180 south, and 270 west.LINE_START_MID_ENDSTART_X—The x-coordinate of the first point of the line or polygon border.START_Y—The y-coordinate of the first point of the line or polygon border.START_Z—The z-coordinate of the first point of the line or polygon border. This field is only added if the input features are z-enabled.START_M—The m-coordinate of the first point of the line or polygon border. This field is only added if the input features are m-enabled.MID_X—The x-coordinate of the point halfway along the length of the line or polygon border.MID_Y—The y-coordinate of the point halfway along the length of the line or polygon border.MID_Z—The z-coordinate of the point halfway along the length of the line or polygon border. This field is only added if the input features are z-enabled.MID_M—The m-coordinate of the point halfway along the length of the line or polygon border. This field is only added if the input features are m-enabled.END_X—The x-coordinate of the last point of the line or polygon border.END_Y—The y-coordinate of the last point of the line or polygon border.END_Z—The z-coordinate of the last point of the line or polygon border. This field is only added if the input features are z-enabled.END_M—The m-coordinate of the last point of the line or polygon border. This field is only added if the input features are m-enabled.PART_COUNTPART_COUNT—The number of parts composing the feature.PERIMETER_LENGTHPERIMETER—The length of the polygon perimeter or border.PERIMETER_LENGTH_GEODESICPERIM_GEO—The shape-preserving geodesic length of the polygon perimeter or border.POINT_COUNTPNT_COUNT—The number of points composing the feature.POINT_X_Y_Z_MPOINT_X—The x-coordinate of the point.POINT_Y—The y-coordinate of the point.POINT_Z—The z-coordinate of the point. This field is only added if the input features are z-enabled.POINT_M—The m-coordinate of the point. This field is only added if the input features are m-enabled.
- If a coordinate system is specified, the length and area calculations will be in the units of that coordinate system unless different units are selected in the Length Unit and Area Unit parameters.
- The attribute fields added by this tool are similar to any fields that you can add to a feature layer. You can overwrite the field values, or delete or rename the fields. The values in these fields are not automatically recalculated after edits. If you edit the features, you'll need to run this tool again to update the field values.
- If the input features have a selection, only the selected features will have values calculated in the added fields; all other features will have null values.
- This tool works with point, multipoint, polyline, polygon, and annotation feature classes. Annotation is treated as a polygon feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features to which new attribute fields will be added to store properties such as length, area, or x-, y-, z-, and m-coordinates. | Feature Layer |
| Geometry Properties | Specifies the geometry or shape properties that will be calculated into new attribute fields. Area—An attribute will be added to store the area of each polygon feature.Geodesic area—An attribute will be added to store the shape-preserving geodesic area of each polygon feature.Centroid coordinates—Attributes will be added to store the centroid coordinates of each feature.Central point coordinates—Attributes will be added to store the coordinates of a central point inside or on each feature.Extent coordinates—Attributes will be added to store the extent coordinates of each feature.Length—An attribute will be added to store the length of each line feature.Geodesic length—An attribute will be added to store the shape-preserving geodesic length of each line feature.3D length—An attribute will be added to store the 3D length of each line feature.Line bearing—An attribute will be added to store the start-to-end bearing of each line feature. Values range from 0 to 360, with 0 meaning north, 90 east, 180 south, and 270 west.Line start, midpoint, and end coordinates—Attributes will be added to store the coordinates of the start, mid, and end points of each feature.Number of parts—An attribute will be added to store the number of parts composing each feature.Length of perimeter—An attribute will be added to store the length of the perimeter or border of each polygon feature.Geodesic length of perimeter—An attribute will be added to store the shape-preserving geodesic length of the perimeter or border of each polygon feature.Number of vertices—An attribute will be added to store the number of points or vertices composing each feature.Point x-, y-, z-, and m-coordinates—Attributes will be added to store the x-, y-, z-, and m-coordinates of each point feature. | String |
| Length Unit (Optional) | Specifies the unit in which the length will be calculated. Feet (United States)—Length in feet (United States)Meters—Length in metersKilometers—Length in kilometersMiles (United States)—Length in miles (United States)Nautical miles (United States)—Length in nautical miles (United States)Yards (United States)—Length in yards (United States) | String |
| Area Unit (Optional) | Specifies the unit in which the area will be calculated. Acres—Area in acresHectares—Area in hectaresSquare miles (United States)—Area in square miles (United States)Square kilometers—Area in square kilometersSquare meters—Area in square metersSquare feet (United States)—Area in square feet (United States)Square yards (United States)—Area in square yards (United States)Square nautical miles (United States)—Area in square nautical miles (United States) | String |
| Coordinate System(Optional) | The coordinate system in which the coordinates, length, and area will be calculated. The coordinate system of the input features is used by default. | Coordinate System |
| Input_Features | The input features to which new attribute fields will be added to store properties such as length, area, or x-, y-, z-, and m-coordinates. | Feature Layer |
| Geometry_Properties[Geometry_Properties,...] | Specifies the geometry or shape properties that will be calculated into new attribute fields. AREA—An attribute will be added to store the area of each polygon feature.AREA_GEODESIC—An attribute will be added to store the shape-preserving geodesic area of each polygon feature.CENTROID—Attributes will be added to store the centroid coordinates of each feature.CENTROID_INSIDE—Attributes will be added to store the coordinates of a central point inside or on each feature.EXTENT—Attributes will be added to store the extent coordinates of each feature.LENGTH—An attribute will be added to store the length of each line feature.LENGTH_GEODESIC—An attribute will be added to store the shape-preserving geodesic length of each line feature.LENGTH_3D—An attribute will be added to store the 3D length of each line feature.LINE_BEARING—An attribute will be added to store the start-to-end bearing of each line feature. Values range from 0 to 360, with 0 meaning north, 90 east, 180 south, and 270 west.LINE_START_MID_END—Attributes will be added to store the coordinates of the start, mid, and end points of each feature.PART_COUNT—An attribute will be added to store the number of parts composing each feature.PERIMETER_LENGTH—An attribute will be added to store the length of the perimeter or border of each polygon feature.PERIMETER_LENGTH_GEODESIC—An attribute will be added to store the shape-preserving geodesic length of the perimeter or border of each polygon feature.POINT_COUNT—An attribute will be added to store the number of points or vertices composing each feature.POINT_X_Y_Z_M—Attributes will be added to store the x-, y-, z-, and m-coordinates of each point feature. | String |
| Length_Unit(Optional) | Specifies the unit in which the length will be calculated. FEET_US—Length in feet (United States)METERS—Length in metersKILOMETERS—Length in kilometersMILES_US—Length in miles (United States)NAUTICAL_MILES—Length in nautical miles (United States)YARDS—Length in yards (United States) | String |
| Area_Unit(Optional) | Specifies the unit in which the area will be calculated. ACRES—Area in acresHECTARES—Area in hectaresSQUARE_MILES_US—Area in square miles (United States)SQUARE_KILOMETERS—Area in square kilometersSQUARE_METERS—Area in square metersSQUARE_FEET_US—Area in square feet (United States)SQUARE_YARDS—Area in square yards (United States)SQUARE_NAUTICAL_MILES—Area in square nautical miles (United States) | String |
| Coordinate_System(Optional) | The coordinate system in which the coordinates, length, and area will be calculated. The coordinate system of the input features is used by default. | Coordinate System |

## Code Samples

### Example 1

```python
arcpy.management.AddGeometryAttributes(Input_Features, Geometry_Properties, {Length_Unit}, {Area_Unit}, {Coordinate_System})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.management.AddGeometryAttributes("roads", "LENGTH;LINE_START_MID_END")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.management.AddGeometryAttributes("roads", "LENGTH;LINE_START_MID_END")
```

### Example 4

```python
# Name: GridCreation.py

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.env.outputCoordinateSystem = arcpy.Describe("roads").spatialReference

# Set local variables
in_features = "roads"
properties = "EXTENT"
length_unit = ""
area_unit = ""
coordinate_system = ""

# Generate the extent coordinates using Add Geometry Properties tool
arcpy.management.AddGeometryAttributes(in_features, properties, length_unit,
                                       area_unit, coordinate_system)

# Use Search Cursor to walk through each feature and generate grids
with arcpy.da.SearchCursor(in_features, ["OID@", "EXT_MIN_X", "EXT_MIN_Y",
                                         "EXT_MAX_X", "EXT_MAX_Y"]) as sCur:
    for row in sCur:
        minX, minY, maxX, maxY = row[1], row[2], row[3], row[4]
        arcpy.management.CreateFishnet("fishnet_{0}".format(row[0]),
                    number_rows = 10,
                    number_columns = 10,
                    template = "{} {} {} {}".format(minX, maxX, minY, maxY),
                    origin_coord = "{} {}".format(minX, minY),
                    y_axis_coord = "{} {}".format(minX, maxY),
                    corner_coord = "{} {}".format(maxX, maxY),
                    geometry_type = "POLYGON",
                    labels = "NO_LABELS")
```

### Example 5

```python
# Name: GridCreation.py

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.env.outputCoordinateSystem = arcpy.Describe("roads").spatialReference

# Set local variables
in_features = "roads"
properties = "EXTENT"
length_unit = ""
area_unit = ""
coordinate_system = ""

# Generate the extent coordinates using Add Geometry Properties tool
arcpy.management.AddGeometryAttributes(in_features, properties, length_unit,
                                       area_unit, coordinate_system)

# Use Search Cursor to walk through each feature and generate grids
with arcpy.da.SearchCursor(in_features, ["OID@", "EXT_MIN_X", "EXT_MIN_Y",
                                         "EXT_MAX_X", "EXT_MAX_Y"]) as sCur:
    for row in sCur:
        minX, minY, maxX, maxY = row[1], row[2], row[3], row[4]
        arcpy.management.CreateFishnet("fishnet_{0}".format(row[0]),
                    number_rows = 10,
                    number_columns = 10,
                    template = "{} {} {} {}".format(minX, maxX, minY, maxY),
                    origin_coord = "{} {}".format(minX, minY),
                    y_axis_coord = "{} {}".format(minX, maxY),
                    corner_coord = "{} {}".format(maxX, maxY),
                    geometry_type = "POLYGON",
                    labels = "NO_LABELS")
```

---

## Add Global IDs (Data Management)

## Summary

Adds global IDs to a list of geodatabase feature classes, tables, and feature datasets.

## Usage

- Global IDs uniquely identify a feature or table row in a geodatabase and across geodatabases.
- If the input dataset is from an enterprise geodatabase, it must be from a database connection established as the data owner.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | A list of geodatabase classes, tables, and feature datasets to which global IDs will be added. | Layer; Table View; Dataset |
| in_datasets[in_dataset,...] | A list of geodatabase classes, tables, and feature datasets to which global IDs will be added. | Layer; Table View; Dataset |

## Code Samples

### Example 1

```python
arcpy.management.AddGlobalIDs(in_datasets)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/MySDEdata.sde" 
arcpy.management.AddGlobalIDs("GDB1.Heather.Roads")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/MySDEdata.sde" 
arcpy.management.AddGlobalIDs("GDB1.Heather.Roads")
```

### Example 4

```python
# Name: AddGlobalIDs_Example2.py
# Description: Add globalIDs to a feature class. The feature class is in an enterprise workspace.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
in_dataset = "GDB1.Heather.Roads"

# Run AddGlobalIDs
arcpy.management.AddGlobalIDs(in_dataset)
```

### Example 5

```python
# Name: AddGlobalIDs_Example2.py
# Description: Add globalIDs to a feature class. The feature class is in an enterprise workspace.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
in_dataset = "GDB1.Heather.Roads"

# Run AddGlobalIDs
arcpy.management.AddGlobalIDs(in_dataset)
```

### Example 6

```python
# Name: AddGlobalIDs_Example2.py
# Description: Add globalIDs to a list of datasets. Both feature classes are in the same enterprise workspace.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
in_dataset1 = "GDB1.Heather.Roads"
in_dataset2 = "GDB1.Heather.Streets"

# Run AddGlobalIDs
arcpy.management.AddGlobalIDs([in_dataset1, in_dataset2])
```

### Example 7

```python
# Name: AddGlobalIDs_Example2.py
# Description: Add globalIDs to a list of datasets. Both feature classes are in the same enterprise workspace.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
in_dataset1 = "GDB1.Heather.Roads"
in_dataset2 = "GDB1.Heather.Streets"

# Run AddGlobalIDs
arcpy.management.AddGlobalIDs([in_dataset1, in_dataset2])
```

---

## Add GPS Metadata Fields (Data Management)

## Summary

Adds GNSS fields to a feature class in a geodatabase.

## Usage

- 0—Unknown
- 1—User defined
- 2—Integrated (System) Location Provider
- 3—External GNSS Receiver
- 4—Network Location Provider
- 0—Fix not valid
- 1—GPS
- 2—Differential GPS
- 4—RTK Fixed
- 5—RTK Float

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input feature class to be updated. The input can be a point, polyline, or polygon feature class. | Feature Layer |
| in_point_features | The input feature class to be updated. The input can be a point, polyline, or polygon feature class. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.AddGPSMetadataFields(in_point_features)
```

### Example 2

```python
import arcpy
arcpy.management.AddGPSMetadataFields(r'c:\data\fgdb.gdb\my_gps_tracks')
```

### Example 3

```python
import arcpy
arcpy.management.AddGPSMetadataFields(r'c:\data\fgdb.gdb\my_gps_tracks')
```

---

## Add Incrementing ID Field (Data Management)

## Summary

Adds a database-maintained, incrementing ID field to an existing table or feature class in a Dameng, IBM Db2, Microsoft Azure SQL Database, Microsoft SQL Server, Oracle, or PostgreSQL database. A database-maintained ID field is required for all feature classes or tables you plan to edit through a feature service.

## Usage

- You cannot run this tool on tables or feature classes that are registered with the geodatabase.
- You must be the owner of the table or feature class to add an ID field to it.
- If you do not specify a name for the field, ObjectID is used by default. If a field named ObjectID already exists, the tool will not run until you provide a different name.
- If a database-maintained, incrementing ID field already exists, this tool will not add another one.
- The following field types will be added:DBMSData typeDamengint32 with identity propertyDb2integer as identityOraclenumber with identityPostgreSQLserial SQL Serverinteger with identity property

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The location and name of the table or feature class to which an ID field will be added. | Table View |
| Field Name (Optional) | The name to be used for the ID field. If no input is provided, the default ObjectID will be used. | String |
| in_table | The location and name of the table or feature class to which an ID field will be added. | Table View |
| field_name(Optional) | The name to be used for the ID field. If no input is provided, the default ObjectID will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddIncrementingIDField(in_table, {field_name})
```

### Example 2

```python
import arcpy
arcpy.management.AddIncrementingIDField("C:/Data/DatabaseConnections/mydb.sde/insp.violations", "FSID")
```

### Example 3

```python
import arcpy
arcpy.management.AddIncrementingIDField("C:/Data/DatabaseConnections/mydb.sde/insp.violations", "FSID")
```

---

## Add Items To Catalog Dataset (Data Management)

## Summary

Adds workspace items and layers—such as geodatabase datasets, raster layers, feature layers, mosaic layers, and other items—to an existing catalog dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Catalog Dataset | The catalog dataset to which the items will be added. | Catalog Layer |
| Input Items | The workspace items, layers, and files from which items will be added to the catalog dataset. The workspace can be a folder, file geodatabase, feature dataset, enterprise database, or a service from a server connection. | Workspace; Feature Layer; Image Service; Raster Layer; Mosaic Layer; LAS Dataset Layer; Layer File; CAD Drawing Dataset; ServerConnection; BIM File Workspace; TIN Layer |
| Input Item Types (Optional) | Specifies the item types that will be added to the catalog dataset from any input workspaces. All supported item types will be added by default.BIM files—BIM file workspaces will be added.BIM floor plans—BIM file floor plans will be added.CAD datasets—CAD drawings will be added.Feature classes—Feature classes will be added.Feature services—Feature services will be added.Image services—Image services will be added.LAS datasets—LAS datasets will be added.LAS files—LAS files will be added.Layer files—Layer files will be added.Map services—Map services will be added.Mosaic datasets—Mosaic datasets will be added.Raster layers—Raster datasets will be added.Scene layer packages—Scene layer packages will be added.TIN datasets—TIN datasets will be added. | String |
| Include subfolders (Optional) | Specifies whether the contents of folders or workspaces specified in the Input Items parameter value will be recursively searched and added to the catalog dataset. This parameter is not applicable to file or enterprise geodatabases.Checked—The contents of folders or workspaces will be recursively searched and added to the catalog dataset. This is the default.Unchecked—The contents of folders or workspaces will not be recursively searched and added to the catalog dataset. | Boolean |
| Footprint Type (Optional) | Specifies whether the reference item's footprint will be the full extent or a convex hull representing the smallest convex polygon for all features.Envelope—The footprint will be a rectangle covering the full extent of the reference item. This is the default.Convex hull—The footprint will be a convex hull enclosing all features from the reference item. | String |
| target_catalog_dataset | The catalog dataset to which the items will be added. | Catalog Layer |
| input_items[input_items,...] | The workspace items, layers, and files from which items will be added to the catalog dataset. The workspace can be a folder, file geodatabase, feature dataset, enterprise database, or a service from a server connection. | Workspace; Feature Layer; Image Service; Raster Layer; Mosaic Layer; LAS Dataset Layer; Layer File; CAD Drawing Dataset; ServerConnection; BIM File Workspace; TIN Layer |
| input_item_types[input_item_types,...](Optional) | Specifies the item types that will be added to the catalog dataset from any input workspaces. All supported item types will be added by default.BIM_FILE_WORKSPACE—BIM file workspaces will be added.BIM_FILE_FLOORPLAN—BIM file floor plans will be added.CAD_DRAWING—CAD drawings will be added.FEATURE_CLASS—Feature classes will be added.FEATURE_SERVICE—Feature services will be added.IMAGE_SERVICE—Image services will be added.LAS_DATASET—LAS datasets will be added.LAS_FILE—LAS files will be added.LAYER_FILE—Layer files will be added.MAP_SERVICE—Map services will be added.MOSAIC_DATASET—Mosaic datasets will be added.RASTER_DATASET—Raster datasets will be added.SCENE_LAYER_PACKAGE—Scene layer packages will be added.TIN—TIN datasets will be added. | String |
| include_subfolders(Optional) | Specifies whether the contents of folders or workspaces specified in the input_items parameter value will be recursively searched and added to the catalog dataset. This parameter is not applicable to file or enterprise geodatabases.INCLUDE_SUBFOLDERS—The contents of folders or workspaces will be recursively searched and added to the catalog dataset. This is the default.NOT_INCLUDE_SUBFOLDERS—The contents of folders or workspaces will not be recursively searched and added to the catalog dataset. | Boolean |
| footprint_type(Optional) | Specifies whether the reference item's footprint will be the full extent or a convex hull representing the smallest convex polygon for all features.ENVELOPE—The footprint will be a rectangle covering the full extent of the reference item. This is the default.CONVEX_HULL—The footprint will be a convex hull enclosing all features from the reference item. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddItemsToCatalogDataset(target_catalog_dataset, input_items, {input_item_types}, {include_subfolders}, {footprint_type})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Dataspace/WhereMyCatalogLayerIs.gdb"
arcpy.management.AddItemsToCatalogDataset(
    "MyCatalogDataset", ["AllMyFeatures.gdb", "AllMyRasters.gdb"], 
    ["FEATURE_CLASS", "RASTER_DATASET"], "INCLUDE_SUBFOLDERS", "ENVELOPE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Dataspace/WhereMyCatalogLayerIs.gdb"
arcpy.management.AddItemsToCatalogDataset(
    "MyCatalogDataset", ["AllMyFeatures.gdb", "AllMyRasters.gdb"], 
    ["FEATURE_CLASS", "RASTER_DATASET"], "INCLUDE_SUBFOLDERS", "ENVELOPE")
```

---

## Add Join (Data Management)

## Summary

Joins a layer to another layer or table based on a common field. Feature layers, table views, and raster layers with a raster attribute table are supported.

## Usage

- The Input Table parameter value can be a feature layer, a table view, or a raster layer with an attribute table. If a data path is used, the layer will be created with the join. The join will always reside in the layer, not with the data.
- To make a permanent join, either use the Join Field tool or use the joined layer as input to one of the following tools: Copy Features, Copy Rows, Export Features, or Export Table. When saving the results to a new feature class or table, the Maintain fully qualified field names environment can be used to control whether the joined output field names will be qualified with the name of the table the field came from. Field aliases are persisted from the layer to the output, except when the output is a shapefile.
- Use the Make Query Layer, Create Database View, or Make Aggregation Query Layer tools to optimize join performance and for more capabilities when you want to join enterprise geodatabase or SQLite database data.
- If the input is a feature class or dataset path, this tool will create and return a new layer with the result of the tool applied.
- When a one-to-many join is produced by the join, the result of the join can be viewed in the attribute table, where a warning message will indicate if the table has duplicate object IDs. Because many geoprocessing tools do not support data with duplicate object IDs and processing such data can produce unexpected results, it is recommended that you first copy the joined layer to a new feature class using the Export Features tool. Then use the new feature class as input to other geoprocessing tools. Optionally, set the Join Operation parameter to Join one to first to prevent duplicate object IDs.
- The Join Operation parameter has three states to adjust the cardinality. The default is no value and will allow the data source to attempt a one-to-many join. The Join one to many option will work only on specific data sources that have an Object ID field. The Join one to first option will use the first match in the table, which may result in different outputs if the Object ID field is changed or the workspace the table is copied to changes. One-to-first joins are not case sensitive; one-to-many joins are case sensitive. If the joined table does have an Object ID, the first match will be the record with the lowest Object ID.
- The following tables include possible outcomes of performing a join with various inputs.The first table shows a one-to-many join. Keeping only matching records will have no effect, as all records have matches.Input tableJoin tableResultInput fieldTypeJoin fieldValueInput fieldTypeJoin fieldValue1A11001A11002B22002B220013001A130024002B2400Add Join example: One-to-many join when each table has an Object ID fieldThe second table uses a join table with no Object ID field; only a one-to-first join is possible. A one-to-first join is also only possible if each table is from a different workspace. A one-to-first join is not a case-sensitive match.Input tableJoin tableResultInput fieldTypeJoin fieldValueInput fieldTypeJoin fieldValue1A11001A11002B22002B220033004400Add Join example: One-to-first join when either table does not have an Object ID field In the last table, the input table has more records than the join table. Keeping all records will keep all of the matching records plus the records from the input table that did not match.Input tableJoin tableResultInput fieldTypeJoin fieldValueInput fieldTypeJoin fieldValue1A11001A11002B22002B22003C13001A13004D24002B24003C<Null><Null>4D<Null><Null>Add Join example: One-to-many join when each table has an Object ID field and the Keep all input records parameter is checked The input table must have an Object ID field to perform a one-to-many join and be in the same workspace.
- Records from the join table can be matched to more than one record if the join table has an Object ID field; otherwise, a one-to-first join will be performed.
- When joining tables, the default option is to keep all records. If a record in the input table doesn't have a match in the join table, that record is given null values for all the fields being appended into the input table from the join table. Input tableJoin tableResultInput fieldTypeJoin fieldValueInput fieldTypeJoin fieldValue1A11001A11002B22002B22003C13001A13004D24002B24003C<Null><Null>4D<Null><Null>When the Keep all input records parameter is unchecked, if a record in the input table doesn't have a match in the join table, that record is removed from the resultant output. If the input table is the attribute table of a layer, features that don't have data joined to them are not shown on the map.Input tableJoin tableResultInput fieldTypeJoin fieldValueInput fieldTypeJoin fieldValue1A11001A11002B22002B220033004400
- The Calculate Field tool will update the first record encountered when using a one-to-many layer and skip the remainder duplicate records. When editing the joined layer field values manually in the table view, the last edit made remains..
- Field properties, such as aliases, visibility, and number formatting, are maintained when a join is added or removed.
- The join persists only for the duration of the layer. A layer can be retained by saving the ArcGIS Pro session or by saving it to a layer file using the Save Layer To File tool. To see the results of a join created in a script tool, the tool must include the layer as a derived output parameter. Similarly, the Updated Input Layer or Table View parameter must be set as a derived output parameter in a model tool to see the joined results.
- In the resulting table, fields will be prefixed with the input's name and a period (.), and all fields from the join table will be prefixed with the join table name and a period as the default.For example, joining landuse, which has fields A and B, to lookup_tab, which has fields C and D, will result in a layer or table view with landuse.A, landuse.B, lookup_tab.C, and lookup_tab.D fields.
- A layer must have unique field names. If both the input and join tables have the same name and are in different workspaces, a join cannot be performed without creating a poorly defined layer.
- Indexing the input field and join field can improve performance. If the Index join fields parameter is checked, an attribute index will be added to both fields. Alternatively, each joining field can be indexed with the Add Attribute Index tool.
- If the join results are unexpected or incomplete, review whether the input field and join field are indexed. If the fields are not indexed, try adding an index. If the fields are indexed, try deleting and re-adding the index to correct any problems with the index. Alternatively, check the Rebuild Join Field Indexes parameter to remove existing indexes and rebuild them.
- If the input layer or table view's fields were modified (renamed or hidden) using the Field Info parameter in the Make Feature Layer or Make Table View tool, the field modifications will not be included in the output joined layer or table view.
- The definition query of the join table will be applied to the input layer or table view by adding a new active query. The previous query is preserved and set to inactive so that the query can be disabled from the joined table if needed. The definition query can be removed using the Remove Join tool.
- If the join table has a definition query, the Keep all input records parameter will have no effect. Manually updating the definition query by appending or OBJECTID is null can fix this if appropriate.
- The Validate Join tool can be used to validate a join between two layers or tables to determine if the layers or tables have valid field names and Object ID fields, if the join produces matching records, if the join is a one-to-one or one-to-many join, and other properties of the join. A button to validate the join is available on the tool dialog box for ease of use.
- This tool ignores selections on the input and join tables. The Join Field tool supports selections. To only join with a selected subset, create a selection layer and use it as input to this tool. Join layer properties are copied when you create a selection layer.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The layer or table view to which the join table will be joined. | Mosaic Layer; Raster Layer; Table View |
| Input Field | The field in the input layer or table view on which the join will be based. | Field |
| Join Table | The table or table view that will be joined to the input layer or table view. | Mosaic Layer; Raster Layer; Table View |
| Join Field | The field in the join table that contains the values on which the join will be based. | Field |
| Keep all input records(Optional) | Specifies whether only records in the input that match a record in the join table will be included in the output or all records in the input layer or table view will be included.Checked—All records in the input layer or table view will be included in the output. This is also known as an outer join. This is the default. Unchecked—Only those records in the input that match a row in the join table will be included in the output. This is also known as an inner join. | Boolean |
| Index join fields(Optional) | Specifies whether table attribute indexes will be added to the input field and join field.Checked—Both fields will be indexed. If the table has an existing index, a new index will not be added. Unchecked—Indexes will not be added. This is the default. | Boolean |
| Rebuild Joined Fields Indexes(Optional) | Specifies whether the indexes of the input field and join field will be removed and rebuilt.Checked—Existing indexes will be removed and a new index will be added.Unchecked—Existing indexes will not be removed or rebuilt. This is the default. | Boolean |
| Join Operation(Optional) | Specifies whether the join will be a one-to-many join or a one-to-first join when the data has a one-to-many cardinality. If no parameter value is specified, the join operation will be based on the data source.Join one to first—The join operation will use the first match. Join one to many—The join operation will perform multiple case-sensitive matches. | String |
| in_layer_or_view | The layer or table view to which the join table will be joined. | Mosaic Layer; Raster Layer; Table View |
| in_field | The field in the input layer or table view on which the join will be based. | Field |
| join_table | The table or table view that will be joined to the input layer or table view. | Mosaic Layer; Raster Layer; Table View |
| join_field | The field in the join table that contains the values on which the join will be based. | Field |
| join_type(Optional) | Specifies whether only records in the input that match a record in the join table will be included in the output or all records in the input layer or table view will be included.KEEP_ALL—All records in the input layer or table view will be included in the output. This is also known as an outer join. This is the default. KEEP_COMMON—Only those records in the input that match a row in the join table will be included in the output. This is also known as an inner join. | Boolean |
| index_join_fields(Optional) | Specifies whether table attribute indexes will be added to the input field and join field.INDEX_JOIN_FIELDS—Both fields will be indexed. If the table has an existing index, a new index will not be added.NO_INDEX_JOIN_FIELDS—Indexes will not be added. This is the default. | Boolean |
| rebuild_index(Optional) | Specifies whether the indexes of the input field and join field will be removed and rebuilt.REBUILD_INDEX— Existing indexes will be removed and a new index will be added.NO_REBUILD_INDEX— Existing indexes will not be removed or rebuilt. This is the default. | Boolean |
| join_operation(Optional) | Specifies whether the join will be a one-to-many join or a one-to-first join when the data has a one-to-many cardinality. If no parameter value is specified, the join operation will be based on the data source.JOIN_ONE_TO_FIRST—The join operation will use the first match. JOIN_ONE_TO_MANY—The join operation will perform multiple case-sensitive matches. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddJoin(in_layer_or_view, in_field, join_table, join_field, {join_type}, {index_join_fields}, {rebuild_index}, {join_operation})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
veg_joined_table = arcpy.management.AddJoin("vegetation", "HOLLAND95", 
                                            "vegtable", "HOLLAND95")
arcpy.management.CopyFeatures(veg_joined_table, "vegjoin")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
veg_joined_table = arcpy.management.AddJoin("vegetation", "HOLLAND95", 
                                            "vegtable", "HOLLAND95")
arcpy.management.CopyFeatures(veg_joined_table, "vegjoin")
```

### Example 4

```python
# Name: AttributeSelection.py
# Purpose: Join a table to a feature class and select the desired attributes

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
# The qualifiedFieldNames environment is used by Copy Features when persisting 
# the join field names.
arcpy.env.qualifiedFieldNames = False

# Set local variables
inFeatures = "vegtype"
joinTable = "vegtable"
joinField = "HOLLAND95"
expression = "vegtable.HABITAT = 1"
outFeature = "vegjoin"

# Join the feature layer to a table
veg_joined_table = arcpy.management.AddJoin(inFeatures, joinField, joinTable, 
                                            joinField)

# Select desired features from veg_layer
arcpy.management.SelectLayerByAttribute(veg_joined_table, "NEW_SELECTION", 
                                        expression)

# Copy the layer to a new permanent feature class
result = arcpy.management.CopyFeatures(veg_joined_table, outFeature)

# See field names and aliases
resultFields = arcpy.ListFields(result)
print([field.name for field in resultFields])
print([field.aliasName for field in resultFields])
```

### Example 5

```python
# Name: AttributeSelection.py
# Purpose: Join a table to a feature class and select the desired attributes

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
# The qualifiedFieldNames environment is used by Copy Features when persisting 
# the join field names.
arcpy.env.qualifiedFieldNames = False

# Set local variables
inFeatures = "vegtype"
joinTable = "vegtable"
joinField = "HOLLAND95"
expression = "vegtable.HABITAT = 1"
outFeature = "vegjoin"

# Join the feature layer to a table
veg_joined_table = arcpy.management.AddJoin(inFeatures, joinField, joinTable, 
                                            joinField)

# Select desired features from veg_layer
arcpy.management.SelectLayerByAttribute(veg_joined_table, "NEW_SELECTION", 
                                        expression)

# Copy the layer to a new permanent feature class
result = arcpy.management.CopyFeatures(veg_joined_table, outFeature)

# See field names and aliases
resultFields = arcpy.ListFields(result)
print([field.name for field in resultFields])
print([field.aliasName for field in resultFields])
```

---

## Add Portal Items To Catalog Dataset (Data Management)

## Summary

Adds ArcGIS Online or ArcGIS Enterprise portal service items—such as feature, map, image, scene, and tile services—to an existing catalog dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Catalog Dataset | The catalog dataset to which portal items will be added. | Catalog Layer |
| Input portal item types (Optional) | Specifies the item types that will be added to the catalog dataset from the portal. All supported item types will be added by default.Feature layers—Feature layers will be added. This option does not add feature collections.Imagery layers—Imagery layers will be added.Map image and tile layers—Map image and tile layers will be added.Scene layers—Scene layers will be added.Vector tile layers—Vector tile layers will be added.Web Feature Service (WFS) layers—Web Feature Service (WFS) layers will be added.Web Map Service (WMS) layers—Web Map Service (WMS) layers will be added.Web Map Tile Service (WMTS) layers—Web Map Tile Service (WMTS) layers will be added. | String |
| Content (Optional) | Specifies the collection in the active portal from which items will be added to the catalog dataset.My Content— Items from your My Content collection will be added. This is the default.My Groups—Items from groups to which you belong will be added.My Organization—Items from your ArcGIS organization will be added. | String |
| Portal folders (Optional) | The portal folders from which items will be added to the catalog dataset. | String |
| Portal groups (Optional) | The portal groups from which items will be added to the catalog dataset. | String |
| Access level (Optional) | Specifies the sharing level that portal items must have to be added to the catalog dataset.Public— Items that are shared with the public will be added to the catalog dataset. This is the default.Organization—Items that are shared with the organization, as well as items you own will be added to the catalog dataset. Items that are shared with the organization and one or more groups will also be added.Shared—Items shared with one or more groups, item owners, and those who have access to the item through group membership will be added to the catalog dataset.Private—Items owned by you will be added to the catalog dataset. Only you or administrators who have access to your content can add these items. | String |
| target_catalog_dataset | The catalog dataset to which portal items will be added. | Catalog Layer |
| input_portal_itemtypes[input_portal_itemtypes,...](Optional) | Specifies the item types that will be added to the catalog dataset from the portal. All supported item types will be added by default.FEATURE_SERVICE—Feature layers will be added. This option does not add feature collections.IMAGE_SERVICE—Imagery layers will be added.MAP_SERVICE—Map image and tile layers will be added.SCENE_SERVICE—Scene layers will be added.VECTOR_TILE_SERVICE—Vector tile layers will be added.WFS—Web Feature Service (WFS) layers will be added.WMS—Web Map Service (WMS) layers will be added.WMTS—Web Map Tile Service (WMTS) layers will be added. | String |
| content(Optional) | Specifies the collection in the active portal from which items will be added to the catalog dataset.MY_CONTENT— Items from your My Content collection will be added. This is the default.MY_GROUPS—Items from groups to which you belong will be added.MY_ORGANIZATION—Items from your ArcGIS organization will be added. | String |
| portal_folders[portal_folders,...](Optional) | The portal folders from which items will be added to the catalog dataset. | String |
| portal_groups[portal_groups,...](Optional) | The portal groups from which items will be added to the catalog dataset. | String |
| access_level(Optional) | Specifies the sharing level that portal items must have to be added to the catalog dataset.PUBLIC— Items that are shared with the public will be added to the catalog dataset. This is the default.ORG—Items that are shared with the organization, as well as items you own will be added to the catalog dataset. Items that are shared with the organization and one or more groups will also be added.SHARED—Items shared with one or more groups, item owners, and those who have access to the item through group membership will be added to the catalog dataset.PRIVATE—Items owned by you will be added to the catalog dataset. Only you or administrators who have access to your content can add these items. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddPortalItemsToCatalogDataset(target_catalog_dataset, {input_portal_itemtypes}, {content}, {portal_folders}, {portal_groups}, {access_level})
```

### Example 2

```python
import arcpy

target_catalog_dataset = r"C:/Dataspace/studyarea.gdb/SampleCatalog"
input_portal_itemtypes = "IMAGE_SERVICE"
content = "MY_CONTENT"
portal_groups = None 
portal_folders = "SampleFolder"
access_level = "PRIVATE"
arcpy.management.AddPortalItemsToCatalogDataset(target_catalog_dataset,
                                        input_portal_itemtypes, content, 
                                        portal_folders, portal_groups,
                                        access_level)
```

### Example 3

```python
import arcpy

target_catalog_dataset = r"C:/Dataspace/studyarea.gdb/SampleCatalog"
input_portal_itemtypes = "IMAGE_SERVICE"
content = "MY_CONTENT"
portal_groups = None 
portal_folders = "SampleFolder"
access_level = "PRIVATE"
arcpy.management.AddPortalItemsToCatalogDataset(target_catalog_dataset,
                                        input_portal_itemtypes, content, 
                                        portal_folders, portal_groups,
                                        access_level)
```

### Example 4

```python
import arcpy

target_catalog_dataset = r"C:/Dataspace/studyarea.gdb/SampleCatalog"
input_portal_itemtypes = ["SCENE_SERVICE", "WFS"]
content = "MY_GROUPS"
portal_groups = "SampleGroup" 
portal_folders = None
access_level = "ORG"
arcpy.management.AddPortalItemsToCatalogDataset(target_catalog_dataset,
                                        input_portal_itemtypes, content,
                                        portal_folders, portal_groups,
                                        access_level)
```

### Example 5

```python
import arcpy

target_catalog_dataset = r"C:/Dataspace/studyarea.gdb/SampleCatalog"
input_portal_itemtypes = ["SCENE_SERVICE", "WFS"]
content = "MY_GROUPS"
portal_groups = "SampleGroup" 
portal_folders = None
access_level = "ORG"
arcpy.management.AddPortalItemsToCatalogDataset(target_catalog_dataset,
                                        input_portal_itemtypes, content,
                                        portal_folders, portal_groups,
                                        access_level)
```

---

## Add Rasters To Mosaic Dataset (Data Management)

## Summary

Adds raster datasets to a mosaic dataset from various sources, including a file, folder, table, or web service.

## Usage

- Raster data that is added is not managed; if the raster data is deleted or moved, the mosaic dataset will be affected.
- You can only add rasters to a mosaic dataset contained in a geodatabase. Those created outside a geodatabase can only contain the contents of a previously created mosaic dataset.
- The Raster Type parameter identifies metadata required for loading data into the mosaic dataset.The raster type is specific to imagery products. It identifies metadata—such as georeferencing, acquisition date, and sensor type—along with a raster format.Learn more about raster types
- The Raster Type Properties page contains information about the supported product types and processing templates based on the selected raster type. You can define various other properties such as Stretch, Gamma, Orthorectification, Band Combination, and Pansharpening types based on the analysis and the data being used.
- The Processing Templates drop-down list includes functions that will be applied to items that will be added to the mosaic dataset and how, or in what order, they will be applied.
- For faster display and processing, the pixel cache for a mosaic dataset can be generated when the Enable Pixel Cache parameter is checked.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The path and name of the mosaic dataset to which the raster data will be added. | Mosaic Layer |
| Raster Type | The type of raster that will be added. The raster type is specific to imagery products. It identifies metadata—such as georeferencing, acquisition date, and sensor type—along with a raster format.If you are using a LAS, LAS Dataset, or Terrain raster type, a cell size must be specified on the Raster Type properties page.To learn more about supported sensors and raster types, see List of supported sensors.Use the Processing Templates drop-down list to choose, import, or create a raster function chain (template) to apply to the mosaic dataset or items. The function chains will be used to process a mosaic dataset or the mosaic dataset items on the fly. You can add, remove, or reorder the function chains.All the template names that are added must be unique.The Processing Templates drop-down list contains functions that will be applied to items that are added to the mosaic dataset and how, or in what order, they will be applied. You can use a single function, such as the Stretch function, or you can chain multiple functions together to create a more advanced product. Most raster types have several preexisting functions associated with them. Use this drop-down list to edit existing functions or add new functions to items that will be added to the mosaic dataset. To edit a template, select it in the Processing Templates drop-down list, and click Edit Raster Function . Once you finish editing the template, click Save to update the template, or click Save As to save it as a new item in the drop-down list. To export a template to disk for use with other mosaic datasets, click the Export button .To create a template, click Create New Raster FunctionTemplate in the Processing Templates drop-down list. For more information, see Raster function template.To import a function chain from disk or from the Raster Function pane, click Import Generic in the Processing Templates drop-down list. If the template was created independently of the raster type template editor, you need to change the name of the primary input raster variable to Dataset. To do this, double-click the first function in the chain and click the Variables tab. Change the value in the Name field of the raster parameter to Dataset. | Raster Type |
| Input Data | Specifies the path and name of the input file, folder, raster dataset, mosaic dataset, table, or service.Not all input options will be available. The selected raster type determines the available options.Dataset—An ArcGIS geographic dataset, such as a raster or mosaic dataset in a geodatabase or table, will be used as input.Workspace—A folder containing multiple raster datasets will be used as input. The folder can contain subfolders. This option is affected by the Include Sub Folders and Input Data Filter parameters.File—One or more raster datasets stored in a folder on disk, an image service definition file (.ISDef), or a raster process definition file (.RPDef) will be used as input. Files that do not correspond to the raster type being added will be ignored. Do not use this option with file formats that are raster datasets, such as TIFF or MrSID files; use the Dataset option instead.Service—A WCS, a map, an image service, or a web service layer file will be used as input. | File; Image Service; LAS Dataset Layer; Layer File; Map Server; Mosaic Layer; Raster Layer; Table View; Terrain Layer; WCS Coverage; WMS Map; Workspace |
| Update Cell Size Ranges(Optional) | Specifies whether the cell size ranges of each raster in the mosaic dataset will be calculated. These values will be written to the attribute table in the minPS and maxPS fields.Checked—The cell size ranges will be calculated for all the rasters in the mosaic dataset. This is the default.Unchecked—The cell size ranges will not be calculated. | Boolean |
| Update Boundary(Optional) | Specifies whether the boundary polygon of a mosaic dataset will be generated or updated. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels.Checked—The boundary will be generated or updated. This is the default.Unchecked—The boundary will not be generated or updated. | Boolean |
| Update Overviews(Optional) | Specifies whether overviews for a mosaic dataset will be defined and generated.Checked—Overviews will be defined and generated.Unchecked—Overviews will not be defined or generated. This is the default. | Boolean |
| Maximum Levels(Optional) | The maximum number of pyramid levels that will be used in the mosaic dataset. For example, a value of 2 will use only the first two pyramid levels from the source raster. Leaving this parameter blank or providing a value of -1 will build pyramids for all levels.This value can affect the display and number of overviews that will be generated. | Long |
| Maximum Cell Size(Optional) | The maximum pyramid cell size that will be used in the mosaic dataset. | Double |
| Minimum Rows or Columns(Optional) | The minimum dimensions of a raster pyramid that will be used in the mosaic dataset. | Long |
| Coordinate System for Input Data(Optional) | The spatial reference system of the input data.Specify a value if the data does not have a coordinate system; otherwise, the coordinate system of the mosaic dataset will be used. This can also be used to override the coordinate system of the input data. | Spatial Reference |
| Input Data Filter(Optional) | A filter for the data being added to the mosaic dataset. You can use SQL expressions to create the data filter. The wildcards for the filter work on the full path to the input data.For example, the following SQL statement will select the rows in which the following object IDs match:OBJECTID IN (19745, 19680, 19681, 19744, 5932, 5931, 5889, 5890, 14551, 14552, 14590, 14591)To add only a TIFF image, add an asterisk before a file extension. *.TIFTo add an image with the word sensor in the file path or file name, add an asterisk before and after the word sensor.*sensor2009*You can also use PERL syntax to create a data filter.REGEX:.*1923.*\|.*1922.* REGEX:.*192[34567].*\|.*194.*\|.*195.* The following PERL syntax with multiple lexical groupings as part of the expression is not supported:REGEX:.* map_mean_.*(?:(?:[a-z0-9]*)_pptPct_(?:[0-9]\|1[0-2]*?)_2[0-9]_*\w*).imgAlternatively, you can use the following syntax:REGEX:.*map_mean_*[a-z0-9]*_pptPct_([0-9]\|1[0-2])_2[0-9]*_\w*.img | String |
| Include Sub Folders(Optional) | Specifies whether subfolders will be recursively explored.Checked—All subfolders will be explored for data. This is the default.Unchecked—Only the top-level folder will be explored for data. | Boolean |
| Add New Datasets Only(Optional) | Specifies how duplicate rasters will be handled. A check will be performed to determine whether each raster has already been added, using the original path and file name. Specify the option to use when a duplicate path and file name are found.Allow duplicates—All rasters will be added even if they already exist in the mosaic dataset. This is the default.Exclude duplicates—Duplicate rasters will not be added.Overwrite duplicates—Duplicate rasters will overwrite existing rasters. | String |
| Build Raster Pyramids(Optional) | Specifies whether pyramids will be built for each source raster.Unchecked—Pyramids will not be built. This is the default.Checked—Pyramids will be built. | Boolean |
| Calculate Statistics(Optional) | Specifies whether statistics will be calculated for each source raster.Unchecked—Statistics will not be calculated. This is the default.Checked—Statistics will be calculated. | Boolean |
| Build Thumbnails (Optional) | Specifies whether thumbnails will be built for each source raster.Unchecked—Thumbnails will not be built. This is the default.Checked—Thumbnails will be built. | Boolean |
| Operation Description(Optional) | The description that will be used to represent the operation of adding raster data. It will be added to the raster type table, which can be used as part of a search or as a reference at another time. | String |
| Force this Coordinate System for Input Data(Optional) | Specifies whether the Coordinate System for Input Data parameter value will be used for all the rasters when loading data into the mosaic dataset. This parameter does not reproject the data; it uses the coordinate system defined in the tool to construct items in the mosaic dataset. The extent of the image will be used, but the projection will be overwritten. Unchecked—The coordinate system of each raster data will be used when loading data. This is the default. If unchecked and the input image does not have a coordinate system (that is, it's unknown), the mosaic dataset coordinate system will be used in constructing mosaic dataset items. If the image has a coordinate system, that coordinate system will be used.Checked—The coordinate system specified in the Coordinate System for Input Data parameter will be used for each raster dataset when loading data. | Boolean |
| Estimate Mosaic Dataset Statistics (Optional) | Specifies whether statistics will be estimated on the mosaic dataset for faster rendering and processing at the mosaic dataset level.Unchecked—Statistics will not be estimated. Statistics generated from each item in the mosaic dataset will be used for display and processing. This is the default.Checked—Statistics will be estimated at the mosaic dataset level. This will use the distribution of pixels to display the mosaic dataset rather than the distribution of the source item in the mosaic dataset. | Boolean |
| Auxiliary Inputs (Optional) | The raster type settings that will be defined on the Raster Type Properties page. This parameter will override the settings defined on the Raster Type Properties page. Auxiliary input options include the following:CameraFile—A .cam or .csv file that stores the camera model information, such as the focal length, sensor pixel size, columns, rows and similar attributes.CameraProperties—A JSON string that defines the camera model.ConstantZ—A value, in meters, that is used to provide an initial estimate of the flight height for each image.CorrectGeoid—A value of 0 represents false, and a value of 1 represents true. This option is only available when a DEM is used.DEM—A DEM used to provide an initial estimate of the flight height for each image.EstimateFlightHeight—An estimated flight height that can be directly used to compute footprint and estimate photo scale.GPSAltRef—Define the altitude used as the reference altitude. 0 indicates above sea level, and 1 is below sea level.GPSFile—A GPS text file, such as a comma-separated values file (.csv), which includes values for the Image Name, Latitude, Longitude, and Altitude fields, and, optionally, the Omega, Phi, and Kappa fields. The geolocation file is provided by the supplier along with the drone imagery.GPSLatRef—Define whether the latitude is north or south latitude. N indicates north latitude, and S indicates south latitude.GPSLonRef—Define whether the longitude is east or west longitude. E indicates east longitude, and W indicates west longitude.GPSSRS—Define the spatial reference of the GPS.GPSVCS—Define the vertical coordinate system of the GPS.IsAltitudeFlightHeight—A Boolean value that defines whether the drone reports heights relative to the takeoff point or altitude for heights relative to a vertical datum. Use a value of 0 when the altitude value is altitude above a datum, and a value of 1 when the altitude value is flight height from a takeoff point.MinimumFlightHeight—Define the minimum flight height. If the estimated flight height of an image is smaller than this number, the image will be considered invalid and will not be added to the image collection for future processing.PanelReflectanceFactor—Define the surface reflectance factor for each band. This option is only available for RedEdge and Altum raster types when Multispectral Processing Template is selected. Type the values in the following format: <band name 1>: <double val>, <band name 2>: <double val>, <band name 3>: <double val>, ... , <band name n>: <double val>. For example, Blue: 0.48, Green: 0.481, Red: 0.488, RedEdge: 0.488.RadiometricQuantityType—An integer value that defines the type of radiometric measurement. This option is only available for RedEdge and Altum raster types when Multispectral Processing Template is selected. The value of RadiometricQuantityType includes the following values:0—DN (Digital Number), default1—Radiance2—Reflectance4—Temperature (thermal band only)ZFactor—The scaling factor used to convert the elevation values. This option is only available when using a DEM.ZOffset—The base value to be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level. This option is only available when using a DEM. | Value Table |
| Enable Pixel Cache(Optional) | Specifies whether the pixel cache will be generated for faster display and processing of the mosaic dataset.Unchecked—The pixel cache will not be generated. This is the default.Checked—The pixel cache will be generated. | Boolean |
| Pixel Cache Location(Optional) | The location of the pixel cache. If no location is provided, the cache will be written to C:\Users\<Username>\AppData\Local\ESRI\rasterproxies\. Once the location is provided, you do not need to redefine the path when adding new rasters to the mosaic dataset. You do need to check the Enable Pixel Cache parameter when adding the new data. | Folder; String |
| in_mosaic_dataset | The path and name of the mosaic dataset to which the raster data will be added. | Mosaic Layer |
| raster_type | The type of raster that will be added. The raster type is specific to imagery products. It identifies metadata—such as georeferencing, acquisition date, and sensor type—along with a raster format.If you are using a LAS, LAS Dataset, or Terrain raster type, an .art file must be used when the cell size is specified.To learn more about supported sensors and raster types, see List of supported sensors.You can identify any function chains that will be used to process a mosaic dataset or the mosaic dataset items on the fly. You can add, remove, or reorder the function chains.All the template names that are added must be unique. | Raster Type |
| input_path[input_path,...] | Specifies the path and name of the input file, folder, raster dataset, mosaic dataset, table, or service.Not all input options will be available. The selected raster type determines the available options.Dataset—An ArcGIS geographic dataset, such as a raster or mosaic dataset in a geodatabase or table, will be used as input.Folder—A folder containing multiple raster datasets will be used as input. The folder can contain subfolders. This option is affected by the Include Sub Folders and Input Data Filter parameters.File—One or more raster datasets stored in a folder on disk, an image service definition file (.ISDef), or a raster process definition file (.RPDef) will be used as input. Files that do not correspond to the raster type being added will be ignored. Do not use this option with file formats that are raster datasets, such as TIFF or MrSID files; use the Dataset option instead.Service—A WCS, a map, an image service, or a web service layer file will be used as input. | File; Image Service; LAS Dataset Layer; Layer File; Map Server; Mosaic Layer; Raster Layer; Table View; Terrain Layer; WCS Coverage; WMS Map; Workspace |
| update_cellsize_ranges(Optional) | Specifies whether the cell size ranges of each raster in the mosaic dataset will be calculated. These values will be written to the attribute table in the minPS and maxPS fields.UPDATE_CELL_SIZES—The cell size ranges will be calculated for all the rasters in the mosaic dataset. This is the default.NO_CELL_SIZES—The cell size ranges will not be calculated. | Boolean |
| update_boundary(Optional) | Specifies whether the boundary polygon of a mosaic dataset will be generated or updated. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels.UPDATE_BOUNDARY—The boundary will be generated or updated. This is the default.NO_BOUNDARY—The boundary will not be generated or updated. | Boolean |
| update_overviews(Optional) | Specifies whether overviews for a mosaic dataset will be defined and generated.UPDATE_OVERVIEWS—Overviews will be defined and generated.NO_OVERVIEWS—Overviews will not be defined or generated. This is the default. | Boolean |
| maximum_pyramid_levels(Optional) | The maximum number of pyramid levels that will be used in the mosaic dataset. For example, a value of 2 will use only the first two pyramid levels from the source raster. Leaving this parameter blank or providing a value of -1 will build pyramids for all levels.This value can affect the display and number of overviews that will be generated. | Long |
| maximum_cell_size(Optional) | The maximum pyramid cell size that will be used in the mosaic dataset. | Double |
| minimum_dimension(Optional) | The minimum dimensions of a raster pyramid that will be used in the mosaic dataset. | Long |
| spatial_reference(Optional) | The spatial reference system of the input data.Specify a value if the data does not have a coordinate system; otherwise, the coordinate system of the mosaic dataset will be used. This can also be used to override the coordinate system of the input data. | Spatial Reference |
| filter(Optional) | A filter for the data being added to the mosaic dataset. You can use SQL expressions to create the data filter. The wildcards for the filter work on the full path to the input data.For example, the following SQL statement will select the rows in which the following object IDs match:OBJECTID IN (19745, 19680, 19681, 19744, 5932, 5931, 5889, 5890, 14551, 14552, 14590, 14591)To add only a TIFF image, add an asterisk before a file extension. *.TIFTo add an image with the word sensor in the file path or file name, add an asterisk before and after the word sensor.*sensor2009*You can also use PERL syntax to create a data filter.REGEX:.*1923.*\|.*1922.* REGEX:.*192[34567].*\|.*194.*\|.*195.* The following PERL syntax with multiple lexical groupings as part of the expression is not supported:REGEX:.* map_mean_.*(?:(?:[a-z0-9]*)_pptPct_(?:[0-9]\|1[0-2]*?)_2[0-9]_*\w*).imgAlternatively, you can use the following syntax:REGEX:.*map_mean_*[a-z0-9]*_pptPct_([0-9]\|1[0-2])_2[0-9]*_\w*.img | String |
| sub_folder(Optional) | Specifies whether subfolders will be recursively explored.SUBFOLDERS—All subfolders will be explored for data. This is the default.NO_SUBFOLDERS—Only the top-level folder will be explored for data. | Boolean |
| duplicate_items_action(Optional) | Specifies how duplicate rasters will be handled. A check will be performed to determine whether each raster has already been added, using the original path and file name. Specify the option to use when a duplicate path and file name are found.ALLOW_DUPLICATES—All rasters will be added even if they already exist in the mosaic dataset. This is the default.EXCLUDE_DUPLICATES—Duplicate rasters will not be added.OVERWRITE_DUPLICATES—Duplicate rasters will overwrite existing rasters. | String |
| build_pyramids(Optional) | Specifies whether pyramids will be built for each source raster.NO_PYRAMIDS—Pyramids will not be built. This is the default.BUILD_PYRAMIDS—Pyramids will be built. | Boolean |
| calculate_statistics(Optional) | Specifies whether statistics will be calculated for each source raster.NO_STATISTICS—Statistics will not be calculated. This is the default.CALCULATE_STATISTICS—Statistics will be calculated. | Boolean |
| build_thumbnails(Optional) | Specifies whether thumbnails will be built for each source raster.NO_THUMBNAILS—Thumbnails will not be built. This is the default.BUILD_THUMBNAILS—Thumbnails will be built. | Boolean |
| operation_description(Optional) | The description that will be used to represent the operation of adding raster data. It will be added to the raster type table, which can be used as part of a search or as a reference at another time. | String |
| force_spatial_reference(Optional) | Specifies the coordinate system that will be used. Use the coordinate system specified in the spatial_reference parameter for all the rasters when loading data into the mosaic dataset.NO_FORCE_SPATIAL_REFERENCE—The coordinate system of each raster data will be used when loading data. This is the default.FORCE_SPATIAL_REFERENCE—The coordinate system specified in the spatial_reference parameter will be used for each raster when loading data. | Boolean |
| estimate_statistics(Optional) | Specifies whether statistics will be estimated on the mosaic dataset for faster rendering and processing at the mosaic dataset level.NO_STATISTICS—Statistics will not be estimated. Statistics generated from each item in the mosaic dataset will be used for display and processing. This is the default.ESTIMATE_STATISTICS—Statistics will be estimated at the mosaic dataset level. This will use the distribution of pixels to display the mosaic dataset rather than the distribution of the source item in the mosaic dataset. | Boolean |
| aux_inputs[aux_inputs,...](Optional) | The raster type settings that will be defined on the Raster Type Properties page. This parameter will override the settings defined on the Raster Type Properties page. Auxiliary input options include the following:CameraFile—A .cam or .csv file that stores the camera model information, such as the focal length, sensor pixel size, columns, rows and similar attributes.CameraProperties—A JSON string that defines the camera model.ConstantZ—A value, in meters, that is used to provide an initial estimate of the flight height for each image.CorrectGeoid—A value of 0 represents false, and a value of 1 represents true. This option is only available when a DEM is used.DEM—A DEM used to provide an initial estimate of the flight height for each image.EstimateFlightHeight—An estimated flight height that can be directly used to compute footprint and estimate photo scale.GPSAltRef—Define the altitude used as the reference altitude. 0 indicates above sea level, and 1 is below sea level.GPSFile—A GPS text file, such as a comma-separated values file (.csv), which includes values for the Image Name, Latitude, Longitude, and Altitude fields, and, optionally, the Omega, Phi, and Kappa fields. The geolocation file is provided by the supplier along with the drone imagery.GPSLatRef—Define whether the latitude is north or south latitude. N indicates north latitude, and S indicates south latitude.GPSLonRef—Define whether the longitude is east or west longitude. E indicates east longitude, and W indicates west longitude.GPSSRS—Define the spatial reference of the GPS.GPSVCS—Define the vertical coordinate system of the GPS.IsAltitudeFlightHeight—A Boolean value that defines whether the drone reports heights relative to the takeoff point or altitude for heights relative to a vertical datum. Use a value of 0 when the altitude value is altitude above a datum, and a value of 1 when the altitude value is flight height from a takeoff point.MinimumFlightHeight—Define the minimum flight height. If the estimated flight height of an image is smaller than this number, the image will be considered invalid and will not be added to the image collection for future processing.PanelReflectanceFactor—Define the surface reflectance factor for each band. This option is only available for RedEdge and Altum raster types when Multispectral Processing Template is selected. Type the values in the following format: <band name 1>: <double val>, <band name 2>: <double val>, <band name 3>: <double val>, ... , <band name n>: <double val>. For example, Blue: 0.48, Green: 0.481, Red: 0.488, RedEdge: 0.488.RadiometricQuantityType—An integer value that defines the type of radiometric measurement. This option is only available for RedEdge and Altum raster types when Multispectral Processing Template is selected. The value of RadiometricQuantityType includes the following values:0—DN (Digital Number), default1—Radiance2—Reflectance4—Temperature (thermal band only)ZFactor—The scaling factor used to convert the elevation values. This option is only available when using a DEM.ZOffset—The base value to be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level. This option is only available when using a DEM. | Value Table |
| enable_pixel_cache(Optional) | Specifies whether the pixel cache will be generated for faster display and processing of the mosaic dataset.NO_PIXEL_CACHE—The pixel cache will not be generated. This is the default.USE_PIXEL_CACHE—The pixel cache will be generated. | Boolean |
| cache_location(Optional) | The location of the pixel cache. If no location is provided, the cache will be written to C:\Users\<Username>\AppData\Local\ESRI\rasterproxies\. Once the location is provided, you do not need to redefine the path when adding new rasters to the mosaic dataset. You do need to set the enable_pixel_cache parameter to USE_PIXEL_CACHE when adding the new data. | Folder; String |

## Code Samples

### Example 1

```python
arcpy.management.AddRastersToMosaicDataset(in_mosaic_dataset, raster_type, input_path, {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {maximum_pyramid_levels}, {maximum_cell_size}, {minimum_dimension}, {spatial_reference}, {filter}, {sub_folder}, {duplicate_items_action}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {operation_description}, {force_spatial_reference}, {estimate_statistics}, {aux_inputs}, {enable_pixel_cache}, {cache_location})
```

### Example 2

```python
import arcpy
arcpy.management.AddRastersToMosaicDataset(
     "c:/data/AddMD.gdb/md_landsat", "Landsat 7 ETM+", 
     "c:/data/landsat7etm", "UPDATE_CELL_SIZES", "UPDATE_BOUNDARY",
     "UPDATE_OVERVIEWS", "2", "#", "#", "GCS_WGS_1984.prj",
     "*.tif", "SUBFOLDERS", "EXCLUDE_DUPLICATES",
     "NO_PYRAMIDS", "NO_STATISTICS", "BUILD_THUMBNAILS", 
     "Add Landsat L1G", "FORCE_SPATIAL_REFERENCE",
     "NO_STATISTICS", "", "USE_PIXEL_CACHE", 
r"C:\test\cachelocation")
```

### Example 3

```python
import arcpy
arcpy.management.AddRastersToMosaicDataset(
     "c:/data/AddMD.gdb/md_landsat", "Landsat 7 ETM+", 
     "c:/data/landsat7etm", "UPDATE_CELL_SIZES", "UPDATE_BOUNDARY",
     "UPDATE_OVERVIEWS", "2", "#", "#", "GCS_WGS_1984.prj",
     "*.tif", "SUBFOLDERS", "EXCLUDE_DUPLICATES",
     "NO_PYRAMIDS", "NO_STATISTICS", "BUILD_THUMBNAILS", 
     "Add Landsat L1G", "FORCE_SPATIAL_REFERENCE",
     "NO_STATISTICS", "", "USE_PIXEL_CACHE", 
r"C:\test\cachelocation")
```

### Example 4

```python
#Add Raster Dataset type Raster to FGDB Mosaic Dataset
#Calculate Cell Size Ranges and Build Boundary
#Build Overviews for Mosaic Dataset upon the 3rd level Raster Dataset pyramid
#Apply TIFF file filter
#Build Pyramids for the source datasets

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
mdname = "AddMD.gdb/md_rasds"
rastype = "Raster Dataset"
inpath = "c:/data/rasds"
updatecs = "UPDATE_CELL_SIZES"
updatebnd = "UPDATE_BOUNDARY"
updateovr = "UPDATE_OVERVIEWS"
maxlevel = "2"
maxcs = "#"
maxdim = "#"
spatialref = "#"
inputdatafilter = "*.tif"
subfolder = "NO_SUBFOLDERS"
duplicate = "EXCLUDE_DUPLICATES"
buildpy = "BUILD_PYRAMIDS"
calcstats = "CALCULATE_STATISTICS"
buildthumb = "NO_THUMBNAILS"
comments = "Add Raster Datasets"
forcesr = "#"
estimatestats = "ESTIMATE_STATISTICS"
auxilaryinput = ""
enablepixcache = "USE_PIXEL_CACHE"
cachelocation = "c:\\test\\cachelocation"

arcpy.management.AddRastersToMosaicDataset(
     mdname,  rastype, inpath, updatecs, updatebnd, updateovr,
     maxlevel, maxcs, maxdim, spatialref, inputdatafilter,
     subfolder, duplicate, buildpy, calcstats, 
     buildthumb, comments, forcesr, estimatestats,
     auxilaryinput, enablepixcache, cachelocation)
```

### Example 5

```python
#Add Raster Dataset type Raster to FGDB Mosaic Dataset
#Calculate Cell Size Ranges and Build Boundary
#Build Overviews for Mosaic Dataset upon the 3rd level Raster Dataset pyramid
#Apply TIFF file filter
#Build Pyramids for the source datasets

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
mdname = "AddMD.gdb/md_rasds"
rastype = "Raster Dataset"
inpath = "c:/data/rasds"
updatecs = "UPDATE_CELL_SIZES"
updatebnd = "UPDATE_BOUNDARY"
updateovr = "UPDATE_OVERVIEWS"
maxlevel = "2"
maxcs = "#"
maxdim = "#"
spatialref = "#"
inputdatafilter = "*.tif"
subfolder = "NO_SUBFOLDERS"
duplicate = "EXCLUDE_DUPLICATES"
buildpy = "BUILD_PYRAMIDS"
calcstats = "CALCULATE_STATISTICS"
buildthumb = "NO_THUMBNAILS"
comments = "Add Raster Datasets"
forcesr = "#"
estimatestats = "ESTIMATE_STATISTICS"
auxilaryinput = ""
enablepixcache = "USE_PIXEL_CACHE"
cachelocation = "c:\\test\\cachelocation"

arcpy.management.AddRastersToMosaicDataset(
     mdname,  rastype, inpath, updatecs, updatebnd, updateovr,
     maxlevel, maxcs, maxdim, spatialref, inputdatafilter,
     subfolder, duplicate, buildpy, calcstats, 
     buildthumb, comments, forcesr, estimatestats,
     auxilaryinput, enablepixcache, cachelocation)
```

---

## Add Relate (Data Management)

## Summary

Relates a layer to another layer or table based on a field value. Feature layers, table views, subtype value layers or tables, and raster layers with a raster attribute table are supported.

## Usage

- If the input is a feature class or dataset path, this tool will create and return a new layer with the result of the tool applied.
- A relate does not modify data; a relate is a property of the layer or table view.
- Records from the relate table can be matched to more than one record in the input layer or table view.
- The relate table can be a geodatabase table or a dBASE file. The relate table does not have to be a layer.
- The input must have an Object ID field. The relate table is not required to contain an Object ID field.
- To save the relate for use in another project, save the layer to a layer file using the Save Layer To File tool. This only applies to layers; table views cannot be saved in this manner.
- To make a permanent relate, use the Create Relationship Class tool.
- Indexing the fields in the input layer or table view and the relate table on which the relate will be based can improve performance. This can be done with the Add Attribute Index tool.
- When the input layer has a relationship and the Automatically select related data property is checked, a selection on the input layer will also select records in the related table.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Layer Name or Table View | The layer or table view to which the relate table will be related. | Mosaic Layer; Raster Layer; Table View |
| Input Relate Field | The primary key field in the input layer or table view on which the relate will be based. | Field |
| Relate Table | The table or table view to be related to the input layer or table view. | Mosaic Layer; Raster Layer; Table View |
| Output Relate Field | The foreign key field in the relate table that will be used to match the primary key. | Field |
| Relate Name | The unique name of a relate. | String |
| Cardinality (Optional) | Specifies the cardinality of the relationship. One to one—The relationship between the input table and related table will be one to one. For example, one record in the input table will have only one matching record in the related table.One to many—The relationship between the input table and related table will be one to many. For example, one record in the input table can have multiple matching records in the related table. This is the default.Many to many—The relationship between the input table and related table will be many to many. For example, many records with the same value in the input table can have multiple matching records in the related table. | String |
| in_layer_or_view | The layer or table view to which the relate table will be related. | Mosaic Layer; Raster Layer; Table View |
| in_field | The primary key field in the input layer or table view on which the relate will be based. | Field |
| relate_table | The table or table view to be related to the input layer or table view. | Mosaic Layer; Raster Layer; Table View |
| relate_field | The foreign key field in the relate table that will be used to match the primary key. | Field |
| relate_name | The unique name of a relate. | String |
| cardinality(Optional) | Specifies the cardinality of the relationship. ONE_TO_ONE—The relationship between the input table and related table will be one to one. For example, one record in the input table will have only one matching record in the related table.ONE_TO_MANY—The relationship between the input table and related table will be one to many. For example, one record in the input table can have multiple matching records in the related table. This is the default.MANY_TO_MANY—The relationship between the input table and related table will be many to many. For example, many records with the same value in the input table can have multiple matching records in the related table. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddRelate(in_layer_or_view, in_field, relate_table, relate_field, relate_name, {cardinality})
```

### Example 2

```python
import arcpy
arcpy.management.AddRelate("Parcel", "ParcelID", "owner_table", "ParcelID", 
                           "Owner2Parcel")
```

### Example 3

```python
import arcpy
arcpy.management.AddRelate("Parcel", "ParcelID", "owner_table", "ParcelID", 
                           "Owner2Parcel")
```

---

## Add Rule To Relationship Class (Data Management)

## Summary

Adds a rule to a relationship class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Relationship Class | The relationship class to which a rule will be added. | Relationship Class |
| Origin Subtype (Optional) | Specifies the subtype of the origin class. If the origin class has subtypes, choose the subtype to which you want to associate a relationship class rule. If the origin class has no subtypes, the relationship rule will apply to all features. | String |
| Origin Minimum (Optional) | Specifies the minimum range cardinality for the origin class if the relationship class is many-to-many. | Long |
| Origin Maximum (Optional) | Specifies the maximum range cardinality for the origin class if the relationship class is many-to-many or one-to-many. | Long |
| Destination Subtype (Optional) | Specifies the subtype of the destination class. If the destination class has subtypes, choose the subtype to which you want to associate a relationship class rule. If the destination class has no subtypes, the relationship rule will apply to all features. | String |
| Destination Minimum (Optional) | Specifies the minimum range cardinality for the destination class if the relationship class is many-to-many or one-to-many. | Long |
| Destination Maximum (Optional) | Specifies the maximum range cardinality for the destination class if the relationship class is many-to-many or one-to-many. | Long |
| in_rel_class | The relationship class to which a rule will be added. | Relationship Class |
| origin_subtype(Optional) | Specifies the subtype of the origin class. If the origin class has subtypes, choose the subtype to which you want to associate a relationship class rule. If the origin class has no subtypes, the relationship rule will apply to all features. | String |
| origin_minimum(Optional) | Specifies the minimum range cardinality for the origin class if the relationship class is many-to-many. | Long |
| origin_maximum(Optional) | Specifies the maximum range cardinality for the origin class if the relationship class is many-to-many or one-to-many. | Long |
| destination_subtype(Optional) | Specifies the subtype of the destination class. If the destination class has subtypes, choose the subtype to which you want to associate a relationship class rule. If the destination class has no subtypes, the relationship rule will apply to all features. | String |
| destination_minimum(Optional) | Specifies the minimum range cardinality for the destination class if the relationship class is many-to-many or one-to-many. | Long |
| destination_maximum(Optional) | Specifies the maximum range cardinality for the destination class if the relationship class is many-to-many or one-to-many. | Long |

## Code Samples

### Example 1

```python
arcpy.management.AddRuleToRelationshipClass(in_rel_class, {origin_subtype}, {origin_minimum}, {origin_maximum}, {destination_subtype}, {destination_minimum}, {destination_maximum})
```

### Example 2

```python
import arcpy

arcpy.management.AddRuleToRelationshipClass(
    "C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", "Residential", 
    0, 1, "House", 1, 3)
```

### Example 3

```python
import arcpy

arcpy.management.AddRuleToRelationshipClass(
    "C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", "Residential", 
    0, 1, "House", 1, 3)
```

---

## Add Rule To Topology (Data Management)

## Summary

Adds a rule to a topology.

## Usage

- You can enter the name of the subtype value to which you want a topology rule to be applied.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology to which the new rule will be added. | Topology Layer |
| Rule Type | Specifies the topology rule that will be added.Must Not Have Gaps (Area)—There must be no voids within a single polygon or between adjacent polygons. All polygons must form a continuous surface. An error will always exist on the perimeter of the surface. You can either ignore this error or mark it as an exception. Use this rule for data that must completely cover an area. For example, soil polygons cannot include gaps or form voids; they must cover an entire area.Must Not Overlap (Area)— The interior of polygons must not overlap. The polygons can share edges or vertices. This rule is used when an area cannot belong to two or more polygons. It is useful for modeling administrative boundaries, such as ZIP Codes or voting districts, and mutually exclusive area classifications, such as land cover or landform type.Must Be Covered By Feature Class Of (Area-Area)—A polygon in one feature class (or subtype) must share all of its area with polygons in another feature class (or subtype). An area in the first feature class that is not covered by polygons from the other feature class is an error. This rule is used when an area of one type, such as a state, should be completely covered by areas of another type, such as counties.Must Cover Each Other (Area-Area)—The polygons of one feature class (or subtype) must share all of their area with the polygons of another feature class (or subtype). Polygons may share edges or vertices. Any area defined in either feature class that is not shared with the other is an error. This rule is used when two systems of classification are used for the same geographic area, and any given point defined in one system must also be defined in the other. One such case occurs with nested hierarchical datasets, such as census blocks and block groups or small watersheds and large drainage basins. The rule can also be applied to nonhierarchically related polygon feature classes, such as soil type and slope class.Must Be Covered By (Area-Area)—The polygons of one feature class (or subtype) must be contained within polygons of another feature class (or subtype). Polygons may share edges or vertices. Any area defined in the contained feature class must be covered by an area in the covering feature class. This rule is used when area features of a given type must be located within features of another type. This rule is useful when modeling areas that are subsets of a larger surrounding area, such as management units within forests or blocks within block groups.Must Not Overlap With (Area-Area)— The interior of polygons in one feature class (or subtype) must not overlap with the interior of polygons in another feature class (or subtype). Polygons of the two feature classes can share edges or vertices or be completely disjointed. This rule is used when an area cannot belong to two separate feature classes. It is useful for combining two mutually exclusive systems of area classification, such as zoning and water body type, in which areas defined within the zoning class cannot also be defined in the water body class and vice versa.Must Be Covered By Boundary Of (Line-Area)—Lines must be covered by the boundaries of area features. This is useful for modeling lines, such as lot lines, that must coincide with the edge of polygon features, such as lots.Must Be Covered By Boundary Of (Point-Area)—Points must fall on the boundaries of area features. This is useful when the point features help support the boundary system, such as boundary markers, which must be found on the edges of certain areas.Must Be Properly Inside (Point-Area)—Points must fall within area features. This is useful when the point features are related to polygons, such as wells and well pads or address points and parcels.Must Not Overlap (Line)—Lines must not overlap with lines in the same feature class (or subtype). This rule is used when line segments should not be duplicated, for example, in a stream feature class. Lines can cross or intersect but cannot share segments.Must Not Intersect (Line)—Line features from the same feature class (or subtype) must not cross or overlap each other. Lines can share endpoints. This rule is used for contour lines that should never cross each other or when the intersection of lines should only occur at endpoints, such as street segments and intersections.Must Not Have Dangles (Line)—A line feature must touch lines from the same feature class (or subtype) at both endpoints. An endpoint that is not connected to another line is called a dangle. This rule is used when line features must form closed loops, such as when they are defining the boundaries of polygon features. It can also be used when lines typically connect to other lines, as with streets. In this case, exceptions can be used when the rule is occasionally violated, as with cul-de-sac or dead-end street segments.Must Not Have Pseudo-Nodes (Line)—A line must connect to at least two other lines at each endpoint. Lines that connect to one other line (or to themselves) are considered to have pseudo nodes. This rule is used when line features must form closed loops, such as when they define the boundaries of polygons or when line features logically must connect to two other line features at each end, as with segments in a stream network, with exceptions being marked for the originating ends of first-order streams.Must Be Covered By Feature Class Of (Line-Line)— Lines from one feature class (or subtype) must be covered by the lines in another feature class (or subtype). This is useful for modeling logically different but spatially coincident lines, such as routes and streets. For example, a bus route feature class must not depart from the streets defined in the street feature class.Must Not Overlap With (Line-Line)—A line from one feature class (or subtype) must not overlap with line features in another feature class (or subtype). This rule is used when line features cannot share the same space. For example, roads must not overlap with railroads, or depression subtypes of contour lines cannot overlap with other contour lines.Must Be Covered By (Point-Line)— Points in one feature class must be covered by lines in another feature class. It does not constrain the covering portion of the line to be an endpoint. This rule is useful for points that fall along a set of lines, such as highway signs along highways.Must Be Covered By Endpoint Of (Point-Line)—Points in one feature class must be covered by the endpoints of lines in another feature class. This rule is similar to the Endpoint Must Be Covered By line rule except that, in cases where the rule is violated, it is the point feature that is marked as an error rather than the line. Boundary corner markers may be constrained to be covered by the endpoints of boundary lines.Boundary Must Be Covered By (Area-Line)— Boundaries of polygon features must be covered by lines in another feature class. This rule is used when area features must have line features that mark the boundaries of the areas. This is usually when the areas have one set of attributes and their boundaries have other attributes. For example, parcels are stored in the geodatabase along with their boundaries. Each parcel is defined by one or more line features that store information about their length or the date surveyed, and every parcel must exactly match its boundaries.Boundary Must Be Covered By Boundary Of (Area-Area)— Boundaries of polygon features in one feature class (or subtype) must be covered by boundaries of polygon features in another feature class (or subtype). This is useful when polygon features in one feature class, such as subdivisions, are composed of multiple polygons in another class, such as parcels, and the shared boundaries must be aligned.Must Not Self-Overlap (Line)—Line features must not overlap themselves. They can cross or touch but must not have coincident segments. This rule is useful for features, such as streets, in which segments might touch in a loop but the same street must not follow the same course twice.Must Not Self-Intersect (Line)—Line features must not cross or overlap themselves. This rule is useful for lines, such as contour lines, that cannot cross themselves.Must Not Intersect Or Touch Interior (Line)—A line in one feature class (or subtype) must only touch other lines of the same feature class (or subtype) at endpoints. Any line segment in which features overlap or any intersection that is not at an endpoint is an error. This rule is useful when lines must only be connected at endpoints, such as lot lines, which must split (only connect to the endpoints of) back lot lines and cannot overlap each other.Endpoint Must Be Covered By (Line-Point)—The endpoints of line features must be covered by point features in another feature class. This is useful for modeling cases in which a fitting must connect two pipes, or a street intersection must be found at the junction of two streets.Contains Point (Area-Point)—A polygon in one feature class must contain at least one point from another feature class. Points must be within the polygon, not on the boundary. This is useful when every polygon must have at least one associated point, such as when parcels must have an address point.Must Be Single Part (Line)—Lines must have only one part. This rule is useful when line features, such as highways, may not have multiple parts.Must Coincide With (Point-Point)—Points in one feature class (or subtype) must be coincident with points in another feature class (or subtype). This is useful when points must be covered by other points, such as transformers that must coincide with power poles in electric distribution networks and observation points that must coincide with stations.Must Be Disjoint (Point)—Points must be separated spatially from other points in the same feature class (or subtype). Any points that overlap are errors. This is useful for ensuring that points are not coincident or duplicated in the same feature class, such as in layers of cities, parcel lot ID points, wells, or street lamp poles.Must Not Intersect With (Line-Line)—Line features from one feature class (or subtype) must not cross or overlap lines from another feature class (or subtype). Lines can share endpoints. This rule is used when lines from two layers should never cross each other or when the intersection of lines should only occur at endpoints, such as streets and railroads.Must Not Intersect or Touch Interior With (Line-Line)—A line in one feature class (or subtype) must only touch other lines of another feature class (or subtype) at endpoints. Any line segment in which features overlap or any intersection that is not at an endpoint is an error. This rule is useful when lines from two layers must only be connected at endpoints.Must Be Inside (Line-Area)—A line must be contained within the boundary of an area feature. This is useful when lines may partially or totally coincide with area boundaries but cannot extend beyond polygons, such as state highways that must be inside state borders and rivers that must be within watersheds.Contains One Point (Area-Point)— Each polygon must contain one point feature and each point feature must fall within a single polygon. This is used when there must be a one-to-one correspondence between features of a polygon feature class and features of a point feature class, such as administrative boundaries and their capital cities. Each point must be properly inside exactly one polygon and each polygon must properly contain exactly one point. Points must be within the polygon, not on the boundary. | String |
| Input Feature Class | The input or origin feature class. | Feature Layer |
| Input Subtype(Optional) | The subtype for the input or origin feature class. Provide the subtype name (not the code). If subtypes do not exist on the input feature class, or you want the rule to be applied to all subtypes in the feature class, leave this parameter blank. | String |
| Destination Feature Class(Optional) | The destination feature class for the topology rule. | Feature Layer |
| Destination Subtype(Optional) | The subtype for the destination feature class. Provide the subtype name (not the code). If subtypes do not exist on the origin feature class, or you want the rule to be applied to all subtypes in the feature class, leave this parameter blank. | String |
| in_topology | The topology to which the new rule will be added. | Topology Layer |
| rule_type | Specifies the topology rule that will be added.Must Not Have Gaps (Area)—There must be no voids within a single polygon or between adjacent polygons. All polygons must form a continuous surface. An error will always exist on the perimeter of the surface. You can either ignore this error or mark it as an exception. Use this rule for data that must completely cover an area. For example, soil polygons cannot include gaps or form voids; they must cover an entire area.Must Not Overlap (Area)— The interior of polygons must not overlap. The polygons can share edges or vertices. This rule is used when an area cannot belong to two or more polygons. It is useful for modeling administrative boundaries, such as ZIP Codes or voting districts, and mutually exclusive area classifications, such as land cover or landform type.Must Be Covered By Feature Class Of (Area-Area)—A polygon in one feature class (or subtype) must share all of its area with polygons in another feature class (or subtype). An area in the first feature class that is not covered by polygons from the other feature class is an error. This rule is used when an area of one type, such as a state, should be completely covered by areas of another type, such as counties.Must Cover Each Other (Area-Area)—The polygons of one feature class (or subtype) must share all of their area with the polygons of another feature class (or subtype). Polygons may share edges or vertices. Any area defined in either feature class that is not shared with the other is an error. This rule is used when two systems of classification are used for the same geographic area, and any given point defined in one system must also be defined in the other. One such case occurs with nested hierarchical datasets, such as census blocks and block groups or small watersheds and large drainage basins. The rule can also be applied to nonhierarchically related polygon feature classes, such as soil type and slope class.Must Be Covered By (Area-Area)—The polygons of one feature class (or subtype) must be contained within polygons of another feature class (or subtype). Polygons may share edges or vertices. Any area defined in the contained feature class must be covered by an area in the covering feature class. This rule is used when area features of a given type must be located within features of another type. This rule is useful when modeling areas that are subsets of a larger surrounding area, such as management units within forests or blocks within block groups.Must Not Overlap With (Area-Area)— The interior of polygons in one feature class (or subtype) must not overlap with the interior of polygons in another feature class (or subtype). Polygons of the two feature classes can share edges or vertices or be completely disjointed. This rule is used when an area cannot belong to two separate feature classes. It is useful for combining two mutually exclusive systems of area classification, such as zoning and water body type, in which areas defined within the zoning class cannot also be defined in the water body class and vice versa.Must Be Covered By Boundary Of (Line-Area)—Lines must be covered by the boundaries of area features. This is useful for modeling lines, such as lot lines, that must coincide with the edge of polygon features, such as lots.Must Be Covered By Boundary Of (Point-Area)—Points must fall on the boundaries of area features. This is useful when the point features help support the boundary system, such as boundary markers, which must be found on the edges of certain areas.Must Be Properly Inside (Point-Area)—Points must fall within area features. This is useful when the point features are related to polygons, such as wells and well pads or address points and parcels.Must Not Overlap (Line)—Lines must not overlap with lines in the same feature class (or subtype). This rule is used when line segments should not be duplicated, for example, in a stream feature class. Lines can cross or intersect but cannot share segments.Must Not Intersect (Line)—Line features from the same feature class (or subtype) must not cross or overlap each other. Lines can share endpoints. This rule is used for contour lines that should never cross each other or when the intersection of lines should only occur at endpoints, such as street segments and intersections.Must Not Have Dangles (Line)—A line feature must touch lines from the same feature class (or subtype) at both endpoints. An endpoint that is not connected to another line is called a dangle. This rule is used when line features must form closed loops, such as when they are defining the boundaries of polygon features. It can also be used when lines typically connect to other lines, as with streets. In this case, exceptions can be used when the rule is occasionally violated, as with cul-de-sac or dead-end street segments.Must Not Have Pseudo-Nodes (Line)—A line must connect to at least two other lines at each endpoint. Lines that connect to one other line (or to themselves) are considered to have pseudo nodes. This rule is used when line features must form closed loops, such as when they define the boundaries of polygons or when line features logically must connect to two other line features at each end, as with segments in a stream network, with exceptions being marked for the originating ends of first-order streams.Must Be Covered By Feature Class Of (Line-Line)— Lines from one feature class (or subtype) must be covered by the lines in another feature class (or subtype). This is useful for modeling logically different but spatially coincident lines, such as routes and streets. For example, a bus route feature class must not depart from the streets defined in the street feature class.Must Not Overlap With (Line-Line)—A line from one feature class (or subtype) must not overlap with line features in another feature class (or subtype). This rule is used when line features cannot share the same space. For example, roads must not overlap with railroads, or depression subtypes of contour lines cannot overlap with other contour lines.Must Be Covered By (Point-Line)— Points in one feature class must be covered by lines in another feature class. It does not constrain the covering portion of the line to be an endpoint. This rule is useful for points that fall along a set of lines, such as highway signs along highways.Must Be Covered By Endpoint Of (Point-Line)—Points in one feature class must be covered by the endpoints of lines in another feature class. This rule is similar to the Endpoint Must Be Covered By line rule except that, in cases where the rule is violated, it is the point feature that is marked as an error rather than the line. Boundary corner markers may be constrained to be covered by the endpoints of boundary lines.Boundary Must Be Covered By (Area-Line)— Boundaries of polygon features must be covered by lines in another feature class. This rule is used when area features must have line features that mark the boundaries of the areas. This is usually when the areas have one set of attributes and their boundaries have other attributes. For example, parcels are stored in the geodatabase along with their boundaries. Each parcel is defined by one or more line features that store information about their length or the date surveyed, and every parcel must exactly match its boundaries.Boundary Must Be Covered By Boundary Of (Area-Area)— Boundaries of polygon features in one feature class (or subtype) must be covered by boundaries of polygon features in another feature class (or subtype). This is useful when polygon features in one feature class, such as subdivisions, are composed of multiple polygons in another class, such as parcels, and the shared boundaries must be aligned.Must Not Self-Overlap (Line)—Line features must not overlap themselves. They can cross or touch but must not have coincident segments. This rule is useful for features, such as streets, in which segments might touch in a loop but the same street must not follow the same course twice.Must Not Self-Intersect (Line)—Line features must not cross or overlap themselves. This rule is useful for lines, such as contour lines, that cannot cross themselves.Must Not Intersect Or Touch Interior (Line)—A line in one feature class (or subtype) must only touch other lines of the same feature class (or subtype) at endpoints. Any line segment in which features overlap or any intersection that is not at an endpoint is an error. This rule is useful when lines must only be connected at endpoints, such as lot lines, which must split (only connect to the endpoints of) back lot lines and cannot overlap each other.Endpoint Must Be Covered By (Line-Point)—The endpoints of line features must be covered by point features in another feature class. This is useful for modeling cases in which a fitting must connect two pipes, or a street intersection must be found at the junction of two streets.Contains Point (Area-Point)—A polygon in one feature class must contain at least one point from another feature class. Points must be within the polygon, not on the boundary. This is useful when every polygon must have at least one associated point, such as when parcels must have an address point.Must Be Single Part (Line)—Lines must have only one part. This rule is useful when line features, such as highways, may not have multiple parts.Must Coincide With (Point-Point)—Points in one feature class (or subtype) must be coincident with points in another feature class (or subtype). This is useful when points must be covered by other points, such as transformers that must coincide with power poles in electric distribution networks and observation points that must coincide with stations.Must Be Disjoint (Point)—Points must be separated spatially from other points in the same feature class (or subtype). Any points that overlap are errors. This is useful for ensuring that points are not coincident or duplicated in the same feature class, such as in layers of cities, parcel lot ID points, wells, or street lamp poles.Must Not Intersect With (Line-Line)—Line features from one feature class (or subtype) must not cross or overlap lines from another feature class (or subtype). Lines can share endpoints. This rule is used when lines from two layers should never cross each other or when the intersection of lines should only occur at endpoints, such as streets and railroads.Must Not Intersect or Touch Interior With (Line-Line)—A line in one feature class (or subtype) must only touch other lines of another feature class (or subtype) at endpoints. Any line segment in which features overlap or any intersection that is not at an endpoint is an error. This rule is useful when lines from two layers must only be connected at endpoints.Must Be Inside (Line-Area)—A line must be contained within the boundary of an area feature. This is useful when lines may partially or totally coincide with area boundaries but cannot extend beyond polygons, such as state highways that must be inside state borders and rivers that must be within watersheds.Contains One Point (Area-Point)— Each polygon must contain one point feature and each point feature must fall within a single polygon. This is used when there must be a one-to-one correspondence between features of a polygon feature class and features of a point feature class, such as administrative boundaries and their capital cities. Each point must be properly inside exactly one polygon and each polygon must properly contain exactly one point. Points must be within the polygon, not on the boundary. | String |
| in_featureclass | The input or origin feature class. | Feature Layer |
| subtype(Optional) | The subtype for the input or origin feature class. Provide the subtype name (not the code). If subtypes do not exist on the input feature class, or you want the rule to be applied to all subtypes in the feature class, leave this parameter blank. | String |
| in_featureclass2(Optional) | The destination feature class for the topology rule. | Feature Layer |
| subtype2(Optional) | The subtype for the destination feature class. Provide the subtype name (not the code). If subtypes do not exist on the origin feature class, or you want the rule to be applied to all subtypes in the feature class, leave this parameter blank. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddRuleToTopology(in_topology, rule_type, in_featureclass, {subtype}, {in_featureclass2}, {subtype2})
```

### Example 2

```python
# Description: Adds a rule to a topology

# Import system modules
import arcpy

# Any intersection of ParcelOutline (BlockLines subtype only) needs to be reviewed
arcpy.management.AddRuleToTopology("C:/data/Landbase.gdb/LegalFabric/topology", 
                                   "Must Not Intersect (Line)",
                                   "C:/data/Landbase.gdb/LegalFabric/ParcelOutline",
                                   "BlockLines")
```

### Example 3

```python
# Description: Adds a rule to a topology

# Import system modules
import arcpy

# Any intersection of ParcelOutline (BlockLines subtype only) needs to be reviewed
arcpy.management.AddRuleToTopology("C:/data/Landbase.gdb/LegalFabric/topology", 
                                   "Must Not Intersect (Line)",
                                   "C:/data/Landbase.gdb/LegalFabric/ParcelOutline",
                                   "BlockLines")
```

---

## Add Spatial Index (Data Management)

## Summary

Adds a spatial index to a shapefile, file geodatabase, mobile geodatabase, or enterprise geodatabase feature class. Use this tool to either add a spatial index to a shapefile or feature class that does not already have one or to re-create an existing spatial index.

## Usage

- ArcGIS uses spatial indexes to quickly locate features in feature classes. Identifying a feature, selecting features by pointing or dragging a box, and panning and zooming all require a spatial index to locate features. The spatial index is defined by using a grid-based system that spans the extent of the features in a feature class, like a locator grid you might find on a common road map.
- By default, ArcGIS creates and maintains a spatial index for geodatabase feature classes. For a geodatabase feature class to not have a spatial index, you must explicitly remove it using the Remove Spatial Index tool.
- Adding a new spatial index to an enterprise geodatabase feature class is a server-intensive operation. It should not be done on large feature classes when a large number of users are logged in to the server.
- For SQL Server and Oracle feature classes, if the index already exists, the index will be dropped and recreated. The bounding box will also be recalculated for SQL Server feature classes.
- SAP HANA feature classes do not support spatial indexes.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | An enterprise geodatabase feature class, file geodatabase feature class, mobile geodatabase feature class, or shapefile to which a spatial index is to be added or rebuilt. | Feature Layer; Mosaic Layer |
| Spatial Grid 1(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |
| Spatial Grid 2(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |
| Spatial Grid 3(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |
| in_features | An enterprise geodatabase feature class, file geodatabase feature class, mobile geodatabase feature class, or shapefile to which a spatial index is to be added or rebuilt. | Feature Layer; Mosaic Layer |
| spatial_grid_1(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |
| spatial_grid_2(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |
| spatial_grid_3(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter will be ignored. | Double |

## Code Samples

### Example 1

```python
arcpy.management.AddSpatialIndex(in_features, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"
arcpy.AddSpatialIndex_management("LPI.Land/LPI.PLSSFirstDivision")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"
arcpy.AddSpatialIndex_management("LPI.Land/LPI.PLSSFirstDivision")
```

### Example 4

```python
# Name: AddSpatialIndex_Example2.py
# Description: Add a spatial index to a enterprise geodatabase feature class.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"

# Execute AddSpatialIndex
arcpy.AddSpatialIndex_management(in_features)
```

### Example 5

```python
# Name: AddSpatialIndex_Example2.py
# Description: Add a spatial index to a enterprise geodatabase feature class.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"

# Execute AddSpatialIndex
arcpy.AddSpatialIndex_management(in_features)
```

---

## Add Spatial Join (Data Management)

## Summary

Joins attributes from one feature to another based on the spatial relationship. The target features and the joined attributes from the join features will be joined.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Features | The attributes from the target features and the attributes from the join features will be joined to the target features layer. However, a subset of attributes can be defined using the Field Map parameter. | Feature Layer |
| Join Features | The attributes from the join features will be joined to the attributes of the target features. See the explanation of the Join Operation parameter for details on how the aggregation of joined attributes are affected by the type of join operation. | Feature Layer |
| Join Operation(Optional) | This parameter is hidden and is not supported. All joins will be performed as a one-to-one join. To achieve a one-to-many join when the output is created in an output feature class, use the Spatial Join tool. | String |
| Keep All Target Features(Optional) | Specifies whether only target features with a spatial relationship with a join feature (known as an inner join) will be preserved or all target features will be preserved, even without a spatial relationship with the join features (known as an outer join).Checked—All features in the target features layer will be preserved. This is the default. Unchecked—Only those features in the target features layer with a spatial relationship with a join feature will be preserved. | Boolean |
| Field Map(Optional) | The fields that will be temporarily joined to the target dataset with their respective properties and source fields. All fields from the join dataset will be included by default. Use the field map to add, delete, rename, and reorder fields, as well as change other field properties.The field map can be used to combine values from two or more input fields into a single output field. | Field Mappings |
| Match Option(Optional) | Specifies the criteria that will be used to match rows.Intersect—The features in the join features will be matched if they intersect a target feature. This is the default. Specify the distance in the Search Radius parameter.Intersect 3D— The features in the join features will be matched if they intersect a target feature in three-dimensional space (x, y, and z). Specify the distance in the Search Radius parameter.Within a distance—The features in the join features will be matched if they are within a specified distance of a target feature. Specify the distance in the Search Radius parameter.Within a distance geodesic—This is the same as Within a distance except that geodesic distance is used rather than planar distance. Distance between features will be calculated using a geodesic formula that takes into account the curvature of the spheroid and correctly handles data near and across the dateline and poles. Use this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations.Within a distance 3D—The features in the join features will be matched if they are within a specified distance of a target feature in three-dimensional space. Specify the distance in the Search Radius parameter.Contains—The features in the join features will be matched if a target feature contains them. The target features must be polygons or polylines. For this option, the target features cannot be points, and the join features can only be polygons when the target features are also polygons.Completely contains—The features in the join features will be matched if a target feature completely contains them. A polygon can completely contain any feature. A point cannot completely contain any feature, not even a point. A polyline can completely contain only polyline and point features.Contains Clementini—This spatial relationship produces the same results as Completely contains except that if the join feature is entirely on the boundary of the target feature (no part is properly inside or outside) the feature will not be matched. Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.Within—The features in the join features will be matched if a target feature is within them. This is the opposite of Contains. For this option, the target features can only be polygons when the join features are also polygons. A point can be a join feature only if a point is the target. Completely within—The features in the join features will be matched if a target feature is completely within them. This is the opposite of Completely contains. Within Clementini—The result will be identical to Within except if the entirety of the feature in the join features is on the boundary of the target feature, the feature will not be matched. Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.Are identical to—The features in the join features will be matched if they are identical to a target feature. Both join and target feature must be of the same shape type—point to point, line to line, or polygon to polygon. Boundary touches—The features in the join features will be matched if they have a boundary that touches a target feature. When the target and join features are lines or polygons, the boundary of the join feature can only touch the boundary of the target feature and no part of the join feature can cross the boundary of the target feature.Share a line segment with—The features in the join features will be matched if they share a line segment with a target feature. The join and target features must be lines or polygons.Crossed by the outline of—The features in the join features will be matched if a target feature is crossed by their outline. The join and target features must be lines or polygons. If polygons are used for the join or target features, the polygon's boundary (line) will be used. Lines that cross at a point will be matched; lines that share a line segment will not be matched.Have their center in—The features in the join features will be matched if a target feature's center falls within them. The center of the feature is calculated as follows: For polygon and multipoint, the geometry's centroid is used, and for line input, the geometry's midpoint is used. Specify the distance in the Search Radius parameter.Closest—The feature in the join features that is closest to a target feature will be matched. See the usage tip for more information. Specify the distance in the Search Radius parameter.Closest geodesic—This is the same as Closest except that geodesic distance is used rather than planar distance. Use this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations Largest overlap—The feature in the join features will be matched with the target feature with the largest overlap. | String |
| Search Radius(Optional) | Join features within this distance of a target feature will be considered for the spatial join. A search radius is only valid when the spatial relationship is specified (the Match Option parameter is set to Intersect, Within a distance, Within a distance geodesic, Have their center in, Closest, or Closest geodesic). For example, using a search radius of 100 meters with the Within a distance spatial relationship will join feature within 100 meters of a target feature. For the three within-a-distance relationships, if no value is specified for Search Radius, a distance of 0 is used. | Linear Unit |
| Distance Field Name(Optional) | The name of the field that contains the distance between the target feature and the closest join feature. This field will be added to the join. This parameter is only valid when the spatial relationship is specified (Match Option is set to Closest or Closest geodesic). The value of this field is -1 if no feature is matched within a search radius. If no field name is provided, the field will not be added to the join. | String |
| Permanently Join Fields(Optional) | Specifies whether fields from the join feature class will be temporarily added to the layer or permanently added to the target feature class.Unchecked—The fields from the join feature class will be temporarily added to the layer by an inner join. This is the default. Checked—The fields from the join feature class will be permanently added to the target feature class. | Boolean |
| Match Fields (Optional) | Pairs of fields from the join features and target features that will be used for attribute matching. Only the records from the join features that share match field values with the target features will participate in the spatial join. | Value Table |
| target_features | The attributes from the target features and the attributes from the join features will be joined to the target features layer. However, a subset of attributes can be defined using the field_mapping parameter. | Feature Layer |
| join_features | The attributes from the join features will be joined to the attributes of the target features. See the explanation of the join_operation parameter for details on how the aggregation of joined attributes are affected by the type of join operation. | Feature Layer |
| join_operation(Optional) | This parameter is not supported. All joins will be performed as a one-to-one join. If you are using positional arguments in Python, use a None type, an empty string ("" or ''), or the JOIN_ONE_TO_ONE keyword. To achieve a one-to-many join when the output is created in an output feature class, use the Spatial Join tool. | String |
| join_type(Optional) | Specifies whether only target features with a spatial relationship with a join feature (known as an inner join) will be preserved or all target features will be preserved, even without a spatial relationship with the join features (known as an outer join).KEEP_ALL—All features in the target features layer will be preserved. This is known as an outer join. This is the default.KEEP_COMMON—Only those features in the target features layer with a spatial relationship with a join feature will be preserved. This is known as an inner join. | Boolean |
| field_mapping(Optional) | The fields that will be temporarily joined to the target dataset with their respective properties and source fields. All fields from the join dataset will be included by default. Use the field map to add, delete, rename, and reorder fields, as well as change other field properties.The field map can be used to combine values from two or more input fields into a single output field.In Python, use the FieldMappings class to define this parameter. | Field Mappings |
| match_option(Optional) | Specifies the criteria that will be used to match rows.INTERSECT—The features in the join features will be matched if they intersect a target feature. This is the default. Specify the distance in the search_radius parameter.INTERSECT_3D— The features in the join features will be matched if they intersect a target feature in three-dimensional space (x, y, and z). Specify the distance in the search_radius parameter.WITHIN_A_DISTANCE—The features in the join features will be matched if they are within a specified distance of a target feature. Specify the distance in the search_radius parameter.WITHIN_A_DISTANCE_GEODESIC—This is the same as WITHIN_A_DISTANCE except that geodesic distance is used rather than planar distance. Distance between features will be calculated using a geodesic formula that takes into account the curvature of the spheroid and correctly handles data near and across the dateline and poles. Use this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations.WITHIN_A_DISTANCE_3D—The features in the join features will be matched if they are within a specified distance of a target feature in three-dimensional space. Specify the distance in the search_radius parameter.CONTAINS—The features in the join features will be matched if a target feature contains them. The target features must be polygons or polylines. For this option, the target features cannot be points, and the join features can only be polygons when the target features are also polygons.COMPLETELY_CONTAINS—The features in the join features will be matched if a target feature completely contains them. A polygon can completely contain any feature. A point cannot completely contain any feature, not even a point. A polyline can completely contain only polyline and point features.CONTAINS_CLEMENTINI—This spatial relationship produces the same results as COMPLETELY_CONTAINS except that if the join feature is entirely on the boundary of the target feature (no part is properly inside or outside) the feature will not be matched. Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.WITHIN—The features in the join features will be matched if a target feature is within them. This is the opposite of CONTAINS. For this option, the target features can only be polygons when the join features are also polygons. A point can be a join feature only if a point is the target. COMPLETELY_WITHIN—The features in the join features will be matched if a target feature is completely within them. This is the opposite of COMPLETELY_CONTAINS. WITHIN_CLEMENTINI—The result will be identical to WITHIN except if the entirety of the feature in the join features is on the boundary of the target feature, the feature will not be matched. Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.ARE_IDENTICAL_TO—The features in the join features will be matched if they are identical to a target feature. Both join and target feature must be of the same shape type—point to point, line to line, or polygon to polygon. BOUNDARY_TOUCHES—The features in the join features will be matched if they have a boundary that touches a target feature. When the target and join features are lines or polygons, the boundary of the join feature can only touch the boundary of the target feature and no part of the join feature can cross the boundary of the target feature.SHARE_A_LINE_SEGMENT_WITH—The features in the join features will be matched if they share a line segment with a target feature. The join and target features must be lines or polygons.CROSSED_BY_THE_OUTLINE_OF—The features in the join features will be matched if a target feature is crossed by their outline. The join and target features must be lines or polygons. If polygons are used for the join or target features, the polygon's boundary (line) will be used. Lines that cross at a point will be matched; lines that share a line segment will not be matched.HAVE_THEIR_CENTER_IN—The features in the join features will be matched if a target feature's center falls within them. The center of the feature is calculated as follows: For polygon and multipoint, the geometry's centroid is used, and for line input, the geometry's midpoint is used. Specify the distance in the search_radius parameter.CLOSEST—The feature in the join features that is closest to a target feature will be matched. See the usage tip for more information. Specify the distance in the search_radius parameter.CLOSEST_GEODESIC—This is the same as CLOSEST except that geodesic distance is used rather than planar distance. Use this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations LARGEST_OVERLAP—The feature in the join features will be matched with the target feature with the largest overlap. | String |
| search_radius(Optional) | Join features within this distance of a target feature will be considered for the spatial join. A search radius is only valid when the spatial relationship is specified (the match_option parameter is set to INTERSECT, WITHIN_A_DISTANCE, WITHIN_A_DISTANCE_GEODESIC, HAVE_THEIR_CENTER_IN, CLOSEST, or CLOSEST_GEODESIC). For example, using a search radius of 100 meters with the WITHIN_A_DISTANCE spatial relationship will join feature within 100 meters of a target feature. For the three within-a-distance relationships, if no value is specified for the search_radius, a distance of 0 is used. | Linear Unit |
| distance_field_name(Optional) | The name of the field that contains the distance between the target feature and the closest join feature. This field will be added to the join. This parameter is only valid when the spatial relationship is specified (match_option is set to CLOSEST or CLOSEST_GEODESIC. The value of this field is -1 if no feature is matched within a search radius. If no field name is provided, the field will not be added to the join. | String |
| permanent_join(Optional) | Specifies whether fields from the join feature class will be temporarily added to the layer or permanently added to the target feature class.NO_PERMANENT_FIELDS—The fields from the join feature class will be temporarily added to the layer by an inner join. This is the default.PERMANENT_FIELDS—The fields from the join feature class will be permanently added to the target feature class. | Boolean |
| match_fields[[join_field, target_field],...](Optional) | Pairs of fields from the join features and target features that will be used for attribute matching. Only the records from the join features that share match field values with the target features will participate in the spatial join. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.AddSpatialJoin(target_features, join_features, {join_operation}, {join_type}, {field_mapping}, {match_option}, {search_radius}, {distance_field_name}, {permanent_join}, {match_fields})
```

### Example 2

```python
import os
import arcpy
arcpy.env.overwriteOutput = True

# Create hexagons
out_gdb = arcpy.env.scratchGDB
hex_fc = os.path.join(out_gdb, 'out_fc_hex_2')

arcpy.management.GenerateTessellation(
    hex_fc, '-10823285.769168 4836611.80759869 -10781728.9441187 4856999.87422328', 
    'HEXAGON', '17269676,2624 Unknown', arcpy.SpatialReference(3857))

# Create 2 random points in each hexagon
count_pts = 2
pts_fc = arcpy.management.CreateRandomPoints(
    out_gdb, 'out_fc_crp_2', constraining_feature_class=hex_fc, 
    number_of_points_or_field=count_pts)[0]

# Join the point attributes based on points within the hexagons
result = arcpy.management.AddSpatialJoin(
    hex_fc, pts_fc, None, None, 'CID', 'COMPLETELY_CONTAINS')
```

### Example 3

```python
import os
import arcpy
arcpy.env.overwriteOutput = True

# Create hexagons
out_gdb = arcpy.env.scratchGDB
hex_fc = os.path.join(out_gdb, 'out_fc_hex_2')

arcpy.management.GenerateTessellation(
    hex_fc, '-10823285.769168 4836611.80759869 -10781728.9441187 4856999.87422328', 
    'HEXAGON', '17269676,2624 Unknown', arcpy.SpatialReference(3857))

# Create 2 random points in each hexagon
count_pts = 2
pts_fc = arcpy.management.CreateRandomPoints(
    out_gdb, 'out_fc_crp_2', constraining_feature_class=hex_fc, 
    number_of_points_or_field=count_pts)[0]

# Join the point attributes based on points within the hexagons
result = arcpy.management.AddSpatialJoin(
    hex_fc, pts_fc, None, None, 'CID', 'COMPLETELY_CONTAINS')
```

---

## Add Subtype (Data Management)

## Summary

Adds a new subtype to the subtypes in the input table.

## Usage

- A field in the feature class or table must be assigned as the subtype field before new subtypes can be added. This is done using the Set Subtype Field tool.
- If you add a subtype whose code already exists, the new subtype will be ignored.
- If you need to change the name (also known as description) of an existing subtype, you would first have to remove the subtype, then add a new subtype with the same code and a new name.
- You can also view and manage subtypes in Subtypes view which can be opened by clicking the Subtypes button found in the Design section of the Data ribbon, or by clicking the Subtypes button on the Fields view ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The feature class or table containing the subtype definition to be updated. | Table View |
| Subtype Code | A unique integer value for the subtype to be added. | Long |
| Subtype Name | A name (also known as description) of the subtype code. | String |
| in_table | The feature class or table containing the subtype definition to be updated. | Table View |
| subtype_code | A unique integer value for the subtype to be added. | Long |
| subtype_description | A name (also known as description) of the subtype code. | String |

## Code Samples

### Example 1

```python
arcpy.management.AddSubtype(in_table, subtype_code, subtype_description)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "TYPECODE")
arcpy.AddSubtype_management("water/fittings", "1", "Bend")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "TYPECODE")
arcpy.AddSubtype_management("water/fittings", "1", "Bend")
```

### Example 4

```python
# Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
    
# Set local parameters
inFeatures = "water/fittings"
 
# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")

# Process: Add Subtypes...
# Store all the suptype values in a dictionary with the subtype code as the 
# "key" and the subtype name as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", 
             "7": "Plug", "8": "Reducer", "9": "Saddle", "10": "Sleeve", 
             "11": "Tap", "12": "Tee", "13": "Weld", "14": "Riser"} 
    
# use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     
			
# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4")
```

### Example 5

```python
# Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
    
# Set local parameters
inFeatures = "water/fittings"
 
# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")

# Process: Add Subtypes...
# Store all the suptype values in a dictionary with the subtype code as the 
# "key" and the subtype name as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", 
             "7": "Plug", "8": "Reducer", "9": "Saddle", "10": "Sleeve", 
             "11": "Tap", "12": "Tee", "13": "Weld", "14": "Riser"} 
    
# use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     
			
# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4")
```

---

## Add XY Coordinates (Data Management)

## Summary

Adds the fields POINT_X and POINT_Y to the point input features and calculates their values. It also appends the POINT_Z and POINT_M fields if the input features are Z- and M-enabled.

## Usage

- If the POINT_X, POINT_Y, POINT_Z, and POINT_M fields exist, their values are recalculated.
- The output POINT_X and POINT_Y field values are based on the coordinate system of the dataset, not the coordinate system of the map display. To force the POINT_X and POINT_Y values to be in a coordinate system different than the input dataset, set the Output Coordinate System environment.
- If points are moved after using Add XY Coordinates, their POINT_X and POINT_Y values, and POINT_Z and POINT_M values—if present—must be recomputed by running Add XY Coordinates again.
- The Project tool does not modify the values of POINT_X, POINT_Y, POINT_Z, or POINT_M fields.
- If the input features are in a geographic coordinate system, POINT_X and POINT_Y represent the longitude and latitude, respectively.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The point features whose x,y coordinates will be appended as POINT_X and POINT_Y fields. | Feature Layer |
| in_features | The point features whose x,y coordinates will be appended as POINT_X and POINT_Y fields. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.AddXY(in_features)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.Copy_management("climate.shp", "climateXYpts.shp")
arcpy.AddXY_management("climateXYpts.shp")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.Copy_management("climate.shp", "climateXYpts.shp")
arcpy.AddXY_management("climateXYpts.shp")
```

### Example 4

```python
# Name: AddXY_Example2.py
# Description: Adding XY points to the climate dataset

# Import system modules
import arcpy
from arcpy import env

# Set workspace
env.workspace = "C:/data"

# Set local variables
in_data= "climate.shp"
in_features = "climateXPpts2.shp"

# Copying data to preserve original dataset
# Execute Copy
arcpy.Copy_management(in_data, in_features)

# Execute AddXY
arcpy.AddXY_management(in_features)
```

### Example 5

```python
# Name: AddXY_Example2.py
# Description: Adding XY points to the climate dataset

# Import system modules
import arcpy
from arcpy import env

# Set workspace
env.workspace = "C:/data"

# Set local variables
in_data= "climate.shp"
in_features = "climateXPpts2.shp"

# Copying data to preserve original dataset
# Execute Copy
arcpy.Copy_management(in_data, in_features)

# Execute AddXY
arcpy.AddXY_management(in_features)
```

---

## Adjust 3D Z (Data Management)

## Summary

Modifies z-values of 3D features.

## Usage

- Consider inverting z-values of bathymetry and other subsurface measurements that use positive values to denote depth.
- Use the Convert From Units and Convert To Units parameters to convert z-values from one common unit of measure to another.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The 3D features with the z-values that will be modified. | Feature Layer |
| Reverse Sign of Z Values(Optional) | Specifies whether features will be inverted along the z-axis.Reverse Z Orientation—The sign of z-values will be inverted causing the feature to flip upside down.Maintain Z Orientation—The sign of z-values will not be inverted; it will be maintained. This is the default. | String |
| Adjust Z Value (Optional) | A numeric value or field from the input features that will be used to adjust the z of each vertex in the input features. A positive value will shift the feature higher, while a negative number will shift it lower along the z-axis. | Double; Field |
| Convert From Units (Optional) | Specifies the existing units of the z-values. This parameter is used in conjunction with the Convert To Units parameter. Millimeters—The units will be millimeters.Centimeters—The units will be centimeters.Meters—The units will be meters.Inches—The units will be inches.Feet—The units will be feet.Yards—The units will be yards.Fathoms—The units will be fathoms. | String |
| Convert To Units(Optional) | Specifies the units that existing z-values will be converted to.Millimeters—The units will be millimeters.Centimeters—The units will be centimeters.Meters—The units will be meters.Inches—The units will be inches.Feet—The units will be feet.Yards—The units will be yards.Fathoms—The units will be fathoms. | String |
| in_features | The 3D features with the z-values that will be modified. | Feature Layer |
| reverse_sign(Optional) | Specifies whether features will be inverted along the z-axis.REVERSE—The sign of z-values will be inverted causing the feature to flip upside down.NO_REVERSE—The sign of z-values will not be inverted; it will be maintained. This is the default. | String |
| adjust_value(Optional) | A numeric value or field from the input features that will be used to adjust the z of each vertex in the input features. A positive value will shift the feature higher, while a negative number will shift it lower along the z-axis. | Double; Field |
| from_units(Optional) | Specifies the existing units of the z-values. This parameter is used in conjunction with the to_units parameter. MILLIMETERS—The units will be millimeters.CENTIMETERS—The units will be centimeters.METERS—The units will be meters.INCHES—The units will be inches.FEET—The units will be feet.YARDS—The units will be yards.FATHOMS—The units will be fathoms. | String |
| to_units(Optional) | Specifies the units that existing z-values will be converted to. MILLIMETERS—The units will be millimeters.CENTIMETERS—The units will be centimeters.METERS—The units will be meters.INCHES—The units will be inches.FEET—The units will be feet.YARDS—The units will be yards.FATHOMS—The units will be fathoms. | String |

## Code Samples

### Example 1

```python
arcpy.management.Adjust3DZ(in_features, {reverse_sign}, {adjust_value}, {from_units}, {to_units})
```

### Example 2

```python
arcpy.env.workspace = "C:/data"
arcpy.Adjust3DZ_management("subsurface_pts.shp", "REVERSE", 0, "METERS", "FEET")
```

### Example 3

```python
arcpy.env.workspace = "C:/data"
arcpy.Adjust3DZ_management("subsurface_pts.shp", "REVERSE", 0, "METERS", "FEET")
```

### Example 4

```python
arcpy.env.workspace = "C:/data"
arcpy.Adjust3DZ_management("subsurface_pts.shp", "REVERSE", 0, "METERS", "FEET")
```

### Example 5

```python
arcpy.env.workspace = "C:/data"
arcpy.Adjust3DZ_management("subsurface_pts.shp", "REVERSE", 0, "METERS", "FEET")
```

---

## Alter Attribute Rule (Data Management)

## Summary

Alters the properties of an attribute rule.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the attribute rule that will be altered. | Table View |
| Rule Name | The name of the attribute rule that will be altered. | String |
| Description (Optional) | The description of the attribute rule.To clear the existing value of the description, choose the Reset option from the drop-down menu. Reset—The existing rule description value will be cleared. | String |
| Error Number(Optional) | The error number of the attribute rule.To clear the existing error number value for a calculation rule, choose the Reset option from the drop-down menu. Error number is a required property for constraint and validation rules and cannot be cleared for those rule types.Reset—The existing rule error number value will be cleared. | String |
| Error Message(Optional) | The error message of the attribute rule.To clear the existing error message value for a calculation rule, choose the Reset option from the drop-down menu. Error message is a required property for constraint and validation rules and cannot be cleared for those rule types.Reset—The existing rule error message value will be cleared. | String |
| Tags(Optional) | The tags for the attribute rule.To clear all tags, click the delete button to remove each tag from the list, and choose Reset from the drop-down menu.Reset—The existing rule tags will be cleared. | String |
| Triggering Events (Optional) | Specifies the editing events that will trigger the attribute rule to take effect. Triggering events are only applicable for constraint rules and immediate calculation rules.The new values will replace existing triggering events. To keep the existing triggering events, leave this parameter empty.Insert—The rule will be applied when a new feature is added.Update—The rule will be applied when a feature is updated.Delete—The rule will be applied when a feature is deleted. | String |
| Script Expression (Optional) | An ArcGIS Arcade expression that defines the rule. To keep the existing expression, leave this parameter empty. If an expression is provided for this parameter, it will replace the existing Arcade expression of the rule. If you alter the script expression of a batch calculation or validation rule, the rule must be reevaluated. | Calculator Expression |
| Exclude From Client Evaluation (Optional) | Specifies whether the application will evaluate the rule locally before applying the edits to the workspace. The default for this parameter corresponds to the existing value set for the rule. That is, if the input rule has the exclude from client evaluation parameter set to false, the default for this parameter will be unchecked so the rule will not be automatically excluded. This parameter is not applicable for validation rules or batch calculation rules.Checked—The rule will be excluded from client evaluation.Unchecked—The rule will not be excluded from client evaluation. | Boolean |
| Triggering Fields (Optional) | A list of fields that will trigger an attribute rule to run when an editing event occurs during an update trigger for calculation and constraint attribute rules. If no fields are specified, the tool will use all fields.To enter multiple triggering fields, use a semicolon delimiter, for example, Field1;Field2;Field3. | String |
| in_table | The table containing the attribute rule that will be altered. | Table View |
| name | The name of the attribute rule that will be altered. | String |
| description(Optional) | The description of the attribute rule.To keep the existing value of the description, leave this parameter empty. To clear the existing value of the description, use the RESET keyword. RESET—The existing rule description value will be cleared. | String |
| error_number(Optional) | The error number of the attribute rule.To keep the existing error number value, leave this parameter empty. To clear the existing error number value for a calculation rule, use the RESET keyword. Error number is a required property for constraint and validation rules and cannot be cleared for those rule types.RESET—The existing rule error number value will be cleared. | String |
| error_message(Optional) | The error message of the attribute rule.To keep the existing error message value, leave this parameter empty. To clear the existing error message value for a calculation rule, use the RESET keyword. Error message is a required property for constraint and validation rules and cannot be cleared for those rule types.RESET—The existing rule error message value will be cleared. | String |
| tags[tags,...](Optional) | The tags for the attribute rule.The new values will replace all existing tags; to keep any current tags, include them in this list. For multiple tags, use a semicolon delimiter, for example, Tag1;Tag2;Tag3. To keep the existing tags, leave this parameter empty. To clear the existing tags, use the RESET keyword. RESET—The existing rule tags will be cleared. | String |
| triggering_events[triggering_events,...](Optional) | Specifies the editing events that will trigger the attribute rule to take effect. Triggering events are only applicable for constraint rules and immediate calculation rules.The new values will replace existing triggering events. To keep the existing triggering events, leave this parameter empty.INSERT—The rule will be applied when a new feature is added.UPDATE—The rule will be applied when a feature is updated.DELETE—The rule will be applied when a feature is deleted. | String |
| script_expression(Optional) | An ArcGIS Arcade expression that defines the rule. To keep the existing expression, leave this parameter empty. If an expression is provided for this parameter, it will replace the existing Arcade expression of the rule. If you alter the script expression of a batch calculation or validation rule, the rule must be reevaluated. | Calculator Expression |
| exclude_from_client_evaluation(Optional) | Specifies whether the application will evaluate the rule locally before applying the edits to the workspace. The default for this parameter corresponds to the existing value set for the rule. That is, if the input rule has the exclude from client evaluation parameter set to false, the default for this parameter will be INCLUDE so the rule will not be automatically excluded. This parameter is not applicable for validation rules or batch calculation rules.EXCLUDE—The rule will be excluded from client evaluation.INCLUDE—The rule will not be excluded from client evaluation. | Boolean |
| triggering_fields[triggering_fields,...](Optional) | A list of fields that will trigger an attribute rule to run when an editing event occurs during an update trigger for calculation and constraint attribute rules. If no fields are specified, the tool will use all fields.To enter multiple triggering fields, use a semicolon delimiter, for example, Field1;Field2;Field3. | String |

## Code Samples

### Example 1

```python
arcpy.management.AlterAttributeRule(in_table, name, {description}, {error_number}, {error_message}, {tags}, {triggering_events}, {script_expression}, {exclude_from_client_evaluation}, {triggering_fields})
```

### Example 2

```python
import arcpy
arcpy.management.AlterAttributeRule("C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes", 
                                    "constraintRuleOP",
                                    "Operating pressure cannot exceed 300",
                                    "999",
                                    "Invalid operating pressure value",
                                    "Pipeline;OP;ExceededValue")
```

### Example 3

```python
import arcpy
arcpy.management.AlterAttributeRule("C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes", 
                                    "constraintRuleOP",
                                    "Operating pressure cannot exceed 300",
                                    "999",
                                    "Invalid operating pressure value",
                                    "Pipeline;OP;ExceededValue")
```

---

## Alter Domain (Data Management)

## Summary

Alters the properties of an existing attribute domain in a workspace.

## Usage

- Domain management involves the following steps: Modify an existing domain using this tool, or create a new domain using the Create Domain tool.Add values to or set the range of values for the domain using the Add Coded Value to Domain tool or Set Value For Range Domain tool.Associate the domain with a feature class using the Assign Domain To Field tool.
- Modify an existing domain using this tool, or create a new domain using the Create Domain tool.
- Add values to or set the range of values for the domain using the Add Coded Value to Domain tool or Set Value For Range Domain tool.
- Associate the domain with a feature class using the Assign Domain To Field tool.
- Coded value domains only support the default value, duplicate split policy, and default value merge policy.
- Range domains support all split and merge policies. After a split or merge operation, the attribute values of output features are calculated based on the numeric values of the input features and the specified split or merge policy.
- The properties of a domain in an enterprise geodatabase workspace can be altered when the Input Workspace value is a database connection file connected as the domain owner or the geodatabase administrator.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase that contains the domain that will be altered. | Workspace |
| Domain Name | The name of the domain the will be altered. | String |
| New Domain Name (Optional) | The new name of the domain. | String |
| New Domain Description (Optional) | The new description of the domain. | String |
| New Split Policy (Optional) | Specifies the split policy that will used for the domain. The behavior of an attribute's values when a feature is split is controlled by its split policy. Use the attribute's default value—The attributes of the two resulting features will be the default value of the attribute of the given feature class or subtype. Duplicate attribute values—The attribute of the two resulting features will be a copy of the original object's attribute value. Use geometric ratio—The attributes of resulting features will be a ratio of the original feature's value. The ratio is based on the proportion into which the original geometry is divided. If the geometry is divided equally, the attribute of each new feature will be one-half the value of the original object's attribute. This option only applies to range domains. | String |
| New Merge Policy (Optional) | Specifies the merge policy that will be used for the domain. When two features are merged into a single feature, merge policies will control attribute values in the new feature. This parameter is only applicable to range domains, as coded value domains can only use the default merge policy.Use the attribute's default value—The attribute of the resulting feature will be the default value of the attribute of the given feature class or subtype. This option only applies to nonnumeric fields and coded value domains. Sum of the values—The attribute of the resulting feature will be on the sum of the values from the original feature's attribute. This option only applies to range domains. Area weighted average—The attribute of the resulting feature will be the weighted average of the attribute values of the original features. The average is based on the original feature's geometry. This option only applies to range domains. | String |
| New Domain Owner (Optional) | The name of the database user that the domain ownership will be transferred to. Ensure that the new domain owner exists in the database; the tool does not check the validity of the owner name specified. This parameter is not applicable for domains created in a file geodatabase. | String |
| in_workspace | The geodatabase that contains the domain that will be altered. | Workspace |
| domain_name | The name of the domain the will be altered. | String |
| new_domain_name(Optional) | The new name of the domain. | String |
| new_domain_description(Optional) | The new description of the domain. | String |
| split_policy(Optional) | Specifies the split policy that will used for the domain. The behavior of an attribute's values when a feature is split is controlled by its split policy. DEFAULT—The attributes of the two resulting features will be the default value of the attribute of the given feature class or subtype. DUPLICATE—The attribute of the two resulting features will be a copy of the original object's attribute value. GEOMETRY_RATIO—The attributes of resulting features will be a ratio of the original feature's value. The ratio is based on the proportion into which the original geometry is divided. If the geometry is divided equally, the attribute of each new feature will be one-half the value of the original object's attribute. This option only applies to range domains. | String |
| merge_policy(Optional) | Specifies the merge policy that will be used for the domain. When two features are merged into a single feature, merge policies will control attribute values in the new feature. This parameter is only applicable to range domains, as coded value domains can only use the default merge policy.DEFAULT—The attribute of the resulting feature will be the default value of the attribute of the given feature class or subtype. This option only applies to nonnumeric fields and coded value domains. SUM_VALUES—The attribute of the resulting feature will be on the sum of the values from the original feature's attribute. This option only applies to range domains. AREA_WEIGHTED—The attribute of the resulting feature will be the weighted average of the attribute values of the original features. The average is based on the original feature's geometry. This option only applies to range domains. | String |
| new_domain_owner(Optional) | The name of the database user that the domain ownership will be transferred to. Ensure that the new domain owner exists in the database; the tool does not check the validity of the owner name specified. This parameter is not applicable for domains created in a file geodatabase. | String |

## Code Samples

### Example 1

```python
arcpy.management.AlterDomain(in_workspace, domain_name, {new_domain_name}, {new_domain_description}, {split_policy}, {merge_policy}, {new_domain_owner})
```

### Example 2

```python
arcpy.env.workspace = "C:/data"
arcpy.AlterDomain_management("montgomery.gdb", "Material", "PipeMaterial", "Valid pipe materials", "DUPLICATE", "DEFAULT")
```

### Example 3

```python
arcpy.env.workspace = "C:/data"
arcpy.AlterDomain_management("montgomery.gdb", "Material", "PipeMaterial", "Valid pipe materials", "DUPLICATE", "DEFAULT")
```

### Example 4

```python
# Name: AlterDomain.py
# Description: Modify an attribute domain to constrain valid date
#              range for wildlife sightings.
# Author: ESRI
 
# Import system modules
import arcpy
from arcpy import env
 
# Set the workspace
env.workspace = "C:/data"
 
# Set local parameters
gdb = "Habitat.gdb"
domName = "CoastalArea"
new_domName = "SightingSeason"
new_desc = "Range of valid dates for sightings"
new_split = "DUPLICATE"
new_merge = "AREA_WEIGHTED"

# Process: Modify the range domain
arcpy.AlterDomain_management(gdb, domName, new_domName, new_desc, new_split, new_merge)
```

### Example 5

```python
# Name: AlterDomain.py
# Description: Modify an attribute domain to constrain valid date
#              range for wildlife sightings.
# Author: ESRI
 
# Import system modules
import arcpy
from arcpy import env
 
# Set the workspace
env.workspace = "C:/data"
 
# Set local parameters
gdb = "Habitat.gdb"
domName = "CoastalArea"
new_domName = "SightingSeason"
new_desc = "Range of valid dates for sightings"
new_split = "DUPLICATE"
new_merge = "AREA_WEIGHTED"

# Process: Modify the range domain
arcpy.AlterDomain_management(gdb, domName, new_domName, new_desc, new_split, new_merge)
```

---

## Alter Field Group (Data Management)

## Summary

Alters the properties of a field group.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The table containing the field group to be altered. | Table View |
| Field Group Name | The name of the field group to be altered. | String |
| New Field Group Name (Optional) | The new, unique name for the field group. | String |
| New Fields (Optional) | The fields that participate in the field group. To modify the fields, enter new field names. Provided values will replace, not append, the current list of fields that participates in the field group. If no values are provided, the fields will not be altered. | String |
| Is Restrictive (Optional) | Specifies whether the field group is restrictive. This parameter allows you to control the editing experience when using contingent values. Checked—The field group is restrictive. Values entered on a field in the field group are restricted to those specified as contingent values. This is the default.Unchecked—The field group is not restrictive. Values can be committed to a field in a field group even if they are not specified as contingent values. | Boolean |
| target_table | The table containing the field group to be altered. | Table View |
| name | The name of the field group to be altered. | String |
| new_name(Optional) | The new, unique name for the field group. | String |
| fields[fields,...](Optional) | The fields that participate in the field group. To modify the fields, enter new field names. Provided values will replace, not append, the current list of fields that participates in the field group. If no values are provided, the fields will not be altered. | String |
| is_restrictive(Optional) | Specifies whether the field group is restrictive. This parameter allows you to control the editing experience when using contingent values.RESTRICT—The field group is restrictive. Values entered on a field in the field group are restricted to those specified as contingent values. This is the default.DO_NOT_RESTRICT—The field group is not restrictive. Values can be committed to a field in a field group even if they are not specified as contingent values. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AlterFieldGroup(target_table, name, {new_name}, {fields}, {is_restrictive})
```

### Example 2

```python
import arcpy
arcpy.AlterFieldGroup_management("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC",
                                 "MyFieldGroup", "MyNewFieldGroupName",
                                 ["Field1", "Field3"],
                                 "DO_NOT_RESTRICT")
```

### Example 3

```python
import arcpy
arcpy.AlterFieldGroup_management("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC",
                                 "MyFieldGroup", "MyNewFieldGroupName",
                                 ["Field1", "Field3"],
                                 "DO_NOT_RESTRICT")
```

---

## Alter Field (Data Management)

## Summary

Renames fields and field aliases or alters field properties.

## Usage

- This tool allows you to rename fields or field aliases for a geodatabase table or feature class.
- You can use this tool to modify the field alias of a field in a table or view that has been registered with the geodatabase.
- If the input field is a required field, only the field alias can be modified.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input geodatabase table or feature class that contains the field that will be altered. | Table View; Raster Layer; Mosaic Layer |
| Field Name | The name of the field that will be altered. If the field is a required field, only the field alias will be altered. | Field |
| New Field Name (Optional) | The new name for the field. | String |
| New Field Alias (Optional) | The new field alias for the field. | String |
| New Field Type (Optional) | Specifies the new field type for the field. This parameter is only applicable if the input table is empty (does not contain records).Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.Text—The field type will be text. Text fields support a string of characters.Date—The field type will be date. Date fields support date and time values. Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.Raster— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | String |
| New Field Length(Optional) | The new length of the field. This sets the maximum number of allowable characters for each record of the field. This parameter is only applicable to fields of type Text or Blob (binary data). If the table is empty, the field length can be increased or decreased. If the table is not empty, the length can only be increased from the current value. | Long |
| New Field IsNullable(Optional) | Specifies whether the field can contain null values. Null values are only supported for fields in a geodatabase. This parameter is only applicable if the table is empty (does not contain records).Checked—The field can contain null values. This is the default.Unchecked—The field cannot contain null values. | Boolean |
| Clear Alias(Optional) | Specifies whether the alias for the input field will be cleared. The New Field Alias parameter value must be empty to clear the alias of the field.Checked—The field alias will be cleared (set to null). The field alias parameter must be empty.Unchecked—The field alias will not be cleared. This is the default. | Boolean |
| in_table | The input geodatabase table or feature class that contains the field that will be altered. | Table View; Raster Layer; Mosaic Layer |
| field | The name of the field that will be altered. If the field is a required field, only the field alias will be altered. | Field |
| new_field_name(Optional) | The new name for the field. | String |
| new_field_alias(Optional) | The new field alias for the field. | String |
| field_type(Optional) | Specifies the new field type for the field. This parameter is only applicable if the input table is empty (does not contain records).SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.TEXT—The field type will be text. Text fields support a string of characters.DATE—The field type will be date. Date fields support date and time values. DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values.TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.RASTER— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | String |
| field_length(Optional) | The new length of the field. This sets the maximum number of allowable characters for each record of the field. This parameter is only applicable to fields of type TEXT or BLOB. If the table is empty, the field length can be increased or decreased. If the table is not empty, the length can only be increased from the current value. | Long |
| field_is_nullable(Optional) | Specifies whether the field can contain null values. Null values are only supported for fields in a geodatabase. This parameter is only applicable if the input table is empty (does not contain records).NULLABLE—The field can contain null values. This is the default. NON_NULLABLE—The field cannot contain null values. | Boolean |
| clear_field_alias(Optional) | Specifies whether the alias for the input field will be cleared. The new_field_alias parameter must be empty to clear the alias of the field.CLEAR_ALIAS—The field alias will be cleared (set to null).DO_NOT_CLEAR—The field alias will not be cleared. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AlterField(in_table, field, {new_field_name}, {new_field_alias}, {field_type}, {field_length}, {field_is_nullable}, {clear_field_alias})
```

### Example 2

```python
import arcpy
arcpy.management.AlterField(
    r'C:\Data\Garbo.gdb\Khyber', 'Elev', 'ELEVATION', 'Elevation in Metres')
```

### Example 3

```python
import arcpy
arcpy.management.AlterField(
    r'C:\Data\Garbo.gdb\Khyber', 'Elev', 'ELEVATION', 'Elevation in Metres')
```

### Example 4

```python
import arcpy

# Set workspace
arcpy.env.workspace = r'C:\Data\Garbo.gdb'

# Loop through feature classes looking for a field named 'elev'
fcList = arcpy.ListFeatureClasses()  # Get a list of feature classes
for fc in fcList:  # Loop through feature classes
    fieldList = arcpy.ListFields(fc)  # Get a list of fields for each feature class
    for field in fieldList:  # Lloop through each field
        if field.name.lower() == 'elev':  # Look for the name elev
            arcpy.management.AlterField(fc, field.name, 'ELEVATION', 'Elevation in Metres')
```

### Example 5

```python
import arcpy

# Set workspace
arcpy.env.workspace = r'C:\Data\Garbo.gdb'

# Loop through feature classes looking for a field named 'elev'
fcList = arcpy.ListFeatureClasses()  # Get a list of feature classes
for fc in fcList:  # Loop through feature classes
    fieldList = arcpy.ListFields(fc)  # Get a list of fields for each feature class
    for field in fieldList:  # Lloop through each field
        if field.name.lower() == 'elev':  # Look for the name elev
            arcpy.management.AlterField(fc, field.name, 'ELEVATION', 'Elevation in Metres')
```

### Example 6

```python
import arcpy

# Set local variables
in_table = "C:/Data/Garbo.gdb/trails"  # Note: empty feature class
field = "condition_rating"  # short int, non nullable field
new_field_name = "notes"
new_field_alias = "Comments on Trail Condition"
field_type = "TEXT"
field_length = 60
field_is_nullable = "NULLABLE"
clear_field_alias = "FALSE"

# Alter the properties of a non nullable, short data type field to become a text field
arcpy.management.AlterField(in_table,
                            field,
                            new_field_name,
                            new_field_alias,
                            field_type,
                            field_length,
                            field_is_nullable,
                            clear_field_alias)
```

### Example 7

```python
import arcpy

# Set local variables
in_table = "C:/Data/Garbo.gdb/trails"  # Note: empty feature class
field = "condition_rating"  # short int, non nullable field
new_field_name = "notes"
new_field_alias = "Comments on Trail Condition"
field_type = "TEXT"
field_length = 60
field_is_nullable = "NULLABLE"
clear_field_alias = "FALSE"

# Alter the properties of a non nullable, short data type field to become a text field
arcpy.management.AlterField(in_table,
                            field,
                            new_field_name,
                            new_field_alias,
                            field_type,
                            field_length,
                            field_is_nullable,
                            clear_field_alias)
```

---

## Alter Fields (multiple) (Data Management)

## Summary

Alters the field properties of multiple fields in a feature class or table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input geodatabase table or feature class that contains the field that will be altered. | Table View; Raster Layer; Mosaic Layer |
| Field Properties (Optional) | The input table fields and their properties that will be altered.Field Name—The name of the field that will be altered. If the field is a required field, only the field alias will be altered.New Field Name (optional)—The new name for the field.New Field Alias (optional)—The new field alias for the field.New Field Type (optional)—Specifies the new field type for the field. This column is only applicable if the input table is empty (does not contain records).New Field Length (optional)—The new length of the field (Long type). This sets the maximum number of allowable characters for each record of the field. This option is only applicable to fields of type Text or Blob (binary data). If the table is empty, the field length can be increased or decreased. If the table is not empty, the length can only be increased from the current value. New Field IsNullable (optional)—Specifies whether the field can contain null values. Null values are only supported for fields in a geodatabase. This option is only applicable if the table is empty (does not contain records). If checked, the field can contain null values. This is the default. If unchecked, the field cannot contain null values.Clear Alias (optional)—Specifies whether the alias for the input field will be cleared. The New Field Alias value must be empty to clear the alias of the field. If checked, the field alias will be cleared (set to null). If unchecked, the field alias will not be cleared. This is the default. Available field types are as follows: Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. Text—The field type will be text. Text fields support a string of characters. Date—The field type will be date. Date fields support date and time values.Date (high precision)—The field type will be high precision date. High precision date fields support date and time values with millisecond time.Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value. Blob (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. Raster imagery—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | Value Table |
| in_table | The input geodatabase table or feature class that contains the field that will be altered. | Table View; Raster Layer; Mosaic Layer |
| field_description[[Field Name, {New Field Name}, {New Field Alias}, {New Field Type}, {New Field Length}, {New Field IsNullable}, {Clear Alias}],...](Optional) | The input table fields and their properties that will be altered.Field Name—The name of the field that will be altered. If the field is a required field, only the field alias will be altered.New Field Name (optional)—The new name for the field.New Field Alias (optional)—The new field alias for the field.New Field Type (optional)—Specifies the new field type for the field. This option is only applicable if the input table is empty (does not contain records).New Field Length (optional)—The new length of the field. This sets the maximum number of allowable characters for each record of the field. This option is only applicable to fields of type TEXT or BLOB. If the table is empty, the field length can be increased or decreased. If the table is not empty, the length can only be increased from the current value. New Field IsNullable (optional)—Specifies whether the field can contain null values. Null values are only supported for fields in a geodatabase. This option is only applicable if the table is empty (does not contain records). If set to True, the field can contain null values. This is the default. If set to False, the field cannot contain null values.Clear Alias (optional)—Specifies whether the alias for the input field will be cleared. The New Field Alias value must be empty to clear the alias of the field. If set to True, the field alias will be cleared (set to null). If set to False, the field alias will not be cleared. This is the default.Available field types are as follows: SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. TEXT—The field type will be text. Text fields support a string of characters. DATE—The field type will be date. Date fields support date and time values. DATEHIGHPRECISION—The field type will be high precision date. High precision date fields support date and time values with millisecond time.DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values. TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. RASTER—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.AlterFields(in_table, {field_description})
```

### Example 2

```python
import arcpy

test_class = arcpy.management.CreateFeatureclass(out_gdb, "new_fc", geometry_type='POLYGON')
field_description = [
    ['SHORT_FIELD', 'SHORT', 'SHORT_FIELD_Alias', '#', '#', '#'],
    ['LONG_FIELD', 'LONG', 'LONG_FIELD_Alias', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER', 'BIGINTEGER_FIELD_Alias', '#', '#', '#'], 
    ['FLOAT_FIELD', 'FLOAT', 'FLOAT_FIELD_Alias', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE', 'DOUBLE_FIELD_Alias', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT', 'TEXT_FIELD_Alias', '#', '#', '#'],
    ['DATE_FIELD', 'DATE', 'DATE_FIELD_Alias', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY', 'DATEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY', 'TIMEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET', 'TIMESTAMPOFFSET_FIELD_Alias', '#', '#', '#']
]
arcpy.management.AddFields(test_class, field_description, None)

alter_field_description = [
    ['SHORT_FIELD', 'SHORT_FIELD_altered', '#', '#', '#', '#'],
    ['LONG_FIELD', 'LONG_FIELD_altered', '#', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER_FIELD_altered', '#', '#', '#', '#'],
    ['FLOAT_FIELD', 'FLOAT_FIELD_altered', '#', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE_FIELD_altered', '#', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT_FIELD_altered', '#', '#', '#', '#'],
    ['DATE_FIELD', 'DATE_FIELD_altered', '#', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY_FIELD_altered', '#', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY_FIELD_altered', '#', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET_FIELD_altered', '#', '#', '#', '#']
]
arcpy.management.AlterFields(in_table=test_class, field_description=alter_field_description)
```

### Example 3

```python
import arcpy

test_class = arcpy.management.CreateFeatureclass(out_gdb, "new_fc", geometry_type='POLYGON')
field_description = [
    ['SHORT_FIELD', 'SHORT', 'SHORT_FIELD_Alias', '#', '#', '#'],
    ['LONG_FIELD', 'LONG', 'LONG_FIELD_Alias', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER', 'BIGINTEGER_FIELD_Alias', '#', '#', '#'], 
    ['FLOAT_FIELD', 'FLOAT', 'FLOAT_FIELD_Alias', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE', 'DOUBLE_FIELD_Alias', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT', 'TEXT_FIELD_Alias', '#', '#', '#'],
    ['DATE_FIELD', 'DATE', 'DATE_FIELD_Alias', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY', 'DATEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY', 'TIMEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET', 'TIMESTAMPOFFSET_FIELD_Alias', '#', '#', '#']
]
arcpy.management.AddFields(test_class, field_description, None)

alter_field_description = [
    ['SHORT_FIELD', 'SHORT_FIELD_altered', '#', '#', '#', '#'],
    ['LONG_FIELD', 'LONG_FIELD_altered', '#', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER_FIELD_altered', '#', '#', '#', '#'],
    ['FLOAT_FIELD', 'FLOAT_FIELD_altered', '#', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE_FIELD_altered', '#', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT_FIELD_altered', '#', '#', '#', '#'],
    ['DATE_FIELD', 'DATE_FIELD_altered', '#', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY_FIELD_altered', '#', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY_FIELD_altered', '#', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET_FIELD_altered', '#', '#', '#', '#']
]
arcpy.management.AlterFields(in_table=test_class, field_description=alter_field_description)
```

### Example 4

```python
import arcpy

test_class = arcpy.management.CreateTable(out_gdb, "new_table")
field_description = [
    ['SHORT_FIELD', 'SHORT', 'SHORT_FIELD_Alias', '#', '#', '#'],
    ['LONG_FIELD', 'LONG', 'LONG_FIELD_Alias', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER', 'BIGINTEGER_FIELD_Alias', '#', '#', '#'],
    ['FLOAT_FIELD', 'FLOAT', 'FLOAT_FIELD_Alias', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE', 'DOUBLE_FIELD_Alias', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT', 'TEXT_FIELD_Alias', '#', '#', '#'],
    ['DATE_FIELD', 'DATE', 'DATE_FIELD_Alias', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY', 'DATEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY', 'TIMEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET', 'TIMESTAMPOFFSET_FIELD_Alias', '#', '#', '#']
]

arcpy.management.AddFields(test_class, field_description, None)

# Change the nullable property to true
alter_field_description = [
    ['SHORT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['LONG_FIELD', '#', '#', '#','#', 'true', '#'],
    ['BIGINTEGER_FIELD', '#', '#', '#','#', 'true', '#'],
    ['FLOAT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DOUBLE_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TEXT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DATE_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DATEONLY_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TIMEONLY_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TIMESTAMPOFFSET_FIELD', '#', '#', '#','#', 'true', '#']
]

arcpy.management.AlterFields(in_table=test_class, field_description=alter_field_description)
```

### Example 5

```python
import arcpy

test_class = arcpy.management.CreateTable(out_gdb, "new_table")
field_description = [
    ['SHORT_FIELD', 'SHORT', 'SHORT_FIELD_Alias', '#', '#', '#'],
    ['LONG_FIELD', 'LONG', 'LONG_FIELD_Alias', '#', '#', '#'],
    ['BIGINTEGER_FIELD', 'BIGINTEGER', 'BIGINTEGER_FIELD_Alias', '#', '#', '#'],
    ['FLOAT_FIELD', 'FLOAT', 'FLOAT_FIELD_Alias', '#', '#', '#'],
    ['DOUBLE_FIELD', 'DOUBLE', 'DOUBLE_FIELD_Alias', '#', '#', '#'],
    ['TEXT_FIELD', 'TEXT', 'TEXT_FIELD_Alias', '#', '#', '#'],
    ['DATE_FIELD', 'DATE', 'DATE_FIELD_Alias', '#', '#', '#'],
    ['DATEONLY_FIELD', 'DATEONLY', 'DATEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMEONLY_FIELD', 'TIMEONLY', 'TIMEONLY_FIELD_Alias', '#', '#', '#'],
    ['TIMESTAMPOFFSET_FIELD', 'TIMESTAMPOFFSET', 'TIMESTAMPOFFSET_FIELD_Alias', '#', '#', '#']
]

arcpy.management.AddFields(test_class, field_description, None)

# Change the nullable property to true
alter_field_description = [
    ['SHORT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['LONG_FIELD', '#', '#', '#','#', 'true', '#'],
    ['BIGINTEGER_FIELD', '#', '#', '#','#', 'true', '#'],
    ['FLOAT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DOUBLE_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TEXT_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DATE_FIELD', '#', '#', '#','#', 'true', '#'],
    ['DATEONLY_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TIMEONLY_FIELD', '#', '#', '#','#', 'true', '#'],
    ['TIMESTAMPOFFSET_FIELD', '#', '#', '#','#', 'true', '#']
]

arcpy.management.AlterFields(in_table=test_class, field_description=alter_field_description)
```

---

## Alter Mosaic Dataset Schema (Data Management)

## Summary

Defines the editing operations that nonowners have when editing a mosaic dataset in an enterprise geodatabase.

## Usage

- This tool is only needed when creating a mosaic dataset in an enterprise geodatabase and a nonowner will be editing the mosaic dataset.
- Use this tool to set up a mosaic dataset that will be published as an image service allowing users to upload or edit the items. A user can only upload data if the raster type is allowed by the mosaic dataset.
- If a nonowner attempts to perform an operation that is not allowed, the operation will fail.
- The Raster Types parameter values control the rasters a nonowner can add using the Add Rasters To Mosaic Dataset tool. If the nonowner tries to add a raster type that has not been specified by this tool, the Add Rasters To Mosaic Dataset tool will fail.
- If you run this tool without choosing any raster types, only the additional mosaic dataset tables will be created. These tables include overview, seamline, color correction, stereo, cell size levels, status, error, and permissions.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset on which the permitted operations will be changed. | Mosaic Layer |
| Operations(Optional) | Specifies the operations that will be permissible for the mosaic dataset. Analysis—A nonowner will be allowed to run the Analyze Mosaic Dataset tool on the mosaic dataset.Boundary—A nonowner will be allowed to create or edit the boundary of the mosaic dataset. This is required if a nonowner will add raster datasets outside of the existing boundary.Cache—A nonowner will be allowed to create a cache for the mosaic dataset.Color correction—A nonowner will be allowed to color correct the mosaic dataset.Definition—A nonowner will be allowed to add multidimensional data or a processing template to the mosaic dataset.Levels— A nonowner will be allowed to calculate cell size ranges for the mosaic dataset.Log—A nonowner will be allowed to create a log table for the mosaic dataset.Overview— A nonowner will be allowed to create overviews for the mosaic dataset.Seamline—A nonowner will be allowed to create seamlines for the mosaic dataset.Stereo— A nonowner will be allowed to define stereo pairs for the mosaic dataset.View—A nonowner will be allowed to edit the image service. The Enable Editor Tracking parameter will be automatically activated when this option is specified. | String |
| Raster Types(Optional) | Specifies the raster types that nonowners can add to the mosaic dataset.To select a custom raster type, provide the location of the custom raster type file.Airborne Digital Sensors—The Leica ADS raster type can be added.Altum—The Altum raster type can be added.ASTER—The ASTER raster type can be added.AVIRIS—The AVIRIS raster type can be added.BlackSky—The BlackSky raster type can be added.CADRG/ECRG—The CADRG/ECRG raster type can be added.CIB—The CIB raster type can be added.Capella—The Capella raster type can be added.Deimos-2—The Deimos-2 raster type can be added.DTED—The DTED raster type can be added.DMCii—The DMCii raster type can be added.DubaiSat-2—The DubaiSat-2 raster type can be added.EMIT—The EMIT raster type can be added.FORMOSAT-2—The FORMOSAT-2 raster type can be added.Frame Camera—The Frame Camera raster type can be added.GeoEye—The GeoEye-1 raster type can be added.GF-1 PMS—The GF-1 PMS raster type can be added.GF-1 WFV—The GF-1 WFV raster type can be added.GF-2 PMS—The GF-2 PMS raster type can be added.GF-4 PMI—The GF-4 PMI raster type can be added.GRIB—The GRIB raster type can be added.HDF—The HDF raster type can be added.HJ 1A/HJ 1B CCD—The HJ 1A/HJ 1B CCD raster type can be added.HRE—The HRE raster type can be added.ICEYE—The ICEYE raster type can be added.IKONOS—The IKONOS raster type can be added.Jilin-1—The Jilin-1 raster type can be added.KOMPSAT-2—The KOMPSAT-2 raster type can be added.KOMPSAT-3—The KOMPSAT-3 raster type can be added.LAS—The LAS raster type can be added.Landsat MSS—The Landsat 1-5 MSS raster type can be added.Landsat TM—The Landsat 4-5 TM raster type can be added.Landsat ETM+—The Landsat 7 ETM+ raster type can be added.Landsat 8—The Landsat 8 raster type can be added.Landsat 9—The Landsat 9 raster type can be added. can be added.Maxar—The Maxar raster type can be added.Match-AT—The Match-AT raster type can be added.NITF—The NITF raster type can be added.NetCDF—The NetCDF raster type can be added.NCDRD—The NCDRD raster type can be added.PlanetScope—The PlanetScope raster type can be added.Pleiades Neo—The Pleiades Neo raster type can be added.Pleiades-1—The Pleiades-1 raster type can be added.QuickBird—The Quickbird raster type can be added.RADARSAT-2—The RADARSAT-2 raster type can be added.RCM—The RCM raster type can be added.RapidEye—The RapidEye raster type can be added.Raster Process Definition—The Raster Process Definition raster type can be added.RedEdge—The RedEdge raster type can be added.Satellogic—The Satellogic raster type can be added.SOCET SET—The SOCET SET raster type can be added.Scanned aerial imagery—The Scanned Aerial Imagery raster type can be added.Sentinel-1—The Sentinel-1 raster type can be added.Sentinel-2—The Sentinel-2 raster type can be added.Sentinel-3—The Sentinel-3 raster type can be added.SkySat-C—The SkySat-C raster type can be added.Spot 5—The SPOT 5 raster type can be added.Spot 6—The SPOT 6 raster type can be added.Spot 7—The SPOT 7 raster type can be added.SuperView-1—The SuperView-1 raster type can be added.TeLEOS-1—The TelEOS-1 raster type can be added.TH-01—The TH-01 raster type can be added.UAV/UAS—The UAV/UAS raster type can be added.Vision-1—The Vision-1 raster type can be added.WorldView-1—The WorldView-1 raster type can be added.WorldView-2— The WorldView-2 raster type can be added.WorldView-3—The WorldView-3 raster type can be added.WorldView-4—The WorldView-4 raster type can be added.ZY1-02C HRC—The ZY1-02C HRC raster type can be added.ZY1-02C PMS—The ZY1-02C PMS raster type can be added.ZY3-CRESDA—The ZY3-CRESDA raster type can be added.ZY3-SASMAC—The ZY3-SASMAC raster type can be added. | String |
| Enable Editor Tracking (Optional) | Specifies whether editor tracking will be activated.Editor tracking can help you maintain accountability and enforce quality-control standards. Unchecked—Editor tracking will not be activated. This is the default.Checked—Editor tracking will be activated.If the View option is specified for the Operations parameter, editor tracking will be automatically activated. | Boolean |
| in_mosaic_dataset | The mosaic dataset on which the permitted operations will be changed. | Mosaic Layer |
| side_tables[operation,...](Optional) | Specifies the operations that will be permissible for the mosaic dataset. ANALYSIS—A nonowner will be allowed to run the Analyze Mosaic Dataset tool on the mosaic dataset.BOUNDARY—A nonowner will be allowed to create or edit the boundary of the mosaic dataset. This is also required if a nonowner will add rasters outside of the existing boundary.CACHE—A nonowner will be allowed to create a cache for the mosaic dataset.COLOR_CORRECTION—A nonowner will be allowed to color correct the mosaic dataset.DEFINITION—A nonowner will be allowed to add multidimensional data or a processing template to the mosaic dataset.LEVELS—A nonowner will be allowed to calculate cell size ranges or create seamlines for the mosaic dataset.LOG—A nonowner will be allowed to create a log table for the mosaic dataset.OVERVIEW—A nonowner will be allowed to create overviews for the mosaic dataset.SEAMLINE—A nonowner will be allowed to create seamlines for the mosaic dataset.STEREO—A nonowner will be allowed to define stereo pairs for the mosaic dataset.VIEW—A nonowner will be allowed to edit the image service. The editor_tracking parameter will be automatically enabled when this option is specified, since the View table must have editor tracking turned on. | String |
| raster_type_names[raster_type,...](Optional) | Specifies the raster types that nonowners can add to the mosaic dataset.To select a custom raster type, provide the location of the custom raster type file.ADS—The Leica ADS raster type can be added.Altum—The Altum raster type can be added.ASTER—The ASTER raster type can be added.AVIRIS—The AVIRIS raster type can be added.BlackSky—The BlackSky raster type can be added.CADRG/ECRG—The CADRG/ECRG raster type can be added.CIB—The CIB raster type can be added.Capella—The Capella raster type can be added.DEIMOS-2—The Deimos-2 raster type can be added.DTED—The DTED raster type can be added.DMCii—The DMCii raster type can be added.DubaiSat-2—The DubaiSat-2 raster type can be added.EMIT—The EMIT raster type can be added.FORMOSAT-2—The FORMOSAT-2 raster type can be added.Frame Camera—The Frame Camera raster type can be added.GeoEye-1—The GeoEye-1 raster type can be added.GF-1 PMS—The GF-1 PMS raster type can be added.GF-1 WFV—The GF-1 WFV raster type can be added.GF-2 PMS—The GF-2 PMS raster type can be added.GF-4 PMI—The GF-4 PMI raster type can be added.GRIB—The GRIB raster type can be added.HDF—The HDF raster type can be added.HJ 1A/1B CCD—The HJ 1A/HJ 1B CCD raster type can be added.HRE—The HRE raster type can be added.ICEYE—The ICEYE raster type can be added.IKONOS—The IKONOS raster type can be added.Jilin-1—The Jilin-1 raster type can be added.KOMPSAT-2—The KOMPSAT-2 raster type can be added.KOMPSAT-3—The KOMPSAT-3 raster type can be added.LAS—The LAS raster type can be added.Landsat 1-5 MSS—The Landsat 1-5 MSS raster type can be added.Landsat 4-5 TM—The Landsat 4-5 TM raster type can be added.Landsat 7 ETM+—The Landsat 7 ETM+ raster type can be added.Landsat 8—The Landsat 8 raster type can be added.Landsat 9—The Landsat 9 raster type can be added. can be added.MAXAR—The Maxar raster type can be added.Match-AT—The Match-AT raster type can be added.NCDRD—The NCDRD raster type can be added.NITF—The NITF raster type can be added.NetCDF—The NetCDF raster type can be added.PlanetScope—The PlanetScope raster type can be added.Pleiades Neo—The Pleiades Neo raster type can be added.Pleiades-1—The Pleiades-1 raster type can be added.QuickBird—The Quickbird raster type can be added.RADARSAT-2—The RADARSAT-2 raster type can be added.RCM—The RCM raster type can be added.RapidEye—The RapidEye raster type can be added.Raster Process Definition—The Raster Process Definition raster type can be added.RedEdge—The RedEdge raster type can be added.Satellogic—The Satellogic raster type can be added.SOCET SET—The SOCET SET raster type can be added.Scanned Aerial Imagery—The Scanned Aerial Imagery raster type can be added.Sentinel-1—The Sentinel-1 raster type can be added.Sentinel-2—The Sentinel-2 raster type can be added.Sentinel-3—The Sentinel-3 raster type can be added.SkySat—The SkySat-C raster type can be added.SPOT 5—The SPOT 5 raster type can be added.SPOT 6—The SPOT 6 raster type can be added.SPOT 7—The SPOT 7 raster type can be added.SuperView-1—The SuperView-1 raster type can be added.TeLEOS-1—The TelEOS-1 raster type can be added.TH-01—The TH-01 raster type can be added.UAV/UAS—The UAV/UAS raster type can be added.Vision-1—The Vision-1 raster type can be added.WorldView-1—The WorldView-1 raster type can be added.WorldView-2— The WorldView-2 raster type can be added.WorldView-3—The WorldView-3 raster type can be added.WorldView-4—The WorldView-4 raster type can be added.ZY1-02C HRC—The ZY1-02C HRC raster type can be added.ZY1-02C PMS—The ZY1-02C PMS raster type can be added.ZY3-CRESDA—The ZY3-CRESDA raster type can be added.ZY3-SASMAC—The ZY3-SASMAC raster type can be added. | String |
| editor_tracking(Optional) | Specifies whether editor tracking will be enabled.Editor tracking can help you maintain accountability and enforce quality-control standards. NO_EDITOR_TRACKING—Editor tracking will not be enabled. This is the default.EDITOR_TRACKING—Editor tracking will be enabled.If the VIEW keyword is specified for the side_tables parameter, editor tracking will be automatically enabled. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AlterMosaicDatasetSchema(in_mosaic_dataset, {side_tables}, {raster_type_names}, {editor_tracking})
```

### Example 2

```python
import arcpy
arcpy.AlterMosaicDatasetSchema_management(
    "C:/folder/mySDEcon.sde/md_01", 
    "ANALYSIS;BOUNDARY;LEVELS;OVERVIEW;VIEW",
    "QuickBird;IKONOS", "EDITOR_TRACKING")
```

### Example 3

```python
import arcpy
arcpy.AlterMosaicDatasetSchema_management(
    "C:/folder/mySDEcon.sde/md_01", 
    "ANALYSIS;BOUNDARY;LEVELS;OVERVIEW;VIEW",
    "QuickBird;IKONOS", "EDITOR_TRACKING")
```

### Example 4

```python
#Alter Mosaic Dataset Schema mainly works on SDE mosaic datasets. The 
#selected side tables of mosaic dataset will be created. If there are
#raster type settings, metadata fields will be create for that raster type.

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
mosaicds = "sdeserver.sde/mosaicds"
ops = "ANALYSIS;BOUNDARY;LEVELS;LOG;OVERVIEW"
rastypes = "QuickBird;IKONOS;Match-AT"

arcpy.AlterMosaicDatasetSchema_management(mosaicds, ops, rastypes)
```

### Example 5

```python
#Alter Mosaic Dataset Schema mainly works on SDE mosaic datasets. The 
#selected side tables of mosaic dataset will be created. If there are
#raster type settings, metadata fields will be create for that raster type.

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
mosaicds = "sdeserver.sde/mosaicds"
ops = "ANALYSIS;BOUNDARY;LEVELS;LOG;OVERVIEW"
rastypes = "QuickBird;IKONOS;Match-AT"

arcpy.AlterMosaicDatasetSchema_management(mosaicds, ops, rastypes)
```

---

## Alter Version (Data Management)

## Summary

Alters the properties of a geodatabase version.

## Usage

- Versioning tools only work with enterprise geodatabase data. File geodatabases do not support versioning.
- When the input workspace is a database connection file, the version name, access, and target owner properties of a branch version can be altered by the geodatabase administrator. The version description can only be altered by the version owner.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The database connection file to the enterprise, workgroup, or desktop geodatabase where the version to be altered is located.For branch versioning, use a feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or the feature layer portal item. | Workspace |
| Input Version | The name of the version to be altered. If altering a branch version from a database connection connected as the geodatabase administrator, the version name must also include the service name, for example, myservice.versionowner.versionname. | String |
| Version Name(Optional) | The new name of the version. | String |
| Version Description(Optional) | The new description of the version. | String |
| Access Permission(Optional) | Specifies the access permission for the version. If no value is specified, the access permission will not be updated.Private—Only the owner can view the version and modify available feature classes.Public—Any user can view the version and modify available feature classes. Protected—Any user can view the version, but only the owner can modify available feature classes. | String |
| Target Owner (Optional) | The name of the portal user to which the version ownership will be transferred. Ensure that the target owner user exists; the tool does not check the validity of the owner name specified. This parameter is only applicable for branch versions. | String |
| in_workspace | The database connection file to the enterprise, workgroup, or desktop geodatabase where the version to be altered is located. The default is to use the workspace defined in the Current Workspace environment.For branch versioning, use a feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or the feature layer portal item. | Workspace |
| in_version | The name of the version to be altered. If altering a branch version from a database connection connected as the geodatabase administrator, the version name must also include the service name, for example, myservice.versionowner.versionname. | String |
| name(Optional) | The new name of the version. | String |
| description(Optional) | The new description of the version. | String |
| access(Optional) | Specifies the access permission for the version. If no value is specified, the access permission will not be updated.PRIVATE—Only the owner can view the version and modify available feature classes.PUBLIC—Any user can view the version and modify available feature classes. PROTECTED—Any user can view the version, but only the owner can modify available feature classes. | String |
| target_owner(Optional) | The name of the portal user to which the version ownership will be transferred. Ensure that the target owner user exists; the tool does not check the validity of the owner name specified. This parameter is only applicable for branch versions. | String |

## Code Samples

### Example 1

```python
arcpy.management.AlterVersion(in_workspace, in_version, {name}, {description}, {access}, {target_owner})
```

### Example 2

```python
# Description: Changes the name of a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/ConnectionFiles/mygeodatabase@gdb.sde"
versionName = "myVersion"
newName = "myVersion2"

# Run AlterVersion
arcpy.management.AlterVersion(inWorkspace, versionName, newName, "#", "PUBLIC")
```

### Example 3

```python
# Description: Changes the name of a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/ConnectionFiles/mygeodatabase@gdb.sde"
versionName = "myVersion"
newName = "myVersion2"

# Run AlterVersion
arcpy.management.AlterVersion(inWorkspace, versionName, newName, "#", "PUBLIC")
```

### Example 4

```python
# Description: Changes the description of a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "https://myserver.mydomain.com/server/rest/services/MyService/FeatureServer"
versionName = "portaluser1.myVersion"
newDesc = "Ready for reconcile and post"

# Sign in to ArcGIS Enterprise
arcpy.SignInToPortal("https://myserver.mydomain.com/portal", 'portaluser1', 'my.password')

# Run AlterVersion
arcpy.management.AlterVersion(inWorkspace, versionName, "", newDesc, "")
```

### Example 5

```python
# Description: Changes the description of a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "https://myserver.mydomain.com/server/rest/services/MyService/FeatureServer"
versionName = "portaluser1.myVersion"
newDesc = "Ready for reconcile and post"

# Sign in to ArcGIS Enterprise
arcpy.SignInToPortal("https://myserver.mydomain.com/portal", 'portaluser1', 'my.password')

# Run AlterVersion
arcpy.management.AlterVersion(inWorkspace, versionName, "", newDesc, "")
```

---

## Analyze Control Points (Data Management)

## Summary

Analyzes the control point coverage and identifies the areas that need additional control points to improve the block adjust result.

## Usage

- You can specify a mask to either exclude or include certain areas.
- Specify a minimum overlap area so you do not end up with small slivers to analyze.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset against which to analyze the control points. | Mosaic Dataset; Mosaic Layer |
| Input Control Points | The input control point feature class.It is normally created from the Compute Tie Points or the Compute Control Points tool. | Feature Layer |
| Output Control Point Coverage Feature Class | A polygon feature class output that contains the control point coverage and the percentage of the area within the corresponding image. | Feature Class |
| Output Overlap Feature Class | A polygon feature class output that contains all the overlap areas between images. | Feature Class |
| Input Mask(Optional) | A polygon feature class used to exclude areas that you do not want in the analysis of the control points computation.The mask field can control the inclusion or exclusion of areas. A value of 1 indicates that the areas defined by the polygons (inside) will be excluded from the computation. A value of 2 indicates the defined polygons (inside) will be included in the computation while areas outside of the polygons will be excluded. | Feature Layer |
| Minimum Overlap Area (Optional) | Specify the minimum percent that the overlap area must be, in relation to the image. Areas that are lower than the specified percent threshold will be excluded from the analysis. Ensure that you do not have areas that are too small; otherwise, you will have small slivers being analyzed. | Double |
| Maximum Overlap Level (Optional) | The maximum number of images that can be overlapped when analyzing the control points.For example, if there are four images in your mosaic dataset, and a maximum overlap value of 3 was specified, then there are ten different combinations that will appear in the Overlap Window. If the four images were named i1, i2, i3, and i4, the ten combinations that would appear are [i1, i2, i3], [i1 i2 i4], [i1 i3 i4], [i2 i3 i4], [i1, i2], [i1, i3], [i1, i4], [i2, i3], [i2, i4], and [i3, i4]. | Long |
| in_mosaic_dataset | The input mosaic dataset against which to analyze the control points. | Mosaic Dataset; Mosaic Layer |
| in_control_points | The input control point feature class.It is normally created from the Compute Tie Points or the Compute Control Points tool. | Feature Layer |
| out_coverage_table | A polygon feature class output that contains the control point coverage and the percentage of the area within the corresponding image. | Feature Class |
| out_overlap_table | A polygon feature class output that contains all the overlap areas between images. | Feature Class |
| in_mask_dataset(Optional) | A polygon feature class used to exclude areas that you do not want in the analysis of the control points computation.The mask field can control the inclusion or exclusion of areas. A value of 1 indicates that the areas defined by the polygons (inside) will be excluded from the computation. A value of 2 indicates the defined polygons (inside) will be included in the computation while areas outside of the polygons will be excluded. | Feature Layer |
| minimum_area(Optional) | Specify the minimum percent that the overlap area must be, in relation to the image. Areas that are lower than the specified percent threshold will be excluded from the analysis. Ensure that you do not have areas that are too small; otherwise, you will have small slivers being analyzed. | Double |
| maximum_level(Optional) | The maximum number of images that can be overlapped when analyzing the control points.For example, if there are four images in your mosaic dataset, and a maximum overlap value of 3 was specified, then there are ten different combinations that will appear in the Overlap Window. If the four images were named i1, i2, i3, and i4, the ten combinations that would appear are [i1, i2, i3], [i1 i2 i4], [i1 i3 i4], [i2 i3 i4], [i1, i2], [i1, i3], [i1, i4], [i2, i3], [i2, i4], and [i3, i4]. | Long |

## Code Samples

### Example 1

```python
arcpy.management.AnalyzeControlPoints(in_mosaic_dataset, in_control_points, out_coverage_table, out_overlap_table, {in_mask_dataset}, {minimum_area}, {maximum_level})
```

### Example 2

```python
import arcpy
arcpy.AnalyzeControlPoints_management(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePts", 
     "c:/BD/BD.gdb/out_coverage", "c:/BD/BD.gdb/out_overlap", 
     "c:/BD/BD.gdb/mask", 5 )
```

### Example 3

```python
import arcpy
arcpy.AnalyzeControlPoints_management(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePts", 
     "c:/BD/BD.gdb/out_coverage", "c:/BD/BD.gdb/out_overlap", 
     "c:/BD/BD.gdb/mask", 5 )
```

### Example 4

```python
#analyze control points
import arcpy
arcpy.env.workspace = "c:/workspace"

#analyze the control points using a mask
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_coverage = "BD.gdb/out_overage"
out_overlap = "BD.gdb/out_overlap"
in_mask = "BD.gdb/mask"

arcpy.AnalyzeControlPoints_management(mdName, in_controlPoint, 
     out_coverage, out_overlap, in_mask, 5)
```

### Example 5

```python
#analyze control points
import arcpy
arcpy.env.workspace = "c:/workspace"

#analyze the control points using a mask
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_coverage = "BD.gdb/out_overage"
out_overlap = "BD.gdb/out_overlap"
in_mask = "BD.gdb/mask"

arcpy.AnalyzeControlPoints_management(mdName, in_controlPoint, 
     out_coverage, out_overlap, in_mask, 5)
```

---

## Analyze Datasets (Data Management)

## Summary

Updates database statistics of base tables, delta tables, and archive tables, along with the statistics on the indexes of those tables. This tool is used in enterprise geodatabases to help get optimal performance from the relational database management system (RDBMS) query optimizer. Stale statistics can affect geodatabase performance.

## Usage

- The input workspace must be a database or enterprise geodatabase. The Analyze Datasets tool does not work with file or mobile geodatabases.
- After data loading, deleting, updating, and compressing operations, it is important to update RDBMS statistics in the database.
- The Include System Tables parameter is used to determine whether the states and state lineage tables will be analyzed. These tables track traditional versions. When this parameter is not checked, the tables are not analyzed; when it's checked, the tables are analyzed.
- This tool is not supported for use on SAP HANA.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database that contains the data to be analyzed. | Workspace |
| Include System Tables | Specifies whether statistics will be gathered on the states and state lineages tables. Unchecked—Statistics will not be gathered on the states and state lineages tables. This is the default.Checked—Statistics will be gathered on the states and state lineages tables.Note:You must be the geodatabase administrator for this parameter to be active.This parameter only applies to geodatabases. If the input workspace is a database, this parameter will be inactive. | Boolean |
| Datasets to Analyze (Optional) | The names of the datasets that will be analyzed. Only datasets that are owned by the connected user will be displayed. | String |
| Analyze Base Tables for Selected Dataset(s) (Optional) | Specifies whether the selected dataset base tables will be analyzed. Note:This parameter only applies to geodatabases. If the input workspace is a database, this parameter will be inactive.Checked—Statistics will be gathered for the base tables for the selected datasets. This is the default.Unchecked—Statistics will not be gathered for the base tables for the selected datasets. | Boolean |
| Analyze Delta Tables for Selected Dataset(s) (Optional) | Specifies whether the selected dataset delta tables will be analyzed. Note:This parameter only applies to geodatabases that contain traditional versions. If the input workspace is a database or does not participate in traditional versioning, this parameter will be inactive.Checked—Statistics will be gathered for the delta tables for the selected datasets. This is the default.Unchecked—Statistics will not be gathered for the delta tables for the selected datasets. | Boolean |
| Analyze Archive Tables for Selected Dataset(s) (Optional) | Specifies whether the selected dataset archive tables will be analyzed. Note:This parameter only applies to geodatabases that contain archive-enabled datasets. If the input workspace is a database, this parameter will be inactive.Checked—Statistics will be gathered for the archive tables for the selected datasets. This is the default.Unchecked—Statistics will not be gathered for the archive tables for the selected datasets. | Boolean |
| input_database | The database that contains the data to be analyzed. | Workspace |
| include_system | Specifies whether statistics will be gathered on the states and state lineages tables. Note:You must be the geodatabase administrator to use this parameter.This parameter only applies to geodatabases. If the input workspace is a database, this parameter will be ignored.NO_SYSTEM—Statistics will not be gathered on the states and state lineages tables. This is the default.SYSTEM—Statistics will be gathered on the states and state lineages tables. | Boolean |
| in_datasets[in_datasets,...](Optional) | The names of the datasets that will be analyzed. An individual dataset or a Python list of datasets can be used. Dataset names use paths relative to the input workspace; full paths are not valid input.The connected user must be the owner of the datasets provided. | String |
| analyze_base(Optional) | Specifies whether the selected dataset base tables will be analyzed.This parameter only applies to geodatabases. If the input workspace is a database, this parameter will be ignored.ANALYZE_BASE— Statistics will be gathered for the base tables for the selected datasets. This is the default.NO_ANALYZE_BASE— Statistics will not be gathered for the base tables for the selected datasets. | Boolean |
| analyze_delta(Optional) | Specifies whether the selected dataset delta tables will be analyzed.This parameter only applies to geodatabases that contain traditional versions. If the input workspace is a database or does not participate in traditional versioning, this parameter will be ignored.ANALYZE_DELTA— Statistics will be gathered for the delta tables for the selected datasets. This is the default.NO_ANALYZE_DELTA— Statistics will not be gathered for the delta tables for the selected datasets. | Boolean |
| analyze_archive(Optional) | Specifies whether the selected dataset archive tables will be analyzed.This parameter only applies to geodatabases that contain archive-enabled datasets. If the input workspace is a database, this parameter will be ignored.ANALYZE_ARCHIVE— Statistics will be gathered for the archive tables for the selected datasets. This is the default.NO_ANALYZE_ARCHIVE— Statistics will not be gathered for the archive tables for the selected datasets. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AnalyzeDatasets(input_database, include_system, {in_datasets}, {analyze_base}, {analyze_delta}, {analyze_archive})
```

### Example 2

```python
# Import system modules
import arcpy

arcpy.AnalyzeDatasets_management("c:/Connections/tenone@sde.sde",
                                 "SYSTEM",
                                 "gdb.city;gdb.state;map.lines",
                                 "ANALYZE_BASE",
                                 "ANALYZE_DELTA",
                                 "ANALYZE_ARCHIVE")
```

### Example 3

```python
# Import system modules
import arcpy

arcpy.AnalyzeDatasets_management("c:/Connections/tenone@sde.sde",
                                 "SYSTEM",
                                 "gdb.city;gdb.state;map.lines",
                                 "ANALYZE_BASE",
                                 "ANALYZE_DELTA",
                                 "ANALYZE_ARCHIVE")
```

### Example 4

```python
# Name: AnalyzeDatasets.py
# Description: analyzes all datasets in an enterprise geodatabase
#              for a given user.

# Import system modules
import arcpy
import os

# set workspace
# the user in this workspace must be the owner of the data to analyze.
workspace = "C:\\MyProject\\MyDataConnection.sde"

# set the workspace environment
arcpy.env.workspace = workspace

# NOTE: Analyze Datasets can accept a Python list of datasets.

# Get the user name for the workspace
userName = arcpy.Describe(workspace).connectionProperties.user

# Get a list of all the datasets the user owns by using a wildcard that incldues the user name
# First, get all the stand alone tables, feature classes and rasters.
dataList = arcpy.ListTables(userName + "*") + arcpy.ListFeatureClasses(userName + "*") + arcpy.ListRasters(userName + "*")

# Next, for feature datasets get all of the datasets and featureclasses
# from the list and add them to the master list.
for dataset in arcpy.ListDatasets(userName + "*", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses(userName + "*") + arcpy.ListDatasets(userName + "*")

# reset the workspace
arcpy.env.workspace = workspace

# Execute analyze datasets
# Note: to use the "SYSTEM" option the workspace user must be an administrator.
arcpy.AnalyzeDatasets_management(workspace, "NO_SYSTEM", dataList, "ANALYZE_BASE","ANALYZE_DELTA","ANALYZE_ARCHIVE")
print("Analyze Complete")
```

### Example 5

```python
# Name: AnalyzeDatasets.py
# Description: analyzes all datasets in an enterprise geodatabase
#              for a given user.

# Import system modules
import arcpy
import os

# set workspace
# the user in this workspace must be the owner of the data to analyze.
workspace = "C:\\MyProject\\MyDataConnection.sde"

# set the workspace environment
arcpy.env.workspace = workspace

# NOTE: Analyze Datasets can accept a Python list of datasets.

# Get the user name for the workspace
userName = arcpy.Describe(workspace).connectionProperties.user

# Get a list of all the datasets the user owns by using a wildcard that incldues the user name
# First, get all the stand alone tables, feature classes and rasters.
dataList = arcpy.ListTables(userName + "*") + arcpy.ListFeatureClasses(userName + "*") + arcpy.ListRasters(userName + "*")

# Next, for feature datasets get all of the datasets and featureclasses
# from the list and add them to the master list.
for dataset in arcpy.ListDatasets(userName + "*", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses(userName + "*") + arcpy.ListDatasets(userName + "*")

# reset the workspace
arcpy.env.workspace = workspace

# Execute analyze datasets
# Note: to use the "SYSTEM" option the workspace user must be an administrator.
arcpy.AnalyzeDatasets_management(workspace, "NO_SYSTEM", dataList, "ANALYZE_BASE","ANALYZE_DELTA","ANALYZE_ARCHIVE")
print("Analyze Complete")
```

---

## Analyze Mosaic Dataset (Data Management)

## Summary

Performs checks on a mosaic dataset for errors and possible improvements.

## Usage

- To examine the analysis results, open the mosaic dataset, right-click the mosaic dataset in the Contents pane and click Data > Analyze Mosaic Dataset. The Analyze Mosaic Dataset pane opens. Click the Messages tab, which allows you to view and interact with the errors, warnings, and messages. When you right-click on an error, warning, or message, a recommended action is listed.
- The errors and warnings are categorized in the following way:A high-priority error indicates that an issue exists where you may not be able to use the mosaic dataset. Some examples of a high-priority error include invalid visibility or inability to open a raster dataset.A medium-priority error indicates an issue that directly affects your mosaic dataset authoring experience. These errors will usually indicate that a geoprocessing tool will likely fail with these mosaic datasets. Some examples of a medium-priority error include an empty mosaic dataset or an invalid raster type ID.A low-priority error indicates that an issue might affect some of the user experiences related to the mosaic dataset or a derived image service. Some examples of a low-priority error include an invalid CenterXY field or broken paths.A high-priority warning indicates an issue related to performance or optimum display. Some examples of a high-priority warning include missing overviews, missing mosaic dataset statistics, and missing raster dataset statistics.A low-priority warning indicates an issue with recommendations that may not have been followed. These recommendations usually ensure smoother and efficient operation, but are not required. Some examples of a low-priority warning include bad metadata, a JPEG quality set too low, too many uncompressed pixels in the mosaic dataset, or missing raster pyramids.The returned messages report statistical facts related to the mosaic dataset.
- A high-priority error indicates that an issue exists where you may not be able to use the mosaic dataset. Some examples of a high-priority error include invalid visibility or inability to open a raster dataset.
- A medium-priority error indicates an issue that directly affects your mosaic dataset authoring experience. These errors will usually indicate that a geoprocessing tool will likely fail with these mosaic datasets. Some examples of a medium-priority error include an empty mosaic dataset or an invalid raster type ID.
- A low-priority error indicates that an issue might affect some of the user experiences related to the mosaic dataset or a derived image service. Some examples of a low-priority error include an invalid CenterXY field or broken paths.
- A high-priority warning indicates an issue related to performance or optimum display. Some examples of a high-priority warning include missing overviews, missing mosaic dataset statistics, and missing raster dataset statistics.
- A low-priority warning indicates an issue with recommendations that may not have been followed. These recommendations usually ensure smoother and efficient operation, but are not required. Some examples of a low-priority warning include bad metadata, a JPEG quality set too low, too many uncompressed pixels in the mosaic dataset, or missing raster pyramids.
- In the results table, many of the errors and warnings can be fixed by right-clicking on the issue.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset you want to analyze. | Mosaic Layer |
| Query Definition (Optional) | An SQL statement that confines your analysis to specific raster datasets within this mosaic dataset. | SQL Expression |
| Checks Performed(Optional) | Choose which parts of the mosaic dataset you want to analyze for known issues.Footprint geometry— Analyze the footprint geometry of each selected mosaic dataset item. This is checked on by default.Function chains— Analyze the function chains for each selected mosaic dataset item.Raster— Analyze the original raster datasets. This is checked on by default.Broken paths— Check for broken paths. This is checked on by default.Source validity— Analyze potential problems with the source data associated with each mosaic dataset item in the selected mosaic dataset. This is a good way to detect issues that may arise during synchronization workflows.Stale overviews— Overviews are stale when the underlying source data has changed. Once the mosaic dataset is analyzed, you can select which items are stale by right-clicking on the error and clicking Select Associated Items on the context menu.Pyramids— Analyze the raster pyramids associated with each mosaic dataset item in the selected mosaic dataset. Test for disconnected auxiliary files, which can occur when they are stored in a raster proxy location.Statistics— Test for disconnected auxiliary statistics files if they are stored in the raster proxy location. Analyze the covariance matrix associated with the raster, when the Gram-Schmidt pan-sharpening method is enabled. Analyze the radiometric pixel depth of a mosaic dataset item against the pixel depth of the mosaic dataset.Performance— Factors that increase performance include compression during transmission and caching items with many raster functions.Information— Generate general information about the mosaic dataset. | String |
| in_mosaic_dataset | The mosaic dataset you want to analyze. | Mosaic Layer |
| where_clause(Optional) | An SQL statement that confines your analysis to specific raster datasets within this mosaic dataset. | SQL Expression |
| checker_keywords[checker_keywords,...](Optional) | Choose which parts of the mosaic dataset you want to analyze for known issues. FOOTPRINT— Analyze the footprint geometry of each selected mosaic dataset item. This is checked on by default.FUNCTION— Analyze the function chains for each selected mosaic dataset item.RASTER— Analyze the original raster datasets. This is checked on by default.PATHS— Check for broken paths. This is checked on by default.SOURCE_VALIDITY— Analyze potential problems with the source data associated with each mosaic dataset item in the selected mosaic dataset. This is a good way to detect issues that may arise during synchronization workflows.STALE— Overviews are stale when the underlying source data has changed. Once the mosaic dataset is analyzed, you can select which items are stale by right-clicking on the error and clicking Select Associated Items on the context menu.PYRAMIDS— Analyze the raster pyramids associated with each mosaic dataset item in the selected mosaic dataset. Test for disconnected auxiliary files, which can occur when they are stored in a raster proxy location.STATISTICS— Test for disconnected auxiliary statistics files if they are stored in the raster proxy location. Analyze the covariance matrix associated with the raster, when the Gram-Schmidt pan-sharpening method is enabled. Analyze the radiometric pixel depth of a mosaic dataset item against the pixel depth of the mosaic dataset.PERFORMANCE— Factors that increase performance include compression during transmission and caching items with many raster functions.INFORMATION— Generate general information about the mosaic dataset. | String |

## Code Samples

### Example 1

```python
arcpy.management.AnalyzeMosaicDataset(in_mosaic_dataset, {where_clause}, {checker_keywords})
```

### Example 2

```python
import arcpy
arcpy.AnalyzeMosaicDataset_management(
     "\\cpu\data\analyze.gdb\mosaicds", "SensorName = 'Landsat-7-ETM+'",
     "FOOTPRINT;FUNCTION;RASTER;PATHS;PYRAMIDS")
```

### Example 3

```python
import arcpy
arcpy.AnalyzeMosaicDataset_management(
     "\\cpu\data\analyze.gdb\mosaicds", "SensorName = 'Landsat-7-ETM+'",
     "FOOTPRINT;FUNCTION;RASTER;PATHS;PYRAMIDS")
```

### Example 4

```python
#Analyze Mosaic Dataset with query definition
#Analyze all components of mosaic dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"


mdname = "analyzemd.gdb/mosaicds"
query = "SensorName = 'Landsat-7-ETM+'"
checks = "FOOTPRINT;FUNCTION;RASTER;PATHS;STALE;PYRAMIDS;PERFORMANCE"

arcpy.AnalyzeMosaicDataset_management(mdname, query, checks)
```

### Example 5

```python
#Analyze Mosaic Dataset with query definition
#Analyze all components of mosaic dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"


mdname = "analyzemd.gdb/mosaicds"
query = "SensorName = 'Landsat-7-ETM+'"
checks = "FOOTPRINT;FUNCTION;RASTER;PATHS;STALE;PYRAMIDS;PERFORMANCE"

arcpy.AnalyzeMosaicDataset_management(mdname, query, checks)
```

---

## Analyze (Data Management)

## Summary

Updates database statistics of business tables, feature tables, and delta tables, along with the statistics of those tables' indexes.

## Usage

- This tool can only be used with data stored in an enterprise geodatabase.
- After data loading, deleting, updating, and compressing operations, it is important to update RDBMS statistics in Oracle, SQL Server, DB2, or Informix databases.
- This tool updates the statistics of business tables, feature tables, raster tables, adds table, and deletes table, along with the statistics on those tables' indexes.
- The Components to Analyze parameter's Add Value button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run, or its derived data does not exist, the Components to Analyze parameter may not be populated with values. The Add Value button allows you to add expected value(s) so you can complete the tool's dialog box and continue to build your model.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The table or feature class to be analyzed. | Layer; Table View; Dataset |
| Components to Analyze | The component type to be analyzed.Business table—Updates business rules statistics. Feature table—Updates feature statistics. Raster table—Updates statistics on raster tables. Adds table—Updates statistics on added datasets. Deletes table—Updates statistics on deleted datasets. | String |
| in_dataset | The table or feature class to be analyzed. | Layer; Table View; Dataset |
| components[components,...] | The component type to be analyzed.BUSINESS—Updates business rules statistics. FEATURE—Updates feature statistics. RASTER—Updates statistics on raster tables. ADDS—Updates statistics on added datasets. DELETES—Updates statistics on deleted datasets. | String |

## Code Samples

### Example 1

```python
arcpy.management.Analyze(in_dataset, components)
```

### Example 2

```python
# Name: Analyze_Example.py
# Description: Gathers statistics for the indexes on the business table of the input dataset

# Import system modules
import arcpy

# Set local variables
inDataset = "c:/Connections/ninefour@gdb.sde/GDB.ctgPrimaryFeature"

# Execute Analyze
arcpy.Analyze_management(inDataset, "BUSINESS")
```

### Example 3

```python
# Name: Analyze_Example.py
# Description: Gathers statistics for the indexes on the business table of the input dataset

# Import system modules
import arcpy

# Set local variables
inDataset = "c:/Connections/ninefour@gdb.sde/GDB.ctgPrimaryFeature"

# Execute Analyze
arcpy.Analyze_management(inDataset, "BUSINESS")
```

---

## Analyze Toolbox For Version (Data Management)

## Summary

Analyzes the contents of a toolbox and identifies compatibility issues with previous versions of ArcGIS software.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input | The input toolbox (.tbx or .atbx file) that will be analyzed. The Python toolbox format (.pyt file) is not supported as an input. | Toolbox |
| Target Version | Specifies the software version that will be used for toolbox compatibility analysis.ArcGIS Desktop 10.6.0—ArcGIS Desktop 10.6.0 will be used for toolbox compatibility issue analysis.ArcGIS Desktop 10.7.0—ArcGIS Desktop 10.7.0 will be used for toolbox compatibility issue analysis.ArcGIS Desktop 10.8.0—ArcGIS Desktop 10.8.0 will be used for toolbox compatibility issue analysis.ArcGIS Desktop 10.8.2—ArcGIS Desktop 10.8.2 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.2—ArcGIS Pro 2.2 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.3—ArcGIS Pro 2.3 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.4—ArcGIS Pro 2.4 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.5—ArcGIS Pro 2.5 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.6—ArcGIS Pro 2.6 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.7—ArcGIS Pro 2.7 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.8—ArcGIS Pro 2.8 will be used for toolbox compatibility issue analysis.ArcGIS Pro 2.9—ArcGIS Pro 2.9 will be used for toolbox compatibility issue analysis.ArcGIS Pro 3.0—ArcGIS Pro 3.0 will be used for toolbox compatibility issue analysis.ArcGIS Pro 3.1—ArcGIS Pro 3.1 will be used for toolbox compatibility issue analysis.ArcGIS Pro 3.2—ArcGIS Pro 3.2 will be used for toolbox compatibility issue analysis.ArcGIS Pro 3.3—ArcGIS Pro 3.3 will be used for toolbox compatibility issue analysis. | String |
| Output File (Optional) | The text file that will be created containing the compatibility issues identified by the analyzers. | File |
| in_toolbox | The input toolbox (.tbx or .atbx file) that will be analyzed. The Python toolbox format (.pyt file) is not supported as an input. | Toolbox |
| version | Specifies the software version that will be used for toolbox compatibility analysis.10.6.0—ArcGIS Desktop 10.6.0 will be used for toolbox compatibility issue analysis.10.7.0—ArcGIS Desktop 10.7.0 will be used for toolbox compatibility issue analysis.10.8.0—ArcGIS Desktop 10.8.0 will be used for toolbox compatibility issue analysis.10.8.2—ArcGIS Desktop 10.8.2 will be used for toolbox compatibility issue analysis.2.2—ArcGIS Pro 2.2 will be used for toolbox compatibility issue analysis.2.3—ArcGIS Pro 2.3 will be used for toolbox compatibility issue analysis.2.4—ArcGIS Pro 2.4 will be used for toolbox compatibility issue analysis.2.5—ArcGIS Pro 2.5 will be used for toolbox compatibility issue analysis.2.6—ArcGIS Pro 2.6 will be used for toolbox compatibility issue analysis.2.7—ArcGIS Pro 2.7 will be used for toolbox compatibility issue analysis.2.8—ArcGIS Pro 2.8 will be used for toolbox compatibility issue analysis.2.9—ArcGIS Pro 2.9 will be used for toolbox compatibility issue analysis.3.0—ArcGIS Pro 3.0 will be used for toolbox compatibility issue analysis.3.1—ArcGIS Pro 3.1 will be used for toolbox compatibility issue analysis.3.2—ArcGIS Pro 3.2 will be used for toolbox compatibility issue analysis.3.3—ArcGIS Pro 3.3 will be used for toolbox compatibility issue analysis. | String |
| report(Optional) | The text file that will be created containing the compatibility issues identified by the analyzers. | File |

## Code Samples

### Example 1

```python
arcpy.management.AnalyzeToolboxForVersion(in_toolbox, version, {report})
```

### Example 2

```python
import arcpy
arcpy.management.AnalyzeToolboxForVersion(r"C:\toolboxes\MyTools.atbx", "3.0")
```

### Example 3

```python
import arcpy
arcpy.management.AnalyzeToolboxForVersion(r"C:\toolboxes\MyTools.atbx", "3.0")
```

---

## Analyze Tools For Pro (Data Management)

## Summary

Analyzes Python scripts and custom geoprocessing tools and toolboxes for functionality that is not supported in ArcGIS Pro.

## Usage

- Any issues identified will be included in the tool messages as warnings.
- ArcGIS Pro uses Python 3. While many scripts may continue to work as they are, others will not. For tips to update Python scripts to work in ArcGIS Pro, see Python migration from 10.x to ArcGIS Pro.
- For Python 2 to Python 3 upgrade issues, Analyze Tools For Pro uses the Python 2to3 utility to review Python code. The Python 2to3 utility can be used to review Python 2.x code using a series of fixers that show how the code can be transformed to valid Python 3.x code.The Python 2to3 utility can be used directly from the command prompt and has a set of options for analyzing Python code that are not available through Analyze Tools For Pro, such as applying only specific fixers or updating the code in place. For more information, see 2to3 — Automated Python 2 to 3 code translation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input | The input can be a geoprocessing toolbox or Python file. | Toolbox; String; File |
| Output File (Optional) | An output text file that includes all issues. | File |
| input | The input can be a geoprocessing toolbox, Python file, or tool name.Note:If a tool name is specified, the tool must be loaded first using the arcpy.ImportToolbox function to be recognized. Include the toolbox alias with tool names. | Toolbox; String; File |
| report(Optional) | An output text file that includes all issues. | File |

## Code Samples

### Example 1

```python
arcpy.management.AnalyzeToolsForPro(input, {report})
```

### Example 2

```python
import arcpy
arcpy.management.AnalyzeToolsForPro('c:/tools/scripts/myutils.py', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

### Example 3

```python
import arcpy
arcpy.management.AnalyzeToolsForPro('c:/tools/scripts/myutils.py', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

### Example 4

```python
import arcpy
arcpy.management.AnalyzeToolsForPro('c:/tools/scripts/mytools.tbx', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

### Example 5

```python
import arcpy
arcpy.management.AnalyzeToolsForPro('c:/tools/scripts/mytools.tbx', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

### Example 6

```python
import arcpy

arcpy.ImportToolbox('c:/tools/scripts/mytools.tbx')
arcpy.management.AnalyzeToolsForPro('mytool_tools', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

### Example 7

```python
import arcpy

arcpy.ImportToolbox('c:/tools/scripts/mytools.tbx')
arcpy.management.AnalyzeToolsForPro('mytool_tools', 'c:/temp/analyze_report.txt')

print(arcpy.GetMessages(1))
```

---

## Append Annotation Feature Classes (Data Management)

## Summary

Creates a geodatabase annotation feature class or appends to an existing annotation feature class by combining annotation from multiple input geodatabase annotation feature classes into a single feature class with annotation classes.

## Usage

- When appending multiple annotation feature classes into a new annotation feature class, the input annotation feature classes must reside in the same database.
- If you select geodatabase annotation features in ArcGIS Pro or build a definition query, only those features will be appended to the output feature class.
- When appending feature-linked annotation feature classes, all the input annotation feature classes must be linked to the same feature class.
- If you select an existing output annotation feature class, the features will be appended to that feature class, and the tool will project the annotation features in the destination spatial reference.
- When appending annotation feature classes with multiple annotation subclasses, the subclasses will be merged if their properties match.
- This tool handles annotation feature classes with different schemas. If the annotation feature classes have the same schema, use the Append tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input features | The input annotation features that will form an annotation class in the output feature class. | Feature Layer |
| Output feature class | A new or existing annotation feature class that will contain an annotation class for each input annotation feature class. | Feature Class |
| Reference scale | The reference scale set in the output feature class. Input features created at a different reference scale will be transformed to match this output reference scale. | Double |
| Create a single annotation class(Optional) | Specifies how annotation features will be added to the output feature class. Checked—All annotation features will be aggregated into one annotation class in the output feature class.Unchecked—Separate annotation classes will be created for each input annotation class in the output feature class unless the classes are named the same and have the same properties. In this case, they will be merged. This is the default. | Boolean |
| Require symbols to be selected from the symbol table(Optional) | Specifies how symbols can be selected for newly created annotation features.Checked—Only the list of symbols in the symbol collection of the output feature class can be used when creating annotation features.Unchecked—Any symbology can be used when creating annotation features. This is the default. | Boolean |
| Create annotation when new features are added (Feature-linked only)(Optional) | Specifies whether feature-linked annotation will be created when a feature is added.Checked—Feature-linked annotation will be created using the label engine when a linked feature is added. This is the default.Unchecked—Feature-linked annotation will not be created when a feature is added. | Boolean |
| Update annotation when the shape of the linked feature is modified (Feature-linked only)(Optional) | Specifies whether feature-linked annotation will be updated when a linked feature changes.Checked—Feature-linked annotation will be updated using the label engine when a linked feature changes. This is the default.Unchecked—Feature-linked annotation will not be updated when a linked feature changes. | Boolean |
| input_features[input_features,...] | The input annotation features that will form an annotation class in the output feature class. | Feature Layer |
| output_featureclass | A new or existing annotation feature class that will contain an annotation class for each input annotation feature class. | Feature Class |
| reference_scale | The reference scale set in the output feature class. Input features created at a different reference scale will be transformed to match this output reference scale. | Double |
| create_single_class(Optional) | Specifies how annotation features will be added to the output feature class.ONE_CLASS_ONLY—All annotation features will be aggregated into one annotation class in the output feature class.CREATE_CLASSES—Separate annotation classes will be created for each input annotation class in the output feature class unless the classes are named the same and have the same properties. In this case, they will be merged. This is the default. | Boolean |
| require_symbol_from_table(Optional) | Specifies how symbols can be selected for newly created annotation features.REQUIRE_SYMBOL—Only the list of symbols in the symbol collection of the output feature class can be used when creating annotation features.NO_SYMBOL_REQUIRED—Any symbology can be used when creating annotation features. This is the default. | Boolean |
| create_annotation_when_feature_added(Optional) | Specifies whether feature-linked annotation will be created when a feature is added.AUTO_CREATE—Feature-linked annotation will be created using the label engine when a linked feature is added. The is the default.NO_AUTO_CREATE—Feature-linked annotation will not be created when a feature is added. | Boolean |
| update_annotation_when_feature_modified(Optional) | Specifies whether feature-linked annotation will be updated when a linked feature changes.AUTO_UPDATE—Feature-linked annotation will be updated using the label engine when a linked feature changes. This is the default.NO_AUTO_UPDATE—Feature-linked annotation will not be updated when a linked feature changes. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AppendAnnotation(input_features, output_featureclass, reference_scale, {create_single_class}, {require_symbol_from_table}, {create_annotation_when_feature_added}, {update_annotation_when_feature_modified})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Cobourg.gdb"
arcpy.management.AppendAnnotation(["highways", "roads"], "transport_anno", 1200, 
                                  "CREATE_CLASSES", "NO_SYMBOL_REQUIRED", 
                                  "AUTO_CREATE", "AUTO_UPDATE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Cobourg.gdb"
arcpy.management.AppendAnnotation(["highways", "roads"], "transport_anno", 1200, 
                                  "CREATE_CLASSES", "NO_SYMBOL_REQUIRED", 
                                  "AUTO_CREATE", "AUTO_UPDATE")
```

### Example 4

```python
# Name: AppendAnnotation_Example.py
# Description: Use AppendAnnotation to append annotation feature classes in a 
#              geodatabase

# import system modules 
import arcpy
import os

# Set environment settings - user specified
# User input geodatabase for annotation location - eg. C:/data/roads.gdb
arcpy.env.workspace = input('Location of geodatabase annotation: ')

# Create list of annotation feature classes within the geodatabase
fcList = arcpy.ListFeatureClasses("", "ANNOTATION")

# Set variables
# User input output feature class name - eg. appendedroadsAnno
outFeatureClass = arcpy.env.workspace + os.sep + \
                  input('Output annotation feature class name: ')
refScale = 1200
createClasses = "CREATE_CLASSES"
symbolReq = "NO_SYMBOL_REQUIRED"
autoCreate = "AUTO_CREATE"
autoUpdate = "AUTO_UPDATE"

# Process: Append the annotation feature classes
print("Appending annotation feature classes...")
arcpy.management.AppendAnnotation(fcList, outFeatureClass, refScale, 
                                  createClasses, symbolReq, autoCreate, 
                                  autoUpdate)

print("Annotation feature classes in {} have been appended into {}".format(
    arcpy.env.workspace, outFeatureClass))
```

### Example 5

```python
# Name: AppendAnnotation_Example.py
# Description: Use AppendAnnotation to append annotation feature classes in a 
#              geodatabase

# import system modules 
import arcpy
import os

# Set environment settings - user specified
# User input geodatabase for annotation location - eg. C:/data/roads.gdb
arcpy.env.workspace = input('Location of geodatabase annotation: ')

# Create list of annotation feature classes within the geodatabase
fcList = arcpy.ListFeatureClasses("", "ANNOTATION")

# Set variables
# User input output feature class name - eg. appendedroadsAnno
outFeatureClass = arcpy.env.workspace + os.sep + \
                  input('Output annotation feature class name: ')
refScale = 1200
createClasses = "CREATE_CLASSES"
symbolReq = "NO_SYMBOL_REQUIRED"
autoCreate = "AUTO_CREATE"
autoUpdate = "AUTO_UPDATE"

# Process: Append the annotation feature classes
print("Appending annotation feature classes...")
arcpy.management.AppendAnnotation(fcList, outFeatureClass, refScale, 
                                  createClasses, symbolReq, autoCreate, 
                                  autoUpdate)

print("Annotation feature classes in {} have been appended into {}".format(
    arcpy.env.workspace, outFeatureClass))
```

---

## Append Control Points (Data Management)

## Summary

Combines control points to an existing control point table.

## Usage

- If both the Z Value Field Name and the Input DEM parameters are set, the Z Value Field Name takes priority. If neither the Z Value Field Name nor the Input DEM parameter is set, the z-value is set to 0 for all ground control points (GCP) and check points.
- Use caution when invoking the Append Option — it is applicable only when the tie points in the input and target control point table have the same transformation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Control Points | The input control point table. This is usually the output from the Compute Tie Points tool. | Feature Class; Feature Layer |
| Input Control Points | A point feature class that stores control points. It could be the control point table created from the Compute Control Points tool, the Compute Tie Points tool, or a point feature class that has ground control points. | Feature Class; Feature Layer; File; String |
| Z Value Field Name (Optional) | The field that stores the control point z-values. If both the Z Value Field Name and the Input DEM parameters are set, the Z value field is used. If neither the Z Value Field Name nor the Input DEM parameter is set, the z-value is set to 0 for all ground control points and check points. | Field |
| Tag Field Name (Optional) | A field in the input control point table that has a unique value. This field will be added to the target control point table, where the tag field can be used to bring in identifiers associated with ground control points. | Field |
| Input DEM (Optional) | A DEM to use to obtain the z-value for the control points in the input control point table.If both the Z Value Field Name and Input DEM parameters are set, the Z value field is used. If neither the Z Value Field Name nor the Input DEM parameter is set, the z-value is set to 0 for all ground control points and check points. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| XY Accuracy (Optional) | The input accuracy for the X and Y coordinates. The accuracy is in the same units as the Input Control Points.This information should be provided by the data provider. If the accuracy information is not available, leave this optional parameter blank. | Double |
| Z Accuracy (Optional) | The input accuracy for the vertical coordinates. The accuracy is in the units of the Input Control Points.This information should be provided by the data provider. If the accuracy information is not available, leave this optional parameter blank. | Double |
| Geoid(Optional) | The geoid correction is required by rational polynomial coefficients (RPC) that reference ellipsoidal heights. Most elevation datasets are referenced to sea level orthometric heights, so this correction would be required in these cases to convert to ellipsoidal heights. Unchecked—No geoid correction is made. Use this option only if your DEM is already expressed in ellipsoidal heights. This is the default.Checked—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |
| Area of Interest (Optional) | Defines an area of interest extent by entering minimum and maximum x- and y-coordinates in the spatial reference of the input control point table. | Envelope; Feature Layer; Feature Class |
| Append Option (Optional) | Specifies how control points will be appended to the control point table.Add all points—Add all points in the input control point table to the target control point table, including GCPs, check points, and all tie points. This is the default.Add GCPs only—Add only GCPs in the input point table to the target control point table.Add GCPs and tie points—Add GCPs and tie points specifically associated with the GCPs to the target control point table.Use caution with this option—it is applicable only when the tie points in the input and target control point table have the same transformation. The tie points might not be in the desired positions if they were computed using a different adjustment process. | String |
| in_master_control_points | The input control point table. This is usually the output from the Compute Tie Points tool. | Feature Class; Feature Layer |
| in_input_control_points | A point feature class that stores control points. It could be the control point table created from the Compute Control Points tool, the Compute Tie Points tool, or a point feature class that has ground control points. | Feature Class; Feature Layer; File; String |
| in_z_field(Optional) | The field that stores the control point z-values. If both the Z Value Field Name and the Input DEM parameters are set, the Z value field is used. If neither the Z Value Field Name nor the Input DEM parameter is set, the z-value is set to 0 for all ground control points and check points. | Field |
| in_tag_field(Optional) | A field in the input control point table that has a unique value. This field will be added to the target control point table, where the tag field can be used to bring in identifiers associated with ground control points. | Field |
| in_dem(Optional) | A DEM to use to obtain the z-value for the control points in the input control point table.If both the Z Value Field Name and Input DEM parameters are set, the Z value field is used. If neither the Z Value Field Name nor the Input DEM parameter is set, the z-value is set to 0 for all ground control points and check points. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| in_xy_accuracy(Optional) | The input accuracy for the X and Y coordinates. The accuracy is in the same units as the in_input_control_points.This information should be provided by the data provider. If the accuracy information is not available, skip this optional parameter. | Double |
| in_z_accuracy(Optional) | The input accuracy for the vertical coordinates. The accuracy is in the units of the in_input_control_points.This information should be provided by the data provider. If the accuracy information is not available, skip this optional parameter. | Double |
| Geoid(Optional) | The geoid correction is required by rational polynomial coefficients (RPC) that reference ellipsoidal heights. Most elevation datasets are referenced to sea level orthometric heights, so this correction would be required in these cases to convert to ellipsoidal heights. NONE—No geoid correction is made. Use NONE only if your DEM is already expressed in ellipsoidal heights. This is the default.GEOID—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |
| area_of_interest(Optional) | Defines an area of interest extent by entering minimum and maximum x- and y-coordinates in the spatial reference of the input control point table. | Envelope; Feature Layer; Feature Class |
| append_option(Optional) | Specifies how control points will be appended to the control point table.ALL—Add all points in the input control point table to the target control point table, including GCPs, check points, and all tie points. This is the default.GCP—Add only GCPs in the input point table to the target control point table.GCPSET—Add GCPs and tie points specifically associated with the GCPs to the target control point table.Use caution with this option—it is applicable only when the tie points in the input and target control point table have the same transformation. The tie points might not be in the desired positions if they were computed using a different adjustment process. | String |

## Code Samples

### Example 1

```python
arcpy.management.AppendControlPoints(in_master_control_points, in_input_control_points, {in_z_field}, {in_tag_field}, {in_dem}, {in_xy_accuracy}, {in_z_accuracy}, {Geoid}, {area_of_interest}, {append_option})
```

### Example 2

```python
#===========================
#AppendControlPoints
'''Usage: AppendControlPoints_management(in_master_control_points, 
{in_z_field}, {in_tag_field}, {in_dem}, {in_xy_accuracy}, 
{in_z_accuracy}, {Geoid}, {area_of_interest}, {append_option}
'''

import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.management.AppendControlPoints("calval_allpoints_dem1", 
    "C:\test\calval_allpoints_dem1", "Score", "Ways", "calval",1, 2, "NONE", 
"-79.6407162269889 43.4853802421312 -79.094324938576 44.0836924137218", "GCP")
```

### Example 3

```python
#===========================
#AppendControlPoints
'''Usage: AppendControlPoints_management(in_master_control_points, 
{in_z_field}, {in_tag_field}, {in_dem}, {in_xy_accuracy}, 
{in_z_accuracy}, {Geoid}, {area_of_interest}, {append_option}
'''

import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.management.AppendControlPoints("calval_allpoints_dem1", 
    "C:\test\calval_allpoints_dem1", "Score", "Ways", "calval",1, 2, "NONE", 
"-79.6407162269889 43.4853802421312 -79.094324938576 44.0836924137218", "GCP")
```

### Example 4

```python
#append control points

import arcpy
arcpy.env.workspace = "c:/workspace"

#append the control points and tie points
target = "BD.gdb/tiePoints"
in_controlPoints = "BD.gdb/controlPoints"
dem = "BD.gdb/dem"
AOI = "-79.6407162269889 43.4853802421312 -79.094324938576 44.0836924137218"
AppOpt = "GCP"


arcpy.management.AppendControlPoints(target, in_controlPoints, "", dem, "", "", "", AOI, appOpt)
```

### Example 5

```python
#append control points

import arcpy
arcpy.env.workspace = "c:/workspace"

#append the control points and tie points
target = "BD.gdb/tiePoints"
in_controlPoints = "BD.gdb/controlPoints"
dem = "BD.gdb/dem"
AOI = "-79.6407162269889 43.4853802421312 -79.094324938576 44.0836924137218"
AppOpt = "GCP"


arcpy.management.AppendControlPoints(target, in_controlPoints, "", dem, "", "", "", AOI, appOpt)
```

---

## Append (Data Management)

## Summary

Appends to, or optionally updates, an existing target dataset with multiple input datasets. Input datasets can be feature classes, tables, shapefiles, rasters, or annotation or dimension feature classes.

## Usage

- Use this tool to add new features or other data from multiple datasets to an existing dataset. This tool can append point, line, or polygon feature classes, tables, rasters, annotation feature classes, or dimension feature classes to an existing dataset of the same type. For example, several tables can be appended to an existing table, or several rasters can be appended to an existing raster dataset, but a line feature class cannot be appended to a point feature class.
- Use the Field Map parameter to control how the attribute information from the input dataset fields is transferred to the target dataset. The Field Map parameter can only be used if Use the Field Map to reconcile schema differences is specified for the Schema Type parameter.
- Use the Field Map parameter to map or match the fields from the input datasets to fields in the target dataset. Use an action to determine how the values from one or multiple fields from the input dataset will be mapped to a single field from the target dataset.The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters. Standard Deviation is not a valid option for single input values.Use the Export option to save a field map as a .fieldmap file.Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the target field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.Fields can also be mapped in a Python script.
- Use an action to determine how the values from one or multiple fields from the input dataset will be mapped to a single field from the target dataset.
- The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.
- When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters.
- Standard Deviation is not a valid option for single input values.
- Use the Export option to save a field map as a .fieldmap file.
- Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.
- Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the target field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.
- Fields can also be mapped in a Python script.
- In Python, when using the FieldMappings object for the field_mapping parameter, add the fields from the target dataset first. Input fields are mapped to the schema of the target fields. When the input fields are added first, the field map may reset or behave unexpectedly.fieldmappings = arcpy.FieldMappings() fieldmappings.addTable(target) fieldmappings.addTable(input)
- This tool will not planarize features when they are added to the target dataset. All features from both the input feature class and the target feature class will remain intact after the append, even if the features overlap. To combine, or planarize, feature geometries, use the Union tool.
- If the Field Matching Type parameter is set to Input fields must match target fields, the schema of the input datasets must match that of the target dataset to append features.If the Field Matching Type parameter is set to Use the field map to reconcile field differences, the schema does not need to match. Fields from the input datasets that do not match the fields of the target dataset will not be mapped to the target dataset unless the mapping is set in the Field Map parameter. Fields in the target dataset that are not mapped to fields from the input datasets will contain null values. If the Field Matching Type parameter is set to Skip and warn if schema does not match, the schema of the input datasets must match that of the target dataset for features to be appended. If an input dataset contains fields that do not match fields in the target dataset, that input dataset will be omitted.
- The number of appended rows are displayed in the tool messages and returned by the Appended Row Count parameter.
- Because the data of the input datasets is written to an existing target dataset that has a predefined schema, you can't use the Field Map parameter to add or remove fields from the target dataset.
- You can use this tool to insert new records and update existing records in a target dataset in the same operation (equivalent to an UPSERT database operation). Use the Matching Fields for Update parameter to specify the fields from the input dataset to match to the fields in the target dataset. For records with matching field values, any fields in the target record will be updated with values from fields with the same name in the input record. The number of updated rows with matched fields are displayed in the tool messages and returned by the Updated Row Count parameter. Geometry will be updated if the Update Geometry parameter is checked.
- The input and target datasets may have different spatial references. When this is the case, the tool projects the input features to the target's coordinate system. For best results, specify an appropriate geographic transformation using the Geographic Transformations environment. If the target dataset is a map layer, the tool may apply a default transformation. In Python, there is no default transformation.
- This tool does not perform edge matching; no adjustment to the geometry of features will be made.
- Map layers can be used as Input Datasets parameter values. If a layer has a selection, only the selected records (features or table rows) will be used.
- You can't use multiple input layers with the same name in this tool. Instead, use the browse button in the Geoprocessing pane to browse to the full paths of each of the Input Datasets values.
- If an input dataset and the target dataset have the same name, the tool will have reduced performance.
- To use the Subtype parameter, the target dataset must have a defined subtype field and assigned subtype codes. In the Subtype parameter, provide a subtype description to assign that subtype to all new data that is appended to the target dataset.
- Use the Enforce Domains parameter to enforce attribute domains for fields in the target dataset. When this parameter is checked, the tool will honor the domain values defined in the target dataset. Data from the input fields that do not conform to these domain values will not be appended..
- This tool can be used directly on the back-end data store, server, or database for web feature layers to ensure optimal performance. For feature services that reference a geodatabase, this tool can be used with a database connection to perform large data loading operations directly to the underlying database. For hosted services on ArcGIS Enterprise, you can use the append operation from the feature layer item page or ArcGIS REST API. For optimal performance with hosted services on ArcGIS Online, administrators and service owners can use this tool with the ArcGIS REST API when the supportAppend property is true. Feature service append capabilities must be enabled for users who are not administrators to use the Append tool with the ArcGIS REST API.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | The input datasets containing the data to be appended to the target dataset. Input datasets can be point, line, or polygon feature classes, tables, rasters, annotation feature classes, or dimensions feature classes. Tables and feature classes can be combined. If a feature class is appended to a table, attributes will be transferred; however, the features will be dropped. If a table is appended to a feature class, the rows from the input table will have null geometry. | Table View; Raster Layer |
| Target Dataset | The existing dataset where the data of the input datasets will be appended. | Table View; Raster Layer |
| Field Matching Type(Optional) | Specifies whether the fields of the input datasets must match the fields of the target dataset for data to be appended.Input fields must match target fields—Fields of the input datasets must match the fields of the target dataset. An error will be returned if the fields do not match. Use the field map to reconcile field differences—Fields of the input datasets do not need to match the fields of the target dataset. Fields of the input datasets that do not match the fields of the target dataset will not be mapped to the target dataset unless the mapping is explicitly set in the Field Map parameter.Skip and warn if schema does not match—Fields of the input datasets must match the fields of the target dataset. If any of the input datasets contain fields that do not match the target dataset, that input dataset will be omitted with a warning message. | String |
| Field Map(Optional) | The field map parameter controls the transfer or mapping of fields from the input datasets to the target dataset. It can only be used when the Field Matching Type parameter is set to Use the field map to reconcile field differences.Because the input datasets are appended to an existing target dataset with predefined fields, you cannot add, remove, reorder, or change the properties of the fields in the field map. The field map can be used to combine values from one or more input fields into a single output field. | Field Mappings |
| Subtype(Optional) | The subtype description that will be assigned to all new data that is appended to the target dataset. | String |
| Expression (Optional) | The SQL expression that will be used to select a subset of the input datasets' records. If multiple input datasets are specified, they will all be evaluated using the expression. If no records match the expression for an input dataset, no records from that dataset will be appended to the target dataset. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |
| Matching Fields for Update (Optional) | The fields from the input datasets that will be used to match to the target dataset. If the values of these fields match, records from the input datasets will update the corresponding records of the target dataset. | Value Table |
| Update Geometry (Optional) | Specifies whether geometry in the target dataset will be updated with geometry from the input datasets if the Matching Fields for Update parameter field values match. Checked—Geometry in the target dataset will be updated if the Matching Fields for Update parameter field values match.Unchecked—Geometry will not be updated. This is the default. | Boolean |
| Enforce Domains (Optional) | Specifies whether field domain rules will be enforced.Checked—Field domain rules will be enforced. If a field cannot be updated, the field value will remain unchanged, and the tool messages will include a warning message.Unchecked—Field domain rules will not be enforced. This is the default | Boolean |
| inputs[inputs,...] | The input datasets containing the data to be appended to the target dataset. Input datasets can be point, line, or polygon feature classes, tables, rasters, annotation feature classes, or dimensions feature classes. Tables and feature classes can be combined. If a feature class is appended to a table, attributes will be transferred; however, the features will be dropped. If a table is appended to a feature class, the rows from the input table will have null geometry. | Table View; Raster Layer |
| target | The existing dataset where the data of the input datasets will be appended. | Table View; Raster Layer |
| schema_type(Optional) | Specifies whether the fields of the input datasets must match the fields of the target dataset for data to be appended.TEST—Fields of the input datasets must match the fields of the target dataset. An error will be returned if the fields do not match. NO_TEST—Fields of the input datasets do not need to match the fields of the target dataset. Fields of the input datasets that do not match the fields of the target dataset will not be mapped to the target dataset unless the mapping is explicitly set in the Field Map parameter.TEST_AND_SKIP—Fields of the input datasets must match the fields of the target dataset. If any of the input datasets contain fields that do not match the target dataset, that input dataset will be omitted with a warning message. | String |
| field_mapping(Optional) | The field map parameter controls the transfer or mapping of fields from the input datasets to the target dataset. It can only be used when the schema_type parameter is set to NO_TEST.Because the input datasets are appended to an existing target dataset with predefined fields, you cannot add, remove, reorder, or change the properties of the fields in the field map. The field map can be used to combine values from one or more input fields into a single output field.In Python, use the FieldMappings class to define this parameter. | Field Mappings |
| subtype(Optional) | The subtype description that will be assigned to all new data that is appended to the target dataset. | String |
| expression(Optional) | The SQL expression that will be used to select a subset of the input datasets' records. If multiple input datasets are specified, they will all be evaluated using the expression. If no records match the expression for an input dataset, no records from that dataset will be appended to the target dataset. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |
| match_fields[[target_field, input_field],...](Optional) | The fields from the input datasets that will be used to match to the target dataset. If the values of these fields match, records from the input datasets will update the corresponding records of the target dataset. | Value Table |
| update_geometry(Optional) | Specifies whether geometry in the target dataset will be updated with geometry from the input datasets if the match_fields parameter field values match.UPDATE_GEOMETRY—Geometry in the target dataset will be updated if the match_fields parameter field values match.NOT_UPDATE_GEOMETRY—Geometry will not be updated. This is the default. | Boolean |
| enforce_domains(Optional) | Specifies whether field domain rules will be enforced.ENFORCE_DOMAINS—Field domain rules will be enforced.NO_ENFORCE_DOMAINS—Field domain rules will not be enforced. This is the default. | Boolean |

## Code Samples

### Example 1

```python
fieldmappings = arcpy.FieldMappings() 
fieldmappings.addTable(target) 
fieldmappings.addTable(input)
```

### Example 2

```python
fieldmappings = arcpy.FieldMappings() 
fieldmappings.addTable(target) 
fieldmappings.addTable(input)
```

### Example 3

```python
arcpy.management.Append(inputs, target, {schema_type}, {field_mapping}, {subtype}, {expression}, {match_fields}, {update_geometry}, {enforce_domains})
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Append(["north.shp", "south.shp", "east.shp", "west.shp"], 
                        "wholecity.shp", "TEST")
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Append(["north.shp", "south.shp", "east.shp", "west.shp"], 
                        "wholecity.shp", "TEST")
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/data/zoning.gdb"
arcpy.management.Append("Zoning_update_2040", "Zoning_2020", "TEST", 
                        match_fields=[["ZONEID", "ZONEID"]], update_geometry="UPDATE_GEOMETRY")
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/data/zoning.gdb"
arcpy.management.Append("Zoning_update_2040", "Zoning_2020", "TEST", 
                        match_fields=[["ZONEID", "ZONEID"]], update_geometry="UPDATE_GEOMETRY")
```

### Example 8

```python
# Name: Append.py
# Description: Use the Append tool to combine several polygon feature classes

# Import system modules 
import arcpy
import os

# Set environment settings
arcpy.env.workspace = "C:/data/towns.gdb"

# Set local variables
outLocation = "C:/data/output.gdb"
outName = "MA_towns.shp"
schemaType = "NO_TEST"
fieldMappings = ""
subtype = ""

# Process: Append to an existing "amherst" polygon feature class
target = os.path.join(outLocation, "amherst")

# All polygon FCs in the workspace are MA town FCs, you want to append these
# to the target FC. The list will resemble ["amherst", "hadley", "pelham",
# "coldspring"]

fcList = arcpy.ListFeatureClasses("", "POLYGON")

# Create FieldMappings object to manage merge output fields
fieldMappings = arcpy.FieldMappings()

# Add the target table to the field mappings class to set the schema
fieldMappings.addTable(target)

# Add input fields for the town name to TOWNNAME field that matches the 
# target dataset since each input dataset has a different field name for 
# this info
fldMap = arcpy.FieldMap()
fldMap.addInputField("amherst","TOWNNAME")
fldMap.addInputField("hadley","NAME")
fldMap.addInputField("pelham","TOWN_NAME")
fldMap.addInputField("coldspring","TOWN")

# Set name of new output field "TOWNNAME"
townName = fldMap.outputField
townName.name, townName.aliasName, townName.type = "TOWNNAME", "TOWNNAME", "TEXT"
fldMap.outputField = townName

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap)

# Do the same for the POPULATION field
fldMap = arcpy.FieldMap()
fldMap.addInputField("amherst","POPULATION")
fldMap.addInputField("hadley","POP")
fldMap.addInputField("pelham","POP_2010")
fldMap.addInputField("coldspring","POP")

# Set name of new output field "POPULATION"
pop = fldMap.outputField
pop.name, pop.aliasName, pop.type = "POPULATION", "POPULATION", "LONG"
fldMap.outputField = pop

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap)

# Process: Append the feature classes to the target feature class
arcpy.management.Append(fcList, os.path.join(outLocation, "amherst"), schemaType, 
                        fieldMappings, subtype)
```

### Example 9

```python
# Name: Append.py
# Description: Use the Append tool to combine several polygon feature classes

# Import system modules 
import arcpy
import os

# Set environment settings
arcpy.env.workspace = "C:/data/towns.gdb"

# Set local variables
outLocation = "C:/data/output.gdb"
outName = "MA_towns.shp"
schemaType = "NO_TEST"
fieldMappings = ""
subtype = ""

# Process: Append to an existing "amherst" polygon feature class
target = os.path.join(outLocation, "amherst")

# All polygon FCs in the workspace are MA town FCs, you want to append these
# to the target FC. The list will resemble ["amherst", "hadley", "pelham",
# "coldspring"]

fcList = arcpy.ListFeatureClasses("", "POLYGON")

# Create FieldMappings object to manage merge output fields
fieldMappings = arcpy.FieldMappings()

# Add the target table to the field mappings class to set the schema
fieldMappings.addTable(target)

# Add input fields for the town name to TOWNNAME field that matches the 
# target dataset since each input dataset has a different field name for 
# this info
fldMap = arcpy.FieldMap()
fldMap.addInputField("amherst","TOWNNAME")
fldMap.addInputField("hadley","NAME")
fldMap.addInputField("pelham","TOWN_NAME")
fldMap.addInputField("coldspring","TOWN")

# Set name of new output field "TOWNNAME"
townName = fldMap.outputField
townName.name, townName.aliasName, townName.type = "TOWNNAME", "TOWNNAME", "TEXT"
fldMap.outputField = townName

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap)

# Do the same for the POPULATION field
fldMap = arcpy.FieldMap()
fldMap.addInputField("amherst","POPULATION")
fldMap.addInputField("hadley","POP")
fldMap.addInputField("pelham","POP_2010")
fldMap.addInputField("coldspring","POP")

# Set name of new output field "POPULATION"
pop = fldMap.outputField
pop.name, pop.aliasName, pop.type = "POPULATION", "POPULATION", "LONG"
fldMap.outputField = pop

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap)

# Process: Append the feature classes to the target feature class
arcpy.management.Append(fcList, os.path.join(outLocation, "amherst"), schemaType, 
                        fieldMappings, subtype)
```

---

## Apply Block Adjustment (Data Management)

## Summary

Applies the geographic adjustments to the mosaic dataset items. This tool uses the solution table from the Compute Block Adjustments tool.

## Usage

- This tool can be used to apply adjustments from a solution table or reset the geographic alignment back to its original state.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset to adjust. | Mosaic Dataset; Mosaic Layer |
| Adjustment Operation | Specifies whether the mosaic dataset will be adjusted using the solution table or whether the mosaic dataset will be reset so there are no adjustments applied.Adjust the mosaic dataset—The mosaic dataset will be adjusted using the input solution table.Reset the mosaic dataset—The mosaic dataset will be reset so there are no adjustments applied to it.Reactivate image status—Images dropped from the adjustment will be restored to active status. Images without the minimum number of control points required for adjustment are dropped from the computation in the standard adjustment operation, such that the images are categorized as Inactive in the footprints table, the maxPS value is set to 0, the imagery is not visible in the map, and the tie points statuses for the dropped images are disabled. This option will restore the Category status to Primary and ensure the maxPS value is resumed. Images that were included in the adjustment process are unaffected by this option. | String |
| Input Solution Table (Optional) | The solution table that will be used when adjusting the mosaic dataset. This is the output from the Compute Block Adjustments tool. | Table View |
| Pan-To-MS Scaling Factor (Optional) | The scaling factor between the pan-sharpened resolution and the multispectral resolution that will be used if the mosaic dataset contains pan-sharpened rasters. | Double |
| Input DEM (Optional) | The DEM that will be used in the block adjustment. This DEM will only be used if it is a higher resolution than any existing DEM in the mosaic dataset. If this input DEM is used, the geometric function of the mosaic dataset will be updated using this input. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| Z offset (Optional) | The vertical offset that will be used to adjust the elevation layer within the mosaic dataset's Geometric function. | Double |
| Control Point Table (Optional) | The input control point table that will have the same adjustments applied as the solution table adjustments. | Table View |
| Adjust Footprints (Optional) | Specifies whether the footprint geometry will be updated using the same transformation that will be applied to the image. Unchecked—The footprint geometry will not be updated. This is the default.Checked—The footprint geometry will be updated. If a control point table is provided, it will also be transformed. | Boolean |
| Solution Point Table (Optional) | The solution point table that will be used to update the status field for the control point table. This parameter is only used when the Control Point Table parameter value is provided. | Table View |
| Adjust Tiepoints (Optional) | Specifies whether the tie points will be updated when a solution point table is provided. This parameter is only used when the Solution Point Table parameter value is provided. Unchecked—The tie points will not be updated. This is the default.Checked—The tie points will be updated when a solution point table is provided. | Boolean |
| in_mosaic_dataset | The input mosaic dataset to adjust. | Mosaic Dataset; Mosaic Layer |
| adjustment_operation | Specifies whether the mosaic dataset will be adjusted using the solution table or whether the mosaic dataset will be reset so there are no adjustments applied.ADJUST—The mosaic dataset will be adjusted using the input solution table.RESET—The mosaic dataset will be reset so there are no adjustments applied to it.REACTIVATE—Images dropped from the adjustment will be restored to active status. Images without the minimum number of control points required for adjustment are dropped from the computation in the standard adjustment operation, such that the images are categorized as Inactive in the footprints table, the maxPS value is set to 0, the imagery is not visible in the map, and the tie points statuses for the dropped images are disabled. This option will restore the Category status to Primary and ensure the maxPS value is resumed. Images that were included in the adjustment process are unaffected by this option. | String |
| input_solution_table(Optional) | The solution table that will be used when adjusting the mosaic dataset. This is the output from the Compute Block Adjustments tool. | Table View |
| pan_to_ms_scaling_factor(Optional) | The scaling factor between the pan-sharpened resolution and the multispectral resolution that will be used if the mosaic dataset contains pan-sharpened rasters. | Double |
| DEM(Optional) | The DEM that will be used in the block adjustment. This DEM will only be used if it is a higher resolution than any existing DEM in the mosaic dataset. If this input DEM is used, the geometric function of the mosaic dataset will be updated using this input. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| zoffset(Optional) | The vertical offset that will be used to adjust the elevation layer within the mosaic dataset's Geometric function. | Double |
| control_point_table(Optional) | The input control point table that will have the same adjustments applied as the solution table adjustments. | Table View |
| adjust_footprints(Optional) | Specifies whether the footprint geometry will be updated using the same transformation that will be applied to the image.NO_ADJUST_FOOTPRINTS—The footprint geometry will not be updated. This is the default.ADJUST_FOOTPRINTS—The footprint geometry will be updated. If a control point table is provided, it will also be transformed. | Boolean |
| solution_point_table(Optional) | The solution point table that will be used to update the status field for the control point table. This parameter is only used when the control_point_table parameter value is provided. | Table View |
| adjust_tiepoints(Optional) | Specifies whether the tie points will be updated when a solution point table is provided. This parameter is only used when the solution_point_table parameter value is provided. NO_ADJUST_TIEPOINTS—The tie points will not be updated. This is the default.ADJUST_TIEPOINTS—The tie points will be updated when a solution point table is provided. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ApplyBlockAdjustment(in_mosaic_dataset, adjustment_operation, {input_solution_table}, {pan_to_ms_scaling_factor}, {DEM}, {zoffset}, {control_point_table}, {adjust_footprints}, {solution_point_table}, {adjust_tiepoints})
```

### Example 2

```python
import arcpy
arcpy.ApplyBlockAdjustment_management(
     "c:/BD/BD.gdb/redQB", "ADJUST",
     "c:/BD/BD.gdb/redQB_sol", 0.25)
```

### Example 3

```python
import arcpy
arcpy.ApplyBlockAdjustment_management(
     "c:/BD/BD.gdb/redQB", "ADJUST",
     "c:/BD/BD.gdb/redQB_sol", 0.25)
```

### Example 4

```python
#apply block adjustment
import arcpy
arcpy.env.workspace = "c:/workspace"

#Apply the block ajustment
mdName = "BD.gdb/redlandsQB"
out_solutionTable = "BD.gdb/redlandsQB_solution"

arcpy.ApplyBlockAdjustment_management(mdName, "ADJUST", 
     out_solutionTable, 0.25)
```

### Example 5

```python
#apply block adjustment
import arcpy
arcpy.env.workspace = "c:/workspace"

#Apply the block ajustment
mdName = "BD.gdb/redlandsQB"
out_solutionTable = "BD.gdb/redlandsQB_solution"

arcpy.ApplyBlockAdjustment_management(mdName, "ADJUST", 
     out_solutionTable, 0.25)
```

---

## Apply Symbology From Layer (Data Management)

## Summary

Applies the symbology from a specified layer or layer file to the input. It can be applied to feature, raster, network analysis, TIN, and geostatistical layers.

## Usage

- The symbology layer must match the data type of the input layer. For example, a feature layer cannot be applied to a raster layer and vice versa.
- The symbology can only be applied to features of the same geometry. For example, point symbology cannot be applied to a polygon layer.
- In addition to symbology, the tool will transfer time field, 3D elevation and offset, label class, and HTML pop-up properties.
- If the input is a feature class or dataset path, this tool will create and return a new layer with the result of the tool applied.
- You can maintain or update the symbology ranges using the Update Symbology Ranges by Data parameter. Default symbology methods are dynamic. For example, the five-class Natural Breaks classification method from the symbology layer is applied to the input layer, and the range values are updated to reflect the Shape_Area values of the input layer. The Maintain ranges option will use the same values as the symbology layer. The Update ranges option will use the five-class Natural Breaks classification method from the input layer's Shape_Area field. The following methods are dynamic:Unique values symbology to a featureThe unique values are updated to reflect the input layer's values.If you do not want the unique values classification to be updated, choose the <all other values> default symbol in the symbology layer.Graduated color symbology to a featureThe range values are updated to the values of the input layer.If you do not want the range output to be updated, choose manual classification.Classified value rendering to a rasterThe range values are updated to reflect the values of the input layer.If you do not want the range output to be updated, choose manual classification.
- Unique values symbology to a featureThe unique values are updated to reflect the input layer's values.If you do not want the unique values classification to be updated, choose the <all other values> default symbol in the symbology layer.
- The unique values are updated to reflect the input layer's values.
- If you do not want the unique values classification to be updated, choose the <all other values> default symbol in the symbology layer.
- Graduated color symbology to a featureThe range values are updated to the values of the input layer.If you do not want the range output to be updated, choose manual classification.
- The range values are updated to the values of the input layer.
- If you do not want the range output to be updated, choose manual classification.
- Classified value rendering to a rasterThe range values are updated to reflect the values of the input layer.If you do not want the range output to be updated, choose manual classification.
- The range values are updated to reflect the values of the input layer.
- If you do not want the range output to be updated, choose manual classification.
- Setting the Update Symbology Ranges by Data parameter to Maintain ranges will copy the labels from the Symbology Layer value to the Input Layer value. If the parameter is set to Update ranges, the labels will be recomputed.
- The symbology persists only for the duration of the layer. A layer can be retained by saving the ArcGIS Pro session or by saving the layer to a layer file using the Save Layer To File tool. To see the symbology created in a script tool, the tool must include the layer as a derived output parameter. Similarly, the Updated Input Layer parameter value must be added as a derived model parameter model tool to see the symbology changes.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The layer to which the symbology will be applied. | Feature Layer; Raster Layer; Layer |
| Symbology Layer | The layer containing the symbology that will be applied to the input layer. Both .lyrx and .lyr files are supported. | Layer |
| Symbology Fields(Optional) | The fields from the input layer that match the symbology fields used in the symbology layer. Symbology fields contain three properties:Field type—The field type: symbology value, normalization, or other type.Source field—The symbology field used by the symbology layer. Use a blank value or "#" if you do not know the source field and want to use the default.Target field—The field from the input layer to use when applying the symbology.Supported field types are as follows:Value field—Primary field used to symbolize valuesNormalization field—Field used to normalize quantitative valuesExclusion clause field—Field used for the symbology exclusion clauseChart renderer pie size field—Field used to set the size of pie chart symbolsRotation X expression field—Field used to set the rotation of symbols on the x-axisRotation Y expression field—Field used to set the rotation of symbols on the y-axisRotation Z expression field—Field used to set the rotation of symbols on the z-axisTransparency expression field—Field used to set the transparency of symbolsTransparency normalization field—Field used to normalize transparency valuesSize expression field—Field used to set the size or width of symbolsColor expression field—Field used to set the color of symbolsPrimitive override expression field—Field used to set various properties on individual symbol layers | Value Table |
| Update Symbology Ranges by Data (Optional) | Specifies whether symbology ranges will be updated.Default—Symbology ranges will be updated, except when the input layer is empty, when the symbology layer uses class breaks (for example, graduated colors or graduated symbols) and the classification method is manual or defined interval, or when the symbology layer uses unique values and the Show all other values option is checked.Update ranges—Symbology ranges will be updated.Maintain ranges—Symbology ranges will not be updated; they will be maintained. | String |
| in_layer | The layer to which the symbology will be applied. | Feature Layer; Raster Layer; Layer |
| in_symbology_layer | The layer containing the symbology that will be applied to the input layer. Both .lyrx and .lyr files are supported. | Layer |
| symbology_fields[[field_type, source_field, target_field],...](Optional) | The fields from the input layer that match the symbology fields used in the symbology layer. Symbology fields contain three properties:Field type—The field type: symbology value, normalization, or other type.Source field—The symbology field used by the symbology layer. Use a blank value or "#" if you do not know the source field and want to use the default.Target field—The field from the input layer to use when applying the symbology.Supported field types are as follows:VALUE_FIELD—Primary field used to symbolize valuesNORMALIZATION_FIELD—Field used to normalize quantitative valuesEXCLUSION_CLAUSE_FIELD—Field used for the symbology exclusion clauseCHART_RENDERER_PIE_SIZE_FIELD—Field used to set the size of pie chart symbolsROTATION_XEXPRESSION_FIELD—Field used to set the rotation of symbols on the x-axisROTATION_YEXPRESSION_FIELD—Field used to set the rotation of symbols on the y-axisROTATION_ZEXPRESSION_FIELD—Field used to set the rotation of symbols on the z-axisTRANSPARENCY_EXPRESSION_FIELD—Field used to set the transparency of symbolsTRANSPARENCY_NORMALIZATION_FIELD—Field used to normalize transparency valuesSIZE_EXPRESSION_FIELD—Field used to set the size or width of symbolsCOLOR_EXPRESSION_FIELD—Field used to set the color of symbolsPRIMITIVE_OVERRIDE_EXPRESSION_FIELD—Field used to set various properties on individual symbol layers | Value Table |
| update_symbology(Optional) | Specifies whether symbology ranges will be updated.DEFAULT—Symbology ranges will be updated, except in the following situations:When the input layer is emptyWhen the symbology layer uses class breaks (for example, graduated colors or graduated symbols) and the classification method is manual or defined intervalWhen the symbology layer uses unique values and the Show all other values option is checkedUPDATE—Symbology ranges will be updated.MAINTAIN—Symbology ranges will not be updated; they will be maintained. | String |

## Code Samples

### Example 1

```python
arcpy.management.ApplySymbologyFromLayer(in_layer, in_symbology_layer, {symbology_fields}, {update_symbology})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data.gdb"
arcpy.management.ApplySymbologyFromLayer("sf_points", "sf_points_water.lyrx")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data.gdb"
arcpy.management.ApplySymbologyFromLayer("sf_points", "sf_points_water.lyrx")
```

### Example 4

```python
# Import system modules
import arcpy

# Set the current workspace
arcpy.env.workspace = "C:/data.gdb"

# Set layer to apply symbology to
inputLayer = "sf_points"

# Set layer that output symbology will be based on
symbologyLayer = "water_symbols_pnt.lyrx"

# Apply the symbology from the symbology layer to the input layer
arcpy.management.ApplySymbologyFromLayer(inputLayer, symbologyLayer)
```

### Example 5

```python
# Import system modules
import arcpy

# Set the current workspace
arcpy.env.workspace = "C:/data.gdb"

# Set layer to apply symbology to
inputLayer = "sf_points"

# Set layer that output symbology will be based on
symbologyLayer = "water_symbols_pnt.lyrx"

# Apply the symbology from the symbology layer to the input layer
arcpy.management.ApplySymbologyFromLayer(inputLayer, symbologyLayer)
```

### Example 6

```python
# Import system modules
import arcpy

# Set the current workspace
arcpy.env.workspace = "C:/data.gdb"

# Set layer to apply symbology to
inputLayer = "InlandEmpireBlocks"

# Set layer that output symbology will be based on
symbologyLayer = "USCensusBlocks.lyrx"

# The symbology layer is symbolized by population normalized by area.
# Symbolize the input by Pop2014 field normalized to Square Miles
symbologyFields = [["VALUE_FIELD", "#", "Pop2014"], 
                   ["NORMALIZATION_FIELD", "#", "SQ_MILES"]]

# Apply the symbology from the symbology layer to the input layer
arcpy.management.ApplySymbologyFromLayer(inputLayer, symbologyLayer, 
                                         symbologyFields)
```

### Example 7

```python
# Import system modules
import arcpy

# Set the current workspace
arcpy.env.workspace = "C:/data.gdb"

# Set layer to apply symbology to
inputLayer = "InlandEmpireBlocks"

# Set layer that output symbology will be based on
symbologyLayer = "USCensusBlocks.lyrx"

# The symbology layer is symbolized by population normalized by area.
# Symbolize the input by Pop2014 field normalized to Square Miles
symbologyFields = [["VALUE_FIELD", "#", "Pop2014"], 
                   ["NORMALIZATION_FIELD", "#", "SQ_MILES"]]

# Apply the symbology from the symbology layer to the input layer
arcpy.management.ApplySymbologyFromLayer(inputLayer, symbologyLayer, 
                                         symbologyFields)
```

### Example 8

```python
# Import system modules
import os
import arcpy

# Get Parameters
layers = arcpy.GetParameter(0)  # Accepts Feature Layers (multivalue)
sym = arcpy.GetParameter(1)  # Accepts a Feature Layer

# Apply symbology to each input layer, store the result objects in a list
results = []
for layer in layers:

    # Derive the name of the output featureclass
    layername = arcpy.Describe(layer).baseName
    outfeature = os.path.join(arcpy.env.scratchGDB, layername + "_out")

    # Copy feature to get output. This step can be replaced by other
				# steps that produce or manipulate a featureclass.
    arcpy.management.CopyFeatures(layer, outfeature)

				# Apply symbology to the final output
    res = arcpy.management.ApplySymbologyFromLayer(outfeature, sym)

    # Append multivalue feature
    results.append(res)

# Set the symbology of the derived output parameter using the 
# list of result objects
arcpy.SetParameter(2, results)
```

### Example 9

```python
# Import system modules
import os
import arcpy

# Get Parameters
layers = arcpy.GetParameter(0)  # Accepts Feature Layers (multivalue)
sym = arcpy.GetParameter(1)  # Accepts a Feature Layer

# Apply symbology to each input layer, store the result objects in a list
results = []
for layer in layers:

    # Derive the name of the output featureclass
    layername = arcpy.Describe(layer).baseName
    outfeature = os.path.join(arcpy.env.scratchGDB, layername + "_out")

    # Copy feature to get output. This step can be replaced by other
				# steps that produce or manipulate a featureclass.
    arcpy.management.CopyFeatures(layer, outfeature)

				# Apply symbology to the final output
    res = arcpy.management.ApplySymbologyFromLayer(outfeature, sym)

    # Append multivalue feature
    results.append(res)

# Set the symbology of the derived output parameter using the 
# list of result objects
arcpy.SetParameter(2, results)
```

---

## Assign Default To Field (Data Management)

## Summary

Creates a default value for a specified field. When a new row is added to the table or feature class, the specified field will be set to this default value.

## Usage

- The default value is dependent on the field type chosen in the Field Name parameter. If you choose a field that is type LONG (long integer), the default value must be type LONG.
- Adding subtypes to the default value is optional. If you add a subtype, there must be a subtype field in the feature class or table. You can set the subtype field using the Set Subtype Field tool.
- You can also view and manage subtypes in Subtypes view which can be opened by clicking the Subtypes button found in the Design section of the Data ribbon, or by clicking the Subtypes button on the Fields view ribbon.
- This tool can also be used to clear the default value of a field or subtype.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class that will have a default value added to one of its fields. | Mosaic Layer; Raster Layer; Table View |
| Field Name | The field to which the default value will be added each time a new row is added to the table or feature class. | Field |
| Default Value(Optional) | The default value to be added to each new table or feature class. The value entered must match the data type of the field. If the field chosen has a coded value domain assigned to it, the values from the coded domain will be included in the parameter value list.. | String |
| Subtype(Optional) | The subtypes that can participate in the default value. | String |
| Clear Value(Optional) | Specifies whether the default value for either the field or the subtype will be cleared. The Default Value parameter must be empty to clear the default value of the field. To clear the default value for the subtype, leave the Default Value parameter empty and specify the subtype to be cleared.Checked—The default value will be cleared (set to null). The default value parameter must be empty.Unchecked—The default value will not be cleared. This is the default. | Boolean |
| in_table | The input table or feature class that will have a default value added to one of its fields. | Mosaic Layer; Raster Layer; Table View |
| field_name | The field to which the default value will be added each time a new row is added to the table or feature class. | Field |
| default_value(Optional) | The default value to be added to each new table or feature class. The value entered must match the data type of the field. | String |
| subtype_code[subtype_code,...](Optional) | The subtypes that can participate in the default value. | String |
| clear_value(Optional) | Specifies whether the default value for either the field or the subtype will be cleared. To clear the default value, the default_value parameter must be passed in as an empty string. To clear the default value for the subtype, you must also specify the subtype to be cleared.CLEAR_VALUE—The default value will be cleared (set to null).DO_NOT_CLEAR—The default value will not be cleared. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.AssignDefaultToField(in_table, field_name, {default_value}, {subtype_code}, {clear_value})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb/Landbase"
arcpy.CopyFeatures_management("blocks", "C:/output/output.gdb/blocks")
arcpy.AssignDefaultToField_management("C:/output/output.gdb/blocks", "Res", 1,
                                      ["0: Non-Residental", "1: Residental"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb/Landbase"
arcpy.CopyFeatures_management("blocks", "C:/output/output.gdb/blocks")
arcpy.AssignDefaultToField_management("C:/output/output.gdb/blocks", "Res", 1,
                                      ["0: Non-Residental", "1: Residental"])
```

### Example 4

```python
# Name: AssignDefaultToField_Example2.py
# Description: Assign a new default to a field along with subtypes
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "c:/data/Montgomery.gdb/Landbase"
 
# Set local variables
inFeatures = "blocks"
outFeatureClass = "c:/output/output.gdb/blocks"
fieldName = "Res"
defaultValue = 1
subTypes = ["0: Non-Residental", "1: Residental"]
 
# Execute CopyFeatures to make new copy of the input
arcpy.CopyFeatures_management(inFeatures, outFeatureClass)
 
# Execute AssignDefaultToField
arcpy.AssignDefaultToField_management(outFeatureClass, fieldName, 
                                      defaultValue, subTypes)
```

### Example 5

```python
# Name: AssignDefaultToField_Example2.py
# Description: Assign a new default to a field along with subtypes
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "c:/data/Montgomery.gdb/Landbase"
 
# Set local variables
inFeatures = "blocks"
outFeatureClass = "c:/output/output.gdb/blocks"
fieldName = "Res"
defaultValue = 1
subTypes = ["0: Non-Residental", "1: Residental"]
 
# Execute CopyFeatures to make new copy of the input
arcpy.CopyFeatures_management(inFeatures, outFeatureClass)
 
# Execute AssignDefaultToField
arcpy.AssignDefaultToField_management(outFeatureClass, fieldName, 
                                      defaultValue, subTypes)
```

---

## Assign Domain To Field (Data Management)

## Summary

Sets the domain for a particular field and, optionally, for a subtype. If no subtype is specified, the domain is only assigned to the specified field.

## Usage

- Domain management involves the following steps:Create the domain using the Create Domain tool.Add values to or set the range of values for the domain using the Add Coded Value to Domain tool or Set Value For Range Domain tool.Associate the domain with a feature class using this tool.
- Create the domain using the Create Domain tool.
- Add values to or set the range of values for the domain using the Add Coded Value to Domain tool or Set Value For Range Domain tool.
- Associate the domain with a feature class using this tool.
- One attribute domain can be associated with multiple fields in the same table, feature class, or subtype as well as in multiple tables and feature classes.
- The Input Table parameter accepts feature layers or table views.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The name of the table or feature class containing the field that will be assigned a domain. | Table View |
| Field Name | The name of the field that will be assigned a domain. | Field |
| Domain Name | The name of a geodatabase domain that will be assigned to the field name. Available domains will be loaded automatically. | String |
| Subtype(Optional) | The subtype code that will be assigned a domain. | String |
| in_table | The name of the table or feature class containing the field that will be assigned a domain. | Table View |
| field_name | The name of the field that will be assigned a domain. | Field |
| domain_name | The name of a geodatabase domain that will be assigned to the field name. Available domains will be loaded automatically. | String |
| subtype_code[subtype_code,...](Optional) | The subtype code that will be assigned a domain. | String |

## Code Samples

### Example 1

```python
arcpy.management.AssignDomainToField(in_table, field_name, domain_name, {subtype_code})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management,AssignDomainToField("montgomery.gdb/Landbase/Parcels", 
                                     "ZONING_S", "ZoningFields", "1: government")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management,AssignDomainToField("montgomery.gdb/Landbase/Parcels", 
                                     "ZONING_S", "ZoningFields", "1: government")
```

### Example 4

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy

# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"
 
# Process: Create the coded value domain
arcpy.management.CreateDomain("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")
    
# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", 
           "ACP": "Asbestos concrete", "COP": "Copper"}

# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.management.AddCodedValueToDomain(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.management.AssignDomainToField(inFeatures, inField, domName)
```

### Example 5

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy

# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"
 
# Process: Create the coded value domain
arcpy.management.CreateDomain("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")
    
# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", 
           "ACP": "Asbestos concrete", "COP": "Copper"}

# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.management.AddCodedValueToDomain(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.management.AssignDomainToField(inFeatures, inField, domName)
```

---

## Batch Build Pyramids (Data Management)

## Summary

Builds pyramids for multiple raster datasets.

## Usage

- Building pyramids improves the display performance of raster datasets.
- Batch building of pyramids is useful when you have a large directory of raster datasets that do not have pyramids.
- Wavelet compressed raster datasets, such as ECW, JPEG2000, and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding.
- Pyramids will not be built for raster datasets that have less than 1,024 pixels in the row or column. Pyramids are not needed since the raster dataset is small enough, and building pyramids will not help improve performance.
- You can choose the compression type for your overview pyramid file using the raster storage environments. Compression will create a smaller .ovr file. The IMAGINE format and older versions of ArcGIS will create reduced-resolution dataset files (.rrd) when compression is not available.
- The default pyramid compression will use the optimal compression type, depending on the data type. You can manually choose LZ77, JPEG, or no compression.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Datasets | The raster datasets for which raster pyramids will be built.Each input should have more than 1,024 rows and 1,024 columns. | Raster Dataset |
| Pyramid levels (Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels. | Long |
| Skip first level(Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales. Unchecked—The first pyramid level will not be skipped; it will be built. This is the default.Checked—The first pyramid level will be skipped; it will not be built. | Boolean |
| Pyramid resampling technique (Optional) | Specifies the resampling technique that will be used to build the pyramids.Nearest neighbor—The new value of a cell will be based on the closest cell when resampling. This is the default.Bilinear—The new value of a cell will be based on a weighted distance average of the four nearest input cell centers.Cubic—The new value of a cell will be determined by fitting a smooth curve through the 16 nearest input cell centers. | String |
| Pyramid compression type (Optional) | Specifies the compression type that will be used when building the pyramids.Default—If the source data is compressed using a wavelet compression, pyramids will be built with the JPEG compression type; otherwise, LZ77 will be used. This is the default.LZ77 Compression—The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type.JPEG—The JPEG compression algorithm will be used to build the pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality.None—No compression will be used when building pyramids. | String |
| Compression quality (Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| Skip Existing (Optional) | Specifies whether pyramids will be built only if they do not exist or built even if they exist. Unchecked—Pyramids will be built even if they already exist; existing pyramids will be overwritten. This is the default.Checked—Pyramids will only be built if they do not exist; existing pyramids will be skipped. | Boolean |
| Input_Raster_Datasets[Input_Raster_Datasets,...] | The raster datasets for which raster pyramids will be built.Each input should have more than 1,024 rows and 1,024 columns. | Raster Dataset |
| Pyramid_levels(Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels. | Long |
| Skip_first_level(Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales.NONE—The first pyramid level will not be skipped; it will be built. This is the default.SKIP_FIRST—The first pyramid level will be skipped; it will not be built. | Boolean |
| Pyramid_resampling_technique(Optional) | Specifies the resampling technique that will be used to build the pyramids.NEAREST—The new value of a cell will be based on the closest cell when resampling. This is the default.BILINEAR—The new value of a cell will be based on a weighted distance average of the four nearest input cell centers.CUBIC—The new value of a cell will be determined by fitting a smooth curve through the 16 nearest input cell centers. | String |
| Pyramid_compression_type(Optional) | Specifies the compression type that will be used when building the pyramids.DEFAULT—If the source data is compressed using a wavelet compression, pyramids will be built with the JPEG compression type; otherwise, LZ77 will be used. This is the default.LZ77—The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type.JPEG—The JPEG compression algorithm will be used to build the pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality.NONE—No compression will be used when building pyramids. | String |
| Compression_quality(Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| Skip_Existing(Optional) | Specifies whether pyramids will be built only if they do not exist or built even if they exist. OVERWRITE—Pyramids will be built even if they already exist; existing pyramids will be overwritten. This is the default.SKIP_EXISTING—Pyramids will only be built if they do not exist; existing pyramids will be skipped. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BatchBuildPyramids(Input_Raster_Datasets, {Pyramid_levels}, {Skip_first_level}, {Pyramid_resampling_technique}, {Pyramid_compression_type}, {Compression_quality}, {Skip_Existing})
```

### Example 2

```python
import arcpy
arcpy.BatchBuildPyramids_management(
     "C:/data/img1.tif;C:/data/img2.img", "6", "SKIP_FIRST",
      "BILINEAR", "JPEG", "50", "SKIP_EXISTING")
```

### Example 3

```python
import arcpy
arcpy.BatchBuildPyramids_management(
     "C:/data/img1.tif;C:/data/img2.img", "6", "SKIP_FIRST",
      "BILINEAR", "JPEG", "50", "SKIP_EXISTING")
```

### Example 4

```python
#Build Pyramids for multiple raster datasets in the workspace
#Skip the dataset that already has pyramid
#Build pyramids with compression and level setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
inras = "image1.tif;image2.img;fgdb.gdb/image3"
pylevels = "6"
skipfirst = "SKIP_FIRST"
resample = "BILINEAR"
compress = "JPEG"
quality = "80"
skipexist = "SKIP_EXISTING"

arcpy.BatchBuildPyramids_management(
     inras, pylevels, skipfirst, resample, compress,
     quality, skipexist)
```

### Example 5

```python
#Build Pyramids for multiple raster datasets in the workspace
#Skip the dataset that already has pyramid
#Build pyramids with compression and level setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
inras = "image1.tif;image2.img;fgdb.gdb/image3"
pylevels = "6"
skipfirst = "SKIP_FIRST"
resample = "BILINEAR"
compress = "JPEG"
quality = "80"
skipexist = "SKIP_EXISTING"

arcpy.BatchBuildPyramids_management(
     inras, pylevels, skipfirst, resample, compress,
     quality, skipexist)
```

---

## Batch Calculate Statistics (Data Management)

## Summary

Calculates statistics for multiple raster datasets.

## Usage

- A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.
- Calculating statistics on the Esri Grid and the RADARSAT2 formats always uses a skip factor of 1.
- The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are quite different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid (for example, if the skip factor is 5 and the closest pyramid level is 4 x 4 pixels, which is level 2), the software will round down to the next pyramid level (in this case, 2) and use that value as the skip factor.
- A skip factor is not used for all raster formats. The raster formats that will calculate statistics and take advantage of the skip factor include TIFF, IMG, NITF, DTED, RAW, ADRG, CIB, CADRG, DIGEST, GIS, LAN, CIT, COT, ERMapper, ENVI DAT, BIL, BIP, BSQ, and geodatabase.
- The Ignore Values option allows you to exclude a specific value from the calculation of statistics. You may want to ignore a value if it is a NoData value or if it will skew your calculation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Datasets | The input raster datasets. | Raster Dataset |
| X Skip Factor(Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Y Skip Factor(Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Ignore values(Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value. | Double |
| Skip Existing (Optional) | Specifies whether statistics will be calculated only when they are missing or will be regenerated even if they exist.Unchecked—Statistics will be calculated even if they already exist, and existing statistics will be overwritten. This is the default.Checked—Statistics will only be calculated if they do not already exist. | Boolean |
| Input_Raster_Datasets[input_raster_dataset,...] | The input raster datasets. | Raster Dataset |
| Number_of_columns_to_skip(Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Number_of_rows_to_skip(Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Ignore_values[ignore_value,...](Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value. | Double |
| Skip_Existing(Optional) | Specifies whether statistics will be calculated only when they are missing or will be regenerated even if they exist.OVERWRITE—Statistics will be calculated even if they already exist, and existing statistics will be overwritten. This is the default.SKIP_EXISTING—Statistics will only be calculated if they do not already exist. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BatchCalculateStatistics(Input_Raster_Datasets, {Number_of_columns_to_skip}, {Number_of_rows_to_skip}, {Ignore_values}, {Skip_Existing})
```

### Example 2

```python
import arcpy
arcpy.management.BatchCalculateStatistics(
     ["C:/data/img1.tif", "C:/data/img2.jp2"],
     5, 5, [0, 255], "SKIP_EXISTING")
```

### Example 3

```python
import arcpy
arcpy.management.BatchCalculateStatistics(
     ["C:/data/img1.tif", "C:/data/img2.jp2"],
     5, 5, [0, 255], "SKIP_EXISTING")
```

### Example 4

```python
# Calculate Statistics for multiple raster datasets with 
# multiple ignore values. 
# Skip datasets that already have the statistics.

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
inras = ["image1.tif", "image2.img", "fgdb.gdb/image3"]
skipcol = 5
skiprow = 5
ignoreval = [0, 255, 21]
skipexist = "SKIP_EXISTING"

arcpy.management.BatchCalculateStatistics(
     inras, skipcol, skiprow, ignoreval,skipexist)
```

### Example 5

```python
# Calculate Statistics for multiple raster datasets with 
# multiple ignore values. 
# Skip datasets that already have the statistics.

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
inras = ["image1.tif", "image2.img", "fgdb.gdb/image3"]
skipcol = 5
skiprow = 5
ignoreval = [0, 255, 21]
skipexist = "SKIP_EXISTING"

arcpy.management.BatchCalculateStatistics(
     inras, skipcol, skiprow, ignoreval,skipexist)
```

---

## Batch Project (Data Management)

## Summary

Changes the coordinate system of a set of input feature classes or feature datasets to a common coordinate system. To change the coordinate system of a single feature class or dataset use the Project tool.

## Usage

- Any valid inputs to the Project tool, such as all feature classes or feature datasets, are also valid inputs for this tool.
- Although both the Output Coordinate System and Template Dataset are optional parameters, one of them must be entered. Keeping both of these parameters empty will cause tool execution to fail.
- If needed, a Geographic Transformation will be calculated for each input dataset based on the output coordinate system, input coordinate system, input dataset’s extent.
- A feature class or dataset with an undefined or Unknown coordinate system must first have its coordinate system defined using the Define Projection tool before it can be used with the tool.
- The names of the input feature classes are used to name the output feature classes. For example, if the input is C:\myworkspace\Gondor.shp, the output feature class will be named Gondor.shp. If the name already exists in the output workspace, a number will be appended (for example, _1) to the end to make it unique (Gondor_1.shp).

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Class or Dataset | The input feature classes or feature datasets whose coordinates are to be converted. | Feature Layer; Feature Dataset |
| Output Workspace | The location of each new output feature class or feature dataset. | Feature Dataset; Workspace |
| Output Coordinate System(Optional) | The coordinate system to be used to project the inputs. The default value is set based on the output coordinate system environment. | Coordinate System |
| Template dataset(Optional) | The feature class or the feature dataset used to specify the output coordinate system used for projection. | Geodataset |
| Transformation(Optional) | The name of the geographic transformation to be applied to convert data between two geographic coordinate systems (datums). | String |
| Input_Feature_Class_or_Dataset[Input_Feature_Class_or_Dataset,...] | The input feature classes or feature datasets whose coordinates are to be converted. | Feature Layer; Feature Dataset |
| Output_Workspace | The location of each new output feature class or feature dataset. | Feature Dataset; Workspace |
| Output_Coordinate_System(Optional) | The coordinate system to be used to project the inputs. Valid values are a SpatialReference object, a file with a .prj extension, or a string representation of a coordinate system. | Coordinate System |
| Template_dataset(Optional) | The feature class or the feature dataset used to specify the output coordinate system used for projection. | Geodataset |
| Transformation(Optional) | The name of the geographic transformation to be applied to convert data between two geographic coordinate systems (datums). | String |

## Code Samples

### Example 1

```python
arcpy.management.BatchProject(Input_Feature_Class_or_Dataset, Output_Workspace, {Output_Coordinate_System}, {Template_dataset}, {Transformation})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/input/batchproject"
arcpy.BatchProject_management(["citylim.shp", "flood.shp", "faultzn.shp"], 
                              "C:/data/output/batchproject", "", 
                              "C:/data/usa.gdb/templatefc")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/input/batchproject"
arcpy.BatchProject_management(["citylim.shp", "flood.shp", "faultzn.shp"], 
                              "C:/data/output/batchproject", "", 
                              "C:/data/usa.gdb/templatefc")
```

### Example 4

```python
# Name: BatchProject.py
# Description: Changes coordinate systems of several datasets in a batch.

import arcpy

# Set workspace environment
arcpy.env.workspace = "C:/data/wgs1972.gdb"

# Input feature classes
input_features = ["cities", "counties", "blocks", "crime"]

# Output workspace
out_workspace = "C:/data/output.gdb"

# Output coordinate system - leave it empty
out_cs = ''

# Template dataset - it has GCS_WGS_1984 coordinate system
template = "C:/data/wgs1984.gdb/stateparks"

# Geographic transformation - 
transformation = "WGS_1972_To_WGS_1984_1"

res = arcpy.BatchProject(input_features, out_workspace, out_cs, template, transformation)
```

### Example 5

```python
# Name: BatchProject.py
# Description: Changes coordinate systems of several datasets in a batch.

import arcpy

# Set workspace environment
arcpy.env.workspace = "C:/data/wgs1972.gdb"

# Input feature classes
input_features = ["cities", "counties", "blocks", "crime"]

# Output workspace
out_workspace = "C:/data/output.gdb"

# Output coordinate system - leave it empty
out_cs = ''

# Template dataset - it has GCS_WGS_1984 coordinate system
template = "C:/data/wgs1984.gdb/stateparks"

# Geographic transformation - 
transformation = "WGS_1972_To_WGS_1984_1"

res = arcpy.BatchProject(input_features, out_workspace, out_cs, template, transformation)
```

---

## Batch Update Fields (Data Management)

## Summary

Transforms fields in a table or feature class based on schema defined in the definition table and creates a new table or feature class.

## Usage

- The input table can be a feature class or a table. The tool outputs a new feature class or table with the updated schema.
- The schema changes for the output table are defined by the Output Schema Definition Table parameter.The following is an example of a definition table:If the Output Schema Definition Table parameter value includes the following field names, as in the example above, the corresponding parameter values will be populated automatically:Target fieldSource fieldTypeDecimals/LengthAliasScriptNote:Script or Source field must be populated.
- Target field
- Source field
- Type
- Decimals/Length
- Alias
- Script
- The tool calculates new fields using existing fields by providing a script field in the definition table.An example value in the Script field of the definition table is !TOTPOP!/!AREA!, which calculates the Population Density field. Field names must be enclosed in exclamation points.
- The tool can create additional fields that are dependent on other field calculations. For example, the Bev_Index field is calculated using the Bev_Per_Capita field, which is also calculated when the tool is run.
- Use the Script File parameter for multiple line Python calculations. To use a script file, create a file with Python functions, and reference the functions in the definition table.The following is an example of script code for a target field named Bev_Per_Capita:A field named Script in the Output Schema Definition Table parameter value with a value of Bev_Per_Capita(!Bev_Total!, !TOTPOP!)A Script File parameter value with the following function: def Bev_Per_Capita(Bev_Total, TOTPOP): return Bev_Total / TOTPOPThe following is an example showing the attribute table before and after running the tool:
- A field named Script in the Output Schema Definition Table parameter value with a value of Bev_Per_Capita(!Bev_Total!, !TOTPOP!)
- A Script File parameter value with the following function: def Bev_Per_Capita(Bev_Total, TOTPOP): return Bev_Total / TOTPOP

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class. | Table View |
| Output Table | The output table or feature class containing the updated fields. | Table |
| Output Schema Definition Table | A table containing the field definitions and calculations that will be used to create the output. | Table View |
| Script File(Optional) | A Python file that stores multiple line Python functions to perform calculations for the Output Table parameter fields. | File |
| Output Field Name(Optional) | The field name from the definition table that contains the target field names for the output table. | Field |
| Source Field Name(Optional) | The field name from the definition table that contains the source field names from the input table. | Field |
| Output Field Type(Optional) | The field in the Output Schema Definition Table parameter value that defines the data types for the output table. The field is expected to be of Text type. The field in the field_definition_table that defines the data types for the output table. The field is expected to be of Text type. The following values are supported:BigInteger—64-bit IntegerBlob—Binary Large ObjectDate—DateDateOnly—Date OnlyDouble—Double-precision floating-point numberGlobalID—Global IDGUID—Globally Unique IdentifierInteger (or Long)—32-bit IntegerRaster—RasterFloat (or Single)—Single-precision floating-point numberShort (or SmallInteger)—16-bit IntegerText (or String)—Character stringTimeOnly—Time OnlyTimestampOffset—Timestamp Offset | Field |
| Output Field Decimals or Length(Optional) | The field name from the definition table that defines the number of decimals or the length of the field for the output fields. | Field |
| Output Field Alias(Optional) | The field name from the definition table that defines the alias names for the fields of the output table. | Field |
| Output Field Script(Optional) | The field name from the definition table that defines the calculations for the output fields. | Field |
| in_table | The input table or feature class. | Table View |
| out_table | The output table or feature class containing the updated fields. | Table |
| field_definition_table | A table containing the field definitions and calculations that will be used to create the output. | Table View |
| script_file(Optional) | A Python file that stores multiple line Python functions to perform calculations for the out_table parameter fields. | File |
| output_field_name(Optional) | The field name from the definition table that contains the target field names for the output table. | Field |
| source_field_name(Optional) | The field name from the definition table that contains the source field names from the input table. | Field |
| output_field_type(Optional) | The field in the Output Schema Definition Table parameter value that defines the data types for the output table. The field is expected to be of Text type. The field in the field_definition_table that defines the data types for the output table. The field is expected to be of Text type. The following values are supported:BigInteger—64-bit IntegerBlob—Binary Large ObjectDate—DateDateOnly—Date OnlyDouble—Double-precision floating-point numberGlobalID—Global IDGUID—Globally Unique IdentifierInteger (or Long)—32-bit IntegerRaster—RasterFloat (or Single)—Single-precision floating-point numberShort (or SmallInteger)—16-bit IntegerText (or String)—Character stringTimeOnly—Time OnlyTimestampOffset—Timestamp Offset | Field |
| output_field_decimals_or_length(Optional) | The field name from the definition table that defines the number of decimals or the length of the field for the output fields. | Field |
| output_field_alias(Optional) | The field name from the definition table that defines the alias names for the fields of the output table. | Field |
| output_field_script(Optional) | The field name from the definition table that defines the calculations for the output fields. | Field |

## Code Samples

### Example 1

```python
def Bev_Per_Capita(Bev_Total, TOTPOP):
    return Bev_Total / TOTPOP
```

### Example 2

```python
def Bev_Per_Capita(Bev_Total, TOTPOP):
    return Bev_Total / TOTPOP
```

### Example 3

```python
arcpy.management.BatchUpdateFields(in_table, out_table, field_definition_table, {script_file}, {output_field_name}, {source_field_name}, {output_field_type}, {output_field_decimals_or_length}, {output_field_alias}, {output_field_script})
```

### Example 4

```python
import arcpy
arcpy.management.BatchUpdateFields(
    "zip_codes", "MyProject.gdb\zip_codes_BatchUpdateFields",
    "DATA_TRANSFORMATION.csv", r"C:\BatchUpdate\script.py", "TARGET",
    "SOURCE", "DATATYPE", "DECIMALS", "ALIAS", "SCRIPT")
```

### Example 5

```python
import arcpy
arcpy.management.BatchUpdateFields(
    "zip_codes", "MyProject.gdb\zip_codes_BatchUpdateFields",
    "DATA_TRANSFORMATION.csv", r"C:\BatchUpdate\script.py", "TARGET",
    "SOURCE", "DATATYPE", "DECIMALS", "ALIAS", "SCRIPT")
```

---

## Bearing Distance To Line (Data Management)

## Summary

Creates a feature class containing geodetic or planar line features from the values in an x-coordinate field, y-coordinate field, bearing field, and distance field of a table.

## Usage

- Output lines are constructed from field values. The field values include the following:The x- and y-coordinates of a starting pointThe distance from the starting pointThe bearing angleThe fields and their values will be included in the output.
- The x- and y-coordinates of a starting point
- The distance from the starting point
- The bearing angle
- When the output lines are geodetic, the x- and y- coordinates and distance are measured on the surface of the earth, and the bearing angle is measured from north. When the output lines are planar, the x- and y- coordinates and distance are measured on the projected plane, and the bearing angle is measured clockwise from grid north (vertical up on the map).
- A geodetic line is a curve on the surface of the earth. However, a geodetic line feature is not stored as a parametric (true) curve in the output; rather, it is stored as a densified polyline representing the path of the geodetic line. If the length of a geodetic line is relatively short, it may be represented by a straight line in the output. As the length of the line increases, more vertices are used to represent the path.
- When the output is a feature class in a geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the Spatial Reference parameter, and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcGIS Pro Measure tool and choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| Output Feature Class | The output feature class containing geodetic or planar lines. | Feature Class |
| X Field | A numerical field in the input table containing the x-coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Y Field | A numerical field in the input table containing the y-coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Distance Field | A numerical field in the input table containing the distances from the starting points for creating the output lines. | Field |
| Distance Units (Optional) | Specifies the units that will be used for the Distance Field parameter.Meters—The units will be meters.Kilometers—The units will be kilometers.Miles—The units will be miles.Nautical miles—The units will be nautical miles.Feet—The units will be feet.U.S. survey feet—The units will be U.S. survey feet. | String |
| Bearing Field | A numerical field in the input table containing bearing angle values for the output line rotation. The angles are measured clockwise from north. | Field |
| Bearing Units (Optional) | Specifies the units of the Bearing Field parameter values.Decimal degrees— The units will be decimal degrees. This is the default.Mils—The units will be mils.Radians—The units will be radians.Gradians—The units will be gradians. | String |
| Line Type (Optional) | Specifies the type of line that will be constructed.Geodesic— A type of geodetic line that most accurately represents the shortest distance between any two points on the surface of the earth will be constructed. This is the default.Great circle—A type of geodetic line that represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth will be constructed. If the Spatial Reference parameter value is a spheroid-based coordinate system, the line is a great elliptic. If the Spatial Reference parameter value is a sphere-based coordinate system, the line is uniquely called a great circle—a circle of the largest radius on the spherical surface.Rhumb line—A type of geodetic line, also known as a loxodrome line, that represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole will be constructed. A rhumb line is shown as a straight line in the Mercator projection.Normal section—A type of geodetic line that represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points will be constructed. The normal section line from point A to point B is different from the line from point B to point A.Planar line—A straight line in the projected plane will be used. A planar line usually does not accurately represent the shortest distance on the surface of the earth as a geodesic line does. This option is not available for geographic coordinate systems. | String |
| ID (Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| Spatial Reference (Optional) | The spatial reference of the output feature class. The default is GCS_WGS_1984 or the input coordinate system if it is not Unknown. | Spatial Reference |
| Preserve attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.Unchecked—The remaining input fields will not be added to the output feature class. This is the default.Checked—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |
| in_table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| out_featureclass | The output feature class containing geodetic or planar lines. | Feature Class |
| x_field | A numerical field in the input table containing the x-coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| y_field | A numerical field in the input table containing the y-coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| distance_field | A numerical field in the input table containing the distances from the starting points for creating the output lines. | Field |
| distance_units(Optional) | Specifies the units that will be used for the distance_field parameter.METERS—The units will be meters.KILOMETERS—The units will be kilometers.MILES—The units will be miles.NAUTICAL_MILES—The units will be nautical miles.FEET—The units will be feet.US_SURVEY_FEET—The units will be U.S. survey feet. | String |
| bearing_field | A numerical field in the input table containing bearing angle values for the output line rotation. The angles are measured clockwise from north. | Field |
| bearing_units(Optional) | Specifies the units of the bearing_field parameter values. DEGREES— The units will be decimal degrees. This is the default.MILS—The units will be mils.RADS—The units will be radians.GRADS—The units will be gradians. | String |
| line_type(Optional) | Specifies the type of line that will be constructed.GEODESIC— A type of geodetic line that most accurately represents the shortest distance between any two points on the surface of the earth will be constructed. This is the default.GREAT_CIRCLE—A type of geodetic line that represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth will be constructed. If the Spatial Reference parameter value is a spheroid-based coordinate system, the line is a great elliptic. If the Spatial Reference parameter value is a sphere-based coordinate system, the line is uniquely called a great circle—a circle of the largest radius on the spherical surface.RHUMB_LINE—A type of geodetic line, also known as a loxodrome line, that represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole will be constructed. A rhumb line is shown as a straight line in the Mercator projection.NORMAL_SECTION—A type of geodetic line that represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points will be constructed. The normal section line from point A to point B is different from the line from point B to point A.PLANAR—A straight line in the projected plane will be used. A planar line usually does not accurately represent the shortest distance on the surface of the earth as a geodesic line does. This option is not available for geographic coordinate systems. | String |
| id_field(Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| spatial_reference(Optional) | The spatial reference of the output feature class. A spatial reference can be specified as any of the following: The path to a .prj file, such as C:/workspace/watershed.prj The path to a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland A SpatialReference object, such as arcpy.SpatialReference("C:/data/Africa/Carthage.prj") | Spatial Reference |
| attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.NO_ATTRIBUTES—The remaining input fields will not be added to the output feature class. This is the default.ATTRIBUTES—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BearingDistanceToLine(in_table, out_featureclass, x_field, y_field, distance_field, {distance_units}, bearing_field, {bearing_units}, {line_type}, {id_field}, {spatial_reference}, {attributes})
```

### Example 2

```python
# Import system modules
import arcpy

# Local variables
input_table = r'c:\workspace\LOBtraffic.dbf'
output_fc = r'c:\workspace\SOPA.gdb\lob_traf001'

# BearingDistanceToLine
arcpy.BearingDistanceToLine_management(input_table, output_fc, 'X', 'Y', 
                                       'NAUTICAL_MILES', 'azim', 'DEGREES', 
                                       'GEODESIC', 'recnum')
```

### Example 3

```python
# Import system modules
import arcpy

# Local variables
input_table = r'c:\workspace\LOBtraffic.dbf'
output_fc = r'c:\workspace\SOPA.gdb\lob_traf001'

# BearingDistanceToLine
arcpy.BearingDistanceToLine_management(input_table, output_fc, 'X', 'Y', 
                                       'NAUTICAL_MILES', 'azim', 'DEGREES', 
                                       'GEODESIC', 'recnum')
```

---

## Build Boundary (Data Management)

## Summary

Updates the extent of the boundary when adding new raster datasets to a mosaic dataset that extend beyond its previous coverage.

## Usage

- Boundaries can only be generated for mosaic datasets stored within a geodatabase.
- If you remove or add rasters, or modify the extent of the footprints, you should use this tool to rebuild the boundary.
- If you modify the shape of the boundary polygon (using editing tools), you can use this tool to recreate the original (unmodified) boundary.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | Select the mosaic dataset where you want to recompute the boundary. | Mosaic Layer |
| Query Definition (Optional) | An SQL query to compute a boundary for select raster datasets. Use this option in conjunction with the Append to Existing Boundary option to save time when adding new raster datasets. | SQL Expression |
| Append To Existing Boundary(Optional) | Use this option when adding new raster datasets to an existing mosaic dataset. Instead of calculating the entire boundary, it will merge the boundary of the new raster datasets with the existing boundary.Checked—Append the perimeter of footprints to the existing boundary. This can save time when adding additional raster data to the mosaic dataset, as the entire boundary will not be recalculated. If there are rasters selected, the boundary will be recalculated to include only the selected footprints. This is the default. Unchecked—Recompute the boundary in its entirety. | Boolean |
| Simplification Method (Optional) | Specifies the simplification method that will be used to reduce the number of vertices, since a dense boundary can affect performance.Choose the simplification method to use to simplify the boundary.None—No simplification method will be used. This is the default.Convex hull—The minimum bounding geometry of the mosaic dataset will be used to simplify the boundary. If there are disconnected footprints, a minimum bounding geometry for each continuous group of footprints will be used to simplify the boundary.Envelope—The envelope of the mosaic dataset will provide a simplified boundary. If there are disconnected footprints, an envelope for each continuous group of footprints will be used to simplify the boundary. | String |
| in_mosaic_dataset | Select the mosaic dataset where you want to recompute the boundary. | Mosaic Layer |
| where_clause(Optional) | An SQL query to compute a boundary for select raster datasets. Use this option in conjunction with setting the append_to_existing parameter to APPEND to save time when adding new raster datasets. | SQL Expression |
| append_to_existing(Optional) | Set this to APPEND when adding new raster datasets to an existing mosaic dataset. Instead of calculating the entire boundary, APPEND will merge the boundary of the new raster datasets with the existing boundary.OVERWRITE—Recompute the boundary in its entirety.APPEND—Append the perimeter of footprints to the existing boundary. This can save time when adding additional raster data to the mosaic dataset, as the entire boundary will not be recalculated. If there are rasters selected, the boundary will be recalculated to include only the selected footprints. This is the default. | Boolean |
| simplification_method(Optional) | Specifies the simplification method that will be used to reduce the number of vertices, since a dense boundary can affect performance.Choose the simplification method to use to simplify the boundary.NONE—No simplification method will be used. This is the default.CONVEX_HULL—The minimum bounding geometry of the mosaic dataset will be used to simplify the boundary. If there are disconnected footprints, a minimum bounding geometry for each continuous group of footprints will be used to simplify the boundary.ENVELOPE—The envelope of the mosaic dataset will provide a simplified boundary. If there are disconnected footprints, an envelope for each continuous group of footprints will be used to simplify the boundary. | String |

## Code Samples

### Example 1

```python
arcpy.management.BuildBoundary(in_mosaic_dataset, {where_clause}, {append_to_existing}, {simplification_method})
```

### Example 2

```python
import arcpy
arcpy.BuildBoundary_management("c:/workspace/Boundary.gdb/md", "#", 
                               "APPEND", "CONVEX_HULL")
```

### Example 3

```python
import arcpy
arcpy.BuildBoundary_management("c:/workspace/Boundary.gdb/md", "#", 
                               "APPEND", "CONVEX_HULL")
```

### Example 4

```python
# Build boundary only for the Quickbird data

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "boundary.gdb/md"
query = "SensorName = 'QuickBird'"
mode = "OVERWRITE"
simplify = "#"

arcpy.BuildBoundary_management(mdname, query, mode, simplify)
```

### Example 5

```python
# Build boundary only for the Quickbird data

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "boundary.gdb/md"
query = "SensorName = 'QuickBird'"
mode = "OVERWRITE"
simplify = "#"

arcpy.BuildBoundary_management(mdname, query, mode, simplify)
```

---

## Build Footprints (Data Management)

## Summary

Computes the extent of every raster in a mosaic dataset. This tool is used when you have added or removed raster datasets from a mosaic dataset and want to recompute the footprints.

## Usage

- If there is a selection on the mosaic dataset layer, only those selected footprints will be recalculated.
- The footprint is used to calculate the boundary. If you modify the shape of the footprints along the perimeter of the mosaic dataset, you need to recalculate the boundary. If you don't choose to use this tool, you can do it later using the Build Boundary tool.
- You cannot rebuild footprints for a referenced mosaic dataset.
- The Approximate Number of Vertices parameter is used to define the complexity of the footprints. The higher the number of vertices will mean the footprint is more accurate and more irregular. Valid values range from 4 to 10,000. You can set your value to be -1 so that no generalization will take place, but this may mean your footprint will have a very large number of vertices.
- The Minimum Region Size, Minimum Thinness Ratio, and Maximum Sliver Size are used to remove holes and slivers in the footprint.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that contains the raster datasets whose footprints you want to compute. | Mosaic Layer |
| Query Definition (Optional) | An SQL expression to select specific raster datasets within the mosaic dataset. | SQL Expression |
| Computation Method (Optional) | Refine the footprints using one of the following methods:Radiometry— Exclude pixels with a value outside of a defined range. This option is generally used to exclude border areas, which do not contain valid data. This is the default.Geometry— Restore the footprint to its original geometry.Copy to sibling— Replace the panchromatic footprint with the multispectral footprint when using a pan-sharpened raster type. This can occur when the panchromatic and multispectral images do not have identical geometries.None—Do not redefine the footprints. | Boolean; String |
| Minimum Data Value(Optional) | Exclude pixels with a value less than this number. | Double |
| Maximum Data Value(Optional) | Exclude pixels with a value greater than this number. | Double |
| Approximate number of vertices (Optional) | Choose between 4 and 10,000. More vertices will improve accuracy but can extend processing time. A value of -1 will calculate all vertices. More vertices will increase accuracy but also the processing time. | Long |
| Shrink distance (Optional) | Clip the footprint by this distance. This can eliminate artifacts from using lossy compression, which causes the edges of the image to overlap into NoData areas.Shrinking of the polygon is used to counteract effects of lossy compression, which causes edges of the image to overlap into NoData areas. | Double |
| Maintain sheet edges (Optional) | Alter the footprints of raster datasets that have been tiled and are adjacent (line up along the seams with little to no overlap).Unchecked—Remove the sheet edges from all the footprints. This is the default.Checked—Maintain the footprints in their original state. | Boolean |
| Skip overviews(Optional) | Adjust the footprints of overviews.Checked—Do not adjust the footprints of overviews. This is the default.Unchecked—Adjust the footprints of overviews and associated raster datasets. | Boolean |
| Update Boundary(Optional) | Update the boundary of the mosaic dataset if you have added or removed imagery that changes the extent.Checked—Update the boundary. This is the default.Unchecked—Do not update the boundary. | Boolean |
| Request Size (Optional) | Set the resampled extent (in columns and rows) for the raster when building footprints. Greater image resolution provides more detail in the raster dataset but increases the processing time. A value of -1 will compute the footprint at the original resolution. | Long |
| Minimum Region Size (Optional) | Avoid small holes in your imagery when using pixel values to create a mask. For example, your imagery may have a range of values from 0 to 255, and to mask clouds, you've excluded values from 245 to 255, which may cause other, noncloud pixels to be masked as well. If those areas are smaller than the number of pixels specified here, they will not be masked out. | Long |
| Simplification Method (Optional) | Reduce the number of vertices in the footprint to improve performance.None—Do not limit the number of vertices. This is the default.Convex hull—Use the minimum bounding box to simplify the footprint.Envelope—Use the envelope of each mosaic dataset item to simplify the footprint. | String |
| Edge tolerance (Optional) | Snap the footprint to the sheet edge if it is within this tolerance. Units are the same as those in the mosaic dataset coordinate system.By default, the value is empty for which the tolerance is computed based on the pixel size corresponding to the requested resampled raster.A value of -1 will compute the tolerance using the average pixel size of the mosaic dataset. | Double |
| Maximum Sliver Size (Optional) | Identify all polygons that are smaller than the square of this value. The value is specified in pixels and is based on the Request Size, not the spatial resolution of the source raster.Regions less than the (Maximum Sliver Size)^2 and less than the Minimum Thinness Ratio are considered slivers and will be removed. | Long |
| Minimum Thinness Ratio(Optional) | Define the thinness of slivers on a scale from 0 to 1.0, where 1.0 represents a circle and 0.0 represents a polygon that approaches a straight line. Polygons that are below both the Maximum Sliver Size and Minimum Thinness Ratio will be removed from the footprint. | Double |
| in_mosaic_dataset | The mosaic dataset that contains the raster datasets whose footprints you want to compute. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select specific raster datasets within the mosaic dataset. | SQL Expression |
| reset_footprint(Optional) | Refine the footprints using one of the following methods:RADIOMETRY— Exclude pixels with a value outside of a defined range. This option is generally used to exclude border areas, which do not contain valid data. This is the default.GEOMETRY— Restore the footprint to its original geometry.COPY_TO_SIBLING— Replace the panchromatic footprint with the multispectral footprint when using a pan-sharpened raster type. This can occur when the panchromatic and multispectral images do not have identical geometries.NONE—Do not redefine the footprints. | Boolean; String |
| min_data_value(Optional) | Exclude pixels with a value less than this number. | Double |
| max_data_value(Optional) | Exclude pixels with a value greater than this number. | Double |
| approx_num_vertices(Optional) | Choose between 4 and 10,000. More vertices will improve accuracy but can extend processing time. A value of -1 will calculate all vertices. More vertices will increase accuracy but also the processing time. | Long |
| shrink_distance(Optional) | Clip the footprint by this distance. This can eliminate artifacts from using lossy compression, which causes the edges of the image to overlap into NoData areas.Shrinking of the polygon is used to counteract effects of lossy compression, which causes edges of the image to overlap into NoData areas. | Double |
| maintain_edges(Optional) | Use this parameter when using raster datasets that have been tiled and are adjacent (line up along the seams with little to no overlap).NO_MAINTAIN_EDGES—Remove the sheet edges from all the footprints. This is the default.MAINTAIN_EDGES—Maintain the footprints in their original state. | Boolean |
| skip_derived_images(Optional) | Adjust the footprints of overviews.SKIP_DERIVED_IMAGES—Do not adjust the footprints of overviews. This is the default. NO_SKIP_DERIVED_IMAGES—Adjust the footprints of overviews and associated raster datasets. | Boolean |
| update_boundary(Optional) | Update the boundary of the mosaic dataset if you have added or removed imagery that changes the extent.UPDATE_BOUNDARY—Update the boundary. This is the default.NO_BOUNDARY—Do not update the boundary. | Boolean |
| request_size(Optional) | Set the resampled extent (in columns and rows) for the raster when building footprints. Greater image resolution provides more detail in the raster dataset but increases the processing time. A value of -1 will compute the footprint at the original resolution. | Long |
| min_region_size(Optional) | Avoid small holes in your imagery when using pixel values to create a mask. For example, your imagery may have a range of values from 0 to 255, and to mask clouds, you've excluded values from 245 to 255, which may cause other, noncloud pixels to be masked as well. If those areas are smaller than the number of pixels specified here, they will not be masked out. | Long |
| simplification_method(Optional) | Reduce the number of vertices in the footprint to improve performance.NONE—Do not limit the number of vertices. This is the default.CONVEX_HULL—Use the minimum bounding box to simplify the footprint.ENVELOPE—Use the envelope of each mosaic dataset item to simplify the footprint. | String |
| edge_tolerance(Optional) | Snap the footprint to the sheet edge if it is within this tolerance. Units are the same as those in the mosaic dataset coordinate system. This is used when maintain_edges is set to MAINTAIN_EDGES.By default, the value is empty for which the tolerance is computed based on the pixel size corresponding to the requested resampled raster.A value of -1 will compute the tolerance using the average pixel size of the mosaic dataset. | Double |
| max_sliver_size(Optional) | Identify all polygons that are smaller than the square of this value. The value is specified in pixels and is based on the request_size, not the spatial resolution of the source raster.Regions less than the (max_sliver_size)2 and less than the min_thinness_ratio are considered slivers and will be removed. | Long |
| min_thinness_ratio(Optional) | Define the thinness of slivers on a scale from 0 to 1.0, where 1.0 represents a circle and 0.0 represents a polygon that approaches a straight line. Polygons that are below both the max_sliver_size and min_thinness_ratio will be removed from the footprint. | Double |

## Code Samples

### Example 1

```python
arcpy.management.BuildFootprints(in_mosaic_dataset, {where_clause}, {reset_footprint}, {min_data_value}, {max_data_value}, {approx_num_vertices}, {shrink_distance}, {maintain_edges}, {skip_derived_images}, {update_boundary}, {request_size}, {min_region_size}, {simplification_method}, {edge_tolerance}, {max_sliver_size}, {min_thinness_ratio})
```

### Example 2

```python
import arcpy
arcpy.BuildFootprints_management(
     "c:/data/Footprints.gdb/md", "#","RADIOMETRY",
     "1", "254", "25", "0", "#", "SKIP_DERIVED_IMAGES", 
     "UPDATE_BOUNDARY", "#", "#", "CONVEX_HULL")
```

### Example 3

```python
import arcpy
arcpy.BuildFootprints_management(
     "c:/data/Footprints.gdb/md", "#","RADIOMETRY",
     "1", "254", "25", "0", "#", "SKIP_DERIVED_IMAGES", 
     "UPDATE_BOUNDARY", "#", "#", "CONVEX_HULL")
```

### Example 4

```python
# Build Footprint by setting the valid pixel value range from 1 to 254
# Allow 25 vertices to be used to draw a single footprint polygon
# Skip the overviews image
# Build new boundary afterwards
# Build footprints based on minimum bounding geometry

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
mdname = "Footprints.gdb/md"
query = "#"
method = "RADIOMETRY"
minval = "1"
maxval = "254"
nvertice = "25"
shrinkdis = "0"
maintainedge = "#"
skipovr = "SKIP_DERIVED_IMAGES"
updatebnd = "UPDATE_BOUNDARY"
requestsize = "#"
minregsize = "#"
simplify = "#"

arcpy.BuildFootprints_management(
     mdname, query, method, minval, maxval, nvertice, shrinkdis,
     maintainedge, skipovr, updatebnd, requestsize, minregsize, 
     simplify)
```

### Example 5

```python
# Build Footprint by setting the valid pixel value range from 1 to 254
# Allow 25 vertices to be used to draw a single footprint polygon
# Skip the overviews image
# Build new boundary afterwards
# Build footprints based on minimum bounding geometry

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
mdname = "Footprints.gdb/md"
query = "#"
method = "RADIOMETRY"
minval = "1"
maxval = "254"
nvertice = "25"
shrinkdis = "0"
maintainedge = "#"
skipovr = "SKIP_DERIVED_IMAGES"
updatebnd = "UPDATE_BOUNDARY"
requestsize = "#"
minregsize = "#"
simplify = "#"

arcpy.BuildFootprints_management(
     mdname, query, method, minval, maxval, nvertice, shrinkdis,
     maintainedge, skipovr, updatebnd, requestsize, minregsize, 
     simplify)
```

---

## Build LAS Dataset Pyramid (Data Management)

## Summary

Constructs or updates a LAS dataset display cache, which optimizes its rendering performance.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The input LAS dataset.It is only possible to construct a LAS dataset pyramid if the LAS dataset has an .lasd extension. The pyramid construction process does not support individual .las or .zlas files. | LAS Dataset Layer |
| Point Selection Method (Optional) | Specifies how the point in each binned region will be selected to construct the pyramid. This parameter is disabled if the LAS dataset contains a pyramid.Lowest Point—The point with the lowest z-value will be selected.Highest Point—The point with the highest z-value will be selected.Closest to Center—The point that is closest to the center of the binned region will be selected.Class Codes and Weights—The point with the highest weight value will be selected. | String |
| Input Class Codes and Weights (Optional) | The weights assigned to each class code that determine which points are retained in each thinning region. This parameter is only enabled when the Class Code Weights option is specified in the Point Selection Method parameter. The class code with the highest weight will be retained in the thinning region. If two class codes with the same weight exist in a given thinning region, the class code with the smallest point source ID will be retained. | Value Table |
| in_las_dataset | The input LAS dataset.It is only possible to construct a LAS dataset pyramid if the LAS dataset has an .lasd extension. The pyramid construction process does not support individual .las or .zlas files. | LAS Dataset Layer |
| point_selection_method(Optional) | Specifies how the point in each binned region will be selected to construct the pyramid. This parameter is disabled if the LAS dataset contains a pyramid.Z_MIN—The point with the lowest z-value will be selected.Z_MAX—The point with the highest z-value will be selected.CLOSEST_TO_CENTER—The point that is closest to the center of the binned region will be selected.CLASS_CODE—The point with the highest weight value will be selected. | String |
| class_codes_weights[class_codes_weights,...](Optional) | The weights assigned to each class code that determine which points are retained in each thinning region. This parameter is only enabled when the Class Code Weights option is specified in the Point Selection Method parameter. The class code with the highest weight will be retained in the thinning region. If two class codes with the same weight exist in a given thinning region, the class code with the smallest point source ID will be retained. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.BuildLasDatasetPyramid(in_las_dataset, {point_selection_method}, {class_codes_weights})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/data'
arcpy.ddd.BuildLasDatasetPyramid('test.lasd', 'MIN_Z')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/data'
arcpy.ddd.BuildLasDatasetPyramid('test.lasd', 'MIN_Z')
```

---

## Build Mosaic Dataset Item Cache (Data Management)

## Summary

Inserts the Cached Raster function as the final step in all function chains within a mosaic dataset.

## Usage

- The Cached Raster function is inserted on the top of every function chain; therefore, it's the last function implemented in the chain.
- If you do not check the Generate Cache parameter (set generate_cache to GENERATE_CACHE in Python) to generate the cache, you can use the Synchronize Mosaic Dataset tool to generate the cache.
- The cache is not moved with the mosaic dataset when it is shared (published) to the server. If you will be building the cache for a mosaic dataset that will be published as an image service, you may want to run this tool on the mosaic dataset after it has been shared. Also, make sure the path to the cache is accessible by the server. If you build the cache prior to publishing the mosaic dataset, you can move the cache to the server and update the cache path stored in the mosaic dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset where you want to apply the cache function. | Mosaic Layer |
| Query Definition (Optional) | An SQL expression to select specific raster datasets within the mosaic dataset on which you want the item cache built. | SQL Expression |
| Define Cache (Optional) | Enable editing on the Cache properties. Checked—Add the Cached Raster function to the selected items. If an item already has this function, it will not add another one. This is the default.Unchecked—No raster cache will be defined. | Boolean |
| Generate Cache (Optional) | Choose to generate the cache files based on the properties defined in the Cached Raster function, such as the location and the compression of the cache.Checked—Cache will be generated. This is the default.Unchecked—Cache will not be generated. | Boolean |
| Cache Path(Optional) | Choose to overwrite the default location to save your cache. If the mosaic dataset is inside of a file geodatabase, by default, the cache is saved in a folder with the same name as the geodatabase and a .cache extension. If the mosaic dataset is inside of an enterprise geodatabase, by default, the cache will be saved inside of that geodatabase. Once created, the cache will always save to the same location. To save the cache to a different location, you need to first use the Repair Mosaic Dataset tool to specify a new location and run this tool again. Once an item cache is created, regenerating an item cache to a different location is not possible by specifying a different cache path and rerunning this tool. It will still generate the item cache in the location where it was generated the first time. However, you can remove this function and insert a new one with the new path or use the Repair Mosaic Dataset tool to modify the cache path and run this tool to generate the item cache in a different location. | Workspace |
| Compression Method (Optional) | Choose how you want to compress your data for faster transmission.Lossless compression— Retain the values of each pixel when generating cache. Lossless has a compression ratio of approximately 2:1.Lossy compression— Appropriate when your imagery is only used as a backdrop. Lossy has the highest compression ratio (20:1) but groups similar pixel values to achieve higher compression.No compression— Do not compress imagery. This will make your imagery slower to transmit but faster to draw because it will not need to be decompressed when viewed. | String |
| Compression Quality (Optional) | Set a compression quality when using the lossy method. The compression quality value is between 1 and 100 percent, with 100 compressing the least. | Long |
| Maximum Allowed Rows (Optional) | Limit the size of the cache dataset by number of rows. If value is more than the number of rows in the dataset, the cache will not generate. | Long |
| Maximum Allowed Columns(Optional) | Limit the size of the cache dataset by number of columns. If value is more than the number of columns in the dataset, the cache will not generate. | Long |
| Request Size Type(Optional) | Resample the cache using one of these two methods:Pixel size factor— Set a scaling factor relative to the pixel size. To not resample the cache, choose Pixel size factor and set the Request Size parameter to 1.Pixel size— Specify a pixel size for the cached raster. | String |
| Request Size(Optional) | Set a value to apply to the Request Size Type. | Double |
| in_mosaic_dataset | The mosaic dataset where you want to apply the cache function. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select specific raster datasets within the mosaic dataset on which you want the item cache built. | SQL Expression |
| define_cache(Optional) | Choose to define the mosaic dataset cache. A Cached Raster function will be inserted to the selected items. If an item already has a Cached Raster function, it will not add another one.DEFINE_CACHE—The Cached Raster function will be added to the selected items. If an item already has this function, it will not add another one. This is the default.NO_DEFINE_CACHE—No raster cache will be defined. | Boolean |
| generate_cache(Optional) | Choose to generate the cache files based on the properties defined in the Cached Raster function, such as the location and the compression of the cache.GENERATE_CACHE—Cache will be generated. This is the default.NO_GENERATE_CACHE—Cache will not be generated. | Boolean |
| item_cache_folder(Optional) | Choose to overwrite the default location to save your cache. If the mosaic dataset is inside of a file geodatabase, by default, the cache is saved in a folder with the same name as the geodatabase and a .cache extension. If the mosaic dataset is inside of an enterprise geodatabase, by default, the cache will be saved inside of that geodatabase. Once created, the cache will always save to the same location. To save the cache to a different location, you need to first use the Repair Mosaic Dataset tool to specify a new location and run this tool again. Once an item cache is created, regenerating an item cache to a different location is not possible by specifying a different cache path and rerunning this tool. It will still generate the item cache in the location where it was generated the first time. However, you can remove this function and insert a new one with the new path or use the Repair Mosaic Dataset tool to modify the cache path and run this tool to generate the item cache in a different location. | Workspace |
| compression_method(Optional) | Choose how you want to compress your data for faster transmission.LOSSLESS— Retain the values of each pixel when generating cache. Lossless has a compression ratio of approximately 2:1.LOSSY— Appropriate when your imagery is only used as a backdrop. Lossy has the highest compression ratio (20:1) but groups similar pixel values to achieve higher compression.NONE— Do not compress imagery. This will make your imagery slower to transmit but faster to draw because it will not need to be decompressed when viewed. | String |
| compression_quality(Optional) | Set a compression quality when using the lossy method. The compression quality value is between 1 and 100 percent, with 100 compressing the least. | Long |
| max_allowed_rows(Optional) | Limit the size of the cache dataset by number of rows. If value is more than the number of rows in the dataset, the cache will not generate. | Long |
| max_allowed_columns(Optional) | Limit the size of the cache dataset by number of columns. If value is more than the number of columns in the dataset, the cache will not generate. | Long |
| request_size_type(Optional) | Resample the cache using one of these two methods:PIXEL_SIZE_FACTOR— Set a scaling factor relative to the pixel size. To not resample the cache, choose PIXEL_SIZE_FACTOR and set the request_size parameter to 1.PIXEL_SIZE— Specify a pixel size for the cached raster. | String |
| request_size(Optional) | Set a value to apply to the request_size_type. | Double |

## Code Samples

### Example 1

```python
arcpy.management.BuildMosaicDatasetItemCache(in_mosaic_dataset, {where_clause}, {define_cache}, {generate_cache}, {item_cache_folder}, {compression_method}, {compression_quality}, {max_allowed_rows}, {max_allowed_columns}, {request_size_type}, {request_size})
```

### Example 2

```python
import arcpy
arcpy.BuildMosaicDatasetItemCache_management(
     "C:/Workspace/itemcache.gdb/md",  "#", "DEFINE_CACHE", 
     "NO_GENERATE_CACHE", "C:/workspace/itemcache", "LOSSY", 
     "80", "#", "#")
```

### Example 3

```python
import arcpy
arcpy.BuildMosaicDatasetItemCache_management(
     "C:/Workspace/itemcache.gdb/md",  "#", "DEFINE_CACHE", 
     "NO_GENERATE_CACHE", "C:/workspace/itemcache", "LOSSY", 
     "80", "#", "#")
```

### Example 4

```python
#Define mosaic dataset item cache without generating the cache file

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "itemcache.gdb/md"
query = "#"
definecache = "DEFINE_CACHE"
generatecache = "NO_GENERATE_CACHE"
cachepath = "C:/workspace/itemcache"
compression = "LOSSY"
compquality = "80"
maxrow = "#"
maxcolumn = "#"

arcpy.BuildMosaicDatasetItemCache_management(
     mdname, query, definecache, generatecache, cachepath, compression, 
     compquality, maxrow, maxcolumn)
```

### Example 5

```python
#Define mosaic dataset item cache without generating the cache file

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "itemcache.gdb/md"
query = "#"
definecache = "DEFINE_CACHE"
generatecache = "NO_GENERATE_CACHE"
cachepath = "C:/workspace/itemcache"
compression = "LOSSY"
compquality = "80"
maxrow = "#"
maxcolumn = "#"

arcpy.BuildMosaicDatasetItemCache_management(
     mdname, query, definecache, generatecache, cachepath, compression, 
     compquality, maxrow, maxcolumn)
```

---

## Build Multidimensional Info (Data Management)

## Summary

Generates multidimensional metadata in the mosaic dataset, including information regarding variables and dimensions.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The input multidimensional mosaic dataset. | Mosaic Layer |
| Variable Field(Optional) | The field in the mosaic dataset that stores the variable names and is used to populate a new field named Variable. If all rasters in the mosaic dataset represent the same variable, type the name of the variable, for example, Temperature.If the Variable field does not already exist, an existing field or string value must be specified. If the Variable field exists, the tool will update the multidimensional information only. | String |
| Dimension Fields(Optional) | The fields in the mosaic dataset that store the dimension information and are used to populate a new field named Dimensions.If the Dimensions field already exists, the tool will update the multidimensional information only. | Value Table |
| Variable Info(Optional) | Specify additional information about the Variable field. | Value Table |
| in_mosaic_dataset | The input multidimensional mosaic dataset. | Mosaic Layer |
| variable_field(Optional) | The field in the mosaic dataset that stores the variable names and is used to populate a new field named Variable. If all rasters in the mosaic dataset represent the same variable, type the name of the variable, for example, Temperature.If the Variable field does not already exist, an existing field or string value must be specified. If the Variable field exists, the tool will update the multidimensional information only. | String |
| dimension_fields[[dimension field, description, units],...](Optional) | The fields in the mosaic dataset that store the dimension information and are used to populate a new field named Dimensions.If the Dimensions field already exists, the tool will update the multidimensional information only. | Value Table |
| variable_desc_units[[variable name, description, units],...](Optional) | Specify additional information about the Variable field. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.BuildMultidimensionalInfo(in_mosaic_dataset, {variable_field}, {dimension_fields}, {variable_desc_units})
```

### Example 2

```python
## Build multidimensional information for a time series mosaic dataset 
## with Landsat 7 imagery.

import arcpy
arcpy.md.BuildMultidimensionalInfo(
	"C:/data/input.gdb/L7TimeSeriesMosaic", "Landsat7", 'AcquisitionDate')
```

### Example 3

```python
## Build multidimensional information for a time series mosaic dataset 
## with Landsat 7 imagery.

import arcpy
arcpy.md.BuildMultidimensionalInfo(
	"C:/data/input.gdb/L7TimeSeriesMosaic", "Landsat7", 'AcquisitionDate')
```

### Example 4

```python
## Build multidimensional information for a mosaic dataset 
## containing sea ice extent imagery over time and water depth.

import arcpy
arcpy.env.workspace = "C:/data"

## Define the input parameters
inputmosaicdataset = "input.gdb/seaice_1982_2019"
variable_field = "measurement"
dimension_fields= "AcquisitionDate;Depth"

arcpy.md.BuildMultidimensionalInfo(
	inputmosaicdataset, variable_field, 
	dimension_fields)
```

### Example 5

```python
## Build multidimensional information for a mosaic dataset 
## containing sea ice extent imagery over time and water depth.

import arcpy
arcpy.env.workspace = "C:/data"

## Define the input parameters
inputmosaicdataset = "input.gdb/seaice_1982_2019"
variable_field = "measurement"
dimension_fields= "AcquisitionDate;Depth"

arcpy.md.BuildMultidimensionalInfo(
	inputmosaicdataset, variable_field, 
	dimension_fields)
```

---

## Build Multidimensional Transpose (Data Management)

## Summary

Transposes a multidimensional raster dataset, which divides the multidimensional data along each dimension to optimize performance when accessing pixel values across all slices.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Multidimensional Raster | The input CRF multidimensional raster dataset. | Raster Layer |
| in_multidimensional_raster | The input CRF multidimensional raster dataset. | Raster Layer |

## Code Samples

### Example 1

```python
arcpy.management.BuildMultidimensionalTranspose(in_multidimensional_raster)
```

### Example 2

```python
import arcpy
## Build the transpose for a sea surface temperature CRF dataset
arcpy.management.BuildMultidimensionalTranspose(
	"C:/Multidimensional/SST_1992_2018")
```

### Example 3

```python
import arcpy
## Build the transpose for a sea surface temperature CRF dataset
arcpy.management.BuildMultidimensionalTranspose(
	"C:/Multidimensional/SST_1992_2018")
```

### Example 4

```python
## import arcpy and set workspace

import arcpy
arcpy.env.workspace = "C:/Workspace/data"

## Build the transpose for a CRF of temperature data

arcpy.management.BuildMultidimensionalTranspose(
	"Temperature_CRF")
```

### Example 5

```python
## import arcpy and set workspace

import arcpy
arcpy.env.workspace = "C:/Workspace/data"

## Build the transpose for a CRF of temperature data

arcpy.management.BuildMultidimensionalTranspose(
	"Temperature_CRF")
```

---

## Build Overviews (Data Management)

## Summary

Defines and generates overviews on a mosaic dataset.

## Usage

- This tool will honor the default mosaic dataset settings, not the properties set on a mosaic dataset layer. For instance, the tool will not accept layer property changes such as band count, mosaic method, extent, or query—these properties will be determined by the mosaic dataset and default properties of the mosaic dataset.
- You must check on Define missing overview tiles, Generate overviews, or both.
- The first overview level is created on the full resolution of the mosaic dataset. Each subsequent level is built on the last overview level that was generated.
- If you need more control over the definition of the overviews, use the Define Overviews tool first.
- By default, the overviews generated for mosaic datasets in a file geodatabase are stored in a folder <gdbname>.Overviews in the same location where the geodatabase containing the mosaic dataset exists. The overviews generated for mosaic datasets on the enterprise geodatabase are stored within the geodatabase containing the mosaic dataset. These locations can be changed by first using the Define Overviews tool and specifying a location.
- You cannot build overviews for a referenced mosaic dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset where you want to build overviews. | Mosaic Layer |
| Query Definition (Optional) | An SQL statement to select specific rasters within the mosaic dataset. The selected rasters will have their overview built. | SQL Expression |
| Define Missing Overview Tiles (Optional) | Identify where overviews are needed and define them.Checked—Automatically identify where overviews are needed and define them. This is the default.Unchecked—Do not define new overviews. | Boolean |
| Generate Overviews (Optional) | Generate all overviews that need to be created or re-created. This includes missing overviews and stale overviews.Checked—Generate all overviews, including those that already exist. This is the default.Unchecked—Generate only the overviews that have been defined but not yet generated. | Boolean |
| Generate Missing Overview Images Only(Optional) | Use if overviews have been defined but not generated.Checked—Overviews that have been defined but not generated will be generated. This is the default.Unchecked—Overviews that have been defined but not generated will not be generated. | Boolean |
| Regenerate Stale Overview Images Only(Optional) | Overviews become stale when you change the underlying raster datasets or modify their properties.Checked—Identify and regenerate stale overviews. This is the default.Unchecked—Do not regenerate stale overviews. | Boolean |
| in_mosaic_dataset | The mosaic dataset where you want to build overviews. | Mosaic Layer |
| where_clause(Optional) | An SQL statement to select specific rasters within the mosaic dataset. The selected rasters will have their overview built. | SQL Expression |
| define_missing_tiles(Optional) | Identify where overviews are needed and define them.DEFINE_MISSING_TILES—Automatically identify where overviews are needed and define them. This is the default.NO_DEFINE_MISSING_TILES— Do not define new overviews. | Boolean |
| generate_overviews(Optional) | Generate all overviews that need to be created or re-created. This includes missing overviews and stale overviews.GENERATE_OVERVIEWS—Generate all overviews, including those that already exist. This is the default.NO_GENERATE_OVERVIEWS—Generate only the overviews that have been defined but not yet generated. | Boolean |
| generate_missing_images(Optional) | Use if overviews have been defined but not generated.GENERATE_MISSING_IMAGES—Generate overviews that have been defined but not generated. This is the default.IGNORE_MISSING_IMAGES— Do not generate overviews that have been defined but not generated. | Boolean |
| regenerate_stale_images(Optional) | Overviews become stale when you change the underlying raster datasets or modify their properties.REGENERATE_STALE_IMAGES—Identify and regenerate stale overviews. This is the default.IGNORE_STALE_IMAGES— Do not regenerate stale overviews. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BuildOverviews(in_mosaic_dataset, {where_clause}, {define_missing_tiles}, {generate_overviews}, {generate_missing_images}, {regenerate_stale_images})
```

### Example 2

```python
import arcpy
arcpy.BuildOverviews_management(
     "C:/Workspace/Overviews.gdb/md", "OBJECTID<5", "DEFINE_MISSING_TILES", 
     "NO_GENERATE_OVERVIEWS", "IGNORE_MISSING_IMAGES", "IGNORE_STALE_IMAGES")
```

### Example 3

```python
import arcpy
arcpy.BuildOverviews_management(
     "C:/Workspace/Overviews.gdb/md", "OBJECTID<5", "DEFINE_MISSING_TILES", 
     "NO_GENERATE_OVERVIEWS", "IGNORE_MISSING_IMAGES", "IGNORE_STALE_IMAGES")
```

### Example 4

```python
# Define Overviews for selected items only

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
arcpy.BuildOverviews_management("Overviews.gdb/md", "OBJECTID<5", 
                                "DEFINE_MISSING_TILES",
                                "NO_GENERATE_OVERVIEWS", "#", "#")
```

### Example 5

```python
# Define Overviews for selected items only

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
arcpy.BuildOverviews_management("Overviews.gdb/md", "OBJECTID<5", 
                                "DEFINE_MISSING_TILES",
                                "NO_GENERATE_OVERVIEWS", "#", "#")
```

---

## Build Pyramids And Statistics (Data Management)

## Summary

Traverses a folder structure, building pyramids and calculating statistics for all the raster datasets it contains. It can also build pyramids and calculate statistics for all the items in a mosaic dataset.

## Usage

- Building pyramids improves the display performance of raster datasets.
- Calculating statistics allows ArcGIS applications to properly stretch and symbolize raster data for display.
- All supported raster formats will be processed.
- If the workspace includes a mosaic dataset, only the statistics associated with the mosaic dataset will be included. The statistics associated with the items within the mosaic dataset will not be included.
- Wavelet compressed raster datasets, such as ECW, JPEG2000, and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data or Workspace | The workspace that contains all the raster datasets or mosaic datasets to be processed.If the workspace includes a mosaic dataset, only the statistics associated with the mosaic dataset will be included. The statistics associated with the items within the mosaic dataset will not be included. | Text File; Workspace; Raster Layer; Mosaic Layer |
| Include Sub-directories(Optional) | Specifies whether subdirectories will be included.Unchecked—Does not include subdirectories.Checked—Includes all the raster datasets within the subdirectories when loading. This is the default.Mosaic datasets must be specified as an input workspace if you want to include the items within the mosaic dataset. Otherwise, only the statistics associated with the mosaic dataset will be used. | Boolean |
| Build Pyramids(Optional) | Specifies whether pyramids will be built.Unchecked—Pyramids will not be built.Checked—Pyramids will be built. This is the default. | Boolean |
| Calculate Statistics(Optional) | Specify whether to calculate statistics.Unchecked—Does not calculate statistics.Checked—Calculates statistics. This is the default. | Boolean |
| Include Source Datasets(Optional) | Specify whether to calculate statistics on the source raster datasets, or calculate statistics on the raster items in a mosaic dataset. This option only applies to mosaic datasets. Unchecked—Statistics will be calculated for each raster item in the mosaic dataset (on each row in the attribute table). Any functions added to the raster item will be applied before generating the statistics. This is the default.Checked—Calculates statistics on the source data of the mosaic dataset. | Boolean |
| Block Field (Optional) | The name of the field within a mosaic dataset's attribute table used to identify items that should be considered one item when performing some calculations and operations. | String |
| Estimate Mosaic Dataset Statistics (Optional) | Specify whether to calculate statistics for the mosaic dataset (not the rasters within it). The statistics are derived from the existing statistics that have been calculated for each raster in the mosaic dataset.Unchecked—Statistics are not calculated for the mosaic dataset. This is the default.Checked—Statistics will be calculated for the mosaic dataset. | Boolean |
| X Skip Factor (Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used. | Long |
| Y Skip Factor (Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used. | Long |
| Ignore Values (Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value. | Long |
| Pyramid levels (Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels.The maximum number of pyramid levels you can specify is 29. Any value of 30 or higher will create a full set of pyramids. | Long |
| Skip first level (Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales. Unchecked—The first pyramid level will not be skipped; it will be built. This is the default.Checked—The first pyramid level will be skipped; it will not be built. | Boolean |
| Pyramid resampling technique (Optional) | Specifies the resampling technique that will be used to build the pyramids.Nearest neighbor—The value of the closest pixel will be used to assign a value to the output pixel when resampling. This is the default.Bilinear—The new value of a pixel will be based on a weighted distance average of the four nearest input pixel centers.Cubic—The new value of a pixel will be based on fitting a smooth curve through the 16 nearest input pixel centers. | String |
| Pyramid compression type (Optional) | Specifies the compression type that will be used when building the raster pyramids.Default—If the source data is compressed using a wavelet compression, pyramids will be built using the JPEG compression type; otherwise, LZ77 will be used. This is the default.LZ77 Compression—The LZ77 compression algorithm will be used to build pyramids. This compression type can be used for any data type.JPEG Compression—The JPEG compression algorithm will be used to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If this compression type is specified, you can then set the Compression quality parameter value.JPEG Luma and Chroma—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components will be used.No compression—No compression will be used when building pyramids. | String |
| Compression quality (1-100) (Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| Skip Existing (Optional) | Specify whether to calculate statistics only where they are missing, or regenerate them even if they exist. Checked—Statistics will only be calculated if they do not already exist. This is the default.Unchecked—Statistics will be calculated even if they already exist; existing statistics will be overwritten. | Boolean |
| Query Definition(Optional) | An SQL expression to select raster datasets that will be processed. | SQL Expression |
| SIPS Mode(Optional) | Specifies whether to enable building of pyramid files using key processes and algorithms defined in the Softcopy Image Processing Standard (SIPS), NGA.STND.0014.Unchecked—Pyramids will be built using standard subsampling methods. This is the default.Checked—Pyramids will be built using SIPS processing. | Boolean |
| in_workspace | The workspace that contains all the raster datasets or mosaic datasets to be processed.If the workspace includes a mosaic dataset, only the statistics associated with the mosaic dataset will be included. The statistics associated with the items within the mosaic dataset will not be included. | Text File; Workspace; Raster Layer; Mosaic Layer |
| include_subdirectories(Optional) | Specifies whether subdirectories will be included.NONE—Does not include subdirectories. INCLUDE_SUBDIRECTORIES—Includes all the raster datasets within the subdirectories when loading. This is the default. If the workspace includes a mosaic dataset, only the statistics associated with mosaic dataset will be included. The statistics associated with the items within the mosaic dataset will not be included. | Boolean |
| build_pyramids(Optional) | Specifies whether pyramids will be built.NONE—Pyramids will not be built.BUILD_PYRAMIDS—Pyramids will be built. This is the default. | Boolean |
| calculate_statistics(Optional) | Specify whether to calculate statistics.NONE—Does not calculate statistics.CALCULATE_STATISTICS—Calculates statistics. This is the default. | Boolean |
| BUILD_ON_SOURCE(Optional) | Specify whether to calculate statistics on the source raster datasets, or calculate statistics on the raster items in a mosaic dataset. This option only applies to mosaic datasets. NONE—Statistics will be calculated for each raster item in the mosaic dataset (on each row in the attribute table). Any functions added to the raster item will be applied before generating the statistics. This is the default.BUILD_ON_SOURCE—Calculates statistics on the source data of the mosaic dataset. | Boolean |
| block_field(Optional) | The name of the field within a mosaic dataset's attribute table used to identify items that should be considered one item when performing some calculations and operations. | String |
| estimate_statistics(Optional) | Specify whether to calculate statistics for the mosaic dataset (not the rasters within it). The statistics are derived from the existing statistics that have been calculated for each raster in the mosaic dataset.NONE—Statistics are not calculated for the mosaic dataset. This is the default.ESTIMATE_STATISTICS—Statistics will be calculated for the mosaic dataset. | Boolean |
| x_skip_factor(Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used. | Long |
| y_skip_factor(Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used. | Long |
| ignore_values[ignore_value,...](Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value. | Long |
| pyramid_level(Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels.The maximum number of pyramid levels you can specify is 29. Any value of 30 or higher will create a full set of pyramids. | Long |
| SKIP_FIRST(Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales.NONE—The first pyramid level will not be skipped; it will be built. This is the default.SKIP_FIRST—The first pyramid level will be skipped; it will not be built. | Boolean |
| resample_technique(Optional) | Specifies the resampling technique that will be used to build the pyramids.NEAREST—The value of the closest pixel will be used to assign a value to the output pixel when resampling. This is the default.BILINEAR—The new value of a pixel will be based on a weighted distance average of the four nearest input pixel centers.CUBIC—The new value of a pixel will be based on fitting a smooth curve through the 16 nearest input pixel centers. | String |
| compression_type(Optional) | Specifies the compression type that will be used when building the raster pyramids.DEFAULT—If the source data is compressed using a wavelet compression, pyramids will be built using the JPEG compression type; otherwise, LZ77 will be used. This is the default.LZ77—The LZ77 compression algorithm will be used to build pyramids. This compression type can be used for any data type.JPEG—The JPEG compression algorithm will be used to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If this compression type is specified, you can then set the Compression quality parameter value.JPEG_YCBCR—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components will be used.NONE—No compression will be used when building pyramids. | String |
| compression_quality(Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| skip_existing(Optional) | Specify whether to calculate statistics only where they are missing, or regenerate them even if they exist.SKIP_EXISTING—Statistics will only be calculated if they do not already exist. This is the default.OVERWRITE—Statistics will be calculated even if they already exist; existing statistics will be overwritten. | Boolean |
| where_clause(Optional) | An SQL expression to select raster datasets that will be processed. | SQL Expression |
| sips_mode(Optional) | Specifies whether to enable building of pyramid files using key processes and algorithms defined in the Softcopy Image Processing Standard (SIPS), NGA.STND.0014.NONE—Pyramids will be built using standard subsampling methods. This is the default.SIPS_MODE—Pyramids will be built using SIPS processing. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BuildPyramidsandStatistics(in_workspace, {include_subdirectories}, {build_pyramids}, {calculate_statistics}, {BUILD_ON_SOURCE}, {block_field}, {estimate_statistics}, {x_skip_factor}, {y_skip_factor}, {ignore_values}, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {compression_type}, {compression_quality}, {skip_existing}, {where_clause}, {sips_mode})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.env.pyramid = "PYRAMIDS 3 BILINEAR JPEG"
arcpy.env.rasterStatistics = "STATISTICS 4 6 (0)"
arcpy.BuildPyramidsandStatistics_management("folder", "INCLUDE_SUBDIRECTORIES",
                                            "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.env.pyramid = "PYRAMIDS 3 BILINEAR JPEG"
arcpy.env.rasterStatistics = "STATISTICS 4 6 (0)"
arcpy.BuildPyramidsandStatistics_management("folder", "INCLUDE_SUBDIRECTORIES",
                                            "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
```

### Example 4

```python
##====================================
##Build Pyramids and Statistics
##Usage: BuildPyramidsandStatistics_management in_workspace {INCLUDE_SUBDIRECTORIES
##                                                 | NONE} {BUILD_PYRAMIDS | NONE}
##                                                 {CALCULATE_STATISTICS | NONE}
    
try:
    import arcpy
    arcpy.env.workspace = r"C:/Workspace"

    ##Define parameters for build pyramids and calculate statitics in environment setting
    arcpy.env.pyramid = "PYRAMIDS 3 BILINEAR JPEG"
    arcpy.env.rasterStatistics = "STATISTICS 4 6 (0)"
    
    ##Build pyramids and calculate statistics for all raster in a folder
    arcpy.BuildPyramidsandStatistics_management("folder", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
    
    ##Build pyramids and calculate statistics for all raster in a GDB
    arcpy.BuildPyramidsandStatistics_management("fgdb.gdb", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
    
    ##Build pyramids and calculate statistics for all raster in a Mosaic Dataset
    arcpy.BuildPyramidsandStatistics_management("fgdb.gdb/md", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")

except:
    print "Build Pyramids and Statistics example failed."
    print arcpy.GetMessages()
```

### Example 5

```python
##====================================
##Build Pyramids and Statistics
##Usage: BuildPyramidsandStatistics_management in_workspace {INCLUDE_SUBDIRECTORIES
##                                                 | NONE} {BUILD_PYRAMIDS | NONE}
##                                                 {CALCULATE_STATISTICS | NONE}
    
try:
    import arcpy
    arcpy.env.workspace = r"C:/Workspace"

    ##Define parameters for build pyramids and calculate statitics in environment setting
    arcpy.env.pyramid = "PYRAMIDS 3 BILINEAR JPEG"
    arcpy.env.rasterStatistics = "STATISTICS 4 6 (0)"
    
    ##Build pyramids and calculate statistics for all raster in a folder
    arcpy.BuildPyramidsandStatistics_management("folder", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
    
    ##Build pyramids and calculate statistics for all raster in a GDB
    arcpy.BuildPyramidsandStatistics_management("fgdb.gdb", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")
    
    ##Build pyramids and calculate statistics for all raster in a Mosaic Dataset
    arcpy.BuildPyramidsandStatistics_management("fgdb.gdb/md", "INCLUDE_SUBDIRECTORIES",
                                                "BUILD_PYRAMIDS", "CALCULATE_STATISTICS")

except:
    print "Build Pyramids and Statistics example failed."
    print arcpy.GetMessages()
```

---

## Build Pyramids (Data Management)

## Summary

Builds raster pyramids for your raster dataset.

## Usage

- Building pyramids improves the display performance of raster datasets.
- You only need to build pyramids once per dataset. The pyramids will be accessed every time you display the raster dataset.
- Pyramids will not be built for raster datasets that have less than 1,024 pixels in the row or column. Pyramids are not needed since the raster dataset is small enough, and building pyramids will not help improve performance.
- Wavelet compressed raster datasets, such as ECW, JPEG2000, and MrSID, do not need to have pyramids built. These formats have internal pyramids that are created upon encoding.
- You can choose the compression type for your overview pyramid file using the raster storage environments. Compression will create a smaller .ovr file. The IMAGINE format and older versions of ArcGIS will create reduced-resolution dataset files (.rrd) when compression is not available.
- The default pyramid compression will use the optimal compression type, depending on the data type. You can manually choose LZ77, JPEG, or no compression.
- JPEG compression can only be used with file formats that can store data according to the JPEG specifications. The list of supported raster dataset file formats lists which specification the JPEG format can support.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Dataset | The raster dataset for which you want to build pyramids.The input should have more than 1,024 rows and 1,024 columns. | Raster Dataset; Raster Layer |
| Pyramid levels (Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels.To delete pyramids, set the number of levels to 0.The maximum number of pyramid levels you can specify is 29. Any value that is 30 or higher will revert to a value of -1, which will create a full set of pyramids. | Long |
| Skip first level (Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales. Unchecked—The first pyramid level will not be skipped; it will be built. This is the default.Checked—The first pyramid level will be skipped; it will not be built. | Boolean |
| Pyramid resampling technique (Optional) | Specifies the resampling technique that will be used to build the pyramids.Nearest—The value of the closest pixel will be used to assign a value to the output pixel when resampling. This is the default.Bilinear—The new value of a pixel will be based on a weighted distance average of the four nearest input pixel centers.Cubic—The new value of a pixel will be based on fitting a smooth curve through the 16 nearest input pixel centers. | String |
| Pyramid compression type(Optional) | Specifies the compression type that will be used when building the raster pyramids.Default—If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method.LZ77—The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type.Jpeg—The JPEG compression algorithm will be used to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality.Jpeg Luma and Chroma—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components will be used to build pyramids.No compression—No compression will be used when building pyramids. | String |
| Compression quality (1-100) (Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| Skip Existing (Optional) | Specifies whether pyramids will be built only when they are missing or will be regenerated even if they exist.Unchecked—Pyramids will be built even if they already exist, and existing pyramids will be overwritten. This is the default.Checked—Pyramids will only be built if they do not already exist. | Boolean |
| in_raster_dataset | The raster dataset for which you want to build pyramids.The input should have more than 1,024 rows and 1,024 columns. | Raster Dataset; Raster Layer |
| pyramid_level(Optional) | The number of reduced-resolution dataset layers that will be built. The default value is -1, which will build full pyramids. A value of 0 will result in no pyramid levels.To delete pyramids, set the number of levels to 0.The maximum number of pyramid levels you can specify is 29. Any value that is 30 or higher will revert to a value of -1, which will create a full set of pyramids. | Long |
| SKIP_FIRST(Optional) | Specifies whether the first pyramid level will be skipped. Skipping the first level will take up slightly less disk space but will slow down performance at these scales.NONE—The first pyramid level will not be skipped; it will be built. This is the default.SKIP_FIRST—The first pyramid level will be skipped; it will not be built. | Boolean |
| resample_technique(Optional) | Specifies the resampling technique that will be used to build the pyramids.NEAREST—The value of the closest pixel will be used to assign a value to the output pixel when resampling. This is the default.BILINEAR—The new value of a pixel will be based on a weighted distance average of the four nearest input pixel centers.CUBIC—The new value of a pixel will be based on fitting a smooth curve through the 16 nearest input pixel centers. | String |
| compression_type(Optional) | Specifies the compression type that will be used when building the raster pyramids.DEFAULT—If the source data is compressed using a wavelet compression, it will build pyramids with the JPEG compression type; otherwise, LZ77 will be used. This is the default compression method.LZ77—The LZ77 compression algorithm will be used to build the pyramids. LZ77 can be used for any data type.JPEG—The JPEG compression algorithm will be used to build pyramids. Only data that adheres to the JPEG compression specification can use this compression type. If JPEG is chosen, you can then set the compression quality.JPEG_YCbCr—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components will be used to build pyramids.NONE—No compression will be used when building pyramids. | String |
| compression_quality(Optional) | The compression quality that will be used when pyramids are built with the JPEG compression type. The value must be between 0 and 100. The values closer to 100 will produce a higher-quality image, but the compression ratio will be lower. | Long |
| skip_existing(Optional) | Specifies whether pyramids will be built only when they are missing or will be regenerated even if they exist.OVERWRITE—Pyramids will be built even if they already exist, and existing pyramids will be overwritten. This is the default.SKIP_EXISTING—Pyramids will only be built if they do not already exist. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BuildPyramids(in_raster_dataset, {pyramid_level}, {SKIP_FIRST}, {resample_technique}, {compression_type}, {compression_quality}, {skip_existing})
```

### Example 2

```python
import arcpy
arcpy.BatchBuildPyramids_management(
     "C:/data/img1.tif;C:/data/img2.img", "6", "SKIP_FIRST",
      "BILINEAR", "JPEG", "50", "SKIP_EXISTING")
```

### Example 3

```python
import arcpy
arcpy.BatchBuildPyramids_management(
     "C:/data/img1.tif;C:/data/img2.img", "6", "SKIP_FIRST",
      "BILINEAR", "JPEG", "50", "SKIP_EXISTING")
```

### Example 4

```python
#Build Pyramids for multiple raster datasets in the workspace
#Skip the dataset that already has pyramid
#Build pyramids with compression and level setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
inras = "image1.tif;image2.img;fgdb.gdb/image3"
pylevels = "6"
skipfirst = "SKIP_FIRST"
resample = "BILINEAR"
compress = "JPEG"
quality = "80"
skipexist = "SKIP_EXISTING"

arcpy.BatchBuildPyramids_management(
     inras, pylevels, skipfirst, resample, compress,
     quality, skipexist)
```

### Example 5

```python
#Build Pyramids for multiple raster datasets in the workspace
#Skip the dataset that already has pyramid
#Build pyramids with compression and level setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
inras = "image1.tif;image2.img;fgdb.gdb/image3"
pylevels = "6"
skipfirst = "SKIP_FIRST"
resample = "BILINEAR"
compress = "JPEG"
quality = "80"
skipexist = "SKIP_EXISTING"

arcpy.BatchBuildPyramids_management(
     inras, pylevels, skipfirst, resample, compress,
     quality, skipexist)
```

---

## Build Raster Attribute Table (Data Management)

## Summary

Adds a raster attribute table to a raster dataset or updates an existing one. This is used primarily with discrete data.

## Usage

- To delete an existing table and create a new one, check the Overwrite parameter. A new raster attribute table will be created, and the existing one will be deleted.
- If you have an existing table and you do not check the Overwrite parameter, the table will be updated. No fields will be deleted, but the values in the table will be updated.
- You cannot build a raster attribute table for a raster dataset that is a pixel type of 32-bit floating point.
- You can generate an attribute table for a multidimensional dataset. When using this tool on a multidimensional mosaic dataset or a multidimensional raster, the output table will contain the pixel count of each class in each slice. The Count_S0 field will contain the pixel count of each class in the first slice of the dataset. The Count_S1 field will contain the pixel count of each class in the second slice, and so on. The variables must contain categorical data such as land cover.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input raster dataset to which a table will be added. This tool will not run if the pixel type is floating point or double precision. | Raster Layer |
| Overwrite(Optional) | Specifies whether the existing table will be overwritten.None—The existing raster attribute table will not be overwritten and any edits will be appended to it. This is the default.Overwrite—The existing raster attribute table will be overwritten and a new raster attribute table will be created. | Boolean |
| Convert colormap(Optional) | Specifies whether the color map will be converted to a raster attribute table. The output raster attribute table will include Red, Green, and Blue fields containing color values from the color map. These fields define the display colors for the corresponding class values.This parameter only applies when the Input Raster parameter value includes an associated color map.Convert Colormap—The color map will be converted to a new raster attribute table.None—The color map will not be converted to a raster attribute table. This is the default. | Boolean |
| in_raster | The input raster dataset to which a table will be added. This tool will not run if the pixel type is floating point or double precision. | Raster Layer |
| overwrite(Optional) | Specifies whether the existing table will be overwritten.NONE—The existing raster attribute table will not be overwritten and any edits will be appended to it. This is the default.Overwrite—The existing raster attribute table will be overwritten and a new raster attribute table will be created. | Boolean |
| convert_colormap(Optional) | Specifies whether the color map will be converted to a raster attribute table. The output raster attribute table will include Red, Green, and Blue fields containing color values from the color map. These fields define the display colors for the corresponding class values.This parameter only applies when the Input Raster parameter value includes an associated color map.Checked—The color map will be converted to a new raster attribute table.Unchecked—The color map will not be converted to a raster attribute table. This is the default.ConvertColormap—The color map will be converted to a new raster attribute table.NONE—The color map will not be converted to a raster attribute table. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BuildRasterAttributeTable(in_raster, {overwrite}, {convert_colormap})
```

### Example 2

```python
import arcpy
arcpy.BuildRasterAttributeTable_management("c:/data/image.tif", "Overwrite")
```

### Example 3

```python
import arcpy
arcpy.BuildRasterAttributeTable_management("c:/data/image.tif", "Overwrite")
```

### Example 4

```python
##====================================
##Build Raster Attribute Table
##Usage: BuildRasterAttributeTable_management in_raster {NONE | Overwrite}
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Build attribute table for single band raster dataset
##Overwrite the existing attribute table file
arcpy.BuildRasterAttributeTable_management("image.tif", "Overwrite")
```

### Example 5

```python
##====================================
##Build Raster Attribute Table
##Usage: BuildRasterAttributeTable_management in_raster {NONE | Overwrite}
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Build attribute table for single band raster dataset
##Overwrite the existing attribute table file
arcpy.BuildRasterAttributeTable_management("image.tif", "Overwrite")
```

---

## Build Seamlines (Data Management)

## Summary

Generate or update seamlines for your mosaic dataset. Seamlines are used to sort overlapping imagery and produce a smoother-looking mosaic.

## Usage

- The seamlines are generated so that there is one seamline per footprint.
- You cannot build seamlines for a referenced mosaic dataset.
- If you plan on color correcting your mosaic dataset, it is recommended that you do so before building seamlines. Color correction is especially important if the Computation Method is set to Radiometry, as color is taken into account when seamlines are built.
- The Sort Method and Sort Ascending parameters are disabled if Update Existing Seamlines option is checked on.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | Select the mosaic dataset on which to build seamlines. | Mosaic Layer |
| Cell Size | Generate seamlines for raster datasets that fall within the following range of spatial resolutions. You can leave this parameter empty and the tool will automatically create seamlines at the appropriate levels.The units for this parameter are the same as the spatial reference of the input mosaic dataset. | Double |
| Sort Method(Optional) | Set a rule to determine which raster will be used to generate seamlines when images overlap.Northwest— Select the raster datasets that have center points closest to the northwest corner of the boundary. This is the default.Closest to viewpoint— Select raster datasets based on a user-defined location and a nadir location for the raster datasets using the Viewpoint tool.By attribute— Select raster datasets based on an attribute from the footprint attribute table. Commonly used attributes include acquisition date, cloud cover, or viewing angle. | String |
| Sort Ascending (Optional) | Sort the raster datasets in ascending or descending order.Checked—Sort the rasters in ascending order. This is the default.Unchecked—Sort the rasters in descending order. | Boolean |
| Sort Attribute(Optional) | Order the raster datasets based on this field when the sort method is By Attribute. The default attribute is ObjectID. | Field |
| Sort Base Value (Optional) | Sort the rasters by their difference between this value and their value in the Sort Attribute parameter. | Variant |
| View Point(Optional) | Set the coordinate location to use when Sort Method is Closest to viewpoint. | Point |
| Computation Method (Optional) | Choose how to build seamlines.The Sort Method parameter applies to each computation method.Geometry—Generate seamlines for overlapping areas based on the intersection of footprints. Areas with no overlapping imagery will merge the footprints. This is the default.Radiometry—Generate seamlines based on the spectral patterns of features within the imagery.Copy footprint—Generate seamlines directly from the footprints.Copy to sibling—Apply the seamlines from another mosaic dataset. The mosaic datasets have to be in the same group. For example, the extent of the panchromatic band does not always match the extent of the multispectral band. This option makes sure they share the same seamline.Edge detection—Generate seamlines over intersecting areas based on the edges of features in the area.Voronoi—Generate seamlines using the area Voronoi diagram.Disparity—Generate seamlines based on the disparity images of stereo pairs. This method can avoid seamlines cutting through buildings. | String |
| Blend Width(Optional) | Blending (feathering) occurs along a seamline between pixels where there are overlapping rasters. The blend width defines how many pixels will be blended.If the blend width value is 10, and you use Both as the blend type, then 5 pixels will be blended on the inside and outside of the seamline. If the value is 10, and the blend type is Inside, then 10 pixels will be blended on the inside of the seamline. | Double |
| Blend Type(Optional) | Determine how to blend one image into another, over the seamlines. Options are to blend inside the seamlines, outside the seamlines, or both inside and outside.Both— Blend using pixels on either side of the seamlines. For example, if the Blend Width is 10 pixels, then five pixels will be blended on the inside and outside of the seamline. This is the default.Inside—Blend inside of the seamline.Outside—Blend outside of the seamline. | String |
| Request Size(Optional) | Specify the number of columns and rows for resampling. The maximum value is 5,000. Increase or decrease this value based on the complexity of your raster data. Greater image resolution provides more detail in the raster dataset but also increases the processing time. | Long |
| Request Size Type (Optional) | Set the units for the Request Size.Pixels—Modify the request size based on the pixel size.This is the default option and resamples the closest image based on the raster pixel size.Pixel scaling factor—Modify the request size by specifying a scaling factor. This option resamples the closest image by multiplying the raster pixel size (from cell size level table) with the pixel size factor. | String |
| Blend Width Units (Optional) | Specify the unit of measurement for blend width.Pixels—Measure using the number of pixels. This is the default.Ground units—Measure using the same units as the mosaic dataset. | String |
| Area of Interest (Optional) | Build seamlines on all the rasters that intersect this polygon. To specify an area of interest, browse to a feature class, or create a polygon graphic on the display. | Feature Set |
| Query Definition (Optional) | SQL expression to build seamlines on specific raster datasets within the mosaic dataset. | SQL Expression |
| Update Existing Seamlines (Optional) | Update seamlines that are affected by the addition or deletion of mosaic dataset items. This option is enabled only if seamlines were generated previously and it will use the existing sort method and sort order to generate seamlines.Unchecked—Regenerates seamlines for all items and ignores existing seamlines, if any. This is the default. Checked—Only update items without seamlines. If any new items overlap with the previously created seamlines, the existing seamlines may be affected.This parameter is disabled if seamlines do not exist. | Boolean |
| Minimum Region Size (Optional) | Specify the minimum region size, in pixel units. Any polygons smaller than this specified threshold will be removed in the seamline result. The default is 100 pixels. This parameter value should be smaller than the sliver area, which is defined as (Maximum Sliver Size) * (Maximum Sliver Size). | Long |
| Minimum Thinness Ratio (Optional) | Define how thin a polygon can be, before it is considered a sliver. This is based on a scale from 0 to 1.0, where a value of 0.0 represents a polygon that is almost a straight line, and a value of 1.0 represents a polygon that is a circle.Slivers are removed when building seamlines. | Double |
| Maximum Sliver Size (Optional) | Specify the maximum size a polygon can be to still be considered a sliver. This parameter is specified in pixels and is based on the Request Size, not the spatial resolution of the source raster. Any polygon that is less than the square of this value is considered a sliver. Any regions that are less than (Maximum Sliver Size)2 are considered slivers. Slivers are removed when building seamlines. | Long |
| in_mosaic_dataset | Select the mosaic dataset on which to build seamlines. | Mosaic Layer |
| cell_size[cell_size,...] | Generate seamlines for raster datasets that fall within the following range of spatial resolutions. You can leave this parameter empty and the tool will automatically create seamlines at the appropriate levels.The units for this parameter are the same as the spatial reference of the input mosaic dataset. | Double |
| sort_method(Optional) | Set a rule to determine which raster will be used to generate seamlines when images overlap.NORTH_WEST— Select the raster datasets that have center points closest to the northwest corner of the boundary. This is the default.CLOSEST_TO_VIEWPOINT— Select raster datasets based on a user-defined location and a nadir location for the raster datasets using the Viewpoint tool.BY_ATTRIBUTE— Select raster datasets based on an attribute from the footprint attribute table. Commonly used attributes include acquisition date, cloud cover, or viewing angle. | String |
| sort_order(Optional) | Choose whether to sort the rasters in ascending order or descending order.ASCENDING— Sort the rasters in ascending order. This is the default.DESCENDING— Sort the rasters in descending order. | Boolean |
| order_by_attribute(Optional) | Order the raster datasets based on this field when the sort method is BY_ATTRIBUTE. The default attribute is ObjectID. | Field |
| order_by_base_value(Optional) | Sort the rasters by their difference between this value and their value in the order_by_attribute parameter. | Variant |
| view_point(Optional) | Set the coordinate location to use when sort_method is CLOSEST_TO_VIEWPOINT. | Point |
| computation_method(Optional) | Choose how to build seamlines.GEOMETRY—Generate seamlines for overlapping areas based on the intersection of footprints. Areas with no overlapping imagery will merge the footprints. This is the default.RADIOMETRY—Generate seamlines based on the spectral patterns of features within the imagery.COPY_FOOTPRINT—Generate seamlines directly from the footprints.COPY_TO_SIBLING—Apply the seamlines from another mosaic dataset. The mosaic datasets have to be in the same group. For example, the extent of the panchromatic band does not always match the extent of the multispectral band. This option makes sure they share the same seamline.EDGE_DETECTION—Generate seamlines over intersecting areas based on the edges of features in the area.VORONOI—Generate seamlines using the area Voronoi diagram.DISPARITY—Generate seamlines based on the disparity images of stereo pairs. This method can avoid seamlines cutting through buildings.The Sort Method parameter applies to each computation method. | String |
| blend_width(Optional) | Blending (feathering) occurs along a seamline between pixels where there are overlapping rasters. The blend width defines how many pixels will be blended.If the blend width value is 10, and you use BOTH as the blend type, then 5 pixels will be blended on the inside and outside of the seamline. If the value is 10, and the blend type is INSIDE, then 10 pixels will be blended on the inside of the seamline. | Double |
| blend_type(Optional) | Determine how to blend one image into another, over the seamlines. Options are to blend inside the seamlines, outside the seamlines, or both inside and outside.BOTH— Blend using pixels on either side of the seamlines. For example, if the Blend Width is 10 pixels, then five pixels will be blended on the inside and outside of the seamline. This is the default.INSIDE—Blend inside of the seamline.OUTSIDE—Blend outside of the seamline. | String |
| request_size(Optional) | Specify the number of columns and rows for resampling. The maximum value is 5,000. Increase or decrease this value based on the complexity of your raster data. Greater image resolution provides more detail in the raster dataset but also increases the processing time. | Long |
| request_size_type(Optional) | Set the units for the Request Size.PIXELS—Modify the request size based on the pixel size.This is the default option and resamples the closest image based on the raster pixel size.PIXELSIZE_FACTOR—Modify the request size by specifying a scaling factor. This option resamples the closest image by multiplying the raster pixel size (from cell size level table) with the pixel size factor. | String |
| blend_width_units(Optional) | Specify the unit of measurement for blend width.PIXELS—Measure using the number of pixels. This is the default.GROUND_UNITS—Measure using the same units as the mosaic dataset. | String |
| area_of_interest(Optional) | Build seamlines on all the rasters that intersect this polygon. To select an area of interest, use an input feature class. | Feature Set |
| where_clause(Optional) | SQL expression to build seamlines on specific raster datasets within the mosaic dataset. | SQL Expression |
| update_existing(Optional) | Update seamlines that are affected by the addition or deletion of the mosaic dataset items.IGNORE_EXISTING—Regenerates seamlines for all items and ignores existing seamlines, if any. This is the default.UPDATE_EXISTING—Only update items without seamlines. If any new items overlap with the previously created seamlines, the existing seamlines may be affected. This parameter is ignored if seamlines do not exist. | Boolean |
| min_region_size(Optional) | Specify the minimum region size, in pixel units. Any polygons smaller than this specified threshold will be removed in the seamline result. The default is 100 pixels. This parameter value should be smaller than the sliver area, which is (max_sliver_size) * (max_sliver_size). | Long |
| min_thinness_ratio(Optional) | Define how thin a polygon can be, before it is considered a sliver. This is based on a scale from 0 to 1.0, where a value of 0.0 represents a polygon that is almost a straight line, and a value of 1.0 represents a polygon that is a circle.Slivers are removed when building seamlines. | Double |
| max_sliver_size(Optional) | Specify the maximum size a polygon can be to still be considered a sliver. This parameter is specified in pixels and is based on the request_size, not the spatial resolution of the source raster. Any polygon that is less than the square of this value is considered a sliver. Any regions that are less than (max_sliver_size)2 are considered slivers. Slivers are removed when building seamlines. | Long |

## Code Samples

### Example 1

```python
arcpy.management.BuildSeamlines(in_mosaic_dataset, cell_size, {sort_method}, {sort_order}, {order_by_attribute}, {order_by_base_value}, {view_point}, {computation_method}, {blend_width}, {blend_type}, {request_size}, {request_size_type}, {blend_width_units}, {area_of_interest}, {where_clause}, {update_existing}, {min_region_size}, {min_thinness_ratio}, {max_sliver_size})
```

### Example 2

```python
import arcpy
arcpy.BuildSeamlines_management("c:/data/Seamlines.gdb/md", "40",
                                "NORTH_WEST", "#", "#", "#", "#",
                                "RADIOMETRY", "5", "INSIDE", "#", 
                                "#", "GROUND_UNITS")
```

### Example 3

```python
import arcpy
arcpy.BuildSeamlines_management("c:/data/Seamlines.gdb/md", "40",
                                "NORTH_WEST", "#", "#", "#", "#",
                                "RADIOMETRY", "5", "INSIDE", "#", 
                                "#", "GROUND_UNITS")
```

### Example 4

```python
# Build seamlines using the  NORTH_WEST sort method

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "Seamlines.gdb/md"
cellsize = "40"
sortmethod = "NORTH_WEST"
sortorder = "#"
orderattribute = "#"
orderbase = "#"
viewpnt = "#"
computemethod = "RADIOMETRY"
blendwidth = "5"
blendtype = "INSIDE"
requestsize = "#"

arcpy.BuildSeamlines_management(
    mdname, cellsize, sortmethod, sortorder, orderattribute, 
    orderbase, viewpnt, computemethod, blendwidth, blendtype, 
    requestsize)
```

### Example 5

```python
# Build seamlines using the  NORTH_WEST sort method

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "Seamlines.gdb/md"
cellsize = "40"
sortmethod = "NORTH_WEST"
sortorder = "#"
orderattribute = "#"
orderbase = "#"
viewpnt = "#"
computemethod = "RADIOMETRY"
blendwidth = "5"
blendtype = "INSIDE"
requestsize = "#"

arcpy.BuildSeamlines_management(
    mdname, cellsize, sortmethod, sortorder, orderattribute, 
    orderbase, viewpnt, computemethod, blendwidth, blendtype, 
    requestsize)
```

---

## Build Stereo Model (Data Management)

## Summary

Builds a stereo model of a mosaic dataset based on a user-provided stereo pair.

## Usage

- The order of the stereo pair in the point cloud calculation is determined by user-defined thresholds for the overlap area, intersection angle, adjustment quality, ground sample distance (GSD) difference, and Omega and Phi quality.
- If you want a specific pair to be used for point cloud generation, set a high value for this pair in the Use field of the stereo table. To open the stereo table, right-click the mosaic layer in the Contents pane and click Open > Stereo.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The mosaic dataset on which the stereo model will be built.Tip:Running the Apply Block Adjustment tool on the input mosaic dataset first will help create a more accurate stereo model. | Mosaic Dataset; Mosaic Layer |
| Minimum Intersection Angle (in degree)(Optional) | The value, in degrees, that defines the minimum angle the stereo pair must meet. The default is 10. | Double |
| Maximum Intersection Angle (in degree)(Optional) | The value, in degrees, that defines the maximum angle the stereo pair must meet. The default is 90. | Double |
| Minimum Area Overlap(Optional) | The percentage of the overlapping area over the whole image. The default is 0.5. | Double |
| Maximum Omega/Phi Difference (in degree)(Optional) | The maximum threshold for the Omega and Phi difference between the two image pairs. The Omega values and Phi values for the image pairs are compared. If the difference between either the two Omega or the two Phi values is above the threshold, the pairs will not be formatted as a stereo pair. | Double |
| Maximum GSD Difference(Optional) | The threshold for the maximum GSD between two images in a pair. If the resolution ratio between the two images is greater than the threshold value, the pairs will not be built as a stereo pair. The default is 2. | Double |
| Group by(Optional) | Builds the stereo model from raster items within the same group, defined by a mosaic dataset field such as RGB, Panchromatic, or Infrared. | Field |
| Only pick stereo models in the same flight line (Optional) | Specifies how the stereo models will be selected. Checked—Stereo pairs will be selected along the same flight line.Unchecked—Stereo pairs will be selected across flight lines. This is the default.Note:This parameter is not applicable to satellite-based sensors. | Boolean |
| in_mosaic_dataset | The mosaic dataset on which the stereo model will be built.Tip:Running the Apply Block Adjustment tool on the input mosaic dataset first will help create a more accurate stereo model. | Mosaic Dataset; Mosaic Layer |
| minimum_angle(Optional) | The value, in degrees, that defines the minimum angle the stereo pair must meet. The default is 10. | Double |
| maximum_angle(Optional) | The value, in degrees, that defines the maximum angle the stereo pair must meet. The default is 90. | Double |
| minimum_overlap(Optional) | The percentage of the overlapping area over the whole image. The default is 0.5. | Double |
| maximum_diff_OP(Optional) | The maximum threshold for the Omega and Phi difference between the two image pairs. The Omega values and Phi values for the image pairs are compared. If the difference between either the two Omega or the two Phi values is above the threshold, the pairs will not be formatted as a stereo pair. | Double |
| maximum_diff_GSD(Optional) | The threshold for the maximum GSD between two images in a pair. If the resolution ratio between the two images is greater than the threshold value, the pairs will not be built as a stereo pair. The default is 2. | Double |
| group_by(Optional) | Builds the stereo model from raster items within the same group, defined by a mosaic dataset field such as RGB, Panchromatic, or Infrared. | Field |
| same_flight(Optional) | Specifies how the stereo models will be selected.SAMEFLIGHT—Stereo pairs will be selected along the same flight line.NO_SAMEFLIGHT—Stereo pairs will be selected across flight lines.Note:This parameter is not applicable to satellite-based sensors. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.BuildStereoModel(in_mosaic_dataset, {minimum_angle}, {maximum_angle}, {minimum_overlap}, {maximum_diff_OP}, {maximum_diff_GSD}, {group_by}, {same_flight})
```

### Example 2

```python
import arcpy
arcpy.management.BuildStereoModel("c:/data/fgdb.gdb/md", 10, 70, 0.6, None, 2, None, "SAMEFLIGHT")
```

### Example 3

```python
import arcpy
arcpy.management.BuildStereoModel("c:/data/fgdb.gdb/md", 10, 70, 0.6, None, 2, None, "SAMEFLIGHT")
```

---

## Calculate Cell Size Ranges (Data Management)

## Summary

Computes the visibility levels of raster datasets in a mosaic dataset based on the spatial resolution.

## Usage

- This tool automatically calculates the cell size ranges for the mosaic dataset items. The calculated cell size ranges are stored in the mosaic dataset attribute table in the MinPS and MaxPS columns. If you need to have specific values for the MinPS and MaxPS columns, you can edit these values manually.
- You cannot calculate cell size ranges for a referenced mosaic dataset.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset to calculate the visibility levels for. | Mosaic Layer |
| Query Definition (Optional) | An SQL expression to select specific rasters in the mosaic dataset on which to calculate visibility levels. If no query is specified, all the mosaic dataset items will have their cell size ranges calculated. | SQL Expression |
| Compute Minimum Cell Sizes(Optional) | Compute the minimum pixel size for each selected raster dataset in the mosaic dataset.Checked—Compute the minimum pixel size. This is the default.Unchecked—Do not compute the minimum pixel size. | Boolean |
| Compute Maximum Cell Sizes(Optional) | Compute the maximum pixel size for each selected raster in the mosaic dataset.Checked—Compute the maximum pixel size. This is the default.Unchecked—Do not compute the maximum pixel size. | Boolean |
| Maximum Cell Size Range Factor(Optional) | Set a multiplication factor to apply to the native resolution. The default is 10, meaning that an image with a resolution of 30 meters will be visible at a scale appropriate for 300 meters. The relationship between cell size and scale is as follows: Cell Size = Scale * 0.0254 / 96 Scale = Cell Size * 96 / 0.0254 | Double |
| Cell Size Tolerance Factor(Optional) | Use this to group images with similar resolutions as having the same nominal resolution. For example 1 m imagery and 0.9 m imagery can be grouped together by setting this factor to 0.1, because they are within 10% of each other. | Double |
| Update Missing Values Only(Optional) | Calculate only the missing cell size range values.Unchecked—Calculate cell size minimum and maximum values for selected rasters within the mosaic dataset. This is the default.Checked—Calculate cell size minimum and maximum values only if they do not exist. | Boolean |
| in_mosaic_dataset | The mosaic dataset to calculate the visibility levels for. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select specific rasters in the mosaic dataset on which to calculate visibility levels. If no query is specified, all the mosaic dataset items will have their cell size ranges calculated. | SQL Expression |
| do_compute_min(Optional) | Compute the minimum pixel size for each selected raster in the mosaic dataset.MIN_CELL_SIZES—Compute the minimum pixel size. This is the default. NO_MIN_CELL_SIZES—Do not compute the minimum pixel size. | Boolean |
| do_compute_max(Optional) | Compute the maximum pixel size for each selected raster in the mosaic dataset.MAX_CELL_SIZES—Compute the maximum pixel size. This is the default.NO_MAX_CELL_SIZES—Do not compute the maximum pixel size. | Boolean |
| max_range_factor(Optional) | Set a multiplication factor to apply to the native resolution. The default is 10, meaning that an image with a resolution of 30 meters will be visible at a scale appropriate for 300 meters. The relationship between cell size and scale is as follows: Cell Size = Scale * 0.0254 / 96 Scale = Cell Size * 96 / 0.0254 | Double |
| cell_size_tolerance_factor(Optional) | Use this to group images with similar resolutions as having the same nominal resolution. For example 1 m imagery and 0.9 m imagery can be grouped together by setting this factor to 0.1, because they are within 10% of each other. | Double |
| update_missing_only(Optional) | Calculate only the missing cell size range values.UPDATE_ALL—Calculate cell size minimum and maximum values for selected rasters within the mosaic dataset. This is the default.UPDATE_MISSING_ONLY— Calculate cell size minimum and maximum values only if they do not exist. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CalculateCellSizeRanges(in_mosaic_dataset, {where_clause}, {do_compute_min}, {do_compute_max}, {max_range_factor}, {cell_size_tolerance_factor}, {update_missing_only})
```

### Example 2

```python
import arcpy
arcpy.CalculateCellSizeRanges_management(
     "C:/Workspace/cellsize.gdb/md", "#", "MIN_CELL_SIZES", 
     "MAX_CELL_SIZES", "20", "1", "UPDATE_MISSING_ONLY")
```

### Example 3

```python
import arcpy
arcpy.CalculateCellSizeRanges_management(
     "C:/Workspace/cellsize.gdb/md", "#", "MIN_CELL_SIZES", 
     "MAX_CELL_SIZES", "20", "1", "UPDATE_MISSING_ONLY")
```

### Example 4

```python
# Calculate the Mininum Cell Size and Maximum Cell Size with default setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "cellsize.gdb/md"
query = "#"
calmin = "MIN_CELL_SIZES"
calmax = "MAX_CELL_SIZES"
maxfactor = "#"
tolerancefactor = "#"
updatemiss = "#"

arcpy.CalculateCellSizeRanges_management(
     mdname, query, calmin, calmax, maxfactor, tolerancefactor, updatemiss)
```

### Example 5

```python
# Calculate the Mininum Cell Size and Maximum Cell Size with default setting

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "cellsize.gdb/md"
query = "#"
calmin = "MIN_CELL_SIZES"
calmax = "MAX_CELL_SIZES"
maxfactor = "#"
tolerancefactor = "#"
updatemiss = "#"

arcpy.CalculateCellSizeRanges_management(
     mdname, query, calmin, calmax, maxfactor, tolerancefactor, updatemiss)
```

---

## Calculate End Date (Data Management)

## Summary

Populates the values for a specified end date field with values calculated using the start date field specified. This tool is useful when the intervals between start date field values are not regular and you want to animate the feature class or table through time or some other value using the Animation toolbar.

## Usage

- The table is first sorted by entity (unique value field) if this is specified, and then by time stamp. With the start date field sorted in ascending order, the end date of any row is the same as the start date of the next row.
- In order to use this tool the start date field must be able to be sorted in ascending order. To test this, open the attribute table for the feature class, right-click the field and click Sort Ascending. If the field cannot be sorted in ascending order, the field must be reformatted before using this tool.
- The end date field value for the last row will be the same as the start date field value.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The feature class or table for which an end date field is calculated based on the start date field specified. | Table View |
| Unique ID Fields(Optional) | The name of the field or fields that can be used to uniquely identify spatial entities. This field or these fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, state name could be the unique value field (the entity). If population figures are per county, you would need to set county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. | Field |
| Start Date Field | The field containing values that will be used to calculate values for the end date field. The start date field and the end date field must be of the same format. | Field |
| End Date Field | The field that will be populated with values based on the start date field specified. The start date field and the end date field must be of the same format. | Field |
| Input_Table | The feature class or table for which an end date field is calculated based on the start date field specified. | Table View |
| Unique_ID_Fields[Unique_ID_Field,...](Optional) | The name of the field or fields that can be used to uniquely identify spatial entities. This field or these fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, state name could be the unique value field (the entity). If population figures are per county, you would need to set county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. | Field |
| Start_Date_Field | The field containing values that will be used to calculate values for the end date field. The start date field and the end date field must be of the same format. | Field |
| End_Date_Field | The field that will be populated with values based on the start date field specified. The start date field and the end date field must be of the same format. | Field |

## Code Samples

### Example 1

```python
arcpy.management.CalculateEndDate(Input_Table, {Unique_ID_Fields}, Start_Date_Field, End_Date_Field)
```

### Example 2

```python
import arcpy
arcpy.CalculateEndDate_management("C:/data/HistPop.shp", "State_FIP;County_FIP", "Start_Date", "End_Date")
```

### Example 3

```python
import arcpy
arcpy.CalculateEndDate_management("C:/data/HistPop.shp", "State_FIP;County_FIP", "Start_Date", "End_Date")
```

### Example 4

```python
# Name: CalculateEndDate_Ex02.py
# Description: Calculate end date based on a start date field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/HistPop.shp"
uniqueIdFields = "State_FIP;County_FIP"
startDateField = "Start_Date"
endDateField = "End_Date"
 
# Execute CalculateEndDate
arcpy.CalculateEndDate_management(inTable, uniqueIdFields, startDateField, endDateField)
```

### Example 5

```python
# Name: CalculateEndDate_Ex02.py
# Description: Calculate end date based on a start date field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/HistPop.shp"
uniqueIdFields = "State_FIP;County_FIP"
startDateField = "Start_Date"
endDateField = "End_Date"
 
# Execute CalculateEndDate
arcpy.CalculateEndDate_management(inTable, uniqueIdFields, startDateField, endDateField)
```

---

## Calculate End Time (Data Management)

## Summary

Calculates the end time of features based on the time values stored in another field.

## Usage

- The table is first sorted by the fields in the ID Fields parameter, then by the Start Time Field. After this sort, the end time of any row is the same as the start time of the next row.
- The End Time Field value for the last row will be the same as the Start Time Field value for that row.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The feature class or table for which an End Time Field is calculated based on the Start Time Field specified. | Table View |
| Start Time Field | The field containing values that will be used to calculate values for the End Time Field. The Start Time Field and the End Time Field must be of the same type. For example, if the Start Time Field is of type LONG, the End Time Field should be of type LONG as well. | Field |
| End Time Field | The field that will be populated with values based on the Start Time Field specified. The Start Time Field and the End Time Field must be of the same format. | Field |
| ID Fields(Optional) | The name of the field or fields that can be used to uniquely identify spatial entities. These fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, the state name could be the unique value field (the entity). If population figures are per county, you would need to set the county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. | Field |
| in_table | The feature class or table for which an End Time Field is calculated based on the Start Time Field specified. | Table View |
| start_field | The field containing values that will be used to calculate values for the End Time Field. The Start Time Field and the End Time Field must be of the same type. For example, if the Start Time Field is of type LONG, the End Time Field should be of type LONG as well. | Field |
| end_field | The field that will be populated with values based on the Start Time Field specified. The Start Time Field and the End Time Field must be of the same format. | Field |
| fields[fields,...](Optional) | The name of the field or fields that can be used to uniquely identify spatial entities. These fields are used to first sort based on entity type if there is more than one entity. For instance, for a feature class representing population values per state over time, the state name could be the unique value field (the entity). If population figures are per county, you would need to set the county name and state name as the unique value fields, since some county names are the same for different states. If there is only one entity, this parameter can be ignored. | Field |

## Code Samples

### Example 1

```python
arcpy.management.CalculateEndTime(in_table, start_field, end_field, {fields})
```

### Example 2

```python
import arcpy
arcpy.CalculateEndTime_management("C:/Data/TemporalData.gdb/CalculateEndTime","Start_Time","End_Time","")
```

### Example 3

```python
import arcpy
arcpy.CalculateEndTime_management("C:/Data/TemporalData.gdb/CalculateEndTime","Start_Time","End_Time","")
```

### Example 4

```python
# Name: CalculateEndTime_Ex02.py
# Description: Calculate end time based on a start time field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/TemporalData.gdb/CalculateEndTime"
uniqueIdFields = ""
startTimeField = "Start_Time"
endTimeField = "End_Time"
 
# Execute CalculateEndDate
arcpy.CalculateEndTime_management(inTable, startTimeField, endTimeField, uniqueIdFields)
```

### Example 5

```python
# Name: CalculateEndTime_Ex02.py
# Description: Calculate end time based on a start time field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/TemporalData.gdb/CalculateEndTime"
uniqueIdFields = ""
startTimeField = "Start_Time"
endTimeField = "End_Time"
 
# Execute CalculateEndDate
arcpy.CalculateEndTime_management(inTable, startTimeField, endTimeField, uniqueIdFields)
```

---

## Calculate Field Python examples

## Code Samples

### Example 1

```python
!CITY_NAME!.capitalize()
```

### Example 2

```python
!CITY_NAME!.capitalize()
```

### Example 3

```python
!CITY_NAME!.rstrip()
```

### Example 4

```python
!CITY_NAME!.rstrip()
```

### Example 5

```python
!STATE_NAME!.replace("california", "California")
```

### Example 6

```python
!STATE_NAME!.replace("california", "California")
```

### Example 7

```python
"{}:{}".format(!FieldA!, !FieldB!)
```

### Example 8

```python
"{}:{}".format(!FieldA!, !FieldB!)
```

### Example 9

```python
4.0 / 3.0 * math.pi * !Radius! ** 3
```

### Example 10

```python
4.0 / 3.0 * math.pi * !Radius! ** 3
```

### Example 11

```python
Expression:
max([!field1!, !field2!, !field3!])
```

### Example 12

```python
Expression:
max([!field1!, !field2!, !field3!])
```

### Example 13

```python
Expression:
sum([!field1!, !field2!, !field3!])
```

### Example 14

```python
Expression:
sum([!field1!, !field2!, !field3!])
```

### Example 15

```python
Expression:
round(!area!, 2)
```

### Example 16

```python
Expression:
round(!area!, 2)
```

### Example 17

```python
Expression:
MetersToFeet((float(!shape.area!)))

Code Block:
import math
def MetersToFeet(area):
    return math.pow(3.2808, 2) * area
```

### Example 18

```python
Expression:
MetersToFeet((float(!shape.area!)))

Code Block:
import math
def MetersToFeet(area):
    return math.pow(3.2808, 2) * area
```

### Example 19

```python
Expression:
Reclass(!WELL_YIELD!)

Code Block:
def Reclass(WellYield):
    if (WellYield >= 0 and WellYield <= 10):
        return 1
    elif (WellYield > 10 and WellYield <= 20):
        return 2
    elif (WellYield > 20 and WellYield <= 30):
        return 3
    elif (WellYield > 30):
        return 4
```

### Example 20

```python
Expression:
Reclass(!WELL_YIELD!)

Code Block:
def Reclass(WellYield):
    if (WellYield >= 0 and WellYield <= 10):
        return 1
    elif (WellYield > 10 and WellYield <= 20):
        return 2
    elif (WellYield > 20 and WellYield <= 30):
        return 3
    elif (WellYield > 30):
        return 4
```

### Example 21

```python
Expression:
!shape.area!
```

### Example 22

```python
Expression:
!shape.area!
```

### Example 23

```python
Expression:
!shape.extent.XMax!
```

### Example 24

```python
Expression:
!shape.extent.XMax!
```

### Example 25

```python
Expression:
getVertexCount(!shape!)

Code Block:
def getVertexCount(feat):    
    partnum = 0

    # Count the number of points in the current multipart feature
    partcount = feat.partCount
    pntcount = 0

    # Enter while loop for each part in the feature (if a singlepart 
    # feature, this will occur only once)
    while partnum < partcount:
        part = feat.getPart(partnum)
        pnt = part.next()

        # Enter while loop for each vertex
        while pnt:
            pntcount += 1   
            pnt = part.next()
   
            # If pnt is null, either the part is finished or there 
            # is an interior ring
            if not pnt: 
                pnt = part.next()
        partnum += 1
    return pntcount
```

### Example 26

```python
Expression:
getVertexCount(!shape!)

Code Block:
def getVertexCount(feat):    
    partnum = 0

    # Count the number of points in the current multipart feature
    partcount = feat.partCount
    pntcount = 0

    # Enter while loop for each part in the feature (if a singlepart 
    # feature, this will occur only once)
    while partnum < partcount:
        part = feat.getPart(partnum)
        pnt = part.next()

        # Enter while loop for each vertex
        while pnt:
            pntcount += 1   
            pnt = part.next()
   
            # If pnt is null, either the part is finished or there 
            # is an interior ring
            if not pnt: 
                pnt = part.next()
        partnum += 1
    return pntcount
```

### Example 27

```python
Expression:
shiftXCoordinate(!SHAPE!)

Code Block:
def shiftXCoordinate(shape):
    shiftValue = 100
    point = shape.getPart(0)
    point.X += shiftValue
    return point
```

### Example 28

```python
Expression:
shiftXCoordinate(!SHAPE!)

Code Block:
def shiftXCoordinate(shape):
    shiftValue = 100
    point = shape.getPart(0)
    point.X += shiftValue
    return point
```

### Example 29

```python
Expression:
!shape!.getLength('PLANAR', 'YARDS')
```

### Example 30

```python
Expression:
!shape!.getLength('PLANAR', 'YARDS')
```

### Example 31

```python
Expression:
!shape!.getArea('PLANAR', 'ACRES')
```

### Example 32

```python
Expression:
!shape!.getArea('PLANAR', 'ACRES')
```

### Example 33

```python
Expression:
!shape!.getLength('GEODESIC', 'YARDS')
```

### Example 34

```python
Expression:
!shape!.getLength('GEODESIC', 'YARDS')
```

### Example 35

```python
Expression:
!shape!.getArea('GEODESIC', 'ACRES')
```

### Example 36

```python
Expression:
!shape!.getArea('GEODESIC', 'ACRES')
```

### Example 37

```python
Expression:
time.strftime("%d/%m/%Y")
```

### Example 38

```python
Expression:
time.strftime("%d/%m/%Y")
```

### Example 39

```python
Expression:
datetime.datetime.now()
```

### Example 40

```python
Expression:
datetime.datetime.now()
```

### Example 41

```python
Expression:
datetime.date.today()
```

### Example 42

```python
Expression:
datetime.date.today()
```

### Example 43

```python
Expression:
datetime.datetime(year=2015, month=3, day=15, hour=13, minute=30, second=0)
```

### Example 44

```python
Expression:
datetime.datetime(year=2015, month=3, day=15, hour=13, minute=30, second=0)
```

### Example 45

```python
Expression:
datetime.datetime.now().day - !OID!
```

### Example 46

```python
Expression:
datetime.datetime.now().day - !OID!
```

### Example 47

```python
Expression:
!field1! + timedelta(days=100)

Code Block:
from datetime import timedelta
```

### Example 48

```python
Expression:
!field1! + timedelta(days=100)

Code Block:
from datetime import timedelta
```

### Example 49

```python
Expression:
!field1!.ctime()
```

### Example 50

```python
Expression:
!field1!.ctime()
```

### Example 51

```python
Expression:
!field1!.strftime('%A')
```

### Example 52

```python
Expression:
!field1!.strftime('%A')
```

### Example 53

```python
Expression:
!field1!.strftime("%m/%d/%Y, %H:%M:%S")
```

### Example 54

```python
Expression:
!field1!.strftime("%m/%d/%Y, %H:%M:%S")
```

### Example 55

```python
Expression:
'1969-07-21 02:56:00'
```

### Example 56

```python
Expression:
'1969-07-21 02:56:00'
```

### Example 57

```python
Expression:
'July 1 2020 12:30:45'
```

### Example 58

```python
Expression:
'July 1 2020 12:30:45'
```

### Example 59

```python
Expression:
time(hour=16, minute=30, second=0)

Code Block:
from datetime import time
```

### Example 60

```python
Expression:
time(hour=16, minute=30, second=0)

Code Block:
from datetime import time
```

### Example 61

```python
Expression:
!date_field!
```

### Example 62

```python
Expression:
!date_field!
```

### Example 63

```python
Expression:
datetime(2000, 12, 31)
```

### Example 64

```python
Expression:
datetime(2000, 12, 31)
```

### Example 65

```python
Expression:
!date_field!
```

### Example 66

```python
Expression:
!date_field!
```

### Example 67

```python
Expression:
datetime.now(tz=timezone.utc)

Code Block:
from datetime import timezone
```

### Example 68

```python
Expression:
datetime.now(tz=timezone.utc)

Code Block:
from datetime import timezone
```

### Example 69

```python
Expression:
datetime.now(ZoneInfo('America/New_York'))

Code Block:
from zoneinfo import ZoneInfo
```

### Example 70

```python
Expression:
datetime.now(ZoneInfo('America/New_York'))

Code Block:
from zoneinfo import ZoneInfo
```

### Example 71

```python
Expression:
!SUB_REGION![-3:]
```

### Example 72

```python
Expression:
!SUB_REGION![-3:]
```

### Example 73

```python
Expression:
!STATE_NAME!.replace("P","p")
```

### Example 74

```python
Expression:
!STATE_NAME!.replace("P","p")
```

### Example 75

```python
Expression:
!SUB_REGION! + " " + !STATE_ABBR!
```

### Example 76

```python
Expression:
!SUB_REGION! + " " + !STATE_ABBR!
```

### Example 77

```python
Expression:
' '.join([i.capitalize() for i in !STATE_NAME!.split(' ')])
```

### Example 78

```python
Expression:
' '.join([i.capitalize() for i in !STATE_NAME!.split(' ')])
```

### Example 79

```python
Expression:
!STATE_NAME!.title()
```

### Example 80

```python
Expression:
!STATE_NAME!.title()
```

### Example 81

```python
Expression:
update_street(!ADDRESS!)

Code Block:
import re
def update_street(street_name):
    return re.sub(r"""\b(St|St.)\Z""",  
                  'Street',
                  street_name)
```

### Example 82

```python
Expression:
update_street(!ADDRESS!)

Code Block:
import re
def update_street(street_name):
    return re.sub(r"""\b(St|St.)\Z""",  
                  'Street',
                  street_name)
```

### Example 83

```python
Expression:
autoIncrement(10, 5)

Code Block:
rec = 0
def autoIncrement(start=1, interval=1):
    global rec
    if rec == 0:
        rec = start
    else:
        rec += interval
    return rec
```

### Example 84

```python
Expression:
autoIncrement(10, 5)

Code Block:
rec = 0
def autoIncrement(start=1, interval=1):
    global rec
    if rec == 0:
        rec = start
    else:
        rec += interval
    return rec
```

### Example 85

```python
Expression:
accumulate(!FieldA!)

Code Block:
total = 0
def accumulate(increment):
    global total
    if total:
        total += increment
    else:
        total = increment
    return total
```

### Example 86

```python
Expression:
accumulate(!FieldA!)

Code Block:
total = 0
def accumulate(increment):
    global total
    if total:
        total += increment
    else:
        total = increment
    return total
```

### Example 87

```python
Expression:
percentIncrease(float(!FieldA!))

Code Block:
lastValue = 0
def percentIncrease(newValue):
    global lastValue
    if lastValue:
        percentage = ((newValue - lastValue) / lastValue)  * 100
    else: 
        percentage = 0
    lastValue = newValue
    return percentage
```

### Example 88

```python
Expression:
percentIncrease(float(!FieldA!))

Code Block:
lastValue = 0
def percentIncrease(newValue):
    global lastValue
    if lastValue:
        percentage = ((newValue - lastValue) / lastValue)  * 100
    else: 
        percentage = 0
    lastValue = newValue
    return percentage
```

### Example 89

```python
Expression:
getRandomValue()

Code Block:
import numpy

def getRandomValue():
    return numpy.random.random()
```

### Example 90

```python
Expression:
getRandomValue()

Code Block:
import numpy

def getRandomValue():
    return numpy.random.random()
```

### Example 91

```python
Expression:
random.randint(0, 10)

Code Block:
import random
```

### Example 92

```python
Expression:
random.randint(0, 10)

Code Block:
import random
```

### Example 93

```python
Expression:
None
```

### Example 94

```python
Expression:
None
```

---

## Calculate Field VBScript examples

## Code Samples

### Example 1

```python
Expression:
Left([MyField], 1)
```

### Example 2

```python
Expression:
Left([MyField], 1)
```

### Example 3

```python
Expression:
Right([MyField], 1)
```

### Example 4

```python
Expression:
Right([MyField], 1)
```

### Example 5

```python
Expression:
Mid([MyField], 14, 4)
```

### Example 6

```python
Expression:
Mid([MyField], 14, 4)
```

### Example 7

```python
Expression:
InStr([MyField], " ")
```

### Example 8

```python
Expression:
InStr([MyField], " ")
```

### Example 9

```python
Expression:
Replace([MyField], "#", "!")
```

### Example 10

```python
Expression:
Replace([MyField], "#", "!")
```

### Example 11

```python
Expression:
Replace([MyField], Chr(13), "!")
```

### Example 12

```python
Expression:
Replace([MyField], Chr(13), "!")
```

### Example 13

```python
Expression:
[MyField1] & " " & [MyField2]
```

### Example 14

```python
Expression:
[MyField1] & " " & [MyField2]
```

### Example 15

```python
Expression:
density

Code Block:
Dim density
If [POP90_SQMI] < 100 Then
density = "low"

ElseIf [POP90_SQMI] < 300 Then
density = "medium"

Else
density = "high"
End If
```

### Example 16

```python
Expression:
density

Code Block:
Dim density
If [POP90_SQMI] < 100 Then
density = "low"

ElseIf [POP90_SQMI] < 300 Then
density = "medium"

Else
density = "high"
End If
```

---

## Calculate Field (Data Management)

## Summary

Calculates the values of a field for a feature class, feature layer, or raster.

## Usage

- To learn more about Python expressions, see Calculate Field Python examples.To learn more about Arcade expressions, see ArcGIS Arcade in the Developer help.To learn more about SQL expressions, see Calculate field values.To learn more about VBScript expressions, see Calculate Field VBScript examples.
- This tool's Arcade expressions use the Field Calculation Arcade profile. An Arcade profile is a context in which an Arcade expression is evaluated and understood. When an Arcade expression is evaluated, the return value is cast to the field type of the output value.
- When used with a selected set of features, such as those created from a query using the Make Feature Layer or Select Layer By Attribute tool, this tool will only update the selected records.
- The calculation can only be applied to one field per operation. To apply multiple calculations, use the Calculate Fields tool.
- Existing field values will be overwritten. To preserve the original values, make a copy of the input table as a backup, or use the Enable Undo option in the Geoprocessing pane.
- For Python calculations, field names must be enclosed in exclamation points (for example, !fieldname!).For Arcade calculations, field names must be prefixed with $feature. (for example, $feature.fieldname).
- To calculate strings to text or character fields, on the dialog box, the string must use double quotation marks (for example, "string"), or in scripting, the string using double quotation marks must also be enclosed in single quotation marks (for example, '"string"').
- To calculate a field to be a numeric value, provide the numeric value in the Expression parameter with no quotation marks around the value.
- You can create complex expressions using the Code Block parameter. Provide the code block either directly on the dialog box or as a string in scripting. The expression and code block are connected. The code block must relate back to the expression; the result of the code block must be passed to the expression.The Code Block parameter is only supported for Python expressions.
- You can use the Python math module and formatting in the Code Block parameter. You can also import additional modules. The math module provides number-theoretic and representation functions, power and logarithmic functions, trigonometric functions, angular conversion functions, hyperbolic functions, and mathematical constants. To learn more about the math module, see the Python help.
- To calculate geometry information in Python, use Geometry object properties. For example, use an expression of !shape.pointCount! to calculate the number of vertices in a feature.Note:The Calculate Geometry Attributes tool supports similar calculations. To calculate area and length in Python, use the getArea and getLength methods with a method and unit type. To calculate the geodesic area of polygons in square kilometers, use the following expression:!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!To calculate the planar length of polylines (or polygons) in yards, use the following expression: !shape.getLength('PLANAR', 'YARDS')See the ArcPy Polygon and Polyline objects for more information.Learn more about geoprocessing tools and linear and areal units
- To calculate the geodesic area of polygons in square kilometers, use the following expression:!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!
- To calculate the planar length of polylines (or polygons) in yards, use the following expression: !shape.getLength('PLANAR', 'YARDS')
- ArcGIS applications use UTF-16-LE encoding to read and write .cal files. Other applications (for example, Notepad) can be used to create or modify .cal files as long as the file is written using UTF-16-LE encoding. A file with any other encoding will not load into the code block.
- When working with joined data, you can only update fields from the origin table. You cannot update fields from the joined table. To perform a calculation on the joined table, perform the calculation directly on that table.
- Python expressions that attempt to concatenate string fields that include a null or divide by zero value will return null for that field value.
- SQL expressions support faster calculations for feature services and enterprise geodatabases. Instead of performing calculations one feature or row at a time, a single request is sent to the server or database, resulting in faster calculations.Only feature services and enterprise geodatabases support SQL expressions. For other formats, use Python or Arcade expressions.Using the SQL option for the Expression Type parameter has the following limitations:The option is only supported for Db2, Oracle, PostgreSQL, SAP HANA, and SQL Server enterprise geodatabases.Calculating field values on joined tables is not supported.Versioned and archived enterprise geodatabase data is not supported.Undoing geoprocessing operations is not supported.See your database vendor documentation for SQL expression help.
- The option is only supported for Db2, Oracle, PostgreSQL, SAP HANA, and SQL Server enterprise geodatabases.
- Calculating field values on joined tables is not supported.
- Versioned and archived enterprise geodatabase data is not supported.
- Undoing geoprocessing operations is not supported.
- The Expression parameter on the tool dialog box has an Insert Values drop-down list where you can add field values from the selected field in the Fields list or domain values from the selected field's domain. Use the domain values to ensure that only valid values for the field domain are inserted into the field.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the field that will be updated with the new calculation. | Mosaic Layer; Raster Layer; Table View |
| Field Name (Existing or New) | The field that will be updated with the new calculation.If a field with the specified name does not exist in the input table, it will be added. | Field |
| Expression | The simple calculation expression used to create a value that will populate the selected rows. | SQL Expression |
| Expression Type(Optional) | Specifies the type of expression that will be used.If the input is a feature service, the default expression type is SQL. For any other type of input, the default expression type is Python.To learn more about Python expressions, see Calculate Field Python examples.To learn more about Arcade expressions, see ArcGIS Arcade in the Developer help.To learn more about SQL expressions, see Calculate field values.SQL expressions support faster calculations for feature services and enterprise geodatabases. Instead of performing calculations one feature or row at a time, a single request is sent to the server or database, resulting in significantly faster calculations.Only feature services and enterprise geodatabases support SQL expressions. For other formats, use Python or Arcade expressions.To learn more about VBScript expressions, see Calculate Field VBScript examples.Python—The Python expression type will be used.Arcade—The Arcade expression type will be used.SQL—The SQL expression type will be used.VBScript—The VBScript expression type will be used. | String |
| Code Block(Optional) | A block of code that will be used for complex Python or VBScript expressions. | String |
| Field Type(Optional) | Specifies the field type of the new field. This parameter is only used when the field name does not exist in the input table.If the field is of type text, the field will have a length of 512, unless the input is a shapefile or dBASE file, in which case the length will be 254. To adjust the length, use the Alter Field tool.Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.Text—The field type will be text. Text fields support a string of characters.Date—The field type will be date. Date fields support date and time values. Date (high precision)—The field type will be high precision date. High precision date fields support date and time values with millisecond time.Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.Raster— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | String |
| Enforce Domains (Optional) | Specifies whether field domain rules will be enforced.Checked—Field domain rules will be enforced. If a field cannot be updated, the field value will remain unchanged, and the tool messages will include a warning message.Unchecked—Field domain rules will not be enforced. This is the default | Boolean |
| in_table | The table containing the field that will be updated with the new calculation. | Mosaic Layer; Raster Layer; Table View |
| field | The field that will be updated with the new calculation.If a field with the specified name does not exist in the input table, it will be added. | Field |
| expression | The simple calculation expression used to create a value that will populate the selected rows. | SQL Expression |
| expression_type(Optional) | Specifies the type of expression that will be used.PYTHON3—The Python expression type will be used.ARCADE—The Arcade expression type will be used.SQL—The SQL expression type will be used.VB—The VBScript expression type will be used.If the input is a feature service, the default expression type is SQL. For any other type of input, the default expression type is PYTHON3.To learn more about Python expressions, see Calculate Field Python examples.To learn more about Arcade expressions, see ArcGIS Arcade in the Developer help.To learn more about SQL expressions, see Calculate field values.SQL expressions support faster calculations for feature services and enterprise geodatabases. Instead of performing calculations one feature or row at a time, a single request is sent to the server or database, resulting in significantly faster calculations.Only feature services and enterprise geodatabases support SQL expressions. For other formats, use Python or Arcade expressions.To learn more about VBScript expressions, see Calculate Field VBScript examples. | String |
| code_block(Optional) | A block of code that will be used for complex Python or VBScript expressions. | String |
| field_type(Optional) | Specifies the field type of the new field. This parameter is only used when the field name does not exist in the input table.If the field is of type text, the field will have a length of 512, unless the input is a shapefile or dBASE file, in which case the length will be 254. To adjust the length, use the Alter Field tool. SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.TEXT—The field type will be text. Text fields support a string of characters.DATE—The field type will be date. Date fields support date and time values. DATEHIGHPRECISION—The field type will be high precision date. High precision date fields support date and time values with millisecond time.DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values.TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets.RASTER— The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | String |
| enforce_domains(Optional) | Specifies whether field domain rules will be enforced.ENFORCE_DOMAINS—Field domain rules will be enforced.NO_ENFORCE_DOMAINS—Field domain rules will not be enforced. This is the default. | Boolean |

## Code Samples

### Example 1

```python
!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!
```

### Example 2

```python
!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!
```

### Example 3

```python
!shape.getLength('PLANAR', 'YARDS')
```

### Example 4

```python
!shape.getLength('PLANAR', 'YARDS')
```

### Example 5

```python
arcpy.management.CalculateField(in_table, field, expression, {expression_type}, {code_block}, {field_type}, {enforce_domains})
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.AddField("vegtable.dbf", "VEG_TYP2", "TEXT", "", "", "20")
arcpy.management.CalculateField("vegtable.dbf", "VEG_TYP2", 
                                '!VEG_TYPE!.split(" ")[-1]', "PYTHON3")
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.AddField("vegtable.dbf", "VEG_TYP2", "TEXT", "", "", "20")
arcpy.management.CalculateField("vegtable.dbf", "VEG_TYP2", 
                                '!VEG_TYPE!.split(" ")[-1]', "PYTHON3")
```

### Example 8

```python
# Name: CalculateField_centroids.py

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "parcels"
fieldName1 = "xCentroid"
fieldName2 = "yCentroid"
fieldPrecision = 18
fieldScale = 11
 
# Add fields
arcpy.management.AddField(inFeatures, fieldName1, "DOUBLE", 
                          fieldPrecision, fieldScale)
arcpy.management.AddField(inFeatures, fieldName2, "DOUBLE", 
                          fieldPrecision, fieldScale)
 
# Calculate centroid
arcpy.management.CalculateField(inFeatures, fieldName1, 
                                "!SHAPE.CENTROID.X!",
                                "PYTHON3")
arcpy.management.CalculateField(inFeatures, fieldName2, 
                                "!SHAPE.CENTROID.Y!",
                                "PYTHON3")
```

### Example 9

```python
# Name: CalculateField_centroids.py

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "parcels"
fieldName1 = "xCentroid"
fieldName2 = "yCentroid"
fieldPrecision = 18
fieldScale = 11
 
# Add fields
arcpy.management.AddField(inFeatures, fieldName1, "DOUBLE", 
                          fieldPrecision, fieldScale)
arcpy.management.AddField(inFeatures, fieldName2, "DOUBLE", 
                          fieldPrecision, fieldScale)
 
# Calculate centroid
arcpy.management.CalculateField(inFeatures, fieldName1, 
                                "!SHAPE.CENTROID.X!",
                                "PYTHON3")
arcpy.management.CalculateField(inFeatures, fieldName2, 
                                "!SHAPE.CENTROID.Y!",
                                "PYTHON3")
```

### Example 10

```python
# Name: CalculateField_ranges.py

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inTable = "parcels"
fieldName = "areaclass"
expression = "getClass(float(!SHAPE.area!))"

codeblock = """
def getClass(area):
    if area <= 1000:
        return 1
    if area > 1000 and area <= 10000:
        return 2
    else:
        return 3"""
 
# Run AddField
arcpy.management.AddField(inTable, fieldName, "SHORT")
 
# Run CalculateField 
arcpy.management.CalculateField(inTable, fieldName, expression, "PYTHON3", 
                                codeblock)
```

### Example 11

```python
# Name: CalculateField_ranges.py

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inTable = "parcels"
fieldName = "areaclass"
expression = "getClass(float(!SHAPE.area!))"

codeblock = """
def getClass(area):
    if area <= 1000:
        return 1
    if area > 1000 and area <= 10000:
        return 2
    else:
        return 3"""
 
# Run AddField
arcpy.management.AddField(inTable, fieldName, "SHORT")
 
# Run CalculateField 
arcpy.management.CalculateField(inTable, fieldName, expression, "PYTHON3", 
                                codeblock)
```

### Example 12

```python
# Name: CalculateField_Random.py

# Import system modules
import arcpy
import random
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
  
# Set local variables
inFeatures = "parcels"
fieldName = "RndValue"
expression = "random.randint(0, 10)"
code_block = "import random"
 
# Run AddField
arcpy.management.AddField(inFeatures, fieldName, "LONG")
 
# Run CalculateField 
arcpy.management.CalculateField(inFeatures, fieldName, expression, "PYTHON3", 
                                code_block)
```

### Example 13

```python
# Name: CalculateField_Random.py

# Import system modules
import arcpy
import random
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
  
# Set local variables
inFeatures = "parcels"
fieldName = "RndValue"
expression = "random.randint(0, 10)"
code_block = "import random"
 
# Run AddField
arcpy.management.AddField(inFeatures, fieldName, "LONG")
 
# Run CalculateField 
arcpy.management.CalculateField(inFeatures, fieldName, expression, "PYTHON3", 
                                code_block)
```

### Example 14

```python
import arcpy
arcpy.env.workspace = "C:/data/fgdb.gdb"
arcpy.management.CalculateField("data", "new_value", "$feature.value1 + $feature.value2", "ARCADE")
```

### Example 15

```python
import arcpy
arcpy.env.workspace = "C:/data/fgdb.gdb"
arcpy.management.CalculateField("data", "new_value", "$feature.value1 + $feature.value2", "ARCADE")
```

### Example 16

```python
import arcpy
feature_service = "<a feature service url>"
arcpy.management.CalculateField("data", "NEW_VALUE", "SAMPLE * (BASELINE - 40)", "SQL")
```

### Example 17

```python
import arcpy
feature_service = "<a feature service url>"
arcpy.management.CalculateField("data", "NEW_VALUE", "SAMPLE * (BASELINE - 40)", "SQL")
```

---

## Calculate Fields (multiple) (Data Management)

## Summary

Calculates the values of two or more fields for a feature class, feature layer, or raster.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the fields that will be updated with the new calculations. | Table View; Raster Layer; Mosaic Layer |
| Expression Type | Specifies the type of expression that will be used.If the input is a feature service, the default expression type is SQL. For any other type of input, the default expression type is Python.To learn more about Python expressions, see Calculate Field Python examples.To learn more about Arcade expressions, see ArcGIS Arcade in the Developer help.To learn more about SQL expressions, see Calculate field values.To learn more about VBScript expressions, see Calculate Field VBScript examples.Python—The Python expression type will be used.Arcade—The Arcade expression type will be used.SQL—The SQL expression type will be used.VBScript—The VBScript expression type will be used. | String |
| Fields | The fields that will be calculated, the expressions to be calculated, and optional where clauses and field types.The optional SQL expression will be used to select a subset of records. Only records that match this where clause will be calculated. If the where clause is left blank, all records will be calculated. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS.If a field with the specified name does not exist in the input table, it will be added. The type of the field will be set using the field type value. If the field type is unspecified, the field will be added as a text field. Available field types are as follows: Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. Text—The field type will be text. Text fields support a string of characters. Date—The field type will be date. Date fields support date and time values.Date (high precision)—The field type will be high precision date. High precision date fields support date and time values with millisecond time.Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value. Blob (binary data)—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID (globally unique identifier)—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. Raster imagery—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | Value Table |
| Code Block (Optional) | A block of code that will be used for complex expressions. A function cannot be used to return multiple values. | String |
| Enforce Domains (Optional) | Specifies whether field domain rules will be enforced.Checked—Field domain rules will be enforced. If a field cannot be updated, the field value will remain unchanged, and the tool messages will include a warning message.Unchecked—Field domain rules will not be enforced. This is the default | Boolean |
| in_table | The table containing the fields that will be updated with the new calculations. | Table View; Raster Layer; Mosaic Layer |
| expression_type | Specifies the type of expression that will be used.PYTHON3—The Python expression type will be used.ARCADE—The Arcade expression type will be used.SQL—The SQL expression type will be used.VB—The VBScript expression type will be used.If the input is a feature service, the default expression type is SQL. For any other type of input, the default expression type is PYTHON3.To learn more about Python expressions, see Calculate Field Python examples.To learn more about Arcade expressions, see ArcGIS Arcade in the Developer help.To learn more about SQL expressions, see Calculate field values.To learn more about VBScript expressions, see Calculate Field VBScript examples. | String |
| fields[[Field Name, Expression, {Where Clause}, {Field Type}],...] | The fields that will be calculated, the expressions to be calculated, and optional where clauses and field types.The optional SQL expression will be used to select a subset of records. Only records that match this where clause will be calculated. If the where clause is left blank, all records will be calculated. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS.If a field with the specified name does not exist in the input table, it will be added. The type of the field will be set using the field type value. If the field type is unspecified, the field will be added as a text field. Available field types are as follows: SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767. LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38. DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308. TEXT—The field type will be text. Text fields support a string of characters. DATE—The field type will be date. Date fields support date and time values. DATEHIGHPRECISION—The field type will be high precision date. High precision date fields support date and time values with millisecond time.DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values. TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.BLOB—The field type will be BLOB. BLOB fields support data stored as a long sequence of binary numbers. You need a custom loader or viewer or a third-party application to load items into a BLOB field or view the contents of a BLOB field. GUID—The field type will be GUID. GUID fields store registry-style strings consisting of 36 characters enclosed in curly brackets. RASTER—The field type will be raster. Raster fields can store raster data in or alongside the geodatabase. All ArcGIS software-supported raster dataset formats can be stored, but it is recommended that only small images be used. | Value Table |
| code_block(Optional) | A block of code that will be used for complex expressions. A function cannot be used to return multiple values. | String |
| enforce_domains(Optional) | Specifies whether field domain rules will be enforced.ENFORCE_DOMAINS—Field domain rules will be enforced.NO_ENFORCE_DOMAINS—Field domain rules will not be enforced. This is the default. | Boolean |

## Code Samples

### Example 1

```python
!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!
```

### Example 2

```python
!shape.getArea('GEODESIC', 'SQUAREKILOMETERS')!
```

### Example 3

```python
!shape.getLength('PLANAR', 'YARDS')
```

### Example 4

```python
!shape.getLength('PLANAR', 'YARDS')
```

### Example 5

```python
arcpy.management.CalculateFields(in_table, expression_type, fields, {code_block}, {enforce_domains})
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.CalculateFields("parcels", "PYTHON3", 
                                 [["xCentroid", "!SHAPE.CENTROID.X!"], 
                                  ["yCentroid", "!SHAPE.CENTROID.Y!"]])
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.CalculateFields("parcels", "PYTHON3", 
                                 [["xCentroid", "!SHAPE.CENTROID.X!"], 
                                  ["yCentroid", "!SHAPE.CENTROID.Y!"]])
```

### Example 8

```python
import arcpy
arcpy.management.CalculateFields("<a feature service url>", "SQL", 
                                 [["ceiling_field", "CEILING(field1)"], 
                                  ["floor_field", "FLOOR(field1)"]])
```

### Example 9

```python
import arcpy
arcpy.management.CalculateFields("<a feature service url>", "SQL", 
                                 [["ceiling_field", "CEILING(field1)"], 
                                  ["floor_field", "FLOOR(field1)"]])
```

### Example 10

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.CalculateFields(
    "parcels", "ARCADE", 
    [["max_value", "Max($feature.field1, $feature.field2)"], 
     ["min_value", "Min($feature.field1, $feature.field2)"]])
```

### Example 11

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"
arcpy.management.CalculateFields(
    "parcels", "ARCADE", 
    [["max_value", "Max($feature.field1, $feature.field2)"], 
     ["min_value", "Min($feature.field1, $feature.field2)"]])
```

### Example 12

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"

# Calculate the Field1 field to -1 if the value is null.
# Calculate the Title field to remove any leading spaces if the first character
# is a space.
arcpy.management.CalculateFields(
    "parcels", "PYTHON3", 
    [['Field1', -1, "Field1 IS NULL"],
     ['Title', '!Address!.lstrip()', "Title LIKE ' %'"]])
```

### Example 13

```python
import arcpy
arcpy.env.workspace = "C:/data/airport.gdb"

# Calculate the Field1 field to -1 if the value is null.
# Calculate the Title field to remove any leading spaces if the first character
# is a space.
arcpy.management.CalculateFields(
    "parcels", "PYTHON3", 
    [['Field1', -1, "Field1 IS NULL"],
     ['Title', '!Address!.lstrip()', "Title LIKE ' %'"]])
```

---

## Calculate Geometry Attributes (Data Management)

## Summary

Adds information to a feature's attribute fields representing the spatial or geometric characteristics and location of each feature, such as length or area and x-, y-, z-coordinates, and m-values.

## Usage

- Length and area calculations will be in the units of the input features' coordinate system unless different units are selected in the Length Unit and Area Unit parameters. If the Coordinate System parameter is specified, the length and area calculations will be in the units of that coordinate system unless different units are specified in the Length Unit and Area Unit parameters.Learn more about length and area units in geoprocessing tools
- The geodesic length and area properties use a shape-preserving algorithm. This produces highly accurate results that are not biased by an assumption that the input line or polygon features are constructed with geodesic arcs between the vertices, which is the assumption regarding traditional geodesic length and area.
- If the input features have a selection, only the selected features will have values calculated in the added fields; all other features will maintain their existing value.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The features with a field that will be updated with geometry calculations. | Feature Layer |
| Geometry Attributes | The fields in which the specified geometry properties will be calculated. You can select an existing field or provide a new field name. If a new field name is provided, the field type is determined by the type of values that are written to the field. Count attributes are written to long integer fields; area, length, and x-, y-, z-coordinate, and m-value attributes are written to double fields; and coordinate notations such as Degrees Minutes Seconds or MGRS are written to text fields.Unless otherwise noted, area and length properties are planar measurements using 2D Cartesian mathematics.Area—An attribute will be added to store the area of each polygon feature.Area (geodesic)—An attribute will be added to store the shape-preserving geodesic area of each polygon feature.Centroid x-coordinate—An attribute will be added to store the centroid x-coordinate of each feature.Centroid y-coordinate—An attribute will be added to store the centroid y-coordinate of each feature.Centroid z-coordinate—An attribute will be added to store the centroid z-coordinate of each feature.Centroid m-value—An attribute will be added to store the centroid m-value of each feature.Central point x-coordinate—An attribute will be added to store the x-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.Central point y-coordinate—An attribute will be added to store the y-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.Central point z-coordinate—An attribute will be added to store the z-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.Central point m-value—An attribute will be added to store the m-value of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.Number of curves—An attribute will be added to store the number of curves in each feature. Curves include elliptical arcs, circular arcs, and Bezier curves.Number of holes—An attribute will be added to store the number of interior holes within each polygon feature.Minimum x-coordinate—An attribute will be added to store the minimum x-coordinate of each feature's extent.Minimum y-coordinate—An attribute will be added to store the minimum y-coordinate of each feature's extent.Minimum z-coordinate—An attribute will be added to store the minimum z-coordinate of each feature's extent.Maximum x-coordinate—An attribute will be added to store the maximum x-coordinate of each feature's extent.Maximum y-coordinate—An attribute will be added to store the maximum y-coordinate of each feature's extent.Maximum z-coordinate—An attribute will be added to store the maximum z-coordinate of each feature's extent.Length—An attribute will be added to store the length of each line feature.Length (geodesic)—An attribute will be added to store the shape-preserving geodesic length of each line feature.Length (3D)—An attribute will be added to store the 3D length of each line feature.Line bearing—An attribute will be added to store the start-to-end bearing of each line feature. Values range from 0 to 360, with 0 meaning north, 90 east, 180 south, 270 west, and so on.Line start x-coordinate—An attribute will be added to store the x-coordinate of the start point of each line feature.Line start y-coordinate—An attribute will be added to store the y-coordinate of the start point of each line feature.Line start z-coordinate—An attribute will be added to store the z-coordinate of the start point of each line feature.Line start m-value—An attribute will be added to store the m-value of the start point of each line feature.Line end x-coordinate—An attribute will be added to store the x-coordinate of the end point of each line feature.Line end y-coordinate—An attribute will be added to store the y-coordinate of the end point of each line feature.Line end z-coordinate—An attribute will be added to store the z-coordinate of the end point of each line feature.Line end m-value—An attribute will be added to store the m-value of the end point of each line feature.Number of parts—An attribute will be added to store the number of parts composing each feature.Number of vertices—An attribute will be added to store the number of points or vertices composing each feature.Perimeter length—An attribute will be added to store the length of the perimeter or border of each polygon feature.Perimeter length (geodesic)—An attribute will be added to store the shape-preserving geodesic length of the perimeter or border of each polygon feature.Point x-coordinate—An attribute will be added to store the x-coordinate of each point feature.Point y-coordinate—An attribute will be added to store the y-coordinate of each point feature.Point z-coordinate—An attribute will be added to store the z-coordinate of each point feature.Point m-value—An attribute will be added to store the m-value of each point feature.Point x- and y-coordinate notation—An attribute will be added to store the x- and y-coordinate of each point feature formatted as a specified coordinate notation. | Value Table |
| Length Unit (Optional) | Specifies the unit that will be used to calculate length. Kilometers—The length unit will be kilometers.Meters—The length unit will be meters.Statute Miles—The length unit will be statute miles.International Nautical Miles—The length unit will be international nautical miles.International Yards—The length unit will be international yards.International Feet—The length unit will be international feet.US Survey Miles—The length unit will be US survey miles.US Survey Nautical Miles—The length unit will be US survey nautical miles.US Survey Yards—The length unit will be US survey yards.US Survey Feet—The length unit will be US survey feet. | String |
| Area Unit (Optional) | Specifies the unit that will be used to calculate area. Square Kilometers—The area unit will be square kilometers.Hectares—The area unit will be hectares.Square Meters—The area unit will be square meters.Square Statute Miles—The area unit will be square statute miles.Square International Nautical Miles—The area unit will be square international nautical miles.International Acres—The area unit will be international acres.Square International Yards—The area unit will be square international yards.Square International Feet—The area unit will be square international feet.Square US Survey Miles—The area unit will be square US survey miles.Square US Survey Nautical Miles—The area unit will be square US survey nautical miles.US Survey Acres—The area unit will be US survey acres.Square US Survey Yards—The area unit will be square US survey yards.Square US Survey Feet—The area unit will be square US survey feet. | String |
| Coordinate System(Optional) | The coordinate system in which the coordinates, length, and area will be calculated. The coordinate system of the input features is used by default. | Coordinate System |
| Coordinate Format(Optional) | Specifies the coordinate format in which the x- and y-coordinates will be calculated. The coordinate format matching the input features' spatial reference units is used by default. Several coordinate formats, including Degrees Minutes Seconds, Degrees Decimal Minutes, and others, require the calculation to be performed in a text field.Same as input—The input features' spatial reference units will be used for coordinate formatting. This is the default.Decimal Degrees—The coordinate format will be Decimal Degrees.Degrees Minutes Seconds (DDD° MM' SSS.ss" <N\|S\|E\|W>)—The coordinate format will be Degrees Minutes Seconds with cardinal direction component at the end (DDD° MM' SSS.ss" <N\|S\|E\|W>).Degrees Minutes Seconds (<N\|S\|E\|W> DDD° MM' SSS.ss")—The coordinate format will be Degrees Minutes Seconds with cardinal direction component at the beginning (<N\|S\|E\|W> DDD° MM' SSS.ss").Degrees Minutes Seconds (<+\|-> DDD° MM' SSS.ss")—The coordinate format will be Degrees Minutes Seconds with positive or negative direction component at the beginning (<+\|-> DDD° MM' SSS.ss").Degrees Minutes Seconds (<+\|-> DDD.MMSSSss)—The coordinate format will be Degrees Minutes Seconds packed into a single value with positive or negative direction component at the beginning (<+\|-> DDD.MMSSSss).Degrees Decimal Minutes (DDD° MM.mmm' <N\|S\|E\|W>)—The coordinate format will be Degrees Decimal Minutes with cardinal direction component at the end (DDD° MM.mmm' <N\|S\|E\|W>).Degrees Decimal Minutes (<N\|S\|E\|W> DDD° MM.mmm')—The coordinate format will be Degrees Decimal Minutes with cardinal direction component at the beginning (<N\|S\|E\|W> DDD° MM.mmm').Degrees Decimal Minutes (<+\|-> DDD° MM.mmm')—The coordinate format will be Degrees Decimal Minutes with positive or negative direction component at the beginning (<+\|-> DDD° MM.mmm').GARS (Global Area Reference System)—The coordinate format will be Global Area Reference System. The Global Area Reference System is based on latitude and longitude, dividing and subdividing the world into cells.GEOREF (World Geographic Reference System)—The coordinate format will be World Geographic Reference System. The World Geographic Reference System is based on the geographic system of latitude and longitude, but using a simpler and more flexible notation.MGRS (Military Grid Reference System)—The coordinate format will be Military Grid Reference System.USNG (United States National Grid)—The coordinate format will be United States National Grid.UTM (Universal Transverse Mercator)—The coordinate format will be Universal Transverse Mercator.UTM with no spaces—The coordinate format will be Universal Transverse Mercator with no spaces. | String |
| in_features | The features with a field that will be updated with geometry calculations. | Feature Layer |
| geometry_property[[Field, Property],...] | The fields in which the specified geometry properties will be calculated. You can select an existing field or provide a new field name. If a new field name is provided, the field type is determined by the type of values that are written to the field. Count attributes are written to long integer fields; area, length, and x-, y-, z-coordinate, and m-value attributes are written to double fields; and coordinate notations such as Degrees Minutes Seconds or MGRS are written to text fields.Unless otherwise noted, area and length properties are planar measurements using 2D Cartesian mathematics.AREA—An attribute will be added to store the area of each polygon feature.AREA_GEODESIC—An attribute will be added to store the shape-preserving geodesic area of each polygon feature.CENTROID_X—An attribute will be added to store the centroid x-coordinate of each feature.CENTROID_Y—An attribute will be added to store the centroid y-coordinate of each feature.CENTROID_Z—An attribute will be added to store the centroid z-coordinate of each feature.CENTROID_M—An attribute will be added to store the centroid m-value of each feature.INSIDE_X—An attribute will be added to store the x-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.INSIDE_Y—An attribute will be added to store the y-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.INSIDE_Z—An attribute will be added to store the z-coordinate of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.INSIDE_M—An attribute will be added to store the m-value of a central point inside or on each feature. This point is the same as the centroid if the centroid is inside the feature; otherwise, it is an inner label point.CURVE_COUNT—An attribute will be added to store the number of curves in each feature. Curves include elliptical arcs, circular arcs, and Bezier curves.HOLE_COUNT—An attribute will be added to store the number of interior holes within each polygon feature.EXTENT_MIN_X—An attribute will be added to store the minimum x-coordinate of each feature's extent.EXTENT_MIN_Y—An attribute will be added to store the minimum y-coordinate of each feature's extent.EXTENT_MIN_Z—An attribute will be added to store the minimum z-coordinate of each feature's extent.EXTENT_MAX_X—An attribute will be added to store the maximum x-coordinate of each feature's extent.EXTENT_MAX_Y—An attribute will be added to store the maximum y-coordinate of each feature's extent.EXTENT_MAX_Z—An attribute will be added to store the maximum z-coordinate of each feature's extent.LENGTH—An attribute will be added to store the length of each line feature.LENGTH_GEODESIC—An attribute will be added to store the shape-preserving geodesic length of each line feature.LENGTH_3D—An attribute will be added to store the 3D length of each line feature.LINE_BEARING—An attribute will be added to store the start-to-end bearing of each line feature. Values range from 0 to 360, with 0 meaning north, 90 east, 180 south, 270 west, and so on.LINE_START_X—An attribute will be added to store the x-coordinate of the start point of each line feature.LINE_START_Y—An attribute will be added to store the y-coordinate of the start point of each line feature.LINE_START_Z—An attribute will be added to store the z-coordinate of the start point of each line feature.LINE_START_M—An attribute will be added to store the m-value of the start point of each line feature.LINE_END_X—An attribute will be added to store the x-coordinate of the end point of each line feature.LINE_END_Y—An attribute will be added to store the y-coordinate of the end point of each line feature.LINE_END_Z—An attribute will be added to store the z-coordinate of the end point of each line feature.LINE_END_M—An attribute will be added to store the m-value of the end point of each line feature.PART_COUNT—An attribute will be added to store the number of parts composing each feature.POINT_COUNT—An attribute will be added to store the number of points or vertices composing each feature.PERIMETER_LENGTH—An attribute will be added to store the length of the perimeter or border of each polygon feature.PERIMETER_LENGTH_GEODESIC—An attribute will be added to store the shape-preserving geodesic length of the perimeter or border of each polygon feature.POINT_X—An attribute will be added to store the x-coordinate of each point feature.POINT_Y—An attribute will be added to store the y-coordinate of each point feature.POINT_Z—An attribute will be added to store the z-coordinate of each point feature.POINT_M—An attribute will be added to store the m-value of each point feature.POINT_COORD_NOTATION—An attribute will be added to store the x- and y-coordinate of each point feature formatted as a specified coordinate notation. | Value Table |
| length_unit(Optional) | Specifies the unit that will be used to calculate length. KILOMETERS—The length unit will be kilometers.METERS—The length unit will be meters.MILES_INT—The length unit will be statute miles.NAUTICAL_MILES_INT—The length unit will be international nautical miles.YARDS_INT—The length unit will be international yards.FEET_INT—The length unit will be international feet.MILES_US—The length unit will be US survey miles.NAUTICAL_MILES—The length unit will be US survey nautical miles.YARDS—The length unit will be US survey yards.FEET_US—The length unit will be US survey feet. | String |
| area_unit(Optional) | Specifies the unit that will be used to calculate area. SQUARE_KILOMETERS—The area unit will be square kilometers.HECTARES—The area unit will be hectares.SQUARE_METERS—The area unit will be square meters.SQUARE_MILES_INT—The area unit will be square statute miles.SQUARE_NAUTICAL_MILES—The area unit will be square international nautical miles.ACRES—The area unit will be international acres.SQUARE_YARDS—The area unit will be square international yards.SQUARE_FEET_INT—The area unit will be square international feet.SQUARE_MILES_US—The area unit will be square US survey miles.SQUARE_NAUTICAL_MILES_US—The area unit will be square US survey nautical miles.ACRES_US—The area unit will be US survey acres.SQUARE_YARDS_US—The area unit will be square US survey yards.SQUARE_FEET_US—The area unit will be square US survey feet. | String |
| coordinate_system(Optional) | The coordinate system in which the coordinates, length, and area will be calculated. The coordinate system of the input features is used by default. | Coordinate System |
| coordinate_format(Optional) | Specifies the coordinate format in which the x- and y-coordinates will be calculated. The coordinate format matching the input features' spatial reference units is used by default. Several coordinate formats, including Degrees Minutes Seconds, Degrees Decimal Minutes, and others, require the calculation to be performed in a text field.SAME_AS_INPUT—The input features' spatial reference units will be used for coordinate formatting. This is the default.DD—The coordinate format will be Decimal Degrees.DMS_DIR_LAST—The coordinate format will be Degrees Minutes Seconds with cardinal direction component at the end (DDD° MM' SSS.ss" <N\|S\|E\|W>).DMS_DIR_FIRST—The coordinate format will be Degrees Minutes Seconds with cardinal direction component at the beginning (<N\|S\|E\|W> DDD° MM' SSS.ss").DMS_POS_NEG—The coordinate format will be Degrees Minutes Seconds with positive or negative direction component at the beginning (<+\|-> DDD° MM' SSS.ss").DMS_PACKED—The coordinate format will be Degrees Minutes Seconds packed into a single value with positive or negative direction component at the beginning (<+\|-> DDD.MMSSSss).DDM_DIR_LAST—The coordinate format will be Degrees Decimal Minutes with cardinal direction component at the end (DDD° MM.mmm' <N\|S\|E\|W>).DDM_DIR_FIRST—The coordinate format will be Degrees Decimal Minutes with cardinal direction component at the beginning (<N\|S\|E\|W> DDD° MM.mmm').DDM_POS_NEG—The coordinate format will be Degrees Decimal Minutes with positive or negative direction component at the beginning (<+\|-> DDD° MM.mmm').GARS—The coordinate format will be Global Area Reference System. The Global Area Reference System is based on latitude and longitude, dividing and subdividing the world into cells.GEOREF—The coordinate format will be World Geographic Reference System. The World Geographic Reference System is based on the geographic system of latitude and longitude, but using a simpler and more flexible notation.MGRS—The coordinate format will be Military Grid Reference System.USNG—The coordinate format will be United States National Grid.UTM—The coordinate format will be Universal Transverse Mercator.UTMNS—The coordinate format will be Universal Transverse Mercator with no spaces. | String |

## Code Samples

### Example 1

```python
arcpy.management.CalculateGeometryAttributes(in_features, geometry_property, {length_unit}, {area_unit}, {coordinate_system}, {coordinate_format})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.management.CalculateGeometryAttributes("roads", [["Length_mi", "LENGTH"], ["Stops", "POINT_COUNT"]], "MILES_US")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.management.CalculateGeometryAttributes("roads", [["Length_mi", "LENGTH"], ["Stops", "POINT_COUNT"]], "MILES_US")
```

### Example 4

```python
# Name: ExtentCreation.py

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.env.outputCoordinateSystem = arcpy.Describe("roads").spatialReference

# Set local variables
in_features = "roads"

# Generate the extent coordinates using CalculateGeometry
arcpy.management.CalculateGeometryAttributes(in_features, [["Left", "EXTENT_MIN_X"],
                                                           ["Bottom", "EXTENT_MIN_Y"],
                                                           ["Right", "EXTENT_MAX_X"],
                                                           ["Top", "EXTENT_MAX_Y"]])
```

### Example 5

```python
# Name: ExtentCreation.py

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = r"C:\data\City.gdb"
arcpy.env.outputCoordinateSystem = arcpy.Describe("roads").spatialReference

# Set local variables
in_features = "roads"

# Generate the extent coordinates using CalculateGeometry
arcpy.management.CalculateGeometryAttributes(in_features, [["Left", "EXTENT_MIN_X"],
                                                           ["Bottom", "EXTENT_MIN_Y"],
                                                           ["Right", "EXTENT_MAX_X"],
                                                           ["Top", "EXTENT_MAX_Y"]])
```

---

## Calculate Statistics (Data Management)

## Summary

Calculates statistics for a raster dataset or a mosaic dataset.

## Usage

- A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster.
- The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are quite different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid (for example, if the skip factor is 5 and the closest pyramid level is 4 x 4 pixels, which is level 2), the software will round down to the next pyramid level (in this case, 2) and use that value as the skip factor.
- A skip factor is not used for all raster formats. The raster formats that will calculate statistics and take advantage of the skip factor include TIFF, IMG, NITF, DTED, RAW, ADRG, CIB, CADRG, DIGEST, GIS, LAN, CIT, COT, ERMapper, ENVI DAT, BIL, BIP, BSQ, and geodatabase.
- When using this tool to calculate statistics on a mosaic dataset, the statistics are calculated for the top-level mosaicked image, not for every raster contained in the mosaic dataset.
- Specifying a skip factor for a mosaic dataset is recommended. as these datasets tend to be very large.
- The Ignore Values option allows you to exclude a specific value from the calculation of statistics. You may want to ignore a value if it is a NoData value or if it will skew your calculation.
- Calculating statistics on the Esri Grid and the RADARSAT2 formats always uses a skip factor of 1.
- When using this tool to calculate statistics on a multidimensional mosaic dataset or a multidimensional raster, the statistics are calculated for each variable in the dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Dataset | The input raster dataset or mosaic dataset. | Mosaic Dataset; Mosaic Layer; Raster Dataset |
| X Skip Factor(Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster. The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Y Skip Factor(Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster. The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| Ignore Values(Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value or the last ignore value used. | Long |
| Skip Existing(Optional) | Specifies whether statistics will be calculated only when they are missing or will be regenerated even if they exist.Unchecked—Statistics will be calculated even if they already exist, and existing statistics will be overwritten. This is the default.Checked—Statistics will only be calculated if they do not already exist. | Boolean |
| Area of Interest (Optional) | The area in the dataset that will be used to calculate statistics, so they are not generated from the entire dataset. You can either browse to a feature class or create a polygon graphic on the display. | Feature Set |
| in_raster_dataset | The input raster dataset or mosaic dataset. | Mosaic Dataset; Mosaic Layer; Raster Dataset |
| x_skip_factor(Optional) | The number of horizontal pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster. The value must be greater than zero and less than or equal to the number of columns in the raster. The default is 1 or the last skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| y_skip_factor(Optional) | The number of vertical pixels between samples.A skip factor controls the portion of the raster that is used when calculating the statistics. The input value indicates the horizontal or vertical skip factor, where a value of 1 will use each pixel and a value of 2 will use every second pixel. The skip factor can only range from 1 to the number of columns/rows in the raster. The value must be greater than zero and less than or equal to the number of rows in the raster. The default is 1 or the last y skip factor used.The skip factors for raster datasets stored in a file geodatabase or an enterprise geodatabase are different. First, if the x and y skip factors are different, the smaller skip factor will be used for both the x and y skip factors. Second, the skip factor is related to the pyramid level that most closely fits the skip factor chosen. If the skip factor value is not equal to the number of pixels in a pyramid layer, the number is rounded down to the next pyramid level, and those statistics are used. | Long |
| ignore_values[ignore_value,...](Optional) | The pixel values that are not to be included in the statistics calculation.The default is no value or the last ignore value used. | Long |
| skip_existing(Optional) | Specifies whether statistics will be calculated only when they are missing or will be regenerated even if they exist.OVERWRITE—Statistics will be calculated even if they already exist, and existing statistics will be overwritten. This is the default.SKIP_EXISTING—Statistics will only be calculated if they do not already exist. | Boolean |
| area_of_interest(Optional) | The feature class that represents the area in the dataset that will be used to calculate statistics, so they are not generated from the entire dataset. | Feature Set |

## Code Samples

### Example 1

```python
arcpy.management.CalculateStatistics(in_raster_dataset, {x_skip_factor}, {y_skip_factor}, {ignore_values}, {skip_existing}, {area_of_interest})
```

### Example 2

```python
import arcpy
arcpy.CalculateStatistics_management(
     "C:/data/image.tif", "5", "5", "0;255", 
     "SKIP_EXISTING", "c:/data/aoi.shp")
```

### Example 3

```python
import arcpy
arcpy.CalculateStatistics_management(
     "C:/data/image.tif", "5", "5", "0;255", 
     "SKIP_EXISTING", "c:/data/aoi.shp")
```

### Example 4

```python
# Calculate Statistics for single raster dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
arcpy.CalculateStatistics_management("image.tif", "4", "6", "0;255;21")
```

### Example 5

```python
# Calculate Statistics for single raster dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
arcpy.CalculateStatistics_management("image.tif", "4", "6", "0;255;21")
```

---

## Change Privileges (Data Management)

## Summary

Establishes or changes user access privileges on the input enterprise database datasets, stand-alone feature classes, or tables.

## Usage

- To edit enterprise geodatabase datasets, both of the following are required:The View parameter must be set to Grant view privileges.The Edit parameter must be set to Grant edit privileges. Edit privileges are dependent on the view privilege, since you cannot edit what you cannot see (view).
- The View parameter must be set to Grant view privileges.
- The Edit parameter must be set to Grant edit privileges.
- If edit privileges are revoked, you can still view the dataset. However, if view privileges are revoked, edit privileges are automatically revoked as well.
- The relational database management system (RDBMS) equivalent command for the View parameter is Select.
- The RDBMS equivalent commands for the Edit parameter are Update, Insert, and Delete. All three are granted or revoked simultaneously by the Edit parameter.
- The parameter descriptions below use the terms user or username. Database roles can also be used in place of usernames. On RDBMS platforms that support operating system groups, the operating system group can also be specified in place of usernames.
- For datasets that are registered as branch versioned, privileges and access are controlled at the portal level. This is managed by sharing items in the organization and the portal user privileges.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The datasets, feature classes, or tables whose access privileges will be changed. | Layer; Table View; Dataset; Address Locator |
| User | The database username whose privileges will be modified. | String |
| View (Select)(Optional) | Specifies the user's view privileges.Do not change to view privileges—No changes will be made to the user's existing view privileges. If the user has view privileges, they will continue to have view privileges. If the user doesn't have view privileges, they will continue to not have view privileges. Grant view privileges—The user will be allowed to view the datasets.Revoke view privileges—The user's view privileges will be removed. | String |
| Edit (Update/Insert/Delete)(Optional) | Specifies the user's edit privileges.Do not change edit privileges— No changes will be made to the user's existing edit privileges. If the user has edit privileges, they will continue to have edit privileges. If the user doesn't have edit privileges, they will continue to not have edit privileges.Grant edit privileges—The user will be allowed to edit the input datasets.Revoke edit privileges—The user's edit privileges will be removed. The user can still view the input dataset. | String |
| in_dataset[in_dataset,...] | The datasets, feature classes, or tables whose access privileges will be changed. | Layer; Table View; Dataset; Address Locator |
| user | The database username whose privileges will be modified. | String |
| View(Optional) | Specifies the user's view privileges.AS_IS—No changes will be made to the user's existing view privileges. If the user has view privileges, they will continue to have view privileges. If the user doesn't have view privileges, they will continue to not have view privileges. GRANT—The user will be allowed to view the datasets.REVOKE—The user's view privileges will be removed. | String |
| Edit(Optional) | Specifies the user's edit privileges.AS_IS— No changes will be made to the user's existing edit privileges. If the user has edit privileges, they will continue to have edit privileges. If the user doesn't have edit privileges, they will continue to not have edit privileges.GRANT—The user will be allowed to edit the input datasets.REVOKE—The user's edit privileges will be removed. The user can still view the input dataset. | String |

## Code Samples

### Example 1

```python
arcpy.management.ChangePrivileges(in_dataset, user, {View}, {Edit})
```

### Example 2

```python
# Name: GrantPrivileges_Example.py
# Description: Grants view and edit privileges to WendelClark

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/Connections/gdb@production.sde/production.GDB.ctgFuseFeature"

# Run ChangePrivileges
arcpy.management.ChangePrivileges(datasetName, "WENDELCLARK", "GRANT", "GRANT")
```

### Example 3

```python
# Name: GrantPrivileges_Example.py
# Description: Grants view and edit privileges to WendelClark

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/Connections/gdb@production.sde/production.GDB.ctgFuseFeature"

# Run ChangePrivileges
arcpy.management.ChangePrivileges(datasetName, "WENDELCLARK", "GRANT", "GRANT")
```

---

## Change Version (Data Management)

## Summary

Modifies the workspace of a layer or table view to connect to the specified version.

## Usage

- The tool only works with feature layers and table views.
- The enterprise geodatabase connection file or feature service that is used to create the input feature layer or table view will not be edited by this tool. Only the open workspace of the input feature layer, table view, topology layer, parcel layer, utility network layer, or trace network layer is changed to connect to the specified version.
- Transactional and historical views are supported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The layer or table view that will connect to the specified version. Note:The sublayers of a topology layer, parcel layer, utility network layer, or trace network layer are not valid inputs. | Feature Layer; Table View; Topology Layer; Parcel Layer; Utility Network Layer; Trace Network Layer |
| Version Type | Specifies the type of version to which the input feature layer will connect.Traditional version—Connect to a defined state of the database (traditional version). Historical version—Connect to a version representing a defined moment in time, often specified by a time or historical marker. Branch version—Connect to a branch version. | String |
| Version Name (Optional) | The name of the version to which the input feature layer will connect. This parameter is optional if you're using a historical version. | String |
| Date and Time (Optional) | The date of the historical version to which the input feature layer will connect. | Date |
| Include participating classes of controller dataset (Optional) | Specifies whether the workspace of participating classes will also change.The parameter is only applicable when the input layer is a topology layer, parcel layer, utility network layer, or trace network layer.Checked—The version of the participating classes of the controller dataset will change if they are from the same workspace as the controller dataset. This is the default.Unchecked—Only the version of the controller dataset will change. | Boolean |
| in_features | The layer or table view that will connect to the specified version. Note:The sublayers of a topology layer, parcel layer, utility network layer, or trace network layer are not valid inputs. | Feature Layer; Table View; Topology Layer; Parcel Layer; Utility Network Layer; Trace Network Layer |
| version_type | Specifies the type of version to which the input feature layer will connect. TRANSACTIONAL—Connect to a defined state of the database (traditional version). HISTORICAL—Connect to a version representing a defined moment in time, often specified by a time or historical marker. BRANCH—Connect to a branch version. | String |
| version_name(Optional) | The name of the version to which the input feature layer will connect. This parameter is optional if you're using a historical version. | String |
| date(Optional) | The date of the historical version to which the input feature layer will connect. | Date |
| include_participating(Optional) | Specifies whether the workspace of participating classes will also change.The parameter is only applicable when the input layer is a topology layer, parcel layer, utility network layer, or trace network layer. INCLUDE—The version of the participating classes of the controller dataset will change if they are from the same workspace as the controller dataset.EXCLUDE—Only the version of the controller dataset will change. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ChangeVersion(in_features, version_type, {version_name}, {date}, {include_participating})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = r'c:\Connections\toolbox.sde'
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.street','RedlandsStreets')
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.streams','RedlandsStreams')
arcpy.MakeFeatureLayer_management(arcpy.SelectLayerByLocation_management("RedlandsStreams","WITHIN_A_DISTANCE","RedlandsStreets","100 Meters","NEW_SELECTION",'#'),
                                  'StreamsNearStreets','','','')
arcpy.ChangeVersion_management('RedlandsStreets','TRANSACTIONAL', 'TOOLBOX.proposedStreets2k9','')
arcpy.MakeFeatureLayer_management(arcpy.SelectLayerByLocation_management("RedlandsStreams","WITHIN_A_DISTANCE","RedlandsStreets","100 Meters","NEW_SELECTION",'#'),
                                  'NewStreamsNearStreets','','','')
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = r'c:\Connections\toolbox.sde'
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.street','RedlandsStreets')
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.streams','RedlandsStreams')
arcpy.MakeFeatureLayer_management(arcpy.SelectLayerByLocation_management("RedlandsStreams","WITHIN_A_DISTANCE","RedlandsStreets","100 Meters","NEW_SELECTION",'#'),
                                  'StreamsNearStreets','','','')
arcpy.ChangeVersion_management('RedlandsStreets','TRANSACTIONAL', 'TOOLBOX.proposedStreets2k9','')
arcpy.MakeFeatureLayer_management(arcpy.SelectLayerByLocation_management("RedlandsStreams","WITHIN_A_DISTANCE","RedlandsStreets","100 Meters","NEW_SELECTION",'#'),
                                  'NewStreamsNearStreets','','','')
```

### Example 4

```python
# Name: ChangeVersion.py
# Description: Use the ChangeVersion tool in a typical versioned analysis workflow.
#              Quick check of additional streams within a pre-determined distance
#              of new proposed street development.

# Import system modules 
import arcpy
import sys
import os

# Set environments
arcpy.env.workspace = sys.path[0] + os.sep + "toolbox.sde"

# Create the layers
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.street','RedlandsStreets')
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.streams','RedlandsStreams')

# Perform analysis on the Default version to determine
# current number of streams within 100 meters of streets.
selection1 = arcpy.SelectLayerByLocation_management("RedlandsStreams",
                                                    "WITHIN_A_DISTANCE",
                                                    "RedlandsStreets",
                                                    "100 Meters",
                                                    "NEW_SELECTION",
                                                    '#')
arcpy.MakeFeatureLayer_management(selection1,'StreamsNearStreets','','','')
print("Streams within 100 Meters of current streets: {}".format(
    arcpy.GetCount_management("StreamsNearStreets")[0]))

# Change to the development version
arcpy.ChangeVersion_management('RedlandsStreets',
                               'TRANSACTIONAL',
                               'TOOLBOX.proposedStreets2k9',
                               '')

# Perform the same analysis on the development version to see the effect of the proposed changes.
selection2 = arcpy.SelectLayerByLocation_management("RedlandsStreams",
                                                   "WITHIN_A_DISTANCE",
                                                   "RedlandsStreets",
                                                   "100 Meters",
                                                   "NEW_SELECTION",
                                                   '#')
arcpy.MakeFeatureLayer_management(selection2,'NewStreamsNearStreets','','','')
print("Streams projected to be within 100 Meters of streets after proposed street additions: {}".format(
    arcpy.GetCount_management("NewStreamsNearStreets")[0]))
```

### Example 5

```python
# Name: ChangeVersion.py
# Description: Use the ChangeVersion tool in a typical versioned analysis workflow.
#              Quick check of additional streams within a pre-determined distance
#              of new proposed street development.

# Import system modules 
import arcpy
import sys
import os

# Set environments
arcpy.env.workspace = sys.path[0] + os.sep + "toolbox.sde"

# Create the layers
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.street','RedlandsStreets')
arcpy.MakeFeatureLayer_management(r'TOOLBOX.Redlands\TOOLBOX.streams','RedlandsStreams')

# Perform analysis on the Default version to determine
# current number of streams within 100 meters of streets.
selection1 = arcpy.SelectLayerByLocation_management("RedlandsStreams",
                                                    "WITHIN_A_DISTANCE",
                                                    "RedlandsStreets",
                                                    "100 Meters",
                                                    "NEW_SELECTION",
                                                    '#')
arcpy.MakeFeatureLayer_management(selection1,'StreamsNearStreets','','','')
print("Streams within 100 Meters of current streets: {}".format(
    arcpy.GetCount_management("StreamsNearStreets")[0]))

# Change to the development version
arcpy.ChangeVersion_management('RedlandsStreets',
                               'TRANSACTIONAL',
                               'TOOLBOX.proposedStreets2k9',
                               '')

# Perform the same analysis on the development version to see the effect of the proposed changes.
selection2 = arcpy.SelectLayerByLocation_management("RedlandsStreams",
                                                   "WITHIN_A_DISTANCE",
                                                   "RedlandsStreets",
                                                   "100 Meters",
                                                   "NEW_SELECTION",
                                                   '#')
arcpy.MakeFeatureLayer_management(selection2,'NewStreamsNearStreets','','','')
print("Streams projected to be within 100 Meters of streets after proposed street additions: {}".format(
    arcpy.GetCount_management("NewStreamsNearStreets")[0]))
```

### Example 6

```python
# Name: ChangeVersionBranchVersioning.py
# Description: Use the ChangeVersion tool to change the branch version of a feature service
#              layer that has the version management capability enabled.

# Import system modules 
import arcpy

# Sign into ArcGIS Enterprise
arcpy.SignInToPortal("https://myserver.mydomain.com/portal", 'portaladmin', 'my.password')

# Create a variable for the feature service URL
myFS = "https://myserver.mydomain.com/server/rest/services/BranchVersioningData/FeatureServer/0"

# Create the layers
arcpy.management.MakeFeatureLayer(myFS, 'myFSLayer')

# Change to a branch version named myVersion
arcpy.management.ChangeVersion('myFSLayer', "BRANCH", "portaladmin.myVersion")
```

### Example 7

```python
# Name: ChangeVersionBranchVersioning.py
# Description: Use the ChangeVersion tool to change the branch version of a feature service
#              layer that has the version management capability enabled.

# Import system modules 
import arcpy

# Sign into ArcGIS Enterprise
arcpy.SignInToPortal("https://myserver.mydomain.com/portal", 'portaladmin', 'my.password')

# Create a variable for the feature service URL
myFS = "https://myserver.mydomain.com/server/rest/services/BranchVersioningData/FeatureServer/0"

# Create the layers
arcpy.management.MakeFeatureLayer(myFS, 'myFSLayer')

# Change to a branch version named myVersion
arcpy.management.ChangeVersion('myFSLayer', "BRANCH", "portaladmin.myVersion")
```

---

## Check Geometry (Data Management)

## Summary

Generates a report of geometry problems in a feature class.

## Usage

- Valid input formats are shapefiles, and feature classes stored in a file geodatabase, enterprise database, enterprise geodatabase, GeoPackage, or SpatiaLite database. For feature classes stored in an enterprise database or enterprise geodatabase, the following spatial types are supported:Microsoft SQL Server—Geometry and GeographyPostgreSQL—PostGIS, Geometry, and GeographyOracle—SDO_GeometrySAP HANA—ST_Geometry License:A Desktop Basic license allows you to apply this tool to only shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database. A Desktop Standard or Desktop Advanced license allows you to also apply this tool to feature classes stored in an enterprise database or enterprise geodatabase.
- Microsoft SQL Server—Geometry and Geography
- PostgreSQL—PostGIS, Geometry, and Geography
- Oracle—SDO_Geometry
- SAP HANA—ST_Geometry
- Feature classes that are stored in an enterprise geodatabase and registered as versioned are not supported.
- The Output Table parameter value will include a record for each geometry problem found. If no problems are found, the table will be empty.
- The contents of the Output Table parameter value, including the PROBLEM field codes, are written in English.
- For point features, only the null geometry problem is possible.
- To review the features that are reported to have geometry problems, you can join the Input Features parameter value to the Output Table parameter value using the Add Join tool, along with the input's OBJECTID or FID field and the output table's FEATURE_ID field. If any of the Input Features parameter values are stored in an enterprise geodatabase, enterprise database, GeoPackage, or SpatiaLite database, the output table will also include a TEXT_ID field that is used for noninteger-based OBJECTID columns.
- The Esri validation option ensures that geometry is topologically correct using the Esri Simplify method. Only the Esri validation is available for data stored in an enterprise geodatabase.
- The Open Geospatial Consortium (OGC) validation method ensures that geometry complies with the OGC specification as defined in OpenGIS Implementation Standard for Geographic information – simple feature access – Part 1: common architecture.
- After a feature's geometry is repaired using the OGC option, any subsequent edit or modification may cause the geometry to no longer comply with the OGC specification. After feature modification, rerun the Check Geometry tool to check for new geometry issues. If necessary, run the Repair Geometry tool.
- Geometry that is validated or repaired using the OGC option will be valid for the Esri option. To learn more about both methods, see What is a simple polygon.
- The problems identified by this tool can be addressed in the following ways:Manually edit and fix the feature with the geometry problems. Some of the problems cannot be fixed through editing.Use the Repair Geometry tool. Some problems associated with data stored in an enterprise database, enterprise geodatabase, GeoPackage, and SpatiaLite database may not be repairable with ArcGIS tools.
- Manually edit and fix the feature with the geometry problems. Some of the problems cannot be fixed through editing.
- Use the Repair Geometry tool. Some problems associated with data stored in an enterprise database, enterprise geodatabase, GeoPackage, and SpatiaLite database may not be repairable with ArcGIS tools.
- The Output Table parameter value has the following fields:CLASS—The full path to and name of the feature class in which the problem was found.FEATURE_ID—The Feature ID (FID) or Object ID (OID) for the feature with the geometry problem.TEXT_ID—This field only exists when Input Features values are stored in an enterprise database, enterprise geodatabase, GeoPackage or SpatiaLite database.PROBLEM—A short description of the problem.
- CLASS—The full path to and name of the feature class in which the problem was found.
- FEATURE_ID—The Feature ID (FID) or Object ID (OID) for the feature with the geometry problem.
- TEXT_ID—This field only exists when Input Features values are stored in an enterprise database, enterprise geodatabase, GeoPackage or SpatiaLite database.
- PROBLEM—A short description of the problem.
- The PROBLEM field will contain one of the following codes:Short segment—Some segments are shorter than allowed by the system units of the spatial reference associated with the geometry.Null geometry—The feature has no geometry or the SHAPE field is empty.Incorrect ring ordering—The polygon is topologically simple, but its rings may not be oriented correctly (outer rings clockwise, inner rings counterclockwise).Incorrect segment orientation—Individual segments are not consistently oriented. The to point of segment i should be incident on the from point of segment i+1.Self intersections—A polygon must not intersect itself. Unclosed rings—The last segment in a ring must have its to point incident on the from point of the first segment.Empty parts—The geometry has multiple parts, and one of them is empty (has no geometry).Duplicate vertex—The geometry has two or more sequential vertices with identical coordinates.Mismatched attributes—The z- or m-coordinate of a line segment's endpoint does not match the z- or m-coordinate of the coincident endpoint on the next segment.Discontinuous parts—One of the geometry's parts is composed of disconnected or discontinuous parts.Empty Z values—The geometry has one or more vertices with an empty z-value (NaN, for example).Bad envelope—The envelope does not match the coordinate extent of the geometry. Bad dataset extent—The extent property of the dataset does not contain all of the features in the dataset. For this problem, the FEATURE_ID value will be -1. For data stored in enterprise geodatabases, the PROBLEM field will contain one of the following codes: NEEDS_REORDERING—The shape must be reordered or have duplicate points removed. SE_INVALID_ENTITY_TYPE—The entity type is invalid. SE_SHAPE_INTEGRITY_ERROR—A shape integrity error occurred. SE_INVALID_SHAPE_OBJECT—The shape object handle is invalid. SE_COORD_OUT_OF_BOUNDS—The specified coordinate exceeds the valid coordinate range. SE_POLY_SHELLS_OVERLAP—Two donuts or two outer shells overlap. SE_TOO_FEW_POINTS—The number of points is less than required for the feature. SE_INVALID_PART_SEPARATOR—A part separator is in the wrong position. SE_INVALID_POLYGON_CLOSURE—A polygon does not close properly. SE_INVALID_OUTER_SHELL—A polygon outer shell does not completely enclose all donuts for the part. SE_ZERO_AREA_POLYGON—A polygon shell has no area. SE_POLYGON_HAS_VERTICAL_LINE—A polygon shell contains a vertical line. SE_OUTER_SHELLS_OVERLAP—A multipart area has overlapping parts. SE_SELF_INTERSECTING—A line string or poly boundary is self-intersecting.
- Short segment—Some segments are shorter than allowed by the system units of the spatial reference associated with the geometry.
- Null geometry—The feature has no geometry or the SHAPE field is empty.
- Incorrect ring ordering—The polygon is topologically simple, but its rings may not be oriented correctly (outer rings clockwise, inner rings counterclockwise).
- Incorrect segment orientation—Individual segments are not consistently oriented. The to point of segment i should be incident on the from point of segment i+1.
- Self intersections—A polygon must not intersect itself.
- Unclosed rings—The last segment in a ring must have its to point incident on the from point of the first segment.
- Empty parts—The geometry has multiple parts, and one of them is empty (has no geometry).
- Duplicate vertex—The geometry has two or more sequential vertices with identical coordinates.
- Mismatched attributes—The z- or m-coordinate of a line segment's endpoint does not match the z- or m-coordinate of the coincident endpoint on the next segment.
- Discontinuous parts—One of the geometry's parts is composed of disconnected or discontinuous parts.
- Empty Z values—The geometry has one or more vertices with an empty z-value (NaN, for example).
- Bad envelope—The envelope does not match the coordinate extent of the geometry.
- Bad dataset extent—The extent property of the dataset does not contain all of the features in the dataset. For this problem, the FEATURE_ID value will be -1.
- NEEDS_REORDERING—The shape must be reordered or have duplicate points removed.
- SE_INVALID_ENTITY_TYPE—The entity type is invalid.
- SE_SHAPE_INTEGRITY_ERROR—A shape integrity error occurred.
- SE_INVALID_SHAPE_OBJECT—The shape object handle is invalid.
- SE_COORD_OUT_OF_BOUNDS—The specified coordinate exceeds the valid coordinate range.
- SE_POLY_SHELLS_OVERLAP—Two donuts or two outer shells overlap.
- SE_TOO_FEW_POINTS—The number of points is less than required for the feature.
- SE_INVALID_PART_SEPARATOR—A part separator is in the wrong position.
- SE_INVALID_POLYGON_CLOSURE—A polygon does not close properly.
- SE_INVALID_OUTER_SHELL—A polygon outer shell does not completely enclose all donuts for the part.
- SE_ZERO_AREA_POLYGON—A polygon shell has no area.
- SE_POLYGON_HAS_VERTICAL_LINE—A polygon shell contains a vertical line.
- SE_OUTER_SHELLS_OVERLAP—A multipart area has overlapping parts.
- SE_SELF_INTERSECTING—A line string or poly boundary is self-intersecting.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature class or layer to be processed.License:A Desktop Basic license only allows shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database as valid input feature formats. A Desktop Standard or Desktop Advanced license also allows feature classes stored in an enterprise database or enterprise geodatabase to be used as valid input feature formats. | Feature Layer |
| Output Table(Optional) | The report (as a table) of the problems discovered. | Table |
| Validation Method(Optional) | Specifies the geometry validation method that will be used to identify geometry problems.Esri—The Esri geometry validation method will be used. This is the default.OGC—The OGC geometry validation method will be used. | String |
| in_features[in_features,...] | The feature class or layer to be processed.License:A Desktop Basic license only allows shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database as valid input feature formats. A Desktop Standard or Desktop Advanced license also allows feature classes stored in an enterprise database or enterprise geodatabase to be used as valid input feature formats. | Feature Layer |
| out_table(Optional) | The report (as a table) of the problems discovered. | Table |
| validation_method(Optional) | Specifies the geometry validation method that will be used to identify geometry problems.ESRI—The Esri geometry validation method will be used. This is the default.OGC—The OGC geometry validation method will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.CheckGeometry(in_features, {out_table}, {validation_method})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/data/data.gdb"
arcpy.management.CheckGeometry(["contours", "roads", "vegetation"], "CheckGeom_Result")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/data/data.gdb"
arcpy.management.CheckGeometry(["contours", "roads", "vegetation"], "CheckGeom_Result")
```

### Example 4

```python
# BatchCheckGeometry.py
# Description: Loop through all the feature classes in a geodatabase, and 
#              generate a report of the problems encountered with feature 
#              geometry.

# Import modules
import arcpy
import os

# The geodatabase in which the feature classes will be checked
arcpy.env.workspace = "C:\\data\\St_Lucia.gdb"
out_table = "checkGeometryResult"
 
# A variable that will hold the list of all the feature classes 
# in the geodatabase
fc_list = []

# Identify all feature classes in the geodatabase
for path, dirnames, fcs in arcpy.da.Walk(arcpy.env.workspace, 
                                         datatype='FeatureClass'):
    for fc in fcs:
        fc_list.append(os.path.join(path, fc))
        
print("Running the check geometry tool on {} feature classes".format(
    len(fc_list)))
arcpy.management.CheckGeometry(fc_list, out_table)

print("{} geometry problems found, see {} for details.".format(
    arcpy.management.GetCount(out_table)[0], out_table))
```

### Example 5

```python
# BatchCheckGeometry.py
# Description: Loop through all the feature classes in a geodatabase, and 
#              generate a report of the problems encountered with feature 
#              geometry.

# Import modules
import arcpy
import os

# The geodatabase in which the feature classes will be checked
arcpy.env.workspace = "C:\\data\\St_Lucia.gdb"
out_table = "checkGeometryResult"
 
# A variable that will hold the list of all the feature classes 
# in the geodatabase
fc_list = []

# Identify all feature classes in the geodatabase
for path, dirnames, fcs in arcpy.da.Walk(arcpy.env.workspace, 
                                         datatype='FeatureClass'):
    for fc in fcs:
        fc_list.append(os.path.join(path, fc))
        
print("Running the check geometry tool on {} feature classes".format(
    len(fc_list)))
arcpy.management.CheckGeometry(fc_list, out_table)

print("{} geometry problems found, see {} for details.".format(
    arcpy.management.GetCount(out_table)[0], out_table))
```

### Example 6

```python
import arcpy

result = arcpy.management.CheckGeometry('c:/data/data.gdb/badgeometryfc')
if result[1] == 'true':
    # Geometry problems found, print the tool messages.
    print(result.getMessages())

else:
    # No problems found
    print("No problems Found")
```

### Example 7

```python
import arcpy

result = arcpy.management.CheckGeometry('c:/data/data.gdb/badgeometryfc')
if result[1] == 'true':
    # Geometry problems found, print the tool messages.
    print(result.getMessages())

else:
    # No problems found
    print("No problems Found")
```

---

## Clear Pixel Cache (Data Management)

## Summary

Clears the pixel cache associated with a mosaic dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The input mosaic dataset with the pixel cache to be deleted. | Mosaic Layer |
| Generated Before (Optional) | All cache generated before this date will be deleted. | Date |
| in_mosaic_dataset | The input mosaic dataset with the pixel cache to be deleted. | Mosaic Layer |
| generated_before(Optional) | All cache generated before this date will be deleted. | Date |

## Code Samples

### Example 1

```python
arcpy.management.ClearPixelCache(in_mosaic_dataset, {generated_before})
```

### Example 2

```python
import arcpy

arcpy.management.ClearPixelCache("c:\\test\\outputdatabase.gdb\mosaicdataset", "10/24/2018 4:15:38 PM")
```

### Example 3

```python
import arcpy

arcpy.management.ClearPixelCache("c:\\test\\outputdatabase.gdb\mosaicdataset", "10/24/2018 4:15:38 PM")
```

### Example 4

```python
#===========================
#Clear Pixel Cache
'''Usage: ClearPixelCache_management(in_mosaic_dataset, {generated_before})'''

import arcpy

#Clear Pixel Cache
mdname = r"c:\test\Clearpixelcahce.gdb\mosaicdataset"
date = "10/25/2018"


arcpy.management.ClearPixelCache(mdname, date)
```

### Example 5

```python
#===========================
#Clear Pixel Cache
'''Usage: ClearPixelCache_management(in_mosaic_dataset, {generated_before})'''

import arcpy

#Clear Pixel Cache
mdname = r"c:\test\Clearpixelcahce.gdb\mosaicdataset"
date = "10/25/2018"


arcpy.management.ClearPixelCache(mdname, date)
```

---

## Clear Workspace Cache (Data Management)

## Summary

Clears information about a workspace that has been cached in memory.

## Usage

- The single input parameter is optional. If you run the tool without specifying a workspace, all contents in the workspace cache will be cleared. To remove cached information about a specific workspace, use that geodatabase, folder, or .sde connection file as input.
- This tool can be used to help disconnect idle enterprise geodatabase connections in a long-running application.
- To clear the workspace cache in a Python script, use this tool as the final call in the script. Use the Python del statement to delete all references to objects or variables that may be pointing to the workspace before calling this tool. If the script uses both ArcPy commands and other Python file and folder management commands, schema locks held by the workspace cache may prevent other Python commands from running successfully. Clearing the workspace cache enables these functions that delete or modify files or folders to succeed and not be blocked by schema locks.
- The first ten workspaces used in an ArcGIS Pro or a stand-alone Python process will be included in the workspace cache. Additional workspaces used in the same process will not be cached.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace (Optional) | The geodatabase, .sde connection file, or folder path representing the workspace that will be removed from the workspace cache. If no value is specified, all contents of the workspace cache will be cleared. | Data Element; Layer |
| in_data(Optional) | The geodatabase, .sde connection file, or folder path representing the workspace that will be removed from the workspace cache. If no value is specified, all contents of the workspace cache will be cleared. | Data Element; Layer |

## Code Samples

### Example 1

```python
arcpy.management.ClearWorkspaceCache({in_data})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/connectionFiles/SQL Server.sde"
arcpy.management.ClearWorkspaceCache()
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/connectionFiles/SQL Server.sde"
arcpy.management.ClearWorkspaceCache()
```

---

## Clip Raster (Data Management)

## Summary

Cuts out a portion of a raster dataset, mosaic dataset, or image service layer.

## Usage

- This tool allows you to extract a portion of a raster dataset based on a template extent. The clip output includes any pixels that intersect the template extent. To extract a portion of a feature dataset, use the Clip tool in the Analysis toolbox.
- The clipped area is specified either by a rectangular envelope using minimum and maximum x- and y-coordinates or by using an output extent file. If the clip extent specified is not aligned with the input raster dataset, this tool verifies that the proper alignment is used. This may cause the output to have a slightly different extent than specified in the tool.
- An existing raster or vector layer can be used as the clip extent. If you are using a feature class as the output extent, you can clip the raster by the minimum bounding rectangle of the feature class or by the polygon geometry of the features. If clipping geometry is used, the pixel depth of the output may be promoted. Ensure that the output format can support the proper pixel depth.
- You can also use the selected features within the display as the clipping extent. If a feature in the feature class is selected and the Use Input Features for Clipping Geometry parameter is checked, the output clips out the selected areas. If a feature in the feature class is selected but the Use Input Features for Clipping Geometry parameter is not checked, the output clips out the minimum bounding rectangle for that feature.
- You can interactively draw a polygon on the raster in the map to be used as the clipping extent. Click the Create new features in the current map to use as input drop-down list and click Polygon. Then move the pointer to the map to digitize a polygon. Double-click to complete the clipping polygon.Note:The only option available for clipping a raster layer is Polygon.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- The extent values must be in the same spatial coordinates and units as the raster dataset.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset, mosaic dataset, or image service to be clipped. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| Rectangle | The four coordinates that define the extent of the bounding box that will be used to clip the raster.If the Output Extent parameter is set, it will automatically populate the x-min, y-min, x-max, and y-max values. Use the Reset button to reset the rectangle extent to the extent of the input raster dataset.If the value specified is not aligned with the input raster dataset, the tool verifies that the proper alignment is used. This may cause the output to have a slightly different extent than specified. | Envelope; Feature Class; Feature Layer |
| Output Raster Dataset | The name, location, and format of the dataset being created. Ensure that it can support the necessary bit depth.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| Output Extent(Optional) | A raster dataset or feature class that will be used as the extent. The clip output includes pixels that intersect the minimum bounding rectangle. If a feature class is used as the output extent and you want to clip the raster based on the polygon features, check the Use Input Features for Clipping Geometry parameter. When this parameter is checked, the pixel depth of the output may be promoted. Ensure that the output format can support the proper pixel depth. | Raster Layer; Feature Layer |
| NoData Value(Optional) | The value for pixels to be considered as NoData. | String |
| Use Input Features for Clipping Geometry(Optional) | Specifies whether the minimum bounding rectangle or the geometry of the specified feature class will be used to clip the data.Checked—The geometry of the specified feature class will be used to clip the data. The pixel depth of the output may be increased; ensure that the output format can support the proper pixel depth.Unchecked—The minimum bounding rectangle will be used to clip the data. This is the default. | Boolean |
| Maintain Clipping Extent (Optional) | Specifies the extent that will be used in the clipping output.Checked—The number of columns and rows will be adjusted and the pixels will be resampled to exactly match the clipping extent specified.Unchecked—The cell alignment of the input raster will be maintained and the output extent will be adjusted accordingly. This is the default. | Boolean |
| in_raster | The raster dataset, mosaic dataset, or image service to be clipped. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| rectangle | The four coordinates that define the extent of the bounding box that will be used to clip the raster. Coordinates are expressed in the order of x-min, y-min, x-max, y-max.If the in_template_dataset parameter is set, it will automatically set this parameter. If the in_template_dataset parameter is a feature layer, the clipping extent is extracted from the bounding box. In this case, the rectangle parameter can be left empty as long as the in_template_dataset parameter value is specified.If both the rectangle and in_template_dataset parameters are set, the rectangle parameter value will be used.If the value specified is not aligned with the input raster dataset, the tool verifies that the proper alignment is used. This may cause the output to have a slightly different extent than specified. | Envelope; Feature Class; Feature Layer |
| out_raster | The name, location, and format of the dataset being created. Ensure that it can support the necessary bit depth.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| in_template_dataset(Optional) | A raster dataset or feature class that will be used as the extent. The clip output includes pixels that intersect the minimum bounding rectangle. If a feature class is used as the output extent and you want to clip the raster based on the polygon features, set the clipping_geometry parameter to ClippingGeometry. This option may promote the pixel depth of the output. Ensure that the output format can support the proper pixel depth. | Raster Layer; Feature Layer |
| nodata_value(Optional) | The value for pixels to be considered as NoData. | String |
| clipping_geometry(Optional) | Specifies whether the minimum bounding rectangle or the geometry of the specified feature class will be used to clip the data.NONE—The minimum bounding rectangle will be used to clip the data. This is the default.ClippingGeometry—The geometry of the specified feature class will be used to clip the data. The pixel depth of the output may be increased; ensure that the output format can support the proper pixel depth. | Boolean |
| maintain_clipping_extent(Optional) | Specifies the extent that will be used in the clipping output.MAINTAIN_EXTENT—The number of columns and rows will be adjusted and the pixels will be resampled to exactly match the clipping extent specified.NO_MAINTAIN_EXTENT—The cell alignment of the input raster will be maintained and the output extent will be adjusted accordingly. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.Clip(in_raster, rectangle, out_raster, {in_template_dataset}, {nodata_value}, {clipping_geometry}, {maintain_clipping_extent})
```

### Example 2

```python
import arcpy
arcpy.Clip_management(
    "c:/data/image.tif","1952602 294196 1953546 296176",
    "c:/data/clip.gdb/clip01", "#", "#", "NONE", "NO_MAINTAIN_EXTENT")
```

### Example 3

```python
import arcpy
arcpy.Clip_management(
    "c:/data/image.tif","1952602 294196 1953546 296176",
    "c:/data/clip.gdb/clip01", "#", "#", "NONE", "NO_MAINTAIN_EXTENT")
```

### Example 4

```python
##Clip Raster Dataset by known extent - Left Bottom Right Top

import arcpy
arcpy.env.workspace = "C:/Workspace"
    

arcpy.Clip_management(
    "image.tif","1952602.23 294196.279 1953546.23 296176.279",
    "clip.gdb/clip", "#", "#", "NONE")
```

### Example 5

```python
##Clip Raster Dataset by known extent - Left Bottom Right Top

import arcpy
arcpy.env.workspace = "C:/Workspace"
    

arcpy.Clip_management(
    "image.tif","1952602.23 294196.279 1953546.23 296176.279",
    "clip.gdb/clip", "#", "#", "NONE")
```

### Example 6

```python
##Clip while maintaining original extent



import arcpy

arcpy.env.workspace = "C:/Workspace"



arcpy.Clip_management("c:\\test\\image.tif", "2536996.21761925 7365614.23930381 2537634.12209192 7366302.3861673", 
                      "c:\\output\\clip.tif", "c:\\test\\clipfeature.shp", "0", "ClippingGeometry", 
                      "MAINTAIN_EXTENT")
```

### Example 7

```python
##Clip while maintaining original extent



import arcpy

arcpy.env.workspace = "C:/Workspace"



arcpy.Clip_management("c:\\test\\image.tif", "2536996.21761925 7365614.23930381 2537634.12209192 7366302.3861673", 
                      "c:\\output\\clip.tif", "c:\\test\\clipfeature.shp", "0", "ClippingGeometry", 
                      "MAINTAIN_EXTENT")
```

---

## Color Balance Mosaic Dataset (Data Management)

## Summary

Color balances a mosaic dataset so that the tiles appear seamless.

## Usage

- Color balancing can only take place if the following is true about your data:All the bands have their statistics calculated.All the bands have their histogram built.None of the raster datasets have an associated color map.
- All the bands have their statistics calculated.
- All the bands have their histogram built.
- None of the raster datasets have an associated color map.
- The bands in the Target Raster parameter value must be in the same order as the bands in the Mosaic Dataset parameter value. Ideally, the number of bands should be the same. If there are more bands in the input mosaic dataset, the bands in the target raster will be used again, sequentially.
- If the bit depth of the input mosaic dataset and the target raster are different, the pixel values will be automatically scaled so that they are both in the same bit depth.
- The actions defined by the Exclude Area Raster, Stretch Type, and Gamma parameters are performed before any color balancing occurs.
- The target color surface is only available if the dodging balancing technique is chosen. When using the dodging technique, each pixel needs a target color, which is picked up from the target color surface. There are five types of target color surfaces that you can choose from: single color, color grid, first order surface, second order surface, and third order surface.
- The Target Raster and Target Raster Object ID parameter values are used to guide color balancing. The color of the Target Raster and Target Raster Object ID parameter values will not be changed, and its color characteristics will be used to color balance the other images in the mosaic dataset. While the color of the target raster is maintained, the global optimization of the entire mosaic dataset may become weaker, so it is not necessary to always get the best overall optimized color balance outcome.When dodging balancing is used—The target color that will be derived depends on the target color surface type that was chosen. For single color, the average value of the reference target image is used. For color grid, the reference target image is resampled to a suitable grid. For the polynomial order surfaces, the coefficients of the polynomial are obtained by least square fitting, from the reference target image.When histogram balancing is used—The target histogram is obtained from the reference target image.When standard deviation balancing is used—The target standard deviation is obtained from the reference target image.
- When dodging balancing is used—The target color that will be derived depends on the target color surface type that was chosen. For single color, the average value of the reference target image is used. For color grid, the reference target image is resampled to a suitable grid. For the polynomial order surfaces, the coefficients of the polynomial are obtained by least square fitting, from the reference target image.
- When histogram balancing is used—The target histogram is obtained from the reference target image.
- When standard deviation balancing is used—The target standard deviation is obtained from the reference target image.
- The DEM Raster parameter is activated when the Balance Method parameter is set to Global Fit. It is recommended that the digital elevation model (DEM) be local rather than online; otherwise, the computation will take longer. When the DEM Raster parameter value is specified, the Z Factor, Z Offset, and Apply Geoid Correction parameters are activated, and their default values are typically used.
- The Input Solution Points parameter is activated when the Balance Method parameter is set to Global Fit. You can use the solution points from block adjustment output to help estimate the locations of image overlap. This is helpful when the image has less than 50 percent overlap with its neighbors. It is recommended that you do not use this option for mosaic datasets with good overlap because it will increase computation time.
- To remove a color correction, right-click the mosaic dataset in the Catalog pane and click Remove > Remove Color Balancing.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that will be color balanced. | Mosaic Layer |
| Balance Method(Optional) | Specifies the balancing method that will be used. Dodging—Each pixel's value will be changed toward a target color. With this method, you must also choose the type of target color surface, which affects the target color. Dodging tends to give the best result in most cases.Global Fit—An optimal pixel value will be determined by globally adjusting the color difference in all overlap areas to the minimum. This method is suitable for a mosaic dataset in which each image has good overlap with each other.Histogram—Each pixel's value will be changed according to its relationship with a target histogram. The target histogram can be derived from all of the rasters, or you can specify a raster. This method works well when all of the rasters have a similar histogram.Standard deviation—Each of the pixel's values will be changed according to its relationship with the histogram of the target raster, within one standard deviation. The standard deviation can be calculated from all of the rasters in the mosaic dataset, or you can specify a target raster. This method works best when all of the rasters have normal distributions. | String |
| Color Surface Type(Optional) | Specifies how the target color of each pixel will be determined.This parameter is active when the Balance Method parameter is set to Dodging.Single color— All the pixels will be altered toward a single color point—the average of all pixels. Use this option when there are only a small number of raster datasets and a few different types of ground objects. If there are too many raster datasets or too many types of ground surfaces, the output color may become blurred.Color grid— Pixels will be altered toward multiple target colors, which are distributed across the mosaic dataset. Use this option when you have a large number of raster datasets or areas with a large number of diverse ground objects.First order— All pixels will be altered toward many points obtained from the two-dimensional polynomial slanted plane. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface.Second order— All input pixels will be altered toward a set of multiple points obtained from the two-dimensional polynomial parabolic surface. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface.Third order— All input pixels will be altered toward multiple points obtained from the cubic surface. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface. | String |
| Target Raster(Optional) | The raster that will be used to color balance the other images. The balance method and color surface type, if applicable, will be derived from this image. | Raster Dataset; Raster Layer; Internet Tiled Layer; Map Server Layer |
| Exclude Area Raster(Optional) | A raster that identifies the locations that will be excluded.Create a mask using the Generate Exclude Area tool. | Raster Layer |
| Stretch Type(Optional) | Specifies how the range of values will be stretched before color balancing.None— The original pixel values will be used. This is the default.Adaptive— An adaptive prestretch will be applied before any processing takes place.Minimum Maximum— The values will be stretched between their minimum and maximum values.Standard deviation— The values will be stretched between the default number of standard deviations. | String |
| Gamma(Optional) | A numeric value that will adjust the overall brightness of an image. A low value will minimize the contrast between moderate values by making them appear darker. Higher values will increase the contrast by making them appear brighter. | Double |
| Block Field (Optional) | A field in the mosaic dataset's attribute table that will be used to identify items that will be considered one item when performing some calculations and operations. | String |
| DEM Raster (Optional) | A DEM to help estimate the overlapped locations in the mosaic dataset.This parameter is active when the Balance Method parameter is set to Global Fit. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| Z Factor(Optional) | A conversion factor that adjusts the units of measure for the vertical (or elevation) units when they are different from the horizontal coordinate (x,y) units of the input surface DEM. It is the number of ground x,y units in one surface z-unit. If the vertical units are meters, set the parameter to 1. If the vertical units are feet, set the parameter to 0.3048. If any other vertical units are used, use this parameter to scale the units to meters.This parameter is active when the DEM Raster parameter is specified. | Double |
| Z Offset(Optional) | A base value that will be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level.This parameter is active when the DEM Raster parameter is specified. | Double |
| Apply Geoid Correction (Optional) | Specifies whether the geoid correction required by rational polynomial coefficients (RPC) that reference ellipsoidal heights will be made. Most elevation datasets are referenced to sea level orthometric heights, so this correction is required in these cases to convert to ellipsoidal heights.This parameter is active when the DEM Raster parameter is specified.Unchecked—No geoid correction will be made. Use this option only if the DEM is already expressed in ellipsoidal heights. This is the default. Checked—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |
| Input Solution Points(Optional) | The solution points from block adjustment output to help accurately estimate the overlapped locations. This parameter is helpful when the image has less than 50 percent overlap with its neighbors. Using this parameter will increase the computation time, so for the ordinary mosaic dataset with good overlaps, you can leave this parameter unspecified.This parameter is active when the Balance Method parameter is set to Global Fit. | Table View |
| Target Raster Object ID(Optional) | The target raster object ID that will be used to color balance the other images. The balance method and color surface type, if applicable, will be derived from this image.This parameter is active when the Balance Method parameter is set to Global Fit. | Long |
| Refine Estimation by Correlation (Optional) | Specifies whether the color balancing estimation for corresponding locations in the overlapped areas will be refined using image correlation.This parameter is helpful for the exact color difference correction but will increase the computation time. If you have a mosaic dataset composed of a large number of images, uncheck this parameter to reduce the computation time.This parameter is active when the Balance Method parameter is set to Global Fit.Unchecked—No refined estimation will be made. Checked—A refined estimation for color balance will be made. This is the default. | Boolean |
| Reduce Shadow Influence (Optional) | Specifies whether the negative influence of shadows on the color balance output will be reduced.This parameter is active when the Balance Method parameter is set to Global Fit.Unchecked—The influence of shadows will not be reduced. This is the default. Checked—The influence of shadows will be reduced. Use this option when the mosaic dataset has a lot of shadows. | Boolean |
| Reduce Cloud Influence (Optional) | Specifies whether the negative influence of clouds on the color balance output will be reduced.This parameter is active when the Balance Method parameter is set to Dodging or Global Fit.Unchecked—The influence of clouds will not be reduced. This is the default. Checked—The influence of clouds will be reduced. | Boolean |
| in_mosaic_dataset | The mosaic dataset that will be color balanced. | Mosaic Layer |
| balancing_method(Optional) | Specifies the balancing method that will be used. DODGING—Each pixel's value will be changed toward a target color. With this method, you must also choose the type of target color surface, which affects the target color. Dodging tends to give the best result in most cases.GLOBAL_FIT—An optimal pixel value will be determined by globally adjusting the color difference in all overlap areas to the minimum. This method is suitable for a mosaic dataset in which each image has good overlap with each other.HISTOGRAM—Each pixel's value will be changed according to its relationship with a target histogram. The target histogram can be derived from all of the rasters, or you can specify a raster. This method works well when all of the rasters have a similar histogram.STANDARD_DEVIATION—Each of the pixel's values will be changed according to its relationship with the histogram of the target raster, within one standard deviation. The standard deviation can be calculated from all of the rasters in the mosaic dataset, or you can specify a target raster. This method works best when all of the rasters have normal distributions. | String |
| color_surface_type(Optional) | Specifies how the target color of each pixel will be determined.This parameter is enabled when the balancing_method parameter is set to DODGING.SINGLE_COLOR— All the pixels will be altered toward a single color point—the average of all pixels. Use this option when there are only a small number of raster datasets and a few different types of ground objects. If there are too many raster datasets or too many types of ground surfaces, the output color may become blurred.COLOR_GRID— Pixels will be altered toward multiple target colors, which are distributed across the mosaic dataset. Use this option when you have a large number of raster datasets or areas with a large number of diverse ground objects.FIRST_ORDER— All pixels will be altered toward many points obtained from the two-dimensional polynomial slanted plane. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface.SECOND_ORDER— All input pixels will be altered toward a set of multiple points obtained from the two-dimensional polynomial parabolic surface. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface.THIRD_ORDER— All input pixels will be altered toward multiple points obtained from the cubic surface. This option tends to create a smoother color change and uses less storage in the auxiliary table, but it may take longer to process compared to the color grid surface. | String |
| target_raster(Optional) | The raster that will be used to color balance the other images. The balance method and color surface type, if applicable, will be derived from this image. | Raster Dataset; Raster Layer; Internet Tiled Layer; Map Server Layer |
| exclude_raster(Optional) | A raster that identifies the locations that will be excluded.Create a mask using the Generate Exclude Area tool. | Raster Layer |
| stretch_type(Optional) | Specifies how the range of values will be stretched before color balancing.NONE— The original pixel values will be used. This is the default.ADAPTIVE— An adaptive prestretch will be applied before any processing takes place.MINIMUM_MAXIMUM— The values will be stretched between their minimum and maximum values.STANDARD_DEVIATION— The values will be stretched between the default number of standard deviations. | String |
| gamma(Optional) | A numeric value that will adjust the overall brightness of an image. A low value will minimize the contrast between moderate values by making them appear darker. Higher values will increase the contrast by making them appear brighter. | Double |
| block_field(Optional) | A field in the mosaic dataset's attribute table that will be used to identify items that will be considered one item when performing some calculations and operations. | String |
| in_DEM_raster(Optional) | A DEM to help estimate the overlapped locations in the mosaic dataset.This parameter is enabled when the balancing_method parameter is set to GLOBAL_FIT. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| ZFactor(Optional) | A conversion factor that adjusts the units of measure for the vertical (or elevation) units when they are different from the horizontal coordinate (x,y) units of the input surface DEM. It is the number of ground x,y units in one surface z-unit. If the vertical units are meters, set the parameter to 1. If the vertical units are feet, set the parameter to 0.3048. If any other vertical units are used, use this parameter to scale the units to meters.This parameter is enabled when the in_DEM_raster parameter is specified. | Double |
| ZOffset(Optional) | A base value that will be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level.This parameter is enabled when the in_DEM_raster parameter is specified. | Double |
| Geoid(Optional) | Specifies whether the geoid correction required by rational polynomial coefficients ( RPC) that reference ellipsoidal heights will be made. Most elevation datasets are referenced to sea level orthometric heights, so this correction is required in these cases to convert to ellipsoidal heights.This parameter is active when the in_DEM_raster parameter is specified.NONE—No geoid correction will be made. Use this option only if the DEM is already expressed in ellipsoidal heights. This is the default.GEOID—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |
| solution_points(Optional) | The solution points from block adjustment output to help accurately estimate the overlapped locations. This parameter is helpful when the image has less than 50 percent overlap with its neighbors. Using this parameter will increase the computation time, so for the ordinary mosaic dataset with good overlaps, you can leave this parameter unspecified.This parameter is enabled when the balancing_method parameter is set to GLOBAL_FIT. | Table View |
| target_objectid(Optional) | The target raster object ID that will be used to color balance the other images. The balance method and color surface type, if applicable, will be derived from this image.This parameter is enabled when the balancing_method parameter is set to GLOBAL_FIT. | Long |
| refine_estimation(Optional) | Specifies whether the color balancing estimation for corresponding locations in the overlapped areas will be refined using image correlation. This parameter is helpful for the exact color difference correction but will increase the computation time. If you have a mosaic dataset composed of a large number of images, specify the NO_REFINE_ESTIMATION option to reduce the computation time.This parameter is enabled when the balancing_method parameter is set to GLOBAL_FIT.NO_REFINE_ESTIMATION—No refined estimation will be made. This is the default.REFINE_ESTIMATION—A refined estimation for color balance will be made | Boolean |
| reduce_shadow(Optional) | Specifies whether the negative influence of shadows on the color balance output will be reduced.This parameter is enabled when the balancing_method parameter is set to GLOBAL_FIT.NO_REDUCE_SHADOW—The influence of shadows will not be reduced. This is the default.REDUCE_SHADOW—The influence of shadows will be reduced. Use this option when the mosaic dataset has a lot of shadows. | Boolean |
| reduce_cloud(Optional) | Specifies whether the negative influence of clouds on the color balance output will be reduced. This parameter is active when the balancing_method parameter is set to DODGING or GLOBAL_FIT.NO_REDUCE_CLOUD—The influence of clouds will not be reduced. This is the default.REDUCE_CLOUD—The influence of clouds will be reduced. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ColorBalanceMosaicDataset(in_mosaic_dataset, {balancing_method}, {color_surface_type}, {target_raster}, {exclude_raster}, {stretch_type}, {gamma}, {block_field}, {in_DEM_raster}, {ZFactor}, {ZOffset}, {Geoid}, {solution_points}, {target_objectid}, {refine_estimation}, {reduce_shadow}, {reduce_cloud})
```

### Example 2

```python
import arcpy
arcpy.ColorBalanceMosaicDataset_management(
     "C:/workspace/CC.gdb/cc1", "DODGING", "SINGLE_COLOR", 
     "C:/workspace/Aerial.lyr",  "#", "STANDARD_DEVIATION", "3", "BLOCKNAME")
```

### Example 3

```python
import arcpy
arcpy.ColorBalanceMosaicDataset_management(
     "C:/workspace/CC.gdb/cc1", "DODGING", "SINGLE_COLOR", 
     "C:/workspace/Aerial.lyr",  "#", "STANDARD_DEVIATION", "3", "BLOCKNAME")
```

### Example 4

```python
#########*#########*##########*#########*#########*#########*#########*&&&&&&&&&&

# Color Correction Mosaic Dataset with target layer

import arcpy
arcpy.env.workspace = "C:/workspace"

mdname = "CC.gdb/cc1"
ccmethod = "DODGING"
dogesurface = "SINGLE_COLOR"
targetras = "C:/workspace/Aerial_photo.lyr"
excluderas = "#"
prestretch = "NONE"
gamma = "#"
blockfield = "#"

arcpy.ColorBalanceMosaicDataset_management(
     mdname, ccmethod, dogesurface, targetras, excluderas,
     prestretch, gamma, blockfield)
```

### Example 5

```python
#########*#########*##########*#########*#########*#########*#########*&&&&&&&&&&

# Color Correction Mosaic Dataset with target layer

import arcpy
arcpy.env.workspace = "C:/workspace"

mdname = "CC.gdb/cc1"
ccmethod = "DODGING"
dogesurface = "SINGLE_COLOR"
targetras = "C:/workspace/Aerial_photo.lyr"
excluderas = "#"
prestretch = "NONE"
gamma = "#"
blockfield = "#"

arcpy.ColorBalanceMosaicDataset_management(
     mdname, ccmethod, dogesurface, targetras, excluderas,
     prestretch, gamma, blockfield)
```

### Example 6

```python
# Color Correction Mosaic Dataset with block field

import arcpy
arcpy.env.workspace = "C:/workspace"

mdname = "CC.gdb/cc2"
ccmethod = "HISTOGRAM"
dogesurface = "#"
targetras = "#"
excluderas = "#"
prestretch = "NONE"
gamma = "#"
blockfield = "BLOCKNAME"

arcpy.ColorBalanceMosaicDataset_management(
     mdname, ccmethod, dogesurface, targetras, excluderas, 
     prestretch, gamma, blockfield)
```

### Example 7

```python
# Color Correction Mosaic Dataset with block field

import arcpy
arcpy.env.workspace = "C:/workspace"

mdname = "CC.gdb/cc2"
ccmethod = "HISTOGRAM"
dogesurface = "#"
targetras = "#"
excluderas = "#"
prestretch = "NONE"
gamma = "#"
blockfield = "BLOCKNAME"

arcpy.ColorBalanceMosaicDataset_management(
     mdname, ccmethod, dogesurface, targetras, excluderas, 
     prestretch, gamma, blockfield)
```

---

## Compact (Data Management)

## Summary

Compacts a file or mobile geodatabase, SQLite database, or Open Geospatial Consortium (OGC) GeoPackage file. Compacting rearranges data storage, often reducing the file's size and improving performance.

## Usage

- If data entry, deletion, or general editing is frequently performed, regularly compact the geodatabase, database, or file to ensure optimal performance.
- A database that is open in ArcGIS Pro for editing cannot be compacted. To compact the database, remove any layers with a source table or feature class in that database from the Contents pane.
- Learn more about compacting a mobile geodatabase

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The file or mobile geodatabase, SQLite database, or GeoPackage that will be compacted. | Workspace |
| in_workspace | The file or mobile geodatabase, SQLite database, or GeoPackage that will be compacted. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.Compact(in_workspace)
```

### Example 2

```python
import arcpy
arcpy.management.Compact("c:/landuse.gdb")
```

### Example 3

```python
import arcpy
arcpy.management.Compact("c:/landuse.gdb")
```

### Example 4

```python
# Name: Compact_Example.py
# Description: compact a file geodatabase

# Import the system modules
import arcpy

# Set local variables
gdbWorkspace = "C:/data/data.gdb"

arcpy.management.Compact(gdbWorkspace)
```

### Example 5

```python
# Name: Compact_Example.py
# Description: compact a file geodatabase

# Import the system modules
import arcpy

# Set local variables
gdbWorkspace = "C:/data/data.gdb"

arcpy.management.Compact(gdbWorkspace)
```

---

## Compare Replica Schema (Data Management)

## Summary

Generates an .xml file that describes schema differences between a replica geodatabase and the relative replica geodatabase.

## Usage

- Modifying the schema of a replica to match the schema of a relative replica is a separate process from data synchronization. Use the following tools for this purpose:Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.Import the changes using the Import Replica Schema tool.To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.
- Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.
- Import the changes using the Import Replica Schema tool.
- To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.
- The output replica schema changes file must be in XML format.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Compare to Replica Geodatabase | The replica geodatabase to which the replica schema will be compared. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| Relative Replica Schema File | The file that contains the relative replica schema that will be used for the comparison. | File |
| Output Replica Schema Changes File | The file that will contain a description of the schema differences. | File |
| in_geodatabase | The replica geodatabase to which the replica schema will be compared. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| in_source_file | The file that contains the relative replica schema that will be used for the comparison. | File |
| output_replica_schema_changes_file | The file that will contain a description of the schema differences. | File |

## Code Samples

### Example 1

```python
arcpy.management.CompareReplicaSchema(in_geodatabase, in_source_file, output_replica_schema_changes_file)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.CompareReplicaSchema("MySDEdata.sde", "RelativeReplicaSchema.xml", "SchemaComparison.xml")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.CompareReplicaSchema("MySDEdata.sde", "RelativeReplicaSchema.xml", "SchemaComparison.xml")
```

### Example 4

```python
# Description: Compare a replica schema (in an enterprise geodatabase 
#              workspace) to its relative replicas schema (in an .xml file).
#              The results of the comparison are created in an .xml file.
#              The relative replica's .xml schema file was created using the 
#              ExportReplicaSchema function.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
in_source_file = "RelativeReplicaSchema.xml"
output_schema_changes = "outputSchemaChanges.xml"

# Run CompareReplicaSchema
arcpy.management.CompareReplicaSchema(in_geodatabase, in_source_file, output_schema_changes)
```

### Example 5

```python
# Description: Compare a replica schema (in an enterprise geodatabase 
#              workspace) to its relative replicas schema (in an .xml file).
#              The results of the comparison are created in an .xml file.
#              The relative replica's .xml schema file was created using the 
#              ExportReplicaSchema function.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
in_source_file = "RelativeReplicaSchema.xml"
output_schema_changes = "outputSchemaChanges.xml"

# Run CompareReplicaSchema
arcpy.management.CompareReplicaSchema(in_geodatabase, in_source_file, output_schema_changes)
```

---

## Composite Bands (Data Management)

## Summary

Creates a single raster dataset from multiple bands.

## Usage

- This tool can also create a raster dataset containing subset of the original raster dataset bands. This is useful if you need to create a new raster dataset with a specific band combination and order.
- The order that the bands are listed in the Multi-value Input control box will determine the order of the bands in the output raster dataset.
- This tool can only output a square pixel size.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- The output raster dataset takes the cell size from the first raster band in the list.
- By default, the output raster dataset takes the extent and the spatial reference of the first raster band with a spatial reference in the list. You can change this by setting the output extent and output coordinate system in the Environment Settings.
- The following are some examples of why you would want to combine single raster datasets into multiband raster datasets:You may have received some satellite data where each band of data is contained in a single file—for example, band1.tif, band2.tif, and band3.tif. To render these raster datasets together to create a color composite, each band needs to be contained within a single raster dataset (for example, allbands.tif).You may have several raster datasets of the same area captured at various times. By displaying these raster datasets as a color composite, you can detect change in the area, such as urban growth or cut forests. To create this color composite, each raster dataset needs to be contained as individual bands within a single raster dataset.In some cases, the output of an analysis operation is a single-band raster dataset. To do further visual analysis, you may need to combine the outputs by rendering your data as a color composite.Combining bands into one raster dataset can help you organize many related single-band rasters.
- You may have received some satellite data where each band of data is contained in a single file—for example, band1.tif, band2.tif, and band3.tif. To render these raster datasets together to create a color composite, each band needs to be contained within a single raster dataset (for example, allbands.tif).
- You may have several raster datasets of the same area captured at various times. By displaying these raster datasets as a color composite, you can detect change in the area, such as urban growth or cut forests. To create this color composite, each raster dataset needs to be contained as individual bands within a single raster dataset.
- In some cases, the output of an analysis operation is a single-band raster dataset. To do further visual analysis, you may need to combine the outputs by rendering your data as a color composite.
- Combining bands into one raster dataset can help you organize many related single-band rasters.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rasters | The raster datasets that you want to use as the bands. | Mosaic Dataset ; Mosaic Layer ; Raster Dataset ; Raster Layer |
| Output Raster | The name, location and format for the raster dataset you are creating. Make sure that it can support the necessary bit-depth.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| in_rasters[in_rasters,...] | The raster datasets that you want to use as the bands. | Mosaic Dataset ; Mosaic Layer ; Raster Dataset ; Raster Layer |
| out_raster | The name, location and format for the raster dataset you are creating. Make sure that it can support the necessary bit-depth.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |

## Code Samples

### Example 1

```python
arcpy.management.CompositeBands(in_rasters, out_raster)
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.CompositeBands_management("band1.tif;band2.tif;band3.tif",
                                "compbands.tif")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.CompositeBands_management("band1.tif;band2.tif;band3.tif",
                                "compbands.tif")
```

### Example 4

```python
##====================================
##Composite Bands
##Usage: CompositeBands_management in_rasters;in_rasters... out_raster

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Compose multi types of single band raster datasets to a TIFF format raster dataset
arcpy.CompositeBands_management("band1.tif;comp.mdb/band2;comp.gdb/bands/Band_3","compbands.tif")
```

### Example 5

```python
##====================================
##Composite Bands
##Usage: CompositeBands_management in_rasters;in_rasters... out_raster

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Compose multi types of single band raster datasets to a TIFF format raster dataset
arcpy.CompositeBands_management("band1.tif;comp.mdb/band2;comp.gdb/bands/Band_3","compbands.tif")
```

---

## Compress File Geodatabase Data (Data Management)

## Summary

Compresses all the contents in a geodatabase, all the contents in a feature dataset, or an individual stand-alone feature class or table.

## Usage

- Once compressed, a feature class or table is read-only and cannot be edited. Compression is ideally suited to mature datasets that do not require further editing. However, if required, a compressed dataset can always be uncompressed to return it to its original, read-write format.
- When you compress a geodatabase, all feature classes and tables within it compress.
- When you compress a feature dataset, all its feature classes compress.
- When you specify a geodatabase as input, this tool compresses all vector feature classes and tables in the geodatabase. It does not compress raster catalogs or raster datasets. If it encounters these in the specified geodatabase, it skips over them. You can individually compress a raster catalog or raster dataset with this tool; however, it makes little sense since the data does not reduce in size. This support is provided strictly as a means to allow ArcPublisher to package to compressed and locked file geodatabase raster catalogs and datasets.
- You cannot individually compress or uncompress a feature class in a feature dataset to produce a mixed state where some feature classes are compressed and others are not. Compressed feature datasets allow you to add an uncompressed feature class through operations such as creating a new, empty feature class, copying and pasting, and importing. However, you cannot edit the uncompressed feature class if there are compressed feature classes in the same feature dataset. Once you've finished adding one or more uncompressed feature classes, you can recompress or uncompress the feature dataset so all its feature classes are either compressed or uncompressed.
- When you display compressed feature class records, they may not display in the same order as they did before you compressed the data. The records display in the order in which they are compressed and stored.
- When using lossless compression, floating-point values will be preserved, but compression will not be as effective. With non-lossless compression, floating-point values will be changed, but not below the limit of adequate precision. For example, state boundaries do not usually need to be measured to nanometer precision. Non-lossless compression is up to 20 percent smaller than lossless.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input file geodatabase data | The geodatabase, feature dataset, feature class, or table to compress. | Feature Dataset; Geometric Network; Raster Layer; Table View; Workspace |
| Lossless compression | Indicates whether lossless compression will be used.Unchecked—Lossless compression will not be used. Checked—Lossless compression will be used. This is the default.Note: For pre-10.0 file geodatabases, lossless compression is not supported. This option cannot be changed and is unchecked and disabled. | Boolean |
| in_data | The geodatabase, feature dataset, feature class, or table to compress. | Feature Dataset; Geometric Network; Raster Layer; Table View; Workspace |
| lossless | Indicates whether lossless compression will be used.Lossless compression—Lossless compression will be used. This is the default.Non-lossless compression—Lossless compression will not be used.This parameter is ignored for pre-10.0 file geodatabases. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CompressFileGeodatabaseData(in_data, lossless)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CompressFileGeodatabaseData("london.gdb", "Lossless compression")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CompressFileGeodatabaseData("london.gdb", "Lossless compression")
```

### Example 4

```python
# Name: CompressFileGeodatabaseData.py
# Description: Use the CompressFileGeodatabaseData tool to compress a geodatabase

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"
lossless = "Lossless compression"

# Process: Compress the data
arcpy.management.CompressFileGeodatabaseData(geodatabase, lossless)
```

### Example 5

```python
# Name: CompressFileGeodatabaseData.py
# Description: Use the CompressFileGeodatabaseData tool to compress a geodatabase

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"
lossless = "Lossless compression"

# Process: Compress the data
arcpy.management.CompressFileGeodatabaseData(geodatabase, lossless)
```

---

## Compress (Data Management)

## Summary

Compresses an enterprise geodatabase by removing states not referenced by a version and redundant rows.

## Usage

- To improve geodatabase performance, compress the geodatabase periodically. A compressed geodatabase is more efficient.
- Once a geodatabase is compressed, deleted records cannot be recovered.
- Compressing a geodatabase not only reduces space requirements but can also reduce overall retrieval times.
- While the Compress tool is running, you can still view data in the geodatabase.
- Only the geodatabase administrator can perform compress.
- This tool is not applicable to enterprise geodatabases that do not use versioning.
- This tool is not applicable to branch versioning.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection file that connects to the enterprise geodatabase to be compressed. Connect as the geodatabase administrator. | Workspace |
| in_workspace | The database connection file that connects to the enterprise geodatabase to be compressed. Connect as the geodatabase administrator. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.Compress(in_workspace)
```

### Example 2

```python
import arcpy
arcpy.Compress_management("c:/Connection to brockville.sde")
```

### Example 3

```python
import arcpy
arcpy.Compress_management("c:/Connection to brockville.sde")
```

---

## Compute Block Adjustment (Data Management)

## Summary

Computes the adjustments to the mosaic dataset. This tool will create a solution table that can be used to apply the actual adjustments.

## Usage

- Use the output control points from the Compute Tie Points tool as the input control points for this tool.
- The output solution table from this tool will be used in the Apply Block Adjustment tool.
- The tool requires the ArcGIS Desktop Advanced license when the Transformation Type value is set to RPC or Frame.
- Many Adjustment Options parameter options are available to optimize the block adjustment solution when the Transformation Type parameter is specified as Frame. See the available settings for the Adjustment Options parameter.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset that will be adjusted. | Mosaic Layer; Mosaic Dataset |
| Input Control Points | The control point table that includes tie points and ground control points.This feature class is usually the output from the Compute Tie Points tool. | Feature Layer |
| Transformation Type | Specifies the type of transformation that will be used when adjusting the mosaic dataset. Zero-order polynomial—A zero-order polynomial will be used in the block adjustment computation. This is commonly used when the data is in flat area. First-order polynomial—A first-order polynomial (affine) will be used in the block adjustment computation. This is the default.Rational Polynomial Coefficients—The rational polynomial coefficients (RPCs) will be used for the transformation. This is used for satellite imagery that contains RPC information in the metadata. This option requires the ArcGIS Desktop Advanced license.Frame camera model—The Frame camera model will be used for the transformation. This is used for aerial imagery that contains the frame camera information in the metadata. This option requires the ArcGIS Desktop Advanced license. | String |
| Output Solution Table | The output solution table containing the adjustments. | Table |
| Output Solution Points (Optional) | The output solution points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| Maximum Residual (Optional) | A threshold that is used in block adjustment computation; points with residuals exceeding the threshold will not be used. This parameter applies when the transformation type is Zero-order polynomial, First-order polynomial, or Frame camera model. If the transformation type is Rational Polynomial Coefficients, the proper threshold for eliminating invalid points will be automatically determined.When the transformation type is Zero-order polynomial or First-order polynomial, the units for this parameter will be map units, and the default value will be 2.When the transformation type is Frame camera model, the units for this parameter will be pixels, and the default value will be 5. | Double |
| Adjustment Options (Optional) | Additional options that will be used to fine-tune the adjustment computation.Note:To set an option in the Geoprocessing pane, type the keyword and the corresponding value in the list box.MinResidual—The minimum residual value, which is the lower threshold value, will be used. When the transformation type is POLYORDER0 or POLYORDER1, the units will be map units and the default minimum residual value will be 0. The minimum residual value and the maximum_residual_value parameter value are used in detecting and removing points that generate large errors from the block adjustment computation.MaxResidualFactor—The maximum residual factor will be used to generate the maximum (upper threshold) residual value if the maximum_residual-value parameter is not defined. In this case, MaxResidualFactor * RMS will be used to calculate the upper threshold value.The minimum residual value and the maximum_residual_factor parameter value are used in detecting and removing points that generate large errors from block adjustment computation.Additional options for the adjustment engine are listed below when Frame is specified for the Transformation Type parameter. The specifications of many of the options are supplied by the data provider.The options include the following:CalibrateF—Calibrate the sensor's focal length for use in the block adjustment. Assign a value of 1 for focal length calibration or 0 for no calibration. The default is 0.CalibratePP—Calibrate the principle point in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.CalibrateP—Calibrate for radial distortion parameters in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.CalibrateK—Calibrate for tangential distortion parameters in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.Note:Calibration parameters, such as perspective data, are usually provided for most professional digital aerial cameras, such as UltraCam or DMC. The calibration options can be 0 if camera calibration parameters are prepared in the camera table.APrioriAccuracyX—Include the accuracy of the x-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveX. If the value is set to 0, the x-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyY—Include the accuracy of the y-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveY. If the value is set to 0, the y-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyZ—Include the accuracy of the z-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveZ. If the value is set to 0, the z-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyXY—Include the accuracy of the planar coordinate provided by the metadata. The units must match PerspectiveX. If the value is set to 0, planar coordinates (x and y) of the image location are not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyXYZ—Include the accuracy of image location provided by the metadata. The units must match PerspectiveX. If the value is set to 0, the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyOmega—Include the accuracy of the Omega angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyPhi—Include the accuracy of the Phi angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyOmegaPhi—Include the accuracy of the Omega or Phi angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyKappa—Include the accuracy of the Kappa angle provided by the airborne Position Orientation System. The units are in decimal degrees.ComputeAntennaOffset—Compute the offset between GNSS antenna center and camera projection center in adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.ComputeShift—Compute the GNSS signal shift in flights in bundle adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.ComputeImagePosteriorStd—Compute the posterior standard deviation of image location and orientation after adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 1.ComputeSolutionPointPosteriorStd—Compute the posterior standard deviation of solution points after adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.rigCamera—Allow processing of a multiple camera rig in the block adjustment. Assign a value of 1 to use the rigCamera module or a value of 0 to not use the rigCamera module. If a value of 1 is assigned, the relationship of multiple cameras in the adjustment will be computed. The default is 0. | Value Table |
| Image Location Accuracy (Optional) | Specifies the geometric accuracy level of the images.This parameter is only active if the Transformation Type parameter is specified as Rational Polynomial Coefficients.If low accuracy is specified, the control points will first be improved by an initial triangulation; then they will be used in the block adjustment calculation. The medium and high accuracy options do not require additional estimation processing.High accuracy—The accuracy will be 30 meters or less.Medium accuracy—The accuracy will be between 31 meters and 100 meters. This is the default.Low accuracy—The accuracy will be more than 100 meters.Very High accuracy—The imagery was collected with a high-accuracy, differential GPS, such as RTK or PPK. This option will keep image locations fixed during block adjustment. | String |
| Output Adjustment Quality Table (Optional) | An output table used to store adjustment quality information. This parameter is only active if the Transformation Type parameter is specified as Rational Polynomial Coefficients. | Table |
| Refine by DEM (Optional) | An input DEM from which elevations will be sampled as ground control points for refining the geometric accuracy of the image network in the adjustment.This parameter is only active when the Transformation Type parameter is specified as Frame camera model. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| Elevation Accuracy of DEM (Optional) | The elevation accuracy of the input DEM. The accuracy value will be used as a weight for the sampled ground control points in the adjustment.This parameter is only active when the Transformation Type parameter is specified as Frame camera model. | Double |
| in_mosaic_dataset | The input mosaic dataset that will be adjusted. | Mosaic Layer; Mosaic Dataset |
| in_control_points | The control point table that includes tie points and ground control points.This feature class is usually the output from the Compute Tie Points tool. | Feature Layer |
| transformation_type | Specifies the type of transformation that will be used when adjusting the mosaic dataset. POLYORDER0—A zero-order polynomial will be used in the block adjustment computation. This is commonly used when the data is in flat area. POLYORDER1—A first-order polynomial (affine) will be used in the block adjustment computation. This is the default.RPC—The rational polynomial coefficients (RPCs) will be used for the transformation. This is used for satellite imagery that contains RPC information in the metadata. This option requires the ArcGIS Desktop Advanced license.Frame—The Frame camera model will be used for the transformation. This is used for aerial imagery that contains the frame camera information in the metadata. This option requires the ArcGIS Desktop Advanced license. | String |
| out_solution_table | The output solution table containing the adjustments. | Table |
| out_solution_point_table(Optional) | The output solution points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| maximum_residual_value(Optional) | A threshold that is used in block adjustment computation; points with residuals exceeding the threshold will not be used. This parameter applies when the transformation type is POLYORDER0, POLYORDER1, or Frame. If the transformation type is RPC, the proper threshold for eliminating invalid points will be automatically determined.When the transformation type is POLYORDER0 or POLYORDER1, the units for this parameter will be map units, and the default value will be 2.When the transformation type is Frame, the units for this parameter will be pixels, and the default value will be 5. | Double |
| adjustment_options[[name, value],...](Optional) | Additional options that will be used to fine-tune the adjustment computation.Note:To set an option in the Geoprocessing pane, type the keyword and the corresponding value in the list box.MinResidual—The minimum residual value, which is the lower threshold value, will be used. When the transformation type is POLYORDER0 or POLYORDER1, the units will be map units and the default minimum residual value will be 0. The minimum residual value and the maximum_residual_value parameter value are used in detecting and removing points that generate large errors from the block adjustment computation.MaxResidualFactor—The maximum residual factor will be used to generate the maximum (upper threshold) residual value if the maximum_residual-value parameter is not defined. In this case, MaxResidualFactor * RMS will be used to calculate the upper threshold value.The minimum residual value and the maximum_residual_factor parameter value are used in detecting and removing points that generate large errors from block adjustment computation.Additional options for the adjustment engine are listed below when Frame is specified for the Transformation Type parameter. The specifications of many of the options are supplied by the data provider.The options include the following:CalibrateF—Calibrate the sensor's focal length for use in the block adjustment. Assign a value of 1 for focal length calibration or 0 for no calibration. The default is 0.CalibratePP—Calibrate the principle point in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.CalibrateP—Calibrate for radial distortion parameters in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.CalibrateK—Calibrate for tangential distortion parameters in the block adjustment. Assign a value of 1 for calibration or 0 for no calibration. The default is 0.Note:Calibration parameters, such as perspective data, are usually provided for most professional digital aerial cameras, such as UltraCam or DMC. The calibration options can be 0 if camera calibration parameters are prepared in the camera table.APrioriAccuracyX—Include the accuracy of the x-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveX. If the value is set to 0, the x-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyY—Include the accuracy of the y-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveY. If the value is set to 0, the y-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyZ—Include the accuracy of the z-coordinate provided by the airborne Position Orientation System. The units must match PerspectiveZ. If the value is set to 0, the z-coordinate of the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyXY—Include the accuracy of the planar coordinate provided by the metadata. The units must match PerspectiveX. If the value is set to 0, planar coordinates (x and y) of the image location are not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyXYZ—Include the accuracy of image location provided by the metadata. The units must match PerspectiveX. If the value is set to 0, the image location is not adjusted in adjustment. This is not recommended for most UAV data.APrioriAccuracyOmega—Include the accuracy of the Omega angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyPhi—Include the accuracy of the Phi angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyOmegaPhi—Include the accuracy of the Omega or Phi angle provided by the airborne Position Orientation System. The units are in decimal degrees.APrioriAccuracyKappa—Include the accuracy of the Kappa angle provided by the airborne Position Orientation System. The units are in decimal degrees.ComputeAntennaOffset—Compute the offset between GNSS antenna center and camera projection center in adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.ComputeShift—Compute the GNSS signal shift in flights in bundle adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.ComputeImagePosteriorStd—Compute the posterior standard deviation of image location and orientation after adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 1.ComputeSolutionPointPosteriorStd—Compute the posterior standard deviation of solution points after adjustment. Assign a value of 1 to compute or 0 for no computation. The default is 0.rigCamera—Allow processing of a multiple camera rig in the block adjustment. Assign a value of 1 to use the rigCamera module or a value of 0 to not use the rigCamera module. If a value of 1 is assigned, the relationship of multiple cameras in the adjustment will be computed. The default is 0. | Value Table |
| location_accuracy(Optional) | Specifies the geometric accuracy level of the images.This parameter is only enabled if the transformation_type parameter is specified as RPC.HIGH—The accuracy will be 30 meters or less.MEDIUM—The accuracy will be between 31 meters and 100 meters.LOW—The accuracy will be more than 100 meters.VERY_HIGH—The imagery was collected with a high-accuracy, differential GPS, such as RTK or PPK. This option will keep image locations fixed during block adjustment.If LOW is specified, the control points will first be improved by an initial triangulation; then they will be used in the block adjustment calculation. The medium and high accuracy options do not require additional estimation processing. | String |
| out_quality_table(Optional) | An output table used to store adjustment quality information. This parameter is only enabled if the transformation_type parameter is specified as RPC. | Table |
| DEM(Optional) | An input DEM from which elevations will be sampled as ground control points for refining the geometric accuracy of the image network in the adjustment.This parameter is only enabled when the transformation_type parameter is specified as Frame. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| elevation_accuracy(Optional) | The elevation accuracy of the input DEM. The accuracy value will be used as a weight for the sampled ground control points in the adjustment.This parameter is only enabled when the transformation_type parameter is specified as Frame. | Double |

## Code Samples

### Example 1

```python
arcpy.management.ComputeBlockAdjustment(in_mosaic_dataset, in_control_points, transformation_type, out_solution_table, {out_solution_point_table}, {maximum_residual_value}, {adjustment_options}, {location_accuracy}, {out_quality_table}, {DEM}, {elevation_accuracy})
```

### Example 2

```python
import arcpy
arcpy.ComputeBlockAdjustment_management(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePoints",
     "POLYORDER1", "c:/BD/BD.gdb/redQB_solution")
```

### Example 3

```python
import arcpy
arcpy.ComputeBlockAdjustment_management(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePoints",
     "POLYORDER1", "c:/BD/BD.gdb/redQB_solution")
```

### Example 4

```python
#compute block adjustment, case 2

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute block adjustment
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_solutionTable = "BD.gdb/redlandsQB_solution"

arcpy.ComputeBlockAdjustment_management(mdName, in_controlPoint, 
     "POLYORDER1", out_solutionTable)
```

### Example 5

```python
#compute block adjustment, case 2

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute block adjustment
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_solutionTable = "BD.gdb/redlandsQB_solution"

arcpy.ComputeBlockAdjustment_management(mdName, in_controlPoint, 
     "POLYORDER1", out_solutionTable)
```

### Example 6

```python
#compute block adjustment, case 3

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute block adjustment specifying an output point table and 
#an setting an adjustment option
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_solutionTable = "BD.gdb/redlandsQB_solution"
out_solutionPoint = "BD.gdb/redlandsQB_solutionPoint"
engineOption = "_BAI c:/workspace/bai.txt; _BAO c:/workspace/bao.txt"

arcpy.ComputeBlockAdjustment_management(mdName, in_controlPoint, 
     "POLYORDER1", out_solutionTable, out_solutionPoint,"0.5", 
     engineOption)
```

### Example 7

```python
#compute block adjustment, case 3

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute block adjustment specifying an output point table and 
#an setting an adjustment option
mdName = "BD.gdb/redlandsQB"
in_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_solutionTable = "BD.gdb/redlandsQB_solution"
out_solutionPoint = "BD.gdb/redlandsQB_solutionPoint"
engineOption = "_BAI c:/workspace/bai.txt; _BAO c:/workspace/bao.txt"

arcpy.ComputeBlockAdjustment_management(mdName, in_controlPoint, 
     "POLYORDER1", out_solutionTable, out_solutionPoint,"0.5", 
     engineOption)
```

---

## Compute Camera Model (Data Management)

## Summary

Estimates the exterior camera model and interior camera model from the EXIF header of the raw image and refines the camera models. The model is then applied to the mosaic dataset with an option to use a tool-generated, high-resolution digital surface model (DSM) to achieve better orthorectification.

## Usage

- A typical workflow might include running the Compute Camera Model tool twice: once with the Estimate Camera Model parameter checked and specifying an Output Control Point Table parameter value, and a second time with the Refine Camera Model parameter checked and using the output from the first run as the Input Tie Point Table parameter value. The goal of this workflow is to first make a quick estimate of the camera model and then create a more accurate camera model.
- When the GPS Location Accuracy parameter is set to Very high GPS accuracy, the orientation parameters of the imagery will be adjusted and the GPS measurements will remain fixed. Additionally, ground control points (GCPs) are not required when this option is specified. GCPs will be marked as check points in the adjustment.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The mosaic dataset on which the camera model will be built and calculated. | Mosaic Dataset; Mosaic Layer |
| Output DSM(Optional) | A DSM raster dataset generated from the adjusted images in the mosaic dataset. If Apply Adjustment is checked, this DSM will replace the DEM in the geometric function to achieve better orthorectification. | Raster Dataset |
| GPS Location Accuracy(Optional) | Specifies the accuracy level of the input images. The tool will search for images in the neighborhood to compute matching points and automatically apply an adjustment strategy based on the accuracy level.High GPS accuracy— The GPS accuracy is 0 to 10 meters, and the tool uses a maximum of 4 by 3 images.Medium GPS accuracy—The GPS accuracy is 10 to 20 meters, and the tool uses a maximum of 4 by 6 images.Low GPS accuracy—The GPS accuracy is 20 to 50 meters, and the tool uses a maximum of 4 by 12 images.Very low GPS accuracy—The GPS accuracy is more than 50 meters, and the tool uses a maximum of 4 by 20 images.Very high GPS accuracy—Imagery was collected with a high-accuracy, differential GPS, such as RTK or PPK. This option will hold image locations fixed during block adjustment. | String |
| Estimate Camera Model(Optional) | Specifies whether the camera model will be estimated by computing the adjustment based on eight times the mosaic dataset's source resolution. Computing the adjustment at this level will be faster but less accurate.Checked—The camera model will be estimated. This is the default.Unchecked—The camera model will not be estimated. | Boolean |
| Refine Camera Model(Optional) | Specifies whether the camera model will be refined by computing the adjustment at the mosaic dataset resolution. Computing the adjustment at this level will provide the most accurate result.Checked—The camera model will be refined by computing the adjustment at the source resolution. This is the default.Unchecked—The camera model will not be refined. This option will be faster, so it is a good option when the computation does not need to be performed at the source resolution. | Boolean |
| Apply Adjustment(Optional) | Specifies whether the calculated adjustment will be applied to the input mosaic dataset.Checked—The calculated adjustment will be applied to the input mosaic dataset. Although not required, it is recommended that you specify this option. This is the default.Unchecked—The calculated adjustment will not be applied to the input mosaic dataset. | Boolean |
| Maximum Residual(Optional) | The maximum residual value allowed to keep a computed control point as a valid control point. The default is 5. | Double |
| Initial Tie Point Resolution(Optional) | The resolution factor at which tie points will be generated when estimating the camera model. The default value is 8, which means eight times the source pixel resolution.For images with only minor differentiation of features, such as agriculture fields, a lower value such as 2 can be used. | Double |
| Output Control Point Table(Optional) | The optional control points feature class. | Feature Class |
| Output Solution Table(Optional) | The optional adjustment solution table. The solution table contains the root mean square (RMS) of the adjustment error and solution matrix. | Table |
| Output Solution Point Table(Optional) | The optional solution point feature class. The solution points are the final controls points used to generate the adjustment solution. | Feature Class |
| Output Flight Path(Optional) | The optional flight path line feature class. | Feature Class |
| Maximum Area Overlap(Optional) | The percentage of overlap between two images to consider them duplicates.For example, if the value is 0.9, it means if an image is 90 percent covered by another image, it will be considered a duplicate and removed. | Double |
| Minimum Control Point Coverage(Optional) | The percentage indicating the control point's coverage on an image. If the coverage is less than the minimum percentage, the image will be unresolved and removed. The default is 0. | Double |
| Remove Off-Strip Images(Optional) | Specifies whether images will be automatically removed if they are too far from the flight strip.Unchecked—Images will not be removed. This is the default.Checked—Images that are too far away from the flight strip will be removed. | Boolean |
| Input Tie Point Table(Optional) | The tie point table that will be used to compute the camera model. If no tie point table is provided, the tool will compute the tie points and estimate the camera model. | Feature Class |
| Additional Options(Optional) | Additional options for the adjustment engine. The specifications of many of the options are supplied by the data provider.The options include the following:CalibrateF—The sensor's focal length will be calibrated for use in the block adjustment. Assign a value of 1 for focal length calibration, or 0 to not calibrate. The default is 1.CalibratePP—The principle point in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.CalibrateP—Radial distortion parameters in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.CalibrateK—Tangential distortion parameters in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.EstimateOPK—The Omega, Phi, and Kappa angles will be calibrated to define the rotation between the image coordinate system and the projected coordinate system. Assign a value of 0 to use orientation angles (roll, pitch, and yaw) from UAV metadata as attitude initials in the block adjustment. Use a value of 1 to estimate orientation angles, and use estimated orientation angles as attitude initials in the block adjustment. The default is 1.Note:For most DJI and Skydio cameras, a value of 0 is recommended.APrioriAccuracyX—The accuracy of the x-coordinate provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyY—The accuracy of the y-coordinate provided by the metadata. The units must match PerspectiveY. This option is not recommended for most UAV data.APrioriAccuracyZ—The accuracy of the z-coordinate provided by the metadata. The units must match PerspectiveZ. This option is not recommended for most UAV data.APrioriAccuracyXY—The accuracy of the planar coordinate provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyXYZ—The accuracy of image location provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyOmega—The accuracy of the Omega angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyPhi—The accuracy of the Phi angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyOmegaPhi—The accuracy of the Omega or Phi angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyKappa—The accuracy of the Kappa angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.ComputeImagePosteriorStd—The posterior standard deviation of image location and orientation after adjustment will be computed. Assign a value of 1 to compute, or 0 to not compute. The default is 1.ComputeSolutionPointPosteriorStd—The posterior standard deviation of solution points after adjustment will be computed. Assign a value of 1 to compute, or 0 to not compute. The default is 0. | Value Table |
| in_mosaic_dataset | The mosaic dataset on which the camera model will be built and calculated. | Mosaic Dataset; Mosaic Layer |
| out_dsm(Optional) | A DSM raster dataset generated from the adjusted images in the mosaic dataset. If apply_adjustment is set to APPLY, this DSM will replace the DEM in the geometric function to achieve better orthorectification. | Raster Dataset |
| gps_accuracy(Optional) | Specifies the accuracy level of the input images. The tool will search for images in the neighborhood to compute matching points and automatically apply an adjustment strategy based on the accuracy level.HIGH— The GPS accuracy is 0 to 10 meters, and the tool uses a maximum of 4 by 3 images.MEDIUM—The GPS accuracy is 10 to 20 meters, and the tool uses a maximum of 4 by 6 images.LOW—The GPS accuracy is 20 to 50 meters, and the tool uses a maximum of 4 by 12 images.VERY_LOW—The GPS accuracy is more than 50 meters, and the tool uses a maximum of 4 by 20 images.VERY_HIGH—Imagery was collected with a high-accuracy, differential GPS, such as RTK or PPK. This option will hold image locations fixed during block adjustment. | String |
| estimate(Optional) | Specifies whether the camera model will be estimated by computing the adjustment based on eight times the mosaic dataset's source resolution. Computing the adjustment at this level will be faster but less accurate.ESTIMATE—The camera model will be estimated. This is the default.NO_ESTIMATE—The camera model will not be estimated. | Boolean |
| refine(Optional) | Specifies whether the camera model will be refined by computing the adjustment at the mosaic dataset resolution. Computing the adjustment at this level will provide the most accurate result.REFINE—The camera model will be refined by computing the adjustment at the source resolution. This is the default.NO_REFINE—The camera model will not be refined. This option will be faster, so it is a good option when the computation does not need to be performed at the source resolution. | Boolean |
| apply_adjustment(Optional) | Specifies whether the calculated adjustment will be applied to the input mosaic dataset.APPLY—The calculated adjustment will be applied to the input mosaic dataset. Although not required, it is recommended that you specify this option. This is the default.NO_APPLY—The calculated adjustment will not be applied to the input mosaic dataset. | Boolean |
| maximum_residual(Optional) | The maximum residual value allowed to keep a computed control point as a valid control point. The default is 5. | Double |
| initial_tiepoint_resolution(Optional) | The resolution factor at which tie points will be generated when estimating the camera model. The default value is 8, which means eight times the source pixel resolution.For images with only minor differentiation of features, such as agriculture fields, a lower value such as 2 can be used. | Double |
| out_control_points(Optional) | The optional control points feature class. | Feature Class |
| out_solution_table(Optional) | The optional adjustment solution table. The solution table contains the root mean square (RMS) of the adjustment error and solution matrix. | Table |
| out_solution_point_table(Optional) | The optional solution point feature class. The solution points are the final controls points used to generate the adjustment solution. | Feature Class |
| out_flight_path(Optional) | The optional flight path line feature class. | Feature Class |
| maximum_overlap(Optional) | The percentage of overlap between two images to consider them duplicates.For example, if the value is 0.9, it means if an image is 90 percent covered by another image, it will be considered a duplicate and removed. | Double |
| minimum_coverage(Optional) | The percentage indicating the control point's coverage on an image. If the coverage is less than the minimum percentage, the image will be unresolved and removed. The default is 0. | Double |
| remove(Optional) | Specifies whether images will be automatically removed if they are too far from the flight strip.NO_REMOVE—Images will not be removed. This is the default.REMOVE—Images that are too far away from the flight strip will be removed. | Boolean |
| in_control_points(Optional) | The tie point table that will be used to compute the camera model. If no tie point table is provided, the tool will compute the tie points and estimate the camera model. | Feature Class |
| options[options,...](Optional) | Additional options for the adjustment engine. The specifications of many of the options are supplied by the data provider.The options include the following:CalibrateF—The sensor's focal length will be calibrated for use in the block adjustment. Assign a value of 1 for focal length calibration, or 0 to not calibrate. The default is 1.CalibratePP—The principle point in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.CalibrateP—Radial distortion parameters in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.CalibrateK—Tangential distortion parameters in the block adjustment will be calibrated. Assign a value of 1 for calibration, or 0 to not calibrate. The default is 1.EstimateOPK—The Omega, Phi, and Kappa angles will be calibrated to define the rotation between the image coordinate system and the projected coordinate system. Assign a value of 0 to use orientation angles (roll, pitch, and yaw) from UAV metadata as attitude initials in the block adjustment. Use a value of 1 to estimate orientation angles, and use estimated orientation angles as attitude initials in the block adjustment. The default is 1.Note:For most DJI and Skydio cameras, a value of 0 is recommended.APrioriAccuracyX—The accuracy of the x-coordinate provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyY—The accuracy of the y-coordinate provided by the metadata. The units must match PerspectiveY. This option is not recommended for most UAV data.APrioriAccuracyZ—The accuracy of the z-coordinate provided by the metadata. The units must match PerspectiveZ. This option is not recommended for most UAV data.APrioriAccuracyXY—The accuracy of the planar coordinate provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyXYZ—The accuracy of image location provided by the metadata. The units must match PerspectiveX. This option is not recommended for most UAV data.APrioriAccuracyOmega—The accuracy of the Omega angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyPhi—The accuracy of the Phi angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyOmegaPhi—The accuracy of the Omega or Phi angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.APrioriAccuracyKappa—The accuracy of the Kappa angle provided by the airborne Position Orientation System (POS). The units are in decimal degrees.ComputeImagePosteriorStd—The posterior standard deviation of image location and orientation after adjustment will be computed. Assign a value of 1 to compute, or 0 to not compute. The default is 1.ComputeSolutionPointPosteriorStd—The posterior standard deviation of solution points after adjustment will be computed. Assign a value of 1 to compute, or 0 to not compute. The default is 0. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.ComputeCameraModel(in_mosaic_dataset, {out_dsm}, {gps_accuracy}, {estimate}, {refine}, {apply_adjustment}, {maximum_residual}, {initial_tiepoint_resolution}, {out_control_points}, {out_solution_table}, {out_solution_point_table}, {out_flight_path}, {maximum_overlap}, {minimum_coverage}, {remove}, {in_control_points}, {options})
```

### Example 2

```python
import arcpy 

arcpy.ComputeCameraModel_management("c:\data\fgdb.gdb\md", "output_DSM.tif", 
                                    "HIGH", "ESTIMATE", "REFINE", "APPLY", "5")
```

### Example 3

```python
import arcpy 

arcpy.ComputeCameraModel_management("c:\data\fgdb.gdb\md", "output_DSM.tif", 
                                    "HIGH", "ESTIMATE", "REFINE", "APPLY", "5")
```

---

## Compute Control Points (Data Management)

## Summary

Creates the control points between the mosaic dataset and the reference image. The control points can then be used in conjunction with tie points to compute the adjustments for the mosaic dataset.

## Usage

- For accurate control point results, use the High similarity option for the Similarity parameter.
- The control points can be combined with tie points, using the Append Control Points tool.
- The control points and tie points are then used within the Compute Block Adjustment tool.
- If you have a mosaic dataset with many items, use caution when specifying the Output Image Features parameter, since your result may take a long time to process.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset that will be used to create control points. | Mosaic Dataset; Mosaic Layer |
| Input Reference Images | The reference images that will be used to create control points for your mosaic dataset. If you have multiple images, create a mosaic dataset from the images and use the mosaic dataset as the reference. | Raster Layer; Raster Dataset; Image Service; Map Server; WMS Map; Mosaic Layer; Internet Tiled Layer; Map Server Layer |
| Output Control Points | The output control point table. This table will contain the control points that were created. | Feature Class |
| Similarity (Optional) | Specifies the similarity level that will be used for matching tie points.Low similarity—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.Medium similarity—The similarity criteria for the matching points will be medium.High similarity—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |
| Output Image Features (Optional) | The output image feature points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| Point Density | Specifies the number of tie points to be created. Low point density—The density of points will be low, creating the fewest number of tie points.Medium point density—The density of points will be medium, creating a moderate number of points.High point density—The density of points will be high, creating the highest number of points. | String |
| Point Distribution | Specifies whether the points will have regular or random distribution. Random point distribution—Points will be generated randomly. Randomly generated points are better for overlapping areas with irregular shapes.Regular point distribution—Points will be generated based on a fixed pattern. Points based on a fixed pattern use the point density to determine how frequently to create points. | String |
| Area of Interest | Limit the area in which tie points are generated to only this polygon feature class. | Feature Layer |
| Image Location Accuracy (Optional) | Specifies the keyword that describes the accuracy of the imagery.Low image location accuracy—Images have a large shift and a large rotation (> 5 degrees).The SIFT algorithm will be used in the point-matching computation.Medium image location accuracy—Images have a medium shift and a small rotation (<5 degrees).The Harris algorithm will be used in the point-matching computation.High image location accuracy—Images have a small shift and a small rotation.The Harris algorithm will be used in the point-matching computation. | String |
| in_mosaic_dataset | The input mosaic dataset that will be used to create control points. | Mosaic Dataset; Mosaic Layer |
| in_reference_images | The reference images that will be used to create control points for your mosaic dataset. If you have multiple images, create a mosaic dataset from the images and use the mosaic dataset as the reference. | Raster Layer; Raster Dataset; Image Service; Map Server; WMS Map; Mosaic Layer; Internet Tiled Layer; Map Server Layer |
| out_control_points | The output control point table. This table will contain the control points that were created. | Feature Class |
| similarity(Optional) | Specifies the similarity level that will be used for matching tie points.LOW—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.MEDIUM—The similarity criteria for the matching points will be medium.HIGH—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |
| out_image_feature_points(Optional) | The output image feature points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| density | Specifies the number of tie points to be created. LOW—The density of points will be low, creating the fewest number of tie points.MEDIUM—The density of points will be medium, creating a moderate number of points.HIGH—The density of points will be high, creating the highest number of points. | String |
| distribution | Specifies whether the points will have regular or random distribution. RANDOM—Points will be generated randomly. Randomly generated points are better for overlapping areas with irregular shapes.REGULAR—Points will be generated based on a fixed pattern. Points based on a fixed pattern use the point density to determine how frequently to create points. | String |
| area_of_interest | Limit the area in which tie points are generated to only this polygon feature class. | Feature Layer |
| location_accuracy(Optional) | Specifies the keyword that describes the accuracy of the imagery.LOW—Images have a large shift and a large rotation (> 5 degrees).The SIFT algorithm will be used in the point-matching computation.MEDIUM—Images have a medium shift and a small rotation (<5 degrees).The Harris algorithm will be used in the point-matching computation.HIGH—Images have a small shift and a small rotation.The Harris algorithm will be used in the point-matching computation. | String |

## Code Samples

### Example 1

```python
arcpy.management.ComputeControlPoints(in_mosaic_dataset, in_reference_images, out_control_points, {similarity}, {out_image_feature_points}, density, distribution, area_of_interest, {location_accuracy})
```

### Example 2

```python
import arcpy
arcpy.ComputeControlPoints_management("c:/block/BD.gdb/redQB", 
     "c:/block/BD.gdb/redQB_tiePoints", "HIGH",
     "c:/block/BD.gdb/redQB_mask", "c:/block/BD.gdb/redQB_imgFeatures")
```

### Example 3

```python
import arcpy
arcpy.ComputeControlPoints_management("c:/block/BD.gdb/redQB", 
     "c:/block/BD.gdb/redQB_tiePoints", "HIGH",
     "c:/block/BD.gdb/redQB_mask", "c:/block/BD.gdb/redQB_imgFeatures")
```

### Example 4

```python
#compute control points

import arcpy
arcpy.env.workspace = "c:/workspace"

#compute control points using a mask 
mdName = "BD.gdb/redlandsQB"
in_mask = "BD.gdb/redlandsQB_mask"
out_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_imageFeature = "BD.gdb/redlandsQB_imageFeatures"

arcpy.ComputeControlPoints_management(mdName, out_controlPoint, 
     "HIGH", in_mask, out_imageFeature)
```

### Example 5

```python
#compute control points

import arcpy
arcpy.env.workspace = "c:/workspace"

#compute control points using a mask 
mdName = "BD.gdb/redlandsQB"
in_mask = "BD.gdb/redlandsQB_mask"
out_controlPoint = "BD.gdb/redlandsQB_tiePoints"
out_imageFeature = "BD.gdb/redlandsQB_imageFeatures"

arcpy.ComputeControlPoints_management(mdName, out_controlPoint, 
     "HIGH", in_mask, out_imageFeature)
```

---

## Compute Depth Map (Data Management)

## Summary

Computes a more accurate CenterZ field value based on the depth map for each image comprising a mosaic dataset. Control points and solution points are used to compute a depth map for each image comprising a mosaic dataset to improve image-to-ground (map) transformation, especially in high oblique cases.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The block adjusted input mosaic dataset. The mosaic dataset must be adjusted before it is used as input for this tool. You can use an ortho mapping workflow in ArcGIS Pro, or a Reality for ArcGIS Pro workflow, to adjust the mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| Control Point Table | The input control point feature class. This point feature class is the output of the Compute Camera Model tool or the Compute Tie Points tool. | Feature Class; Table View |
| Solution Point Table | The input solution point feature class. This point feature class is the output of the Compute Camera Model tool or the Compute Tie Points tool. | Feature Class; Table View |
| Query Definition (Optional) | An SQL expression that will be used to select items in the mosaic dataset to include in the depth map. | SQL Expression |
| Skip Existing (Optional) | Specifies whether a depth map CenterZ value will be computed only for rasters without a CenterZ value, or computed for all mosaic dataset items including those with an existing CenterZ value.Unchecked—A depth map CenterZ value will be computed for every mosaic dataset item, including items with an existing CenterZ value. This is the default.Checked—A depth map CenterZ value will only be computed for rasters that do not have a CenterZ value. | Boolean |
| Adjust Footprints (Optional) | Specifies whether the footprint geometry will be updated using the same transformation that was applied to the image. Unchecked—The footprint geometry will not be updated. This is the default.Checked—The footprint geometry will be updated to the image geometry. | Boolean |
| in_mosaic_dataset | The block adjusted input mosaic dataset. The mosaic dataset must be adjusted before it is used as input for this tool. You can use an ortho mapping workflow in ArcGIS Pro, or a Reality for ArcGIS Pro workflow, to adjust the mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| control_point_table | The input control point feature class. This point feature class is the output of the Compute Camera Model tool or the Compute Tie Points tool. | Feature Class; Table View |
| solution_point_table | The input solution point feature class. This point feature class is the output of the Compute Camera Model tool or the Compute Tie Points tool. | Feature Class; Table View |
| where_clause(Optional) | An SQL expression that will be used to select items in the mosaic dataset to include in the depth map. | SQL Expression |
| skip_existing(Optional) | Specifies whether a depth map CenterZ value will be computed only for rasters without a CenterZ value, or computed for all mosaic dataset items including those with an existing CenterZ value.NO_SKIP_EXISTING—A depth map CenterZ value will be computed for every mosaic dataset item, including items with an existing CenterZ value. This is the default.SKIP_EXISTING—A depth map CenterZ value will only be computed for rasters that do not have a CenterZ value. | Boolean |
| adjust_footprints(Optional) | Specifies whether the footprint geometry will be updated using the same transformation that was applied to the image.NO_ADJUST_FOOTPRINTS—The footprint geometry will not be updated. This is the default.ADJUST_FOOTPRINTS—The footprint geometry will be updated to the image geometry. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ComputeDepthMap(in_mosaic_dataset, control_point_table, solution_point_table, {where_clause}, {skip_existing}, {adjust_footprints})
```

### Example 2

```python
# Import system modules
import arcpy  

# Execute
arcpy.management.ComputeDepthMap(in_mosaic_dataset= r"C:\CDM.gdb\YVWD", control_point_table=r" C:\CDM.gdb\YVWD_ControlPoints", solution_point_table= r"C:\CDM.gdb\YVWD_SolutionPoints", skip_existing="SKIP_EXISTING", adjust_footprints="NO_ADJUST_FOOTPRINTS")
```

### Example 3

```python
# Import system modules
import arcpy  

# Execute
arcpy.management.ComputeDepthMap(in_mosaic_dataset= r"C:\CDM.gdb\YVWD", control_point_table=r" C:\CDM.gdb\YVWD_ControlPoints", solution_point_table= r"C:\CDM.gdb\YVWD_SolutionPoints", skip_existing="SKIP_EXISTING", adjust_footprints="NO_ADJUST_FOOTPRINTS")
```

### Example 4

```python
# Import system modules
import arcpy  

# Define input parameters
in_mosaic_dataset= r"C:\CDM_RM.gdb\YVWD"
control_point_table=r"C:\CDM_RM.gdb\YVWD_ControlPoints"
solution_point_table= r"C:\CDM_RM.gdb\YVWD_SolutionPoints",
where_clause= "OBJECTID > 2",
skip_existing="SKIP_EXISTING",
adjust_footprints="ADJUST_FOOTPRINTS"

# Execute
arcpy.management.ComputeDepthMap(in_mosaic_dataset, control_point_table, solution_point_table, where_clause, skip_existing, adjust_footprints)
```

### Example 5

```python
# Import system modules
import arcpy  

# Define input parameters
in_mosaic_dataset= r"C:\CDM_RM.gdb\YVWD"
control_point_table=r"C:\CDM_RM.gdb\YVWD_ControlPoints"
solution_point_table= r"C:\CDM_RM.gdb\YVWD_SolutionPoints",
where_clause= "OBJECTID > 2",
skip_existing="SKIP_EXISTING",
adjust_footprints="ADJUST_FOOTPRINTS"

# Execute
arcpy.management.ComputeDepthMap(in_mosaic_dataset, control_point_table, solution_point_table, where_clause, skip_existing, adjust_footprints)
```

---

## Compute Dirty Area (Data Management)

## Summary

Identifies areas within a mosaic dataset that have changed since a specified point in time. This is used commonly when a mosaic dataset is updated or synchronized, or when derived products, such as cache, need to be updated. This tool will enable you to limit such processes to only the areas that have changed.

## Usage

- This tool constructs a polygon that defines regions containing one or more mosaic dataset items that have been modified since a specified point in time.
- This allows tools and applications that depend on the mosaic dataset for the construction of derived products, such as cache, to perform partial updates since the last time the derived products were synchronized with the mosaic dataset.
- The date and time parameter can be specified in one of two following ways: XML time stingNon-XML string
- XML time sting
- Non-XML string
- A valid XML time string must be in one of the following formats:YYYY-MM-DDThh:mm:ssYYYY-MM-DDThh:mm:ss.ssssZYYYY-MM-DDThh:mm:ss.ssss-00:00YYYY-MM-DDThh:mm:ss+00:00YYYYYYYY-MMYYYY-MM-DDYYYY-MM-DDZYYYY-MM-DDThhYYYY-MM-DDThhZYYYY-MM-DDThh:mmYYYY-MM-DDThh:mmZYYYY-MM-DDThh:mm:ssYYYY-MM-DDThh:mm:ssZ
- YYYY-MM-DDThh:mm:ss
- YYYY-MM-DDThh:mm:ss.ssssZ
- YYYY-MM-DDThh:mm:ss.ssss-00:00
- YYYY-MM-DDThh:mm:ss+00:00
- YYYY
- YYYY-MM
- YYYY-MM-DD
- YYYY-MM-DDZ
- YYYY-MM-DDThh
- YYYY-MM-DDThhZ
- YYYY-MM-DDThh:mm
- YYYY-MM-DDThh:mmZ
- YYYY-MM-DDThh:mm:ss
- YYYY-MM-DDThh:mm:ssZ
- The last possible part of the XML time strings is the time zone. The time zones specified with a Z refers to Zulu Time, or Greenwich Mean Time. You can also specify a time zone by using the positive or negative hours from Zulu Time. If you do not specify a time zone, then the local time zone will be used.
- Valid Non-XML time strings can take on any format shown below:YYYY/MM/DD hh:mm:ss.sYYYY/MM/DD hh:mm:ssYYYY/MM/DD hh:mmYYYY/MM/DD hhYYYY/MMYYYY-MM-DDhhmmss.sYYYY-MM-DDhhmmssYYYY-MM-DDhhmmYYYY-MM-DDhhYYYY-MM-DDYYYY-MMYYYYMMDDhhmmss.sYYYYMMDDhhmmssYYYYMMDDhhmmYYYYMMDDhhYYYYMMDDYYYYMMYYYY
- YYYY/MM/DD hh:mm:ss.s
- YYYY/MM/DD hh:mm:ss
- YYYY/MM/DD hh:mm
- YYYY/MM/DD hh
- YYYY/MM
- YYYY-MM-DDhhmmss.s
- YYYY-MM-DDhhmmss
- YYYY-MM-DDhhmm
- YYYY-MM-DDhh
- YYYY-MM-DD
- YYYY-MM
- YYYYMMDDhhmmss.s
- YYYYMMDDhhmmss
- YYYYMMDDhhmm
- YYYYMMDDhh
- YYYYMMDD
- YYYYMM
- YYYY

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that you want to analyze for changes. | Mosaic Layer |
| Query Definition (Optional) | SQL expression to select specific rasters within the mosaic dataset on which to compute dirty areas. | SQL Expression |
| Start Date and Time | Compute the areas that have changed since the input time.XML time syntax:YYYY-MM-DDThh:mm:ssYYYY-MM-DDThh:mm:ss.ssssZ2002-10-10T12:00:00.ssss-00:002002-10-10T12:00:00+00:00Non-XML time syntax:2002/12/25 23:59:58.123 | String |
| Output Feature Class | The feature class containing the areas that have changed. | Feature Class |
| in_mosaic_dataset | The mosaic dataset that you want to analyze for changes. | Mosaic Layer |
| where_clause(Optional) | SQL expression to select specific rasters within the mosaic dataset on which to compute dirty areas. | SQL Expression |
| timestamp | Compute the areas that have changed since the input time.XML time syntax:YYYY-MM-DDThh:mm:ssYYYY-MM-DDThh:mm:ss.ssssZ2002-10-10T12:00:00.ssss-00:002002-10-10T12:00:00+00:00Non-XML time syntax:2002/12/25 23:59:58.123 | String |
| out_feature_class | The feature class containing the areas that have changed. | Feature Class |

## Code Samples

### Example 1

```python
arcpy.management.ComputeDirtyArea(in_mosaic_dataset, {where_clause}, timestamp, out_feature_class)
```

### Example 2

```python
import arcpy
arcpy.ComputeDirtyArea_management("c:/workspace/fgdb.gdb/md", "#", 
                                  "2010-01-12T18:00:00.00-08:00", "dirtyarea.shp")
```

### Example 3

```python
import arcpy
arcpy.ComputeDirtyArea_management("c:/workspace/fgdb.gdb/md", "#", 
                                  "2010-01-12T18:00:00.00-08:00", "dirtyarea.shp")
```

### Example 4

```python
##===========================
##Compute Dirty Area
##Usage: ComputeDirtyArea_management in_mosaic_dataset {where_clause} timestamp
##                                   out_feature_class

import arcpy
arcpy.env.workspace = "c:/workspace"

# Find the area changed after 6:00pm Jan 12th 2010
arcpy.ComputeDirtyArea_management("fgdb.gdb/md", "#", "2010-01-12T18:00:00.00-08:00", "dirtyarea.shp")
```

### Example 5

```python
##===========================
##Compute Dirty Area
##Usage: ComputeDirtyArea_management in_mosaic_dataset {where_clause} timestamp
##                                   out_feature_class

import arcpy
arcpy.env.workspace = "c:/workspace"

# Find the area changed after 6:00pm Jan 12th 2010
arcpy.ComputeDirtyArea_management("fgdb.gdb/md", "#", "2010-01-12T18:00:00.00-08:00", "dirtyarea.shp")
```

---

## Compute Fiducials (Data Management)

## Summary

Computes the fiducial coordinates in image and film space for each image in a mosaic dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset created from scanned aerial photos using scanned raster type or frame camera raster type. | Mosaic Layer |
| Output Fiducial Table | The output table that stores all the fiducial coordinate information in image and film space. | Table |
| Query Definition (Optional) | A query definition string that defines a subset of rasters for computing fiducials. | SQL Expression |
| Fiducial Templates (Optional) | The fiducial template table that contains required fields for storing fiducial pictures and other properties. | Table View; File; String |
| Film Coordinate System (Optional) | A keyword that defines the film coordinate system of the scanned aerial photograph. It is used in computing fiducial information and affine transformation construction. No change—Maintain the coordinate system of the mosaic dataset. Do not change the film coordinate system of the scanned aerial photograph. Maintain the coordinate system of the mosaic dataset. X right, Y up—The origin of the scanned photo's coordinate system is the center, and positive X points right and positive Y points up.X up, Y left—The origin of the scanned photo's coordinate system is the center, and positive X points up and positive Y points left.X left, Y down—The origin of the scanned photo's coordinate system is the center, and positive X points left and positive Y points down. X down, Y right—The origin of the scanned photo's coordinate system is the center, and positive X points down and positive Y points right. | String |
| in_mosaic_dataset | The mosaic dataset created from scanned aerial photos using scanned raster type or frame camera raster type. | Mosaic Layer |
| out_fiducial_table | The output table that stores all the fiducial coordinate information in image and film space. | Table |
| where_clause(Optional) | A query definition string that defines a subset of rasters for computing fiducials. | SQL Expression |
| fiducial_templates(Optional) | The fiducial template table that contains required fields for storing fiducial pictures and other properties. | Table View; File; String |
| film_coordinate_system(Optional) | A keyword that defines the film coordinate system of the scanned aerial photograph. It is used in computing fiducial information and affine transformation construction. NO_CHANGE—Maintain the coordinate system of the mosaic dataset. Do not change the film coordinate system of the scanned aerial photograph. Maintain the coordinate system of the mosaic dataset. X_RIGHT_Y_UP—The origin of the scanned photo's coordinate system is the center, and positive X points right and positive Y points up.X_UP_Y_LEFT—The origin of the scanned photo's coordinate system is the center, and positive X points up and positive Y points left.X_LEFT_Y_DOWN—The origin of the scanned photo's coordinate system is the center, and positive X points left and positive Y points down. X_DOWN_Y_RIGHT—The origin of the scanned photo's coordinate system is the center, and positive X points down and positive Y points right. | String |

## Code Samples

### Example 1

```python
arcpy.management.ComputeFiducials(in_mosaic_dataset, out_fiducial_table, {where_clause}, {fiducial_templates}, {film_coordinate_system})
```

### Example 2

```python
arcpy.ComputeFiducials_management(r"c:\test\orthomapping.gdb\orthomosaicdataset",
                       r"c:\test\orthomapping.gdb\out_table", "objectID = 1",
                       r"c:\test\fiducilatemplate.csv", "X_RIGHT_Y_UP")
```

### Example 3

```python
arcpy.ComputeFiducials_management(r"c:\test\orthomapping.gdb\orthomosaicdataset",
                       r"c:\test\orthomapping.gdb\out_table", "objectID = 1",
                       r"c:\test\fiducilatemplate.csv", "X_RIGHT_Y_UP")
```

### Example 4

```python
import arcpy

in_mosaic_dataset = "c:\\test\\ortho.gdb\\ortho_md"
out_fiducial_table = "c:\\test\\ortho.gdb\\fiducial_table"
where_clause = ""
fiducial_template = "c:\\test\\fiducilatemplate.csv"
film_coordinate_system = "NO_CHANGE"


arcpy.ComputeFiducials_management(in_mosaic_dataset, out_fiducial_table,
where_clause,fiducial_template, film_coordinate_system)
```

### Example 5

```python
import arcpy

in_mosaic_dataset = "c:\\test\\ortho.gdb\\ortho_md"
out_fiducial_table = "c:\\test\\ortho.gdb\\fiducial_table"
where_clause = ""
fiducial_template = "c:\\test\\fiducilatemplate.csv"
film_coordinate_system = "NO_CHANGE"


arcpy.ComputeFiducials_management(in_mosaic_dataset, out_fiducial_table,
where_clause,fiducial_template, film_coordinate_system)
```

---

## Compute Mosaic Candidates (Data Management)

## Summary

Finds the image candidates in a mosaic dataset that best represent the mosaic area.

## Usage

- This tool is useful when working with orthomosaic projects that may include overlapping UAV or UAS data in a mosaic dataset. In these cases, finding the optimal images in the mosaic dataset will generate a good block adjustment result for the orthomosaic.
- Use this tool before running the Build Seamlines or Color Balance Mosaic Dataset tool. The Candidate field in the mosaic dataset footprint table determines which images will be used in these other tools.
- The Candidate field in the mosaic dataset footprint table can have a value of 0, 1, or 2. A value of 0 means that the image is not an appropriate candidate. A value of 1 means that this image is an appropriate candidate according to the tool output, and it will automatically be used by algorithms that use the Candidate field. A value of 2 can be manually added so the selected image will automatically be used by algorithms that use the Candidate field.
- The value in the Candidate field can be manually edited to include or exclude specific images.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input mosaic dataset | The input mosaic dataset with densely overlapped images. | Mosaic Dataset; Mosaic Layer |
| Maximum Area Overlap(Optional) | The maximum amount of overlap between the mosaic dataset and the footprint of each image in the mosaic dataset. If the percentage of overlap is greater than this threshold, the image is excluded since it will have too much redundant information.The percentage is expressed as a decimal. For example, a maximum overlap of 60 percent is expressed as 0.6. | Double |
| Maximum Area Loss Allowed(Optional) | The maximum percentage of area that can be excluded by the candidate images. After the tool finds the best candidate images based on the Maximum Area Overlap parameter value, it checks whether the maximum excluded area is below the threshold specified. If the excluded area is greater than the specified threshold, the tool will add candidate images to fill in some of the voids that were missing. These excluded areas will typically be along the border of the mosaic dataset.The percentage is expressed as a decimal. For example, a maximum excluded area of 5 percent is expressed as 0.05. | Double |
| Maximum Obliqueness Angle of Image (Optional) | The maximum image obliqueness angle that will be used to filter images. Any image with an obliqueness angle larger than this value will not be used as a candidate. This parameter is measured in degrees. The default value is 15. | Double |
| in_mosaic_dataset | The input mosaic dataset with densely overlapped images. | Mosaic Dataset; Mosaic Layer |
| maximum_overlap(Optional) | The maximum amount of overlap between the mosaic dataset and the footprint of each image in the mosaic dataset. If the percentage of overlap is greater than this threshold, the image is excluded since it will have too much redundant information.The percentage is expressed as a decimal. For example, a maximum overlap of 60 percent is expressed as 0.6. | Double |
| maximum_area_loss(Optional) | The maximum percentage of area that can be excluded by the candidate images. After the tool finds the best candidate images based on the maximum_overlap parameter value, it checks whether the maximum excluded area is below the threshold specified. If the excluded area is greater than the specified threshold, the tool will add candidate images to fill in some of the voids that were missing. These excluded areas will typically be along the border of the mosaic dataset.The percentage is expressed as a double. For example, a maximum excluded area of 5 percent is expressed as 0.05. | Double |
| maximum_obliqueness_angle(Optional) | The maximum image obliqueness angle that will be used to filter images. Any image with an obliqueness angle larger than this value will not be used as a candidate. This parameter is measured in degrees. The default value is 15. | Double |

## Code Samples

### Example 1

```python
arcpy.management.ComputeMosaicCandidates(in_mosaic_dataset, {maximum_overlap}, {maximum_area_loss}, {maximum_obliqueness_angle})
```

### Example 2

```python
Import arcpy
arcpy.ComputeMosaicCandidates_management("c:/data/fgdb.gdb/md", 0.6, 0.05)
```

### Example 3

```python
Import arcpy
arcpy.ComputeMosaicCandidates_management("c:/data/fgdb.gdb/md", 0.6, 0.05)
```

---

## Compute Pansharpen Weights (Data Management)

## Summary

Calculates an optimal set of pan sharpened weights for new or custom sensor data.

## Usage

- This tool will calculate an optimal set of pan sharpened weights, which can be used in other tools that require pan sharpened weights.
- If a raster product is used as the Input Raster, the band order within the raster product template will be honored.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | A multispectral raster that has a panchromatic band. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| Panchromatic Image | The panchromatic band associated with the multispectral raster. | Raster Layer |
| Band Indexes (Optional) | The band order for the pan sharpened weights.If a raster product is used as the Input Raster, the band order within the raster product template will be used. | String |
| in_raster | A multispectral raster that has a panchromatic band. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| in_panchromatic_image | The panchromatic band associated with the multispectral raster. | Raster Layer |
| band_indexes(Optional) | The band order for the pan sharpened weights.If a raster product is used as the in_raster parameter, the band order within the raster product template will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.ComputePansharpenWeights(in_raster, in_panchromatic_image, {band_indexes})
```

### Example 2

```python
import arcpy
arcpy.ComputePansharpenWeights_management(
    "c:/data/rgb.tif", "c:/data/image.tif", "3 2 1 4")
```

### Example 3

```python
import arcpy
arcpy.ComputePansharpenWeights_management(
    "c:/data/rgb.tif", "c:/data/image.tif", "3 2 1 4")
```

### Example 4

```python
#Run Compute Pan Sharpen Weights tool using the bands 4,3,2,1 

import arcpy

InMSraster = "C:\\Landsat7\\L71046029_02920050705_MTL.txt\Multispectral" 
InPANraster = "C:\\Landsat7\\L71046029_02920050705_MTL.txt\Panchromatic"
band_index = "3 2 1 5"

arcpy.ComputePansharpenWeights_management(InMSraster, InPANraster, band_index)
```

### Example 5

```python
#Run Compute Pan Sharpen Weights tool using the bands 4,3,2,1 

import arcpy

InMSraster = "C:\\Landsat7\\L71046029_02920050705_MTL.txt\Multispectral" 
InPANraster = "C:\\Landsat7\\L71046029_02920050705_MTL.txt\Panchromatic"
band_index = "3 2 1 5"

arcpy.ComputePansharpenWeights_management(InMSraster, InPANraster, band_index)
```

### Example 6

```python
#Compute the pansharpening weights and use the results in the 
#create pansharpening tool.

try:
    import arcpy
    
    InRGBraster = "C:\\temp\\rgb.img"
    InPanraster = "C:\\temp\\pan.tif"
    
    #Compute Pan Sharpen Weights  
    out_pan_weight = arcpy.ComputePansharpenWeights_management(
        InRGBraster, InPanraster, "3 2 1 4")
    
    #Get results 
    pansharpen_weights = out_pan_weight.getOutput(0)
    
    #Split the results string for weights of each band
    pansplit = pansharpen_weights.split(";")
    
    #Run the Create pan sharpened raster dataset tool. 
    arcpy.CreatePansharpenedRasterDataset_management(
        InRGBraster, "3", "2", "1", "4", "C:\\temp\\pansharpened_raster.tif",
        InPanraster, "Gram-Schmidt", pansplit[0].split(" ")[1],  
        pansplit[1].split(" ")[1], pansplit[2].split(" ")[1],
        pansplit[3].split(" ")[1])
    
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err[0])
```

### Example 7

```python
#Compute the pansharpening weights and use the results in the 
#create pansharpening tool.

try:
    import arcpy
    
    InRGBraster = "C:\\temp\\rgb.img"
    InPanraster = "C:\\temp\\pan.tif"
    
    #Compute Pan Sharpen Weights  
    out_pan_weight = arcpy.ComputePansharpenWeights_management(
        InRGBraster, InPanraster, "3 2 1 4")
    
    #Get results 
    pansharpen_weights = out_pan_weight.getOutput(0)
    
    #Split the results string for weights of each band
    pansplit = pansharpen_weights.split(";")
    
    #Run the Create pan sharpened raster dataset tool. 
    arcpy.CreatePansharpenedRasterDataset_management(
        InRGBraster, "3", "2", "1", "4", "C:\\temp\\pansharpened_raster.tif",
        InPanraster, "Gram-Schmidt", pansplit[0].split(" ")[1],  
        pansplit[1].split(" ")[1], pansplit[2].split(" ")[1],
        pansplit[3].split(" ")[1])
    
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err[0])
```

---

## Compute Tie Points (Data Management)

## Summary

Computes the tie points between overlapped mosaic dataset items. The tie points can then be used to compute the block adjustments for the mosaic dataset.

## Usage

- The tie points can be combined with control points using the Append Control Points tool.
- The tie points and the optional control points are then used as the inputs for the Compute Block Adjustment tool.
- If you have a mosaic dataset with many items, use caution when specifying the Output Image Features parameter value, since the result may take a long time to process.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset that will be used to create tie points. | Mosaic Layer; Mosaic Dataset |
| Output Control Points | The output control point table. The table will contain the tie points created by this tool. | Feature Class |
| Similarity (Optional) | Specifies the similarity level that will be used for matching tie points.Low similarity—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.Medium similarity—The similarity criteria for the matching points will be medium.High similarity—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |
| Input Mask (Optional) | A polygon feature class used to exclude areas that will not be included in the computation of control points. The mask field can control the inclusion or exclusion of areas. A value of 1 indicates that the areas defined by the polygons (inside) will be excluded from the computation. A value of 2 indicates the defined polygons (inside) will be included in the computation while areas outside of the polygons will be excluded. | Feature Layer |
| Output Image Features(Optional) | The output image feature points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| Point Density | Specifies the number of tie points to be created. Low point density—The density of points will be low, creating the fewest number of tie points.Medium point density—The density of points will be medium, creating a moderate number of points.High point density—The density of points will be high, creating the highest number of points. | String |
| Point Distribution | Specifies whether the points will have regular or random distribution. Random point distribution—Points will be generated randomly. Randomly generated points are better for overlapping areas with irregular shapes.Regular point distribution—Points will be generated based on a fixed pattern. Points based on a fixed pattern use the point density to determine how frequently to create points. | String |
| Image Location Accuracy | Specifies the keyword that describes the accuracy of the imagery.Low image location accuracy—Images have a large shift and a large rotation (> 5 degrees).The SIFT algorithm will be used in the point-matching computation.Medium image location accuracy—Images have a medium shift and a small rotation (<5 degrees).The Harris algorithm will be used in the point-matching computation.High image location accuracy—Images have a small shift and a small rotation.The Harris algorithm will be used in the point-matching computation. | String |
| Additional Options(Optional) | Additional options for the adjustment engine. The options are only used by third-party adjustment engines. | Value Table |
| in_mosaic_dataset | The input mosaic dataset that will be used to create tie points. | Mosaic Layer; Mosaic Dataset |
| out_control_points | The output control point table. The table will contain the tie points created by this tool. | Feature Class |
| similarity(Optional) | Specifies the similarity level that will be used for matching tie points.LOW—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.MEDIUM—The similarity criteria for the matching points will be medium.HIGH—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |
| in_mask_dataset(Optional) | A polygon feature class used to exclude areas that will not be included in the computation of control points. The mask field can control the inclusion or exclusion of areas. A value of 1 indicates that the areas defined by the polygons (inside) will be excluded from the computation. A value of 2 indicates the defined polygons (inside) will be included in the computation while areas outside of the polygons will be excluded. | Feature Layer |
| out_image_features(Optional) | The output image feature points table. This will be saved as a polygon feature class. This output can be quite large. | Feature Class |
| density | Specifies the number of tie points to be created. LOW—The density of points will be low, creating the fewest number of tie points.MEDIUM—The density of points will be medium, creating a moderate number of points.HIGH—The density of points will be high, creating the highest number of points. | String |
| distribution | Specifies whether the points will have regular or random distribution. RANDOM—Points will be generated randomly. Randomly generated points are better for overlapping areas with irregular shapes.REGULAR—Points will be generated based on a fixed pattern. Points based on a fixed pattern use the point density to determine how frequently to create points. | String |
| location_accuracy | Specifies the keyword that describes the accuracy of the imagery.LOW—Images have a large shift and a large rotation (> 5 degrees).The SIFT algorithm will be used in the point-matching computation.MEDIUM—Images have a medium shift and a small rotation (<5 degrees).The Harris algorithm will be used in the point-matching computation.HIGH—Images have a small shift and a small rotation.The Harris algorithm will be used in the point-matching computation. | String |
| options[options,...](Optional) | Additional options for the adjustment engine. The options are only used by third-party adjustment engines. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.ComputeTiePoints(in_mosaic_dataset, out_control_points, {similarity}, {in_mask_dataset}, {out_image_features}, density, distribution, location_accuracy, {options})
```

### Example 2

```python
import arcpy
arcpy.ComputeTiePoints_management("c:/workspace/BD.gdb/redQB", 
     "c:/workspace/BD.gdb/redQB_tiePoints", "MEDIUM")
```

### Example 3

```python
import arcpy
arcpy.ComputeTiePoints_management("c:/workspace/BD.gdb/redQB", 
     "c:/workspace/BD.gdb/redQB_tiePoints", "MEDIUM")
```

### Example 4

```python
#compute tie points

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute tie points for a mosaic dataset
mdName = "BD.gdb/redlandsQB"
out_tiePoint = "BD.gdb/redlandsQB_tiePoints"

arcpy.ComputeTiePoints_management(mdName, out_tiePoint, "MEDIUM")
```

### Example 5

```python
#compute tie points

import arcpy
arcpy.env.workspace = "c:/workspace"

#Compute tie points for a mosaic dataset
mdName = "BD.gdb/redlandsQB"
out_tiePoint = "BD.gdb/redlandsQB_tiePoints"

arcpy.ComputeTiePoints_management(mdName, out_tiePoint, "MEDIUM")
```

---

## Configure Geodatabase Log File Tables (Data Management)

## Summary

Alters the type of log file tables used by an earlier release enterprise geodatabase to maintain lists of records cached by ArcGIS.

## Usage

- This tool only functions with enterprise geodatabase version 10.9.0.2.7 or earlier in IBM Db2 or enterprise geodatabase version 10.7.0.2.3 or earlier in Oracle.
- Only the geodatabase administrator can run the Configure Geodatabase Log File Tables tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | A database connection (.sde file) to the enterprise geodatabase where the log file table configuration will be changed. The connection must be made as the geodatabase administrator. | Workspace |
| Log File Type | Specifies the type of log file tables the geodatabase will use.Session log file—Session-based log file tables for selection sets will be used. Session-based log file tables are dedicated to a single session and may contain multiple selection sets.Shared log file—Shared log file tables for selection sets will be used. Shared log file tables are shared by all sessions that connect as the same user. | String |
| Number of pooled session-based log file tables to be owned by the administrator(Optional) | The number of tables included in the pool that the geodatabase will use if a pool of session-based log file tables owned by the geodatabase administrator is used. | Long |
| Create session-based log file tables in the TempDB database (Microsoft SQL Server only) (Optional) | This parameter is no longer applicable in any supported ArcGIS release. | Boolean |
| input_database | A database connection (.sde file) to the enterprise geodatabase where the log file table configuration will be changed. The connection must be made as the geodatabase administrator. | Workspace |
| log_file_type | Specifies the type of log file tables the geodatabase will use.SESSION_LOG_FILE—Session-based log file tables for selection sets will be used. Session-based log file tables are dedicated to a single session and may contain multiple selection sets.SHARED_LOG_FILE—Shared log file tables for selection sets will be used. Shared log file tables are shared by all sessions that connect as the same user. | String |
| log_file_pool_size(Optional) | The number of tables included in the pool that the geodatabase will use if a pool of session-based log file tables owned by the geodatabase administrator is used. | Long |
| use_tempdb(Optional) | This parameter is no longer applicable in any supported ArcGIS release. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConfigureGeodatabaseLogFileTables(input_database, log_file_type, {log_file_pool_size}, {use_tempdb})
```

### Example 2

```python
import arcpy
input_database = "c:\\temp\\ProductionGDB.sde"
log_file_type = "SESSION_LOG_FILE"
log_file_pool_size = 100

arcpy.ConfigureGeodatabaseLogFileTables_management(
    input_database, log_file_type, log_file_pool_size)
```

### Example 3

```python
import arcpy
input_database = "c:\\temp\\ProductionGDB.sde"
log_file_type = "SESSION_LOG_FILE"
log_file_pool_size = 100

arcpy.ConfigureGeodatabaseLogFileTables_management(
    input_database, log_file_type, log_file_pool_size)
```

---

## Consolidate Layer (Data Management)

## Summary

Consolidates one or more layers by copying all referenced data sources into a single folder.

## Usage

- A warning is issued when this tool encounters an unsupported layer type. The unsupported layer will not be written to the output.
- When the Convert data to file geodatabase parameter is checked, the following occurs:Each unique data source will have a file geodatabase created in the consolidated folder or package.Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- Each unique data source will have a file geodatabase created in the consolidated folder or package.
- Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.
- Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- When the Convert data to file geodatabase parameter is not checked, the following occurs:The data source format of the input layers will be preserved when possible.ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- The data source format of the input layers will be preserved when possible.
- ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.
- In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.
- Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. By default, joined or related data sources will be consolidated in their entirety or, depending on the Select Related Rows parameter value, based on the extent specified for the Extent parameter.
- For feature layers, the Extent parameter is used to select the features that will be consolidated. For raster layers, the Extent parameter is used to clip the raster datasets.
- Some datasets reference other datasets. For example, a topology dataset may reference four feature classes. Other examples of datasets that reference other datasets include geometric networks, networks, and locators. When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.
- If the Schema only parameter is checked, only the schema of the input data sources will be consolidated or packaged. A schema is the structure or design of a feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on. Data and records will not be consolidated or packaged.
- Data sources that do not support schema only will not be consolidated or packaged. If the Schema only parameter is checked and the tool encounters a layer that is not supported for schema only, a warning message appears and that layer will be skipped. If the only layer specified is unsupported for schema only, the tool will fail.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The input layers that will be consolidated. | Layer |
| Output Folder | The output folder that will contain the layer files and consolidated data. If the specified folder does not exist, a new folder will be created. | Folder |
| Convert data to file geodatabase (Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.Checked—All data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To include enterprise geodatabase data, check the Include Enterprise Geodatabase data instead of referencing the data parameter.Unchecked—Data formats will be preserved when possible. This is the default. | Boolean |
| Include Enterprise Geodatabase data instead of referencing the data (Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format.Checked—All enterprise geodatabase data sources will be converted to a file geodatabase. This is the default.Unchecked—All enterprise geodatabase data sources will be preserved and will be referenced in the resulting package. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The specified extent will be applied to all layers. This is the default.Checked—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| Schema only (Optional) | Specifies whether only the schema of the input layers will be consolidated or packaged.Unchecked—All features and records for input layers will be included in the consolidated folder or package. This is the default.Checked—Only the schema of the input layers will be consolidated or packaged. No features or records will be consolidated or packaged in the output folder. | Boolean |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Preserve mobile geodatabase (Optional) | Specifies whether input mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.This parameter overrides the Convert data to file geodatabase parameter when the input data is a mobile geodatabase. Unchecked—Mobile geodatabase data will be converted to file geodatabase format. This is the default.Checked—Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| Exclude Network Dataset (Optional) | For network analysis layers, specifies whether the network dataset will also be consolidated.Unchecked—The network dataset will be included and consolidated. This is the default.Checked—The network dataset will not be included. Only the selected network analysis layers will be consolidated. | Boolean |
| in_layer[in_layer,...] | The input layers that will be consolidated. | Layer |
| output_folder | The output folder that will contain the layer files and consolidated data. If the specified folder does not exist, a new folder will be created. | Folder |
| convert_data(Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.CONVERT— Data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set the convert_arcsde_data parameter to CONVERT_ARCSDE.PRESERVE—Data formats will be preserved when possible. This is the default. | Boolean |
| convert_arcsde_data(Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format. CONVERT_ARCSDE— Enterprise geodatabase data will be converted to a file geodatabase and will be included in the consolidated folder or package. This is the default.PRESERVE_ARCSDE— Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ARCSDE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| schema_only(Optional) | Specifies whether only the schema of the input layers will be consolidated or packaged.ALL— All features and records will be consolidated or packaged. This is the default.SCHEMA_ONLY— Only the schema of the input layers will be consolidated or packaged. | Boolean |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| preserve_sqlite(Optional) | Specifies whether mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.This parameter overrides the convert_data parameter when the input data is a mobile geodatabase.CONVERT_SQLITE—Mobile geodatabase data will be converted to file geodatabase format. This is the default.PRESERVE_SQLITE— Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| exclude_network_dataset(Optional) | For network analysis layers, specifies whether the network dataset will also be consolidated.INCLUDE_NETWORK_DATASET—The network dataset will be included and consolidated. This is the default.EXCLUDE_NETWORK_DATASET— The network dataset will not be included. Only the network analysis layers will be consolidated. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConsolidateLayer(in_layer, output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {select_related_rows}, {preserve_sqlite}, {exclude_network_dataset})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers' 
arcpy.management.ConsolidateLayer('Parcels.lyr', 'Consolidated_folder', 'PRESERVE', 'CONVERT_ARCSDE')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers' 
arcpy.management.ConsolidateLayer('Parcels.lyr', 'Consolidated_folder', 'PRESERVE', 'CONVERT_ARCSDE')
```

### Example 4

```python
# Import system modules
import os 
import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers' 

# Loop through the workspace, find all the layer files (.lyr) and create a consolidated folder for each 
# layer file found using the same name as the original layer file.
for lyr in arcpy.ListFiles('*.lyr'):
    print('Consolidating {}'.format(lyr))
    arcpy.management.ConsolidateLayer(lyr, os.path.splitext(lyr)[0], 'PRESERVE', 'CONVERT_ARCSDE')
```

### Example 5

```python
# Import system modules
import os 
import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/arcgis/ArcTutor/BuildingaGeodatabase/Layers' 

# Loop through the workspace, find all the layer files (.lyr) and create a consolidated folder for each 
# layer file found using the same name as the original layer file.
for lyr in arcpy.ListFiles('*.lyr'):
    print('Consolidating {}'.format(lyr))
    arcpy.management.ConsolidateLayer(lyr, os.path.splitext(lyr)[0], 'PRESERVE', 'CONVERT_ARCSDE')
```

---

## Consolidate Locator (Data Management)

## Summary

Consolidate a locator or composite locator by copying all locators into a single folder.

## Usage

- Locators stored in a geodatabase are not accessible. If you want to consolidate a composite locator, make sure that the participating locators are not stored in a geodatabase.
- A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Locator | The input locator or composite locator that will be consolidated. | Address Locator |
| Output Folder | The output folder that will contain the consolidated locator or composite locator with its participating locators. If the specified folder does not exist, a new folder will be created. | Folder |
| Composite locator only: copy participating locators in enterprise database instead of referencing them(Optional) |  | Boolean |
| in_locator | The input locator or composite locator that will be consolidated. | Address Locator |
| output_folder | The output folder that will contain the consolidated locator or composite locator with its participating locators. If the specified folder does not exist, a new folder will be created. | Folder |
| copy_arcsde_locator(Optional) | This parameter has no effect in ArcGIS Pro. It remains only to support backward compatibility. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConsolidateLocator(in_locator, output_folder, {copy_arcsde_locator})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/MyData/Locators" 
arcpy.ConsolidateLocator_Management('Atlanta_composite', 'Consolidate_folder')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/MyData/Locators" 
arcpy.ConsolidateLocator_Management('Atlanta_composite', 'Consolidate_folder')
```

### Example 4

```python
# Name: ConsolidateLocator.py
# Description:  Find all the locators that reside in a specified folder and create a consolidated folder for each locator.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/MyData/Locators"

# Loop through the workspace, find all the loc and create a consolidated folder using the same 
# name as the original locator
for loc in arcpy.ListFiles("*.loc"):
    print("Consolidating " + loc)
    arcpy.ConsolidateLocator_Management(loc, os.path.splitext(loc)[0])
```

### Example 5

```python
# Name: ConsolidateLocator.py
# Description:  Find all the locators that reside in a specified folder and create a consolidated folder for each locator.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/MyData/Locators"

# Loop through the workspace, find all the loc and create a consolidated folder using the same 
# name as the original locator
for loc in arcpy.ListFiles("*.loc"):
    print("Consolidating " + loc)
    arcpy.ConsolidateLocator_Management(loc, os.path.splitext(loc)[0])
```

---

## Consolidate Map (Data Management)

## Summary

Consolidates a map and all referenced data sources to a specified output folder.

## Usage

- A warning is issued when this tool encounters an unsupported layer type. The unsupported layer will not be written to the output.
- When the Convert data to file geodatabase parameter is checked, the following occurs:Each unique data source will have a file geodatabase created in the consolidated folder or package.Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- Each unique data source will have a file geodatabase created in the consolidated folder or package.
- Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.
- Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- When the Convert data to file geodatabase parameter is not checked, the following occurs:The data source format of the input layers will be preserved when possible.ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- The data source format of the input layers will be preserved when possible.
- ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.
- In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.
- Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. By default, joined or related data sources will be consolidated in their entirety or, depending on the Select Related Rows parameter value, based on the extent specified for the Extent parameter.
- Some datasets reference other datasets. For example, a topology dataset may reference four feature classes. Other examples of datasets that reference other datasets include geometric networks, networks, and locators. When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.
- For feature layers, the Extent parameter is used to select the features that will be consolidated. For raster layers, the Extent parameter is used to clip the raster datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | The map (.mapx file) that will be consolidated. When running this tool in ArcGIS Pro, the input can be a map, scene, or basemap. | Map |
| Output Folder | The output folder that will contain the consolidated map and data. If the specified folder does not exist, a folder will be created. | Folder |
| Convert data to file geodatabase (Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.Checked—All data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To include enterprise geodatabase data, check the Include Enterprise Geodatabase data instead of referencing the data parameter.Unchecked—Data formats will be preserved when possible. This is the default. | Boolean |
| Include Enterprise Geodatabase data instead of referencing the data (Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format.Checked—All enterprise geodatabase data sources will be converted to a file geodatabase. This is the default.Unchecked—All enterprise geodatabase data sources will be preserved and will be referenced in the resulting package. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The specified extent will be applied to all layers. This is the default.Checked—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| Preserve Mobile Geodatabase (Optional) | Specifies whether input mobile geodatabase data will be preserved as mobile geodatabase in the output. If the input data is a mobile geodatabase network dataset, the output will always be mobile geodatabase. Unchecked—Mobile geodatabase data will be converted to file geodatabase. This is the default.Checked—Mobile geodatabase data will be preserved as SQLite in the consolidated folder. | Boolean |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Consolidate to a single file geodatabase(Optional) | Specifies whether map layers will be consolidated to a single file geodatabase or to multiple file geodatabases based on the number of unique data sources in the input map.Unchecked—The output will include a file geodatabase for each unique data source in the input map. Each unique data source in the input map will remain as a dedicated data source in the output. For example, if the input map has two layers, and each layer has a data source in a different enterprise geodatabase, the output layer data sources will be in separate geodatabases. This is the default.Checked—All map layers will be consolidated into a single file geodatabase. | Boolean |
| in_map[in_map,...] | The map (.mapx file) that will be consolidated. When running this tool in ArcGIS Pro, the input can be a map, scene, or basemap. | Map |
| output_folder | The output folder that will contain the consolidated map and data. If the specified folder does not exist, a folder will be created. | Folder |
| convert_data(Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.CONVERT— Data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set the convert_arcsde_data parameter to CONVERT_ARCSDE.PRESERVE—Data formats will be preserved when possible. This is the default. | Boolean |
| convert_arcsde_data(Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format. CONVERT_ARCSDE— Enterprise geodatabase data will be converted to a file geodatabase and will be included in the consolidated folder or package. This is the default.PRESERVE_ARCSDE— Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ARCSDE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| preserve_sqlite(Optional) | Specifies whether input mobile geodatabase data will be preserved as mobile geodatabase in the output. If the input data is a mobile geodatabase network dataset, the output will always be mobile geodatabase.PRESERVE_SQLITE—Mobile geodatabase data will be preserved as SQLite in the consolidated folder.CONVERT_SQLITE—Mobile geodatabase data will be converted to file geodatabase. This is the default. | Boolean |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| consolidate_to_one_fgdb(Optional) | Specifies whether map layers will be consolidated to a single file geodatabase or to multiple file geodatabases based on the number of unique data sources in the input map.SINGLE_OUTPUT_WORKSPACE—All map layers will be consolidated into a single file geodatabase.MULTIPLE_OUTPUT_WORKSPACES—The output will include a file geodatabase for each unique data source in the input map. Each unique data source in the input map will remain as a dedicated data source in the output. For example, if the input map has two layers, and each layer has a data source in a different enterprise geodatabase, the output layer data sources will be in separate geodatabases. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConsolidateMap(in_map, output_folder, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {preserve_sqlite}, {select_related_rows}, {consolidate_to_one_fgdb})
```

### Example 2

```python
# In this code, it is assumed that a map named "World1" exists in the current project
import arcpy
arcpy.management.ConsolidateMap('World1', 'c:/projects/World', 'PRESERVE', 'CONVERT_ARCSDE')
```

### Example 3

```python
# In this code, it is assumed that a map named "World1" exists in the current project
import arcpy
arcpy.management.ConsolidateMap('World1', 'c:/projects/World', 'PRESERVE', 'CONVERT_ARCSDE')
```

---

## Consolidate Project (Data Management)

## Summary

Consolidates an ArcGIS Pro project (.aprx file) and referenced maps and data into a folder.

## Usage

- The data and elements in the project will be consolidated and included in the .aprx file, including maps, the data referenced by the layers, toolboxes, styles, layouts, and connections when appropriate.
- The Share outside of organization parameter controls whether the data referenced from networked resources will be consolidated into the folder. Unchecked—Packages will be created for your internal environment, meaning the data will not be consolidated. Data stored on UNC paths, enterprise geodatabase layers, feature services, styles, and connections will remain referenced in the consolidated project.Checked—All required data sources for the package will be copied to the consolidated folder. Items that would be unavailable outside your network—such as enterprise geodatabase connections, feature services, and data stored on shared folders—will be converted or copied to the output folder.Note:Data and maps will be consolidated if the project references them from a local path, such as c:\gisdata\landrecords.gdb\, regardless of the Share outside of organization parameter value.
- Unchecked—Packages will be created for your internal environment, meaning the data will not be consolidated. Data stored on UNC paths, enterprise geodatabase layers, feature services, styles, and connections will remain referenced in the consolidated project.
- Checked—All required data sources for the package will be copied to the consolidated folder. Items that would be unavailable outside your network—such as enterprise geodatabase connections, feature services, and data stored on shared folders—will be converted or copied to the output folder.
- Templates can be created using the Consolidate as template parameter. You can use a project template to define layers, maps, and data, as well as required connections when creating a project. Learn more about creating a project template
- Use the Analyze Tools For Pro and Analyze Toolbox for Version tools to analyze toolboxes that are part of a project before consolidation. Errors that are identified can stop the consolidation process. You must either fix the errors or remove the tool from the project.
- Connections—such as folder, server, database, workflow, and data reviewer—will only be included in an internal package. These items will be removed if the package is created to be shared outside your organization.
- Items such as attachments, styles, and tasks will be referenced for internal packages if they were part of the project from a UNC path. These items will be copied and packaged in all other situations.
- For feature layers, use the Extent parameter to select the features that will be consolidated. For raster layers, use the Extent parameter to clip the raster datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Project | The project (.aprx file) that will be consolidated. | File |
| Output Folder | The output folder that will contain the consolidated project and data. If the specified folder does not exist, a folder will be created. | Folder |
| Share outside of organization(Optional) | Specifies whether the project and all data will be consolidated into a single folder (for sharing outside your organization) or referenced to network data (for sharing within your organization).Note:Data and maps will be consolidated and packaged if the project references them from a local path, such as c:\gisdata\landrecords.gdb\, regardless of this parameter value.Unchecked—Data stored on networked resources such as enterprise geodatabases, feature services, and file shares (UNC path) will be referenced as is. The data will not be converted or included in the package. This is the default.Checked—Data stored on networked resources will be converted or copied into the package. The format of the data will be preserved when possible. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply Extent only to enterprise geodatabase layers (Optional) | Specifies whether the extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The extent will be applied to all layers. This is the default.Checked—The extent will be applied to enterprise geodatabase layers only. | Boolean |
| Consolidate as template (Optional) | Specifies whether the project will be consolidated as a template or a regular project. Templates can include maps, layouts, connections to databases and servers, and so on. A project template allows you to standardize a series of maps for use in a project and ensure that the correct layers are immediately available for use. Unchecked—The project will be consolidated as a project into a folder. This is the default. Checked—The project will be consolidated as a template into a folder. | Boolean |
| Preserve Mobile Geodatabase (Optional) | Specifies whether mobile geodatabases will be preserved or converted to file geodatabases. Note:This parameter applies only to mobile geodatabases (.geodatabase) used primarily for offline workflows in ArcGIS Runtime apps. SQLite databases with .sqlite or .gpkg file extensions will be converted to file geodatabases.Unchecked—Mobile geodatabases will be converted to file geodatabases. This is the default.Checked—Mobile geodatabases will be preserved. | Boolean |
| Version (Optional) | Specifies the ArcGIS Pro version to which objects such as projects, maps, and layers will be persisted. Saving to an earlier version is useful if the project will be used with earlier versions of ArcGIS Pro.Note:A project saved to an earlier version may lose functionality or properties that are only available in later versions.Current version— The contents of the consolidated folder will match the current version of the ArcGIS Pro release. ArcGIS Pro 2.2— The contents of the consolidated folder will be ArcGIS Pro version 2.2.ArcGIS Pro 2.3—The contents of the consolidated folder will be ArcGIS Pro version 2.3.ArcGIS Pro 2.4—The contents of the consolidated folder will be ArcGIS Pro version 2.4.ArcGIS Pro 2.5—The contents of the consolidated folder will be ArcGIS Pro version 2.5.ArcGIS Pro 2.6—The contents of the consolidated folder will be ArcGIS Pro version 2.6.ArcGIS Pro 2.7—The contents of the consolidated folder will be ArcGIS Pro version 2.7.ArcGIS Pro 2.8—The contents of the consolidated folder will be ArcGIS Pro version 2.8.ArcGIS Pro 2.9—The contents of the consolidated folder will be ArcGIS Pro version 2.9.ArcGIS Pro 3.0—The contents of the consolidated folder will be ArcGIS Pro version 3.0.ArcGIS Pro 3.1—The contents of the consolidated folder will be ArcGIS Pro version 3.1.ArcGIS Pro 3.2—The contents of the consolidated folder will be ArcGIS Pro version 3.2.ArcGIS Pro 3.3—The contents of the consolidated folder will be ArcGIS Pro version 3.3.ArcGIS Pro 3.4—The contents of the consolidated folder will be ArcGIS Pro version 3.4. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| in_project | The project (.aprx file) that will be consolidated. | File |
| output_folder | The output folder that will contain the consolidated project and data. If the specified folder does not exist, a folder will be created. | Folder |
| sharing_internal(Optional) | Specifies whether the project and all data will be consolidated into a single folder (for sharing outside your organization) or referenced to network data (for sharing within your organization).INTERNAL—Data stored on networked resources such as enterprise geodatabases, feature services, and file shares (UNC path) will be referenced as is. The data will not be converted or included in the package. This is the default.EXTERNAL—Data stored on networked resources will be converted or copied into the package. The format of the data will be preserved when possible. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_enterprise_geo(Optional) | Specifies whether the extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The extent will be applied to all layers. This is the default.ENTERPRISE_ONLY—The extent will be applied to enterprise geodatabase layers only. | Boolean |
| package_as_template(Optional) | Specifies whether the project will be consolidated as a template or a regular project. Templates can include maps, layouts, connections to databases and servers, and so on. A project template allows you to standardize a series of maps for use in a project and ensure that the correct layers are immediately available for use.Learn more about creating a project templatePROJECT_PACKAGE—The project will be consolidated as a project into a folder. This is the default. PROJECT_TEMPLATE—The project will be consolidated as a template into a folder | Boolean |
| preserve_sqlite(Optional) | Specifies whether mobile geodatabases will be preserved or converted to file geodatabases.Note:This parameter applies only to mobile geodatabases (.geodatabase) used primarily for offline workflows in ArcGIS Runtime apps. SQLite databases with .sqlite or .gpkg file extensions will be converted to file geodatabases.CONVERT_SQLITE—Mobile geodatabases will be converted to file geodatabases. This is the default.PRESERVE_SQLITE—Mobile geodatabases will be preserved. | Boolean |
| version(Optional) | Specifies the ArcGIS Pro version to which objects such as projects, maps, and layers will be persisted. Saving to an earlier version is useful if the project will be used with earlier versions of ArcGIS Pro.Note:A project saved to an earlier version may lose functionality or properties that are only available in later versions.CURRENT— The contents of the consolidated folder will match the current version of the ArcGIS Pro release. 2.2— The contents of the consolidated folder will be ArcGIS Pro version 2.2.2.3—The contents of the consolidated folder will be ArcGIS Pro version 2.3.2.4—The contents of the consolidated folder will be ArcGIS Pro version 2.4.2.5—The contents of the consolidated folder will be ArcGIS Pro version 2.5.2.6—The contents of the consolidated folder will be ArcGIS Pro version 2.6.2.7—The contents of the consolidated folder will be ArcGIS Pro version 2.7.2.8—The contents of the consolidated folder will be ArcGIS Pro version 2.8.2.9—The contents of the consolidated folder will be ArcGIS Pro version 2.9.3.0—The contents of the consolidated folder will be ArcGIS Pro version 3.0.3.1—The contents of the consolidated folder will be ArcGIS Pro version 3.1.3.2—The contents of the consolidated folder will be ArcGIS Pro version 3.2.3.3—The contents of the consolidated folder will be ArcGIS Pro version 3.3.3.4—The contents of the consolidated folder will be ArcGIS Pro version 3.4. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConsolidateProject(in_project, output_folder, {sharing_internal}, {extent}, {apply_extent_to_enterprise_geo}, {package_as_template}, {preserve_sqlite}, {version}, {select_related_rows})
```

### Example 2

```python
import arcpy
arcpy.management.ConsolidateProject(r'c:\projects\SE_Pipelines\PipeDevelopment.aprx',
                                    r'E:\gisShare\projects\Pipelines\SE_Pipeline',
                                    'INTERNAL')
```

### Example 3

```python
import arcpy
arcpy.management.ConsolidateProject(r'c:\projects\SE_Pipelines\PipeDevelopment.aprx',
                                    r'E:\gisShare\projects\Pipelines\SE_Pipeline',
                                    'INTERNAL')
```

### Example 4

```python
import os
import arcpy

enterpriseProjectDir = r"\\centralFileServer\gisData\ArcGISProProjects"
localProjectDir = "c:\\GISdata\\localProjects"

walk = arcpy.da.Walk(enterpriseProjectDir, datatype="Project")

for dirpath, dirnames, filenames in walk:
    for fname in filenames:
        project = os.path.join(dirpath, fame)
        outputFolder = os.path.join(localProjectDir, 
                                    os.path.splitext(os.path.basename(project))[0])
        print("Consolidating: {0} to {1}".format(project, outputFolder))
        arcpy.management.ConsolidateProject(project, outputFolder, "INTERNAL")
```

### Example 5

```python
import os
import arcpy

enterpriseProjectDir = r"\\centralFileServer\gisData\ArcGISProProjects"
localProjectDir = "c:\\GISdata\\localProjects"

walk = arcpy.da.Walk(enterpriseProjectDir, datatype="Project")

for dirpath, dirnames, filenames in walk:
    for fname in filenames:
        project = os.path.join(dirpath, fame)
        outputFolder = os.path.join(localProjectDir, 
                                    os.path.splitext(os.path.basename(project))[0])
        print("Consolidating: {0} to {1}".format(project, outputFolder))
        arcpy.management.ConsolidateProject(project, outputFolder, "INTERNAL")
```

---

## Consolidate Toolbox (Data Management)

## Summary

Consolidates one or more toolboxes into a specified output folder.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Toolbox | The toolboxes to be consolidated. | Toolbox |
| Output Folder | The output folder that will contain the consolidated toolbox. If the specified folder does not exist, a folder will be created. | Folder |
| Version (Optional) | Specifies the target ArcGIS Pro version of the consolidated toolbox. Specifying a version allows toolboxes to be used by previous versions of ArcGIS.Current version—The consolidated folder will contain tools compatible with the current version of the ArcGIS Pro release. This is the default.ArcGIS Pro 2.2— The consolidated folder will contain tools compatible with ArcGIS Pro version 2.2.ArcGIS Pro 2.3—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.3.ArcGIS Pro 2.4—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.4.ArcGIS Pro 2.5—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.5.ArcGIS Pro 2.6—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.6.ArcGIS Pro 2.7—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.7.ArcGIS Pro 2.8—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.8.ArcGIS Pro 2.9—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.9.ArcGIS Pro 3.0—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.0.ArcGIS Pro 3.1—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.1.ArcGIS Pro 3.2—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.2.ArcGIS Pro 3.3—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.3.ArcGIS Pro 3.4—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.4. | String |
| in_toolbox[in_toolbox,...] | The toolboxes to be consolidated. | Toolbox |
| output_folder | The output folder that will contain the consolidated toolbox. If the specified folder does not exist, a folder will be created. | Folder |
| version(Optional) | Specifies the target ArcGIS Pro version of the consolidated toolbox. Specifying a version allows toolboxes to be used by previous versions of ArcGIS.CURRENT—The consolidated folder will contain tools compatible with the current version of the ArcGIS Pro release. This is the default.2.2— The consolidated folder will contain tools compatible with ArcGIS Pro version 2.2.2.3—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.3.2.4—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.4.2.5—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.5.2.6—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.6.2.7—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.7.2.8—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.8.2.9—The consolidated folder will contain tools compatible with ArcGIS Pro version 2.9.3.0—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.0.3.1—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.1.3.2—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.2.3.3—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.3.3.4—The consolidated folder will contain tools compatible with ArcGIS Pro version 3.4. | String |

## Code Samples

### Example 1

```python
arcpy.management.ConsolidateToolbox(in_toolbox, output_folder, {version})
```

### Example 2

```python
import arcpy
arcpy.management.ConsolidateToolbox(r'C:\data\BufferPoints.tbx', 
                                    r'C:\project\Buffer_Pnts', "CURRENT")
```

### Example 3

```python
import arcpy
arcpy.management.ConsolidateToolbox(r'C:\data\BufferPoints.tbx', 
                                    r'C:\project\Buffer_Pnts', "CURRENT")
```

### Example 4

```python
# Name: ConsolidateToolboxEx2.py
# Description: Find all the toolboxes that reside in a specified folder and 
#              create a consolidated folder for each.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/Toolboxes"

# Loop through the workspace, find all the toolboxes (.tbx), and create a 
# consolidated folder for each toolbox found using the same name as the original 
# toolbox.
for tbx in arcpy.ListFiles("*.tbx"):
    print("Consolidating " +  tbx)
    arcpy.management.ConsolidateToolbox(tbx, os.path.splitext(tbx)[0], "CURRENT")
```

### Example 5

```python
# Name: ConsolidateToolboxEx2.py
# Description: Find all the toolboxes that reside in a specified folder and 
#              create a consolidated folder for each.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/Toolboxes"

# Loop through the workspace, find all the toolboxes (.tbx), and create a 
# consolidated folder for each toolbox found using the same name as the original 
# toolbox.
for tbx in arcpy.ListFiles("*.tbx"):
    print("Consolidating " +  tbx)
    arcpy.management.ConsolidateToolbox(tbx, os.path.splitext(tbx)[0], "CURRENT")
```

---

## Convert Coordinate Notation (Data Management)

## Summary

Converts coordinate notations contained in one or two fields from one notation format to another.

## Usage

- The coordinate system of the values stored in the X Field (Longitude) and Y Field (Latitude) values is specified with the Input Coordinate System parameter. The default is GCS_WGS_1984 unless the input table is a feature class, in which case the default is the coordinate system of the input features.
- The following formats are supported: Decimal degrees (DD)Degrees decimal minutes (DDM)Degrees-minutes-seconds (DMS)Global Area Reference System (GARS)World Geographic Reference System (GEOREF and GEOREF 16)Universal Transverse Mercator (UTM)United States National Grid (USNG and USNG 16)Military Grid Reference System (MGRS and MGRS 16)
- Decimal degrees (DD)
- Degrees decimal minutes (DDM)
- Degrees-minutes-seconds (DMS)
- Global Area Reference System (GARS)
- World Geographic Reference System (GEOREF and GEOREF 16)
- Universal Transverse Mercator (UTM)
- United States National Grid (USNG and USNG 16)
- Military Grid Reference System (MGRS and MGRS 16)
- For the Input Coordinate Format parameter options DD 1, DDM 1, and DMS 1, the latitude and longitude values must represent a location that is concatenated together and stored in a single field. For the DD 2, DDM 2, and DMS 2 options, the latitude and longitude values are represented by two separate fields. For the DD numeric option, the latitude and longitude values are stored in two separate fields of type double.For the Gars, Georef, Georef 16, UTM zones, UTM bands, USNG, USNG 16, MGRS and MGRS 16 options, the values are single-string coordinate formats, meaning only one field contains the coordinate.
- For the Input Coordinate Format and Output Coordinate Format parameters, the MGRS 16 (example: 11SLT7858811533670379), USNG 16, and Georef 16 options have higher precision (16 digits) than the equivalent MGRS (example: 11SLT78588115), USNG, and Georef options (8 digits).
- All fields from the input table, including the OID field and the input format fields, will be transferred to the output point feature class. The OID field values can be used to differentiate valid notations that are converted when the Exclude records with invalid notation parameter is checked.Output field names will be matched to the name of the output coordinate notation. For example, if the output format is MGRS, the new output field name will be MGRS.If a field with the same name as the input field already exists in the output, the name of the copied field will be appended with a unique number.
- Uncheck the Exclude records with invalid notation parameter to retain information regarding all input records. The Check Geometry tool can be used to identify records with invalid notations.If there are records with invalid notations, the tool messages will include the path of the log file that contains IDs of all invalid records.
- The Add XY Coordinates tool can be used to add two fields—POINT_X and POINT_Y—to the output point feature class. These fields contain the coordinates of a point in the unit of the coordinate system of the feature class.
- The tool can convert numeric notations DD, DMS, and DDM to other output formats, as long as the input values of DD, DMS, and DDM formats do not contain a letter character.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or text file. Point features are also valid. | Table View |
| Output Feature Class | The output point feature class. The attribute table will contain all fields of the input table along with the fields containing converted values in the output format. | Feature Class |
| X Field (Longitude) | A field from the input table containing the longitude value. For the Input Coordinate Format parameter's DD 2, DD numeric, DDM 2, and DMS 2 options, this is the longitude field.For the DD 1, DDM 1, and DMS 1 options, this field contains both latitude and longitude values in a single string.For the Gars, Georef, Georef 16, UTM zones, UTM bands, USNG, USNG 16, MGRS, and MGRS 16 options, this field contains an alphanumeric notation in a single text field. | Field |
| Y Field (Latitude) | A field from the input table containing the latitude value. For the Input Coordinate Format parameter's DD 2, DD numeric, DDM 2, and DMS 2 options, this is the latitude field. This parameter is inactive when one of the single-string formats is chosen. | Field |
| Input Coordinate Format | Specifies the coordinate format of the input fields.DD, DDM, DMS, and UTM are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a qualifier tell more about the field values.DD 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD 2—Longitude and latitude values are in two separate fields.This is the default.DDM 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DDM 2— Longitude and latitude values are in two separate fields.DMS 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DMS 2—Longitude and latitude values are in two separate fields.Gars—Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells.Georef—World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles.Georef 16—World Geographic Reference System in 16-digit precision.UTM zones—The letter N or S after the UTM zone number designates only North or South hemisphere.UTM bands—The letter after the UTM zone number designates one of the 20 latitude bands. N or S does not designate a hemisphere.USNG—United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum.USNG 16—United States National Grid in 16-digit higher precision.MGRS—Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids.MGRS 16—Military Grid Reference System in 16-digit precision.Shape—Only available when a point feature layer is selected as input. The coordinates of each point are used to define the output format. | String |
| Output Coordinate Format | Specifies the coordinate format to which the input notations will be converted.DD, DDM, DMS, and UTM are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a qualifier tell more about the field values.DD 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD 2—Longitude and latitude values are in two separate fields.DD numeric—Longitude and latitude values are in two separate fields of type Double. Values in the West and South are denoted by a minus sign.DDM 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DDM 2— Longitude and latitude values are in two separate fields.DMS 1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DMS 2—Longitude and latitude values are in two separate fields.Gars—Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells.Georef—World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles.Georef 16—World Geographic Reference System in 16-digit precision.UTM zones—The letter N or S after the UTM zone number designates only North or South hemisphere.UTM bands—The letter after the UTM zone number designates one of the 20 latitude bands. N or S does not designate a hemisphere.USNG—United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum.USNG 16—United States National Grid in 16-digit higher precision.MGRS—Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids.MGRS 16—Military Grid Reference System in 16-digit precision. | String |
| ID (Optional) | This parameter is no longer used, as all fields are transferred to output table. | Field |
| Output Coordinate System (Optional) | The spatial reference of the output feature class. The default is GCS_WGS_1984.The tool projects the output to the spatial reference specified. If the input and output coordinate systems are in a different datum, a default transformation will be used based on the coordinate systems of the input and the output and the extent of the data. | Spatial Reference |
| Input Coordinate System (Optional) | The spatial reference of the input data. If the input spatial reference cannot be obtained from the input table, a default of GCS_WGS_1984 will be used. | Coordinate System |
| Exclude records with invalid notation (Optional) | Specifies whether records with invalid notation will be excluded.Unchecked—Invalid records will be excluded and only valid records will be converted to points in the output. This is the default.Checked—Valid records will be converted to points in the output and invalid records will be included as null geometry. | Boolean |
| in_table | The input table or text file. Point features are also valid. | Table View |
| out_featureclass | The output point feature class. The attribute table will contain all fields of the input table along with the fields containing converted values in the output format. | Feature Class |
| x_field | A field from the input table containing the longitude value. For the input_coordinate_format parameter's DD_2, DD_NUMERIC, DDM_2, and DMS_2 options, this is the longitude field. For the DD_1, DDM_1, and DMS_1 options, this field contains both latitude and longitude values in a single string.For the GARS, GEOREF, GEOREF16, UTM_ZONES, UTM_BANDS, USNG, USNG16, MGRS, and MGRS16 options, this field contains an alphanumeric system of notation in a single text field. | Field |
| y_field | A field from the input table containing the latitude value. For the input_coordinate_format parameter's DD_2, DD_NUMERIC, DDM_2, and DMS_2 options, this is the longitude field. This parameter is ignored when one of the single-string formats is chosen. | Field |
| input_coordinate_format | Specifies the coordinate format of the input fields.DD_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD_2—Longitude and latitude values are in two separate fields.This is the default.DDM_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DDM_2— Longitude and latitude values are in two separate fields.DMS_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DMS_2—Longitude and latitude values are in two separate fields.GARS—Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells.GEOREF—World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles.GEOREF16—World Geographic Reference System in 16-digit precision.UTM_ZONES—The letter N or S after the UTM zone number designates only North or South hemisphere.UTM_BANDS—The letter after the UTM zone number designates one of the 20 latitude bands. N or S does not designate a hemisphere.USNG—United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum.USNG16—United States National Grid in 16-digit higher precision.MGRS—Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids.MGRS16—Military Grid Reference System in 16-digit precision.SHAPE—Only available when a point feature layer is selected as input. The coordinates of each point are used to define the output format.DD, DDM, DMS, and UTM are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a qualifier tell more about the field values. | String |
| output_coordinate_format | Specifies the coordinate format to which the input notations will be converted.DD_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash. DD_2—Longitude and latitude values are in two separate fields.DD_NUMERIC—Longitude and latitude values are in two separate fields of type Double. Values in the West and South are denoted by a minus sign.DDM_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DDM_2— Longitude and latitude values are in two separate fields.DMS_1—Both longitude and latitude values are in a single field. Two values are separated by a space, a comma, or a slash.DMS_2—Longitude and latitude values are in two separate fields.GARS—Global Area Reference System. Based on latitude and longitude, it divides and subdivides the world into cells.GEOREF—World Geographic Reference System. A grid-based system that divides the world into 15-degree quadrangles and then subdivides into smaller quadrangles.GEOREF16—World Geographic Reference System in 16-digit precision.UTM_ZONES—The letter N or S after the UTM zone number designates only North or South hemisphere.UTM_BANDS—The letter after the UTM zone number designates one of the 20 latitude bands. N or S does not designate a hemisphere.USNG—United States National Grid. Almost exactly the same as MGRS but uses North American Datum 1983 (NAD83) as its datum.USNG16—United States National Grid in 16-digit higher precision.MGRS—Military Grid Reference System. Follows the UTM coordinates and divides the world into 6-degree longitude and 20 latitude bands, but MGRS then further subdivides the grid zones into smaller 100,000-meter grids. These 100,000-meter grids are then divided into 10,000-meter, 1,000-meter, 100-meter, 10-meter, and 1-meter grids.MGRS16—Military Grid Reference System in 16-digit precision.DD, DDM, DMS, and UTM are also valid keywords; they can be used just by typing in (on dialog) or passing the value in scripting. However, keywords with underscore and a qualifier tell more about the field values. | String |
| id_field(Optional) | This parameter is ignored, as all fields are transferred to output table. | Field |
| spatial_reference(Optional) | The spatial reference of the output feature class. The default is GCS_WGS_1984.The tool projects the output to the spatial reference specified. If the input and output coordinate systems are in a different datum, a default transformation will be used based on the coordinate systems of the input and the output and the extent of the data. | Spatial Reference |
| in_coor_system(Optional) | The spatial reference of the input data. If the input spatial reference cannot be obtained from the input table, a default of GCS_WGS_1984 will be used. | Coordinate System |
| exclude_invalid_records(Optional) | Specifies whether to exclude records with invalid notation.EXCLUDE_INVALID—Invalid records will be excluded and only valid records will be converted to points in the output. This is the default.INCLUDE_INVALID—Valid records will be converted to points in the output and invalid records will be included as null geometry. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConvertCoordinateNotation(in_table, out_featureclass, x_field, y_field, input_coordinate_format, output_coordinate_format, {id_field}, {spatial_reference}, {in_coor_system}, {exclude_invalid_records})
```

### Example 2

```python
# import arcpy module
import arcpy

# set locals variables
in_tab = r"c:\data\notation.gdb\loc_mgrs"
out_pts = r"c:\data\notation.gdb\loc_final"

# call Convert Coordinate Notation with MGRS as input field.
# leaving out spatial reference parameter will default to WGS 1984 
arcpy.ConvertCoordinateNotation_management(in_tab, out_pts, "m10d", "#", "MGRS", "DD_1")
```

### Example 3

```python
# import arcpy module
import arcpy

# set locals variables
in_tab = r"c:\data\notation.gdb\loc_mgrs"
out_pts = r"c:\data\notation.gdb\loc_final"

# call Convert Coordinate Notation with MGRS as input field.
# leaving out spatial reference parameter will default to WGS 1984 
arcpy.ConvertCoordinateNotation_management(in_tab, out_pts, "m10d", "#", "MGRS", "DD_1")
```

### Example 4

```python
# imports
import arcpy

arcpy.env.workspace = r"c:\data\mtf.gdb"

# set parameter values
input_table = 'rit_up_DD'
output_points = 'ritLOC'
x_field = 'LON'
y_field = 'LAT'
input_format = 'DD_2'
output_format = 'GARS'
id_field = 'CITY_NAME'
spatial_ref = arcpy.SpatialReference('WGS 1984')

try:
    arcpy.ConvertCoordinateNotation_management(input_table, output_points, x_field, y_field, 
                                               input_format, output_format, id_field, spatial_ref)
    print(arcpy.GetMessages(0))

except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 5

```python
# imports
import arcpy

arcpy.env.workspace = r"c:\data\mtf.gdb"

# set parameter values
input_table = 'rit_up_DD'
output_points = 'ritLOC'
x_field = 'LON'
y_field = 'LAT'
input_format = 'DD_2'
output_format = 'GARS'
id_field = 'CITY_NAME'
spatial_ref = arcpy.SpatialReference('WGS 1984')

try:
    arcpy.ConvertCoordinateNotation_management(input_table, output_points, x_field, y_field, 
                                               input_format, output_format, id_field, spatial_ref)
    print(arcpy.GetMessages(0))

except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 6

```python
# imports
import arcpy

arcpy.env.workspace = r"c:\data\ccn.gdb"

# export_utm58 table contains coordinates in UTM_BANDS format 
# where N and S indicate latitude bands, 
# for example, 58S4144921393176 - here 58S is latitude band
input_table = 'export_utm58'

# the coordinate values in output point table will be in UTM_ZONES format
# for example, 58N4144921393176 - note that it is now 58N because
# the point is in UTM 58 North zone
output_points = 'utm_zone18'

spatial_ref = arcpy.SpatialReference('WGS 1984')

try:
    arcpy.ConvertCoordinateNotation_management(input_table, output_points, "LOCS", "", 
                                               "UTM_BANDS", "UTM_ZONES", "", spatial_ref)
    print(arcpy.GetMessages(0))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 7

```python
# imports
import arcpy

arcpy.env.workspace = r"c:\data\ccn.gdb"

# export_utm58 table contains coordinates in UTM_BANDS format 
# where N and S indicate latitude bands, 
# for example, 58S4144921393176 - here 58S is latitude band
input_table = 'export_utm58'

# the coordinate values in output point table will be in UTM_ZONES format
# for example, 58N4144921393176 - note that it is now 58N because
# the point is in UTM 58 North zone
output_points = 'utm_zone18'

spatial_ref = arcpy.SpatialReference('WGS 1984')

try:
    arcpy.ConvertCoordinateNotation_management(input_table, output_points, "LOCS", "", 
                                               "UTM_BANDS", "UTM_ZONES", "", spatial_ref)
    print(arcpy.GetMessages(0))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 8

```python
# imports
import arcpy

# output from Convert Coordinate Notation tool
# for DD_2 (and also for DD_1) format, the output values are in string format
# for example, for DD_1, the output values may be '43.63872N 116.24135W'
in_table = r"c:\data\ccn.gdb\ccn_dd1"

# add a field of type DOUBLE to store the numeric longitude value
arcpy.AddField_management(in_table, "DDLonDbl", "DOUBLE")

# now call CalculateField tool to convert the values, 'W' is negative
expr = """def convertToDouble(fldval):
    val = float(fldval[:-1])
    if fldval[-1:] == 'W':
        return val * -1.0
    else:
        return val"""

# DDLon field contains longitudes in a string field
arcpy.CalculateField_management(in_table,"DDLonDbl","convertToDouble(!DDLon!)","PYTHON_9.3",expr)
    
# add another field to store the numeric longitude value
arcpy.AddField_management(in_table, "DDLatDbl", "DOUBLE")

# call CalculateField again to convert the values, 'S' is negative
expr = """def convertToDouble(fldval):
    val = float(fldval[:-1])
    if fldval[-1:] == 'S':
        return val * -1.0
    else:
        return val"""

# DDLat field contains latitudes in a string field
arcpy.CalculateField_management(in_table,"DDLatDbl","convertToDouble(!DDLat!)","PYTHON_9.3",expr)
```

### Example 9

```python
# imports
import arcpy

# output from Convert Coordinate Notation tool
# for DD_2 (and also for DD_1) format, the output values are in string format
# for example, for DD_1, the output values may be '43.63872N 116.24135W'
in_table = r"c:\data\ccn.gdb\ccn_dd1"

# add a field of type DOUBLE to store the numeric longitude value
arcpy.AddField_management(in_table, "DDLonDbl", "DOUBLE")

# now call CalculateField tool to convert the values, 'W' is negative
expr = """def convertToDouble(fldval):
    val = float(fldval[:-1])
    if fldval[-1:] == 'W':
        return val * -1.0
    else:
        return val"""

# DDLon field contains longitudes in a string field
arcpy.CalculateField_management(in_table,"DDLonDbl","convertToDouble(!DDLon!)","PYTHON_9.3",expr)
    
# add another field to store the numeric longitude value
arcpy.AddField_management(in_table, "DDLatDbl", "DOUBLE")

# call CalculateField again to convert the values, 'S' is negative
expr = """def convertToDouble(fldval):
    val = float(fldval[:-1])
    if fldval[-1:] == 'S':
        return val * -1.0
    else:
        return val"""

# DDLat field contains latitudes in a string field
arcpy.CalculateField_management(in_table,"DDLatDbl","convertToDouble(!DDLat!)","PYTHON_9.3",expr)
```

---

## Convert Raster Function Template (Data Management)

## Summary

Converts a raster function template between formats (rft.xml, json, and binary).

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Function Template | The input raster function template file. The input template file can be XML, JSON, or binary format. | File; String |
| Output Raster Function Template File | The output raster function template file path and file name. | File |
| Format (Optional) | The output function template file format. XML—XML output format.JSON—JSON output format. This is the default.Binary—Binary output format. | String |
| in_raster_function_template | The input raster function template file. The input template file can be XML, JSON, or binary format. | File; String |
| out_raster_function_template_file | The output raster function template file path and file name. | File |
| format(Optional) | The output function template file format. XML—XML output format.JSON—JSON output format. This is the default.BINARY—Binary output format. | String |

## Code Samples

### Example 1

```python
arcpy.management.ConvertRasterFunctionTemplate(in_raster_function_template, out_raster_function_template_file, {format})
```

### Example 2

```python
##Convert Raster Function Template

## Usage: ConvertRasterFunctionTemplate_management(in_raster_function_template,
##                   out_raster_function_template_file, {JSON | XML | BINARY})


import arcpy

#Convert rft.xml to json

arcpy.ConvertRasterFunctionTemplate_management("c:\\test\\NDVI.rft.xml", "C:\\Test\\json_NDVI.rft.json")
```

### Example 3

```python
##Convert Raster Function Template

## Usage: ConvertRasterFunctionTemplate_management(in_raster_function_template,
##                   out_raster_function_template_file, {JSON | XML | BINARY})


import arcpy

#Convert rft.xml to json

arcpy.ConvertRasterFunctionTemplate_management("c:\\test\\NDVI.rft.xml", "C:\\Test\\json_NDVI.rft.json")
```

### Example 4

```python
#import arcpy module
import arcpy

#Set input parameters
in_json = "c:\\test\\aspect.json"
out_rftxml = "c:\\test\\aspect.rft.xml"
out_file_type = "XML"

#Convert json to rft.xml
arcpy.ConvertRasterFunctionTemplate_management(in_json, out_rftxml, out_file_type)
```

### Example 5

```python
#import arcpy module
import arcpy

#Set input parameters
in_json = "c:\\test\\aspect.json"
out_rftxml = "c:\\test\\aspect.rft.xml"
out_file_type = "XML"

#Convert json to rft.xml
arcpy.ConvertRasterFunctionTemplate_management(in_json, out_rftxml, out_file_type)
```

---

## Convert Schema Report (Data Management)

## Summary

Converts a JSON or XLSX formatted schema report to another schema report format or to an XML workspace document that can be used to create a geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Schema Report | The JSON or XLSX schema report that will be converted. | File |
| Output Location | The folder where the output files will be placed | Folder |
| Name | The name of the file outputs. | String |
| Output Formats | Specifies the file types that will be included in the output folder.JSON—The output folder will include a .json file.PDF—The output folder will include a .pdf file.HTML—The output folder will include an .html file.XLSX—The output folder will include an Excel .xlsx file.XML—The output folder will include an .xml file. | String |
| schema_report | The JSON or XLSX schema report that will be converted. | File |
| out_location | The folder where the output files will be placed | Folder |
| name | The name of the file outputs. | String |
| formats[formats,...] | Specifies the file types that will be included in the output folder.JSON—The output folder will include a .json file.PDF—The output folder will include a .pdf file.HTML—The output folder will include an .html file.XLSX—The output folder will include an Excel .xlsx file.XML—The output folder will include an .xml file. | String |

## Code Samples

### Example 1

```python
arcpy.management.ConvertSchemaReport(schema_report, out_location, name, formats)
```

### Example 2

```python
import arcpy
arcpy.management.ConvertSchemaReport("C:\folder\TEST.xlsx", "C:\location\folder",
                                     "NEW", ["JSON", "XLSX", "HTML", "PDF", "XML"])
```

### Example 3

```python
import arcpy
arcpy.management.ConvertSchemaReport("C:\folder\TEST.xlsx", "C:\location\folder",
                                     "NEW", ["JSON", "XLSX", "HTML", "PDF", "XML"])
```

### Example 4

```python
# Name: ConvertSchemaReport_Example.py
# Description: ConvertSchemaReport of a file geodatabase

# Import the system modules
import arcpy

# Set local variables
out_location=r"C:\location\folder"

arcpy.management.ConvertSchemaReport(schema_report=r"C:\folder\TEST.xlsx", out_location,
                                     name="NEW", formats=["JSON", "XLSX", "HTML", "PDF", "XML"])
```

### Example 5

```python
# Name: ConvertSchemaReport_Example.py
# Description: ConvertSchemaReport of a file geodatabase

# Import the system modules
import arcpy

# Set local variables
out_location=r"C:\location\folder"

arcpy.management.ConvertSchemaReport(schema_report=r"C:\folder\TEST.xlsx", out_location,
                                     name="NEW", formats=["JSON", "XLSX", "HTML", "PDF", "XML"])
```

---

## Convert Temporal Field (Data Management)

## Summary

Transfers temporal values stored in a field to another field. The tool can be used to convert between field types (text, numeric, or datetime fields) or to convert the values to a different format such as dd/MM/yy HH:mm:ss to yyyy-MM-dd.

## Usage

- If the input time field is a text or numeric field, choose or specify the appropriate format for the Input Format parameter. Learn more about supported time field formats
- If the Input Field parameter value is a text field, custom formats can be added to the Input Format parameter.For text fields that are compliant with ISO-8601 formatting, use yyyy-MM-ddTHH:mm:ss.s as the input format. This format can handle inputs that use either a UTC designator (Z) or UTC offsets (±hh:mm).Learn more about converting string time values to date format
- Outputs to a text field with yyyy-MM-ddTHH:mm:ss.s format do not support the addition of a UTC (Z) or UTC offset (±hh:mm) designator.
- If the Time Zone environment is specified, the environment value's for the Time Zone Offset parameter will be used by default, unless the parameter value is also specified.
- If the Input Field parameter value is a Date field and the Output Field Type parameter value is Timestamp offset, you can specify the String type for the Time Zone Offset parameter and choose a time zone that will be uniformly applied to all rows in the output. For example, selecting Pacific Standard Time as an offset for an input date field with a value 2002-08-20 12:00:00 PM will produce an output value of 2002-08-20 12:00:00 PM -08:00. All other rows in the input Date field will also have the -08:00 offset applied. Alternatively, specify the Field type, and select a field containing the time zone offset data. Field values can be numeric, such as -5, or string, such as Eastern Standard Time. See the Time Zone Offset parameter for a list of all time zones supported as string values. When a field is specified, the tool will use the row value as the offset value. Using a field for the offset can be useful when the date values for the input rows are from different time zones. For example, if the input Date field contains the values 2001-01-01 7:00:00 AM, 2001-01-02, 9:00:00 AM, and 2001-01-02 12:00:00 PM, and the offset field contains the values -8, -5, and 0 for the corresponding rows, the tool will output the values 2001-01-01 7:00:00 AM -08:00, 2001-01-02 9:00:00 AM -05:00, and 2001-01-02 12:00:00 PM +00:00.Note:The tool will not perform any time or date conversion when the Input Field parameter value is a Date field and the Output Field Type parameter value is Timestamp offset. If a time or date conversion needs to occur, use the Convert Time Zone tool to convert the date field to the appropriate date and time before using this tool to apply the corresponding timestamp offset value.
- If the Input Field parameter value is a Timestamp Offset field and the Output Field Type parameter value is a Date, Date Only, or Time Only field, the tool will perform the appropriate time and date conversion to the specified Time Zone Offset parameter value. For example, a Timestamp Offset field value of 2011-06-02 4:30:00.000 PM -08:00 converted to Date type, with the Time Zone Offset parameter set to the String type and a value of UTC will have an output value of 2011-06-03 12:30:00 AM.
- The Output Format parameter can be specified as Time only, but Time Only is not an accepted Input Format parameter value. This is because the Time only output format lacks sufficient information to be converted to other formats such as Timestamp Offset, Date, or Date Only.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The layer or table that contains the field containing the time values that will be converted. | Table View |
| Input Field | The field containing the time values. The field can be of type short, long, float, double, text, date, timestamp offset, or date only. | Field |
| Input Format(Optional) | The format of the time values in the Input Field parameter value. You can choose a standard time format from the drop-down list or specify a custom format. The parameter is not supported when the input time field is of type date.Note:The format strings are case sensitive.If the data type of the time field is date, date only, or timestamp offset, no time format is required.If the data type of the time field is numeric (short, long, float, double, or big integer), a list of standard numeric time formats is provided in the drop-down list.If the data type of the time field is string, a list of standard string time formats is provided in the drop-down list. For string fields, you can also specify a custom time format. For example, the time values may have been stored in a string field in one of the standard formats such as yyyy/MM/dd HH:mm:ss or in a custom format such as dd/MM/yyyy HH:mm:ss. For the custom format, you can also specify the a.m. or p.m. designator.For ISO-8601 compliant strings, use yyyy-MM-ddTHH:mm:ss.s as the input format. This input selection can handle inputs that use either a UTC designator (Z) or UTC offsets (±hh:mm).Commonly used formats are listed below:yyyy—Year represented by four digits MM—Month as digits with leading zero for single-digit monthsMMM—Month as a three-letter abbreviation dd—Day of month as digits with leading zero for single-digit daysddd—Day of week as a three-letter abbreviationhh—Hours with leading zero for single-digit hours and a 12-hour clockHH—Hours with leading zero for single-digit hours and a 24-hour clockmm—Minutes with leading zero for single-digit minutesss—Seconds with leading zero for single-digit secondst—One character time marker string, such as A or Ptt—Multicharacter time marker string, such as AM or PMunix_us—UNIX time in microsecondsunix_ms—UNIX time in millisecondsunix_s—UNIX time in secondsunix_hex—UNIX time in hexadecimal | String |
| Output Field Name | The name of the field to be added in which the converted time values will be stored. | String |
| Output Field Type (Optional) | Specifies the field type of the output time field.Date only—The field type will be date only. Date only fields support date values with no time values.Date—The field type will be date. Date fields support date and time values. Time only—The field type will be time only. Time only fields support time values with no date values.Timestamp offset—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.Text—The field type will be text. Text fields support a string of characters.Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. | String |
| Output Format (Optional) | The format of the output time values. Supported output time formats depend on the Output Field Type parameter value. A custom format can also be used to convert the value to a different format or to extract a portion of the value (such as the year). For a list of custom formats, see the Input Format parameter explanation. This parameter is not used when the Output Field Type parameter value is Date.Note:If the data type of the output time field isn't long enough to store the converted time value, the output value will be truncated. | String |
| Time Zone Offset(Optional) | The time zone or field containing a numeric offset from the input table that will be used as the output time zone. If the Time Zone environment is specified, the tool will use the that environment value by default. UTC—The time zone will be UTC.Dateline Standard Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian Standard Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian Standard Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas Standard Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan Standard Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific Standard Time (Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific Standard Time—The time zone will be Pacific Standard Time (UTC-08:00).US Mountain Standard Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain Standard Time (Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain Standard Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon Standard Time—The time zone will be Yukon Standard Time (UTC-07:00).Central America Standard Time—The time zone will be Central America Standard Time (UTC-06:00).Central Standard Time—The time zone will be Central Standard Time (UTC-06:00).Easter Island Standard Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central Standard Time (Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada Central Standard Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA Pacific Standard Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern Standard Time (Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern Standard Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti Standard Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba Standard Time—The time zone will be Cuba Standard Time (UTC-05:00).US Eastern Standard Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks And Caicos Standard Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay Standard Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic Standard Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela Standard Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central Brazilian Standard Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA Western Standard Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific SA Standard Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland Standard Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins Standard Time—The time zone will be Tocantins Standard Time (UTC-03:00).E. South America Standard Time—The time zone will be E. South America Standard Time (UTC-03:00).SA Eastern Standard Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina Standard Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland Standard Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo Standard Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes Standard Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint Pierre Standard Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia Standard Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic Standard Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores Standard Time—The time zone will be Azores Standard Time (UTC-01:00).Cape Verde Standard Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT Standard Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich Standard Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao Tome Standard Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco Standard Time—The time zone will be Morocco Standard Time (UTC+00:00).W. Europe Standard Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central Europe Standard Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance Standard Time—The time zone will be Romance Standard Time (UTC+01:00).Central European Standard Time—The time zone will be Central European Standard Time (UTC+01:00).W. Central Africa Standard Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan Standard Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB Standard Time—The time zone will be GTB Standard Time (UTC+02:00).Middle East Standard Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt Standard Time—The time zone will be Egypt Standard Time (UTC+02:00).E. Europe Standard Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria Standard Time—The time zone will be Syria Standard Time (UTC+02:00).West Bank Standard Time—The time zone will be West Bank Standard Time (UTC+02:00).South Africa Standard Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE Standard Time—The time zone will be FLE Standard Time (UTC+02:00).Israel Standard Time—The time zone will be Israel Standard (UTC+02:00).South Sudan Standard Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad Standard Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan Standard Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya Standard Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia Standard Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic Standard Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey Standard Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab Standard Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus Standard Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian Standard Time—The time zone will be Russian Standard Time (UTC+03:00).E. Africa Standard Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd Standard Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran Standard Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian Standard Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan Standard Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan Standard Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia Time Zone 3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius Standard Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov Standard Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian Standard Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus Standard Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan Standard Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West Asia Standard Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg Standard Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan Standard Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda Standard Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India Standard Time—The time zone will be India Standard Time (UTC+05:30).Sri Lanka Standard Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal Standard Time—The time zone will be Nepal Standard Time (UTC+05:45).Central Asia Standard Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh Standard Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk Standard Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar Standard Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE Asia Standard Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai Standard Time—The time zone will be Altai Standard Time (UTC+07:00).W. Mongolia Standard Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North Asia Standard Time—The time zone will be North Asia Standard Time (UTC+07:00).N. Central Asia Standard Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk Standard Time—The time zone will be Tomsk Standard Time (UTC+07:00).China Standard Time—The time zone will be China Standard Time (UTC+08:00).North Asia East Standard Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore Standard Time—The time zone will be Singapore Standard Time (UTC+08:00).W. Australia Standard Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei Standard Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar Standard Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus Central W. Standard Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal Standard Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo Standard Time—The time zone will be Tokyo Standard Time (UTC+09:00).North Korea Standard Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea Standard Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk Standard Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen. Australia Standard Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS Central Standard Time—The time zone will be AUS Central Standard Time (UTC+09:30).E. Australia Standard Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS Eastern Standard Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West Pacific Standard Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania Standard Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok Standard Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord Howe Standard Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville Standard Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia Time Zone 10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan Standard Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk Standard Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin Standard Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central Pacific Standard Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia Time Zone 11—The time zone will be Russia Time Zone 11 (UTC+11:00).New Zealand Standard Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji Standard Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka Standard Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham Islands Standard Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga Standard Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa Standard Time—The time zone will be Samoa Standard Time (UTC+13:00).Line Islands Standard Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String; Field |
| in_table | The layer or table that contains the field containing the time values that will be converted. | Table View |
| input_time_field | The field containing the time values. The field can be of type short, long, float, double, text, date, timestamp offset, or date only. | Field |
| input_time_format(Optional) | The format of the time values in the input_time_field parameter value. The parameter is not supported when the input time field is of type date.Note:The format strings are case sensitive.If the data type of the time field is date, date only, or timestamp offset, no time format is required.If the data type of the time field is numeric (short, long, float, double, or big integer), a list of standard numeric time formats is provided in the drop-down list.If the data type of the time field is string, a list of standard string time formats is provided in the drop-down list. For string fields, you can also specify a custom time format. For example, the time values may have been stored in a string field in one of the standard formats such as yyyy/MM/dd HH:mm:ss or in a custom format such as dd/MM/yyyy HH:mm:ss. For the custom format, you can also specify the a.m. or p.m. designator.For ISO-8601 compliant strings, use yyyy-MM-ddTHH:mm:ss.s as the input format. This input selection can handle inputs that use either a UTC designator (Z) or UTC offsets (±hh:mm).Commonly used formats are listed below:yyyy—Year represented by four digits MM—Month as digits with leading zero for single-digit monthsMMM—Month as a three-letter abbreviation dd—Day of month as digits with leading zero for single-digit daysddd—Day of week as a three-letter abbreviationhh—Hours with leading zero for single-digit hours and a 12-hour clockHH—Hours with leading zero for single-digit hours and a 24-hour clockmm—Minutes with leading zero for single-digit minutesss—Seconds with leading zero for single-digit secondst—One character time marker string, such as A or Ptt—Multicharacter time marker string, such as AM or PMunix_us—UNIX time in microsecondsunix_ms—UNIX time in millisecondsunix_s—UNIX time in secondsunix_hex—UNIX time in hexadecimal | String |
| output_time_field | The name of the field to be added in which the converted time values will be stored. | String |
| output_time_type(Optional) | Specifies the field type of the output time field.DATE—The field type will be date. Date fields support date and time values. DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values.TIMESTAMPOFFSET—The field type will be timestamp offset. Timestamp offset fields support a date, time, and offset from a UTC value.TEXT—The field type will be text. Text fields support a string of characters.LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253. | String |
| output_time_format(Optional) | The format of the output time values. Supported output time formats depend on the output_time_type parameter value. A custom format can also be used to convert the value to a different format or to extract a portion of the value (such as the year). For a list of custom formats, see the input_time_format parameter explanation. This parameter is not used when the output_time_type parameter value is DATE.Note:If the data type of the output time field isn't long enough to store the converted time value, the output value will be truncated. | String |
| timezone_or_field(Optional) | The time zone or field containing a numeric offset from the input table that will be used as the output time zone. If the Time Zone environment is specified, the tool will use the that environment value by default. UTC—The time zone will be UTC.Dateline_Standard_Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian_Standard_Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian_Standard_Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas_Standard_Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan_Standard_Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific_Standard_Time_(Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific_Standard_Time—The time zone will be Pacific Standard Time (UTC-08:00).US_Mountain_Standard_Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain_Standard_Time_(Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain_Standard_Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon_Standard_Time—The time zone will be Yukon Standard Time (UTC-07:00).Central_America_Standard_Time—The time zone will be Central America Standard Time (UTC-06:00).Central_Standard_Time—The time zone will be Central Standard Time (UTC-06:00).Easter_Island_Standard_Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central_Standard_Time_(Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada_Central_Standard_Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA_Pacific_Standard_Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern_Standard_Time_(Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern_Standard_Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti_Standard_Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba_Standard_Time—The time zone will be Cuba Standard Time (UTC-05:00).US_Eastern_Standard_Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks_And_Caicos_Standard_Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay_Standard_Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic_Standard_Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela_Standard_Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central_Brazilian_Standard_Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA_Western_Standard_Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific_SA_Standard_Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland_Standard_Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins_Standard_Time—The time zone will be Tocantins Standard Time (UTC-03:00).E._South_America_Standard_Time—The time zone will be E. South America Standard Time (UTC-03:00).SA_Eastern_Standard_Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina_Standard_Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland_Standard_Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo_Standard_Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes_Standard_Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint_Pierre_Standard_Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia_Standard_Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic_Standard_Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores_Standard_Time—The time zone will be Azores Standard Time (UTC-01:00).Cape_Verde_Standard_Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT_Standard_Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich_Standard_Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao_Tome_Standard_Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco_Standard_Time—The time zone will be Morocco Standard Time (UTC+00:00).W._Europe_Standard_Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central_Europe_Standard_Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance_Standard_Time—The time zone will be Romance Standard Time (UTC+01:00).Central_European_Standard_Time—The time zone will be Central European Standard Time (UTC+01:00).W._Central_Africa_Standard_Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan_Standard_Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB_Standard_Time—The time zone will be GTB Standard Time (UTC+02:00).Middle_East_Standard_Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt_Standard_Time—The time zone will be Egypt Standard Time (UTC+02:00).E._Europe_Standard_Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria_Standard_Time—The time zone will be Syria Standard Time (UTC+02:00).West_Bank_Standard_Time—The time zone will be West Bank Standard Time (UTC+02:00).South_Africa_Standard_Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE_Standard_Time—The time zone will be FLE Standard Time (UTC+02:00).Israel_Standard_Time—The time zone will be Israel Standard (UTC+02:00).South_Sudan_Standard_Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad_Standard_Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan_Standard_Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya_Standard_Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia_Standard_Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic_Standard_Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey_Standard_Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab_Standard_Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus_Standard_Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian_Standard_Time—The time zone will be Russian Standard Time (UTC+03:00).E._Africa_Standard_Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd_Standard_Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran_Standard_Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian_Standard_Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan_Standard_Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan_Standard_Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia_Time_Zone_3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius_Standard_Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov_Standard_Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian_Standard_Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus_Standard_Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan_Standard_Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West_Asia_Standard_Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg_Standard_Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan_Standard_Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda_Standard_Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India_Standard_Time—The time zone will be India Standard Time (UTC+05:30).Sri_Lanka_Standard_Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal_Standard_Time—The time zone will be Nepal Standard Time (UTC+05:45).Central_Asia_Standard_Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh_Standard_Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk_Standard_Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar_Standard_Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE_Asia_Standard_Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai_Standard_Time—The time zone will be Altai Standard Time (UTC+07:00).W._Mongolia_Standard_Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North_Asia_Standard_Time—The time zone will be North Asia Standard Time (UTC+07:00).N._Central_Asia_Standard_Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk_Standard_Time—The time zone will be Tomsk Standard Time (UTC+07:00).China_Standard_Time—The time zone will be China Standard Time (UTC+08:00).North_Asia_East_Standard_Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore_Standard_Time—The time zone will be Singapore Standard Time (UTC+08:00).W._Australia_Standard_Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei_Standard_Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar_Standard_Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus_Central_W._Standard_Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal_Standard_Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo_Standard_Time—The time zone will be Tokyo Standard Time (UTC+09:00).North_Korea_Standard_Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea_Standard_Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk_Standard_Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen._Australia_Standard_Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS_Central_Standard_Time—The time zone will be AUS Central Standard Time (UTC+09:30).E._Australia_Standard_Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS_Eastern_Standard_Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West_Pacific_Standard_Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania_Standard_Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok_Standard_Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord_Howe_Standard_Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville_Standard_Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia_Time_Zone_10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan_Standard_Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk_Standard_Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin_Standard_Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central_Pacific_Standard_Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia_Time_Zone_11—The time zone will be Russia Time Zone 11 (UTC+11:00).New_Zealand_Standard_Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji_Standard_Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka_Standard_Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham_Islands_Standard_Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga_Standard_Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa_Standard_Time—The time zone will be Samoa Standard Time (UTC+13:00).Line_Islands_Standard_Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String; Field |

## Code Samples

### Example 1

```python
arcpy.management.ConvertTimeField(in_table, input_time_field, {input_time_format}, output_time_field, {output_time_type}, {output_time_format}, {timezone_or_field})
```

### Example 2

```python
import arcpy
arcpy.ConvertTimeField_management("C:/Data/TemporalData.gdb/Input_Table","Input_Time","1033;MMMM dd, yyyy HH:mm:ss;AM;PM","Output_Time")
```

### Example 3

```python
import arcpy
arcpy.ConvertTimeField_management("C:/Data/TemporalData.gdb/Input_Table","Input_Time","1033;MMMM dd, yyyy HH:mm:ss;AM;PM","Output_Time")
```

### Example 4

```python
# Name: ConvertTimeField_Ex02.py
# Description: Convert a time field to date field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:\Data\TemporalData.gdb\Input_Table"
inputTimeField = "Input_Time"
inputTimeFormat = "1033;MMMM dd, yyyy HH:mm:ss;AM;PM"
outputDateField = "Output_Time"

# Execute CalculateEndDate
arcpy.ConvertTimeField_management(inTable, inputTimeField, inputTimeFormat, outputDateField)
```

### Example 5

```python
# Name: ConvertTimeField_Ex02.py
# Description: Convert a time field to date field
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:\Data\TemporalData.gdb\Input_Table"
inputTimeField = "Input_Time"
inputTimeFormat = "1033;MMMM dd, yyyy HH:mm:ss;AM;PM"
outputDateField = "Output_Time"

# Execute CalculateEndDate
arcpy.ConvertTimeField_management(inTable, inputTimeField, inputTimeFormat, outputDateField)
```

---

## Convert Time Zone (Data Management)

## Summary

Converts time values recorded in a date field from one time zone to another time zone.

## Usage

- The input time values to be converted must be stored in a date field.
- The added output time field will be a date type field.
- If your data was collected in a time zone that observes Daylight Saving Time, you should choose the parameters to honor Daylight Saving Time in the input and output fields.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input feature class or table that contains the time stamps that will be transformed to a different time zone. | Table View |
| Input Time Field | The input field that contains the time stamps that will be transformed to a different time zone. | Field |
| Input Time Zone | The input time zone in which the time stamps were collected.UTC—The time zone will be UTC.Dateline Standard Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian Standard Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian Standard Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas Standard Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan Standard Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific Standard Time (Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific Standard Time—The time zone will be Pacific Standard Time (UTC-08:00).US Mountain Standard Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain Standard Time (Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain Standard Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon Standard Time—The time zone will be Yukon Standard Time (UTC-07:00).Central America Standard Time—The time zone will be Central America Standard Time (UTC-06:00).Central Standard Time—The time zone will be Central Standard Time (UTC-06:00).Easter Island Standard Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central Standard Time (Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada Central Standard Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA Pacific Standard Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern Standard Time (Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern Standard Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti Standard Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba Standard Time—The time zone will be Cuba Standard Time (UTC-05:00).US Eastern Standard Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks And Caicos Standard Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay Standard Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic Standard Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela Standard Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central Brazilian Standard Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA Western Standard Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific SA Standard Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland Standard Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins Standard Time—The time zone will be Tocantins Standard Time (UTC-03:00).E. South America Standard Time—The time zone will be E. South America Standard Time (UTC-03:00).SA Eastern Standard Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina Standard Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland Standard Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo Standard Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes Standard Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint Pierre Standard Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia Standard Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic Standard Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores Standard Time—The time zone will be Azores Standard Time (UTC-01:00).Cape Verde Standard Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT Standard Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich Standard Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao Tome Standard Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco Standard Time—The time zone will be Morocco Standard Time (UTC+00:00).W. Europe Standard Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central Europe Standard Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance Standard Time—The time zone will be Romance Standard Time (UTC+01:00).Central European Standard Time—The time zone will be Central European Standard Time (UTC+01:00).W. Central Africa Standard Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan Standard Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB Standard Time—The time zone will be GTB Standard Time (UTC+02:00).Middle East Standard Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt Standard Time—The time zone will be Egypt Standard Time (UTC+02:00).E. Europe Standard Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria Standard Time—The time zone will be Syria Standard Time (UTC+02:00).West Bank Standard Time—The time zone will be West Bank Standard Time (UTC+02:00).South Africa Standard Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE Standard Time—The time zone will be FLE Standard Time (UTC+02:00).Israel Standard Time—The time zone will be Israel Standard (UTC+02:00).South Sudan Standard Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad Standard Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan Standard Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya Standard Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia Standard Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic Standard Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey Standard Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab Standard Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus Standard Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian Standard Time—The time zone will be Russian Standard Time (UTC+03:00).E. Africa Standard Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd Standard Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran Standard Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian Standard Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan Standard Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan Standard Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia Time Zone 3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius Standard Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov Standard Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian Standard Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus Standard Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan Standard Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West Asia Standard Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg Standard Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan Standard Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda Standard Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India Standard Time—The time zone will be India Standard Time (UTC+05:30).Sri Lanka Standard Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal Standard Time—The time zone will be Nepal Standard Time (UTC+05:45).Central Asia Standard Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh Standard Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk Standard Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar Standard Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE Asia Standard Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai Standard Time—The time zone will be Altai Standard Time (UTC+07:00).W. Mongolia Standard Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North Asia Standard Time—The time zone will be North Asia Standard Time (UTC+07:00).N. Central Asia Standard Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk Standard Time—The time zone will be Tomsk Standard Time (UTC+07:00).China Standard Time—The time zone will be China Standard Time (UTC+08:00).North Asia East Standard Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore Standard Time—The time zone will be Singapore Standard Time (UTC+08:00).W. Australia Standard Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei Standard Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar Standard Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus Central W. Standard Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal Standard Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo Standard Time—The time zone will be Tokyo Standard Time (UTC+09:00).North Korea Standard Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea Standard Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk Standard Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen. Australia Standard Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS Central Standard Time—The time zone will be AUS Central Standard Time (UTC+09:30).E. Australia Standard Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS Eastern Standard Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West Pacific Standard Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania Standard Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok Standard Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord Howe Standard Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville Standard Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia Time Zone 10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan Standard Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk Standard Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin Standard Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central Pacific Standard Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia Time Zone 11—The time zone will be Russia Time Zone 11 (UTC+11:00).New Zealand Standard Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji Standard Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka Standard Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham Islands Standard Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga Standard Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa Standard Time—The time zone will be Samoa Standard Time (UTC+13:00).Line Islands Standard Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String |
| Output Time Field | The output field in which the time stamps transformed to the desired output time zone will be stored. | String |
| Output Time Zone | The time zone to which the time stamps will be transformed. By default, the output time zone is the same as the input time zone.UTC—The time zone will be UTC.Dateline Standard Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian Standard Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian Standard Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas Standard Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan Standard Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific Standard Time (Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific Standard Time—The time zone will be Pacific Standard Time (UTC-08:00).US Mountain Standard Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain Standard Time (Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain Standard Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon Standard Time—The time zone will be Yukon Standard Time (UTC-07:00).Central America Standard Time—The time zone will be Central America Standard Time (UTC-06:00).Central Standard Time—The time zone will be Central Standard Time (UTC-06:00).Easter Island Standard Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central Standard Time (Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada Central Standard Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA Pacific Standard Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern Standard Time (Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern Standard Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti Standard Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba Standard Time—The time zone will be Cuba Standard Time (UTC-05:00).US Eastern Standard Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks And Caicos Standard Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay Standard Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic Standard Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela Standard Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central Brazilian Standard Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA Western Standard Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific SA Standard Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland Standard Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins Standard Time—The time zone will be Tocantins Standard Time (UTC-03:00).E. South America Standard Time—The time zone will be E. South America Standard Time (UTC-03:00).SA Eastern Standard Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina Standard Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland Standard Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo Standard Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes Standard Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint Pierre Standard Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia Standard Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic Standard Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores Standard Time—The time zone will be Azores Standard Time (UTC-01:00).Cape Verde Standard Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT Standard Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich Standard Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao Tome Standard Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco Standard Time—The time zone will be Morocco Standard Time (UTC+00:00).W. Europe Standard Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central Europe Standard Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance Standard Time—The time zone will be Romance Standard Time (UTC+01:00).Central European Standard Time—The time zone will be Central European Standard Time (UTC+01:00).W. Central Africa Standard Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan Standard Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB Standard Time—The time zone will be GTB Standard Time (UTC+02:00).Middle East Standard Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt Standard Time—The time zone will be Egypt Standard Time (UTC+02:00).E. Europe Standard Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria Standard Time—The time zone will be Syria Standard Time (UTC+02:00).West Bank Standard Time—The time zone will be West Bank Standard Time (UTC+02:00).South Africa Standard Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE Standard Time—The time zone will be FLE Standard Time (UTC+02:00).Israel Standard Time—The time zone will be Israel Standard (UTC+02:00).South Sudan Standard Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad Standard Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan Standard Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya Standard Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia Standard Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic Standard Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey Standard Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab Standard Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus Standard Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian Standard Time—The time zone will be Russian Standard Time (UTC+03:00).E. Africa Standard Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd Standard Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran Standard Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian Standard Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan Standard Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan Standard Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia Time Zone 3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius Standard Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov Standard Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian Standard Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus Standard Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan Standard Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West Asia Standard Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg Standard Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan Standard Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda Standard Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India Standard Time—The time zone will be India Standard Time (UTC+05:30).Sri Lanka Standard Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal Standard Time—The time zone will be Nepal Standard Time (UTC+05:45).Central Asia Standard Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh Standard Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk Standard Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar Standard Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE Asia Standard Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai Standard Time—The time zone will be Altai Standard Time (UTC+07:00).W. Mongolia Standard Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North Asia Standard Time—The time zone will be North Asia Standard Time (UTC+07:00).N. Central Asia Standard Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk Standard Time—The time zone will be Tomsk Standard Time (UTC+07:00).China Standard Time—The time zone will be China Standard Time (UTC+08:00).North Asia East Standard Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore Standard Time—The time zone will be Singapore Standard Time (UTC+08:00).W. Australia Standard Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei Standard Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar Standard Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus Central W. Standard Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal Standard Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo Standard Time—The time zone will be Tokyo Standard Time (UTC+09:00).North Korea Standard Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea Standard Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk Standard Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen. Australia Standard Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS Central Standard Time—The time zone will be AUS Central Standard Time (UTC+09:30).E. Australia Standard Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS Eastern Standard Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West Pacific Standard Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania Standard Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok Standard Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord Howe Standard Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville Standard Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia Time Zone 10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan Standard Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk Standard Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin Standard Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central Pacific Standard Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia Time Zone 11—The time zone will be Russia Time Zone 11 (UTC+11:00).New Zealand Standard Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji Standard Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka Standard Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham Islands Standard Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga Standard Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa Standard Time—The time zone will be Samoa Standard Time (UTC+13:00).Line Islands Standard Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String |
| Input time field values are adjusted for Daylight Saving Time(Optional) | Specifies whether the time stamps were collected while observing Daylight Saving Time rules in the input time zone. When reading the time values to convert the time zone, the time values will be adjusted to account for the shift in time during Daylight Saving Time.By default, this option is checked and the input time values are adjusted to account for the shift in time due to the Daylight Saving Time rules observed in the input time zone.Checked—The input time values are adjusted for Daylight Saving Time.Unchecked—The input time values are not adjusted for Daylight Saving Time. | Boolean |
| Output time field values are adjusted for Daylight Saving Time(Optional) | Indicates whether the output time values will account for the shift in time due to the Daylight Saving Time rules observed in the output time zone.By default, this option is checked and the output time values are adjusted to account for the shift in time due to the Daylight Saving Time rules observed in the output time zone.Checked—The output time values are adjusted for Daylight Saving Time in the output time zone.Unchecked—The output time values are not adjusted for Daylight Saving Time in the output time zone. | Boolean |
| in_table | The input feature class or table that contains the time stamps that will be transformed to a different time zone. | Table View |
| input_time_field | The input field that contains the time stamps that will be transformed to a different time zone. | Field |
| input_time_zone | The input time zone in which the time stamps were collected.UTC—The time zone will be UTC.Dateline_Standard_Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian_Standard_Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian_Standard_Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas_Standard_Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan_Standard_Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific_Standard_Time_(Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific_Standard_Time—The time zone will be Pacific Standard Time (UTC-08:00).US_Mountain_Standard_Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain_Standard_Time_(Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain_Standard_Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon_Standard_Time—The time zone will be Yukon Standard Time (UTC-07:00).Central_America_Standard_Time—The time zone will be Central America Standard Time (UTC-06:00).Central_Standard_Time—The time zone will be Central Standard Time (UTC-06:00).Easter_Island_Standard_Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central_Standard_Time_(Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada_Central_Standard_Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA_Pacific_Standard_Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern_Standard_Time_(Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern_Standard_Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti_Standard_Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba_Standard_Time—The time zone will be Cuba Standard Time (UTC-05:00).US_Eastern_Standard_Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks_And_Caicos_Standard_Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay_Standard_Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic_Standard_Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela_Standard_Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central_Brazilian_Standard_Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA_Western_Standard_Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific_SA_Standard_Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland_Standard_Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins_Standard_Time—The time zone will be Tocantins Standard Time (UTC-03:00).E._South_America_Standard_Time—The time zone will be E. South America Standard Time (UTC-03:00).SA_Eastern_Standard_Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina_Standard_Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland_Standard_Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo_Standard_Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes_Standard_Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint_Pierre_Standard_Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia_Standard_Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic_Standard_Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores_Standard_Time—The time zone will be Azores Standard Time (UTC-01:00).Cape_Verde_Standard_Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT_Standard_Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich_Standard_Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao_Tome_Standard_Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco_Standard_Time—The time zone will be Morocco Standard Time (UTC+00:00).W._Europe_Standard_Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central_Europe_Standard_Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance_Standard_Time—The time zone will be Romance Standard Time (UTC+01:00).Central_European_Standard_Time—The time zone will be Central European Standard Time (UTC+01:00).W._Central_Africa_Standard_Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan_Standard_Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB_Standard_Time—The time zone will be GTB Standard Time (UTC+02:00).Middle_East_Standard_Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt_Standard_Time—The time zone will be Egypt Standard Time (UTC+02:00).E._Europe_Standard_Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria_Standard_Time—The time zone will be Syria Standard Time (UTC+02:00).West_Bank_Standard_Time—The time zone will be West Bank Standard Time (UTC+02:00).South_Africa_Standard_Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE_Standard_Time—The time zone will be FLE Standard Time (UTC+02:00).Israel_Standard_Time—The time zone will be Israel Standard (UTC+02:00).South_Sudan_Standard_Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad_Standard_Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan_Standard_Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya_Standard_Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia_Standard_Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic_Standard_Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey_Standard_Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab_Standard_Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus_Standard_Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian_Standard_Time—The time zone will be Russian Standard Time (UTC+03:00).E._Africa_Standard_Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd_Standard_Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran_Standard_Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian_Standard_Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan_Standard_Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan_Standard_Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia_Time_Zone_3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius_Standard_Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov_Standard_Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian_Standard_Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus_Standard_Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan_Standard_Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West_Asia_Standard_Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg_Standard_Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan_Standard_Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda_Standard_Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India_Standard_Time—The time zone will be India Standard Time (UTC+05:30).Sri_Lanka_Standard_Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal_Standard_Time—The time zone will be Nepal Standard Time (UTC+05:45).Central_Asia_Standard_Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh_Standard_Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk_Standard_Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar_Standard_Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE_Asia_Standard_Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai_Standard_Time—The time zone will be Altai Standard Time (UTC+07:00).W._Mongolia_Standard_Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North_Asia_Standard_Time—The time zone will be North Asia Standard Time (UTC+07:00).N._Central_Asia_Standard_Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk_Standard_Time—The time zone will be Tomsk Standard Time (UTC+07:00).China_Standard_Time—The time zone will be China Standard Time (UTC+08:00).North_Asia_East_Standard_Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore_Standard_Time—The time zone will be Singapore Standard Time (UTC+08:00).W._Australia_Standard_Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei_Standard_Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar_Standard_Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus_Central_W._Standard_Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal_Standard_Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo_Standard_Time—The time zone will be Tokyo Standard Time (UTC+09:00).North_Korea_Standard_Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea_Standard_Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk_Standard_Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen._Australia_Standard_Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS_Central_Standard_Time—The time zone will be AUS Central Standard Time (UTC+09:30).E._Australia_Standard_Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS_Eastern_Standard_Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West_Pacific_Standard_Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania_Standard_Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok_Standard_Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord_Howe_Standard_Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville_Standard_Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia_Time_Zone_10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan_Standard_Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk_Standard_Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin_Standard_Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central_Pacific_Standard_Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia_Time_Zone_11—The time zone will be Russia Time Zone 11 (UTC+11:00).New_Zealand_Standard_Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji_Standard_Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka_Standard_Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham_Islands_Standard_Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga_Standard_Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa_Standard_Time—The time zone will be Samoa Standard Time (UTC+13:00).Line_Islands_Standard_Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String |
| output_time_field | The output field in which the time stamps transformed to the desired output time zone will be stored. | String |
| output_time_zone | The time zone to which the time stamps will be transformed. By default, the output time zone is the same as the input time zone.UTC—The time zone will be UTC.Dateline_Standard_Time—The time zone will be Dateline Standard Time (UTC-12:00).UTC-11—The time zone will be UTC-11 (UTC-11:00).Aleutian_Standard_Time—The time zone will be Aleutian Standard Time (UTC-10:00).Hawaiian_Standard_Time—The time zone will be Hawaiian Standard Time (UTC-10:00).Marquesas_Standard_Time—The time zone will be Marquesas Standard Time (UTC-09:30).Alaskan_Standard_Time—The time zone will be Alaskan Standard Time (UTC-09:00).UTC-09—The time zone will be UTC-09 (UTC-09:00).Pacific_Standard_Time_(Mexico)—The time zone will be Pacific Standard Time (Mexico) (UTC-08:00).UTC-08—The time zone will be UTC-08 (UTC-08:00).Pacific_Standard_Time—The time zone will be Pacific Standard Time (UTC-08:00).US_Mountain_Standard_Time—The time zone will be US Mountain Standard Time (UTC-07:00).Mountain_Standard_Time_(Mexico)—The time zone will be Mountain Standard Time (Mexico) (UTC-07:00).Mountain_Standard_Time—The time zone will be Mountain Standard Time (UTC-07:00).Yukon_Standard_Time—The time zone will be Yukon Standard Time (UTC-07:00).Central_America_Standard_Time—The time zone will be Central America Standard Time (UTC-06:00).Central_Standard_Time—The time zone will be Central Standard Time (UTC-06:00).Easter_Island_Standard_Time—The time zone will be Easter Island Standard Time (UTC-06:00).Central_Standard_Time_(Mexico)—The time zone will be Central Standard Time (Mexico) (UTC-06:00).Canada_Central_Standard_Time—The time zone will be Canada Central Standard Time (UTC-06:00).SA_Pacific_Standard_Time—The time zone will be SA Pacific Standard Time (UTC-05:00).Eastern_Standard_Time_(Mexico)—The time zone will be Eastern Standard Time (Mexico) (UTC-05:00).Eastern_Standard_Time—The time zone will be Eastern Standard Time (UTC-05:00).Haiti_Standard_Time—The time zone will be Haiti Standard Time (UTC-05:00).Cuba_Standard_Time—The time zone will be Cuba Standard Time (UTC-05:00).US_Eastern_Standard_Time—The time zone will be US Eastern Standard Time (UTC-05:00).Turks_And_Caicos_Standard_Time—The time zone will be Turks And Caicos Standard Time (UTC-04:00).Paraguay_Standard_Time—The time zone will be Paraguay Standard Time (UTC-04:00).Atlantic_Standard_Time—The time zone will be Atlantic Standard Time (UTC-04:00).Venezuela_Standard_Time—The time zone will be Venezuela Standard Time (UTC-04:00).Central_Brazilian_Standard_Time—The time zone will be Central Brazilian Standard Time (UTC-04:00).SA_Western_Standard_Time—The time zone will be SA Western Standard Time (UTC-04:00).Pacific_SA_Standard_Time—The time zone will be Pacific SA Standard Time (UTC-04:00).Newfoundland_Standard_Time—The time zone will be Newfoundland Standard Time (UTC-03:30).Tocantins_Standard_Time—The time zone will be Tocantins Standard Time (UTC-03:00).E._South_America_Standard_Time—The time zone will be E. South America Standard Time (UTC-03:00).SA_Eastern_Standard_Time—The time zone will be SA Eastern Standard Time (UTC-03:00).Argentina_Standard_Time—The time zone will be Argentina Standard Time (UTC-03:00).Greenland_Standard_Time—The time zone will be Greenland Standard Time (UTC-03:00).Montevideo_Standard_Time—The time zone will be Montevideo Standard Time (UTC-03:00).Magallanes_Standard_Time—The time zone will be Magallanes Standard Time (UTC-03:00).Saint_Pierre_Standard_Time—The time zone will be Saint Pierre Standard Time (UTC-03:00).Bahia_Standard_Time—The time zone will be Bahia Standard Time (UTC-03:00).UTC-02—The time zone will be UTC-02 (UTC-02:00).Mid-Atlantic_Standard_Time—The time zone will be Mid-Atlantic Standard Time (UTC-02:00).Azores_Standard_Time—The time zone will be Azores Standard Time (UTC-01:00).Cape_Verde_Standard_Time—The time zone will be Cape Verde Standard Time (UTC-01:00).GMT_Standard_Time—The time zone will be GMT Standard Time (UTC+00:00).Greenwich_Standard_Time—The time zone will be Greenwich Standard Time (UTC+00:00).Sao_Tome_Standard_Time—The time zone will be Sao Tome Standard Time (UTC+00:00).Morocco_Standard_Time—The time zone will be Morocco Standard Time (UTC+00:00).W._Europe_Standard_Time—The time zone will be W. Europe Standard Time (UTC+01:00).Central_Europe_Standard_Time—The time zone will be Central Europe Standard Time (UTC+01:00).Romance_Standard_Time—The time zone will be Romance Standard Time (UTC+01:00).Central_European_Standard_Time—The time zone will be Central European Standard Time (UTC+01:00).W._Central_Africa_Standard_Time—The time zone will be W. Central Africa Standard Time (UTC+01:00).Jordan_Standard_Time—The time zone will be Jordan Standard Time (UTC+02:00).GTB_Standard_Time—The time zone will be GTB Standard Time (UTC+02:00).Middle_East_Standard_Time—The time zone will be Middle East Standard Time (UTC+02:00).Egypt_Standard_Time—The time zone will be Egypt Standard Time (UTC+02:00).E._Europe_Standard_Time—The time zone will be E. Europe Standard Time (UTC+02:00).Syria_Standard_Time—The time zone will be Syria Standard Time (UTC+02:00).West_Bank_Standard_Time—The time zone will be West Bank Standard Time (UTC+02:00).South_Africa_Standard_Time—The time zone will be South Africa Standard Time (UTC+02:00).FLE_Standard_Time—The time zone will be FLE Standard Time (UTC+02:00).Israel_Standard_Time—The time zone will be Israel Standard (UTC+02:00).South_Sudan_Standard_Time—The time zone will be South Sudan Standard Time (UTC+02:00).Kaliningrad_Standard_Time—The time zone will be Kaliningrad Standard Time (UTC+02:00).Sudan_Standard_Time—The time zone will be Sudan Standard Time (UTC+02:00).Libya_Standard_Time—The time zone will be Libya Standard Time (UTC+02:00).Namibia_Standard_Time—The time zone will be Namibia Standard Time (UTC+02:00).Arabic_Standard_Time—The time zone will be Arabic Standard Time (UTC+03:00).Turkey_Standard_Time—The time zone will be Turkey Standard Time (UTC+03:00).Arab_Standard_Time—The time zone will be Arab Standard Time (UTC+03:00).Belarus_Standard_Time—The time zone will be Belarus Standard Time (UTC+03:00).Russian_Standard_Time—The time zone will be Russian Standard Time (UTC+03:00).E._Africa_Standard_Time—The time zone will be E. Africa Standard Time (UTC+03:00).Volgograd_Standard_Time—The time zone will be Volgograd Standard Time (UTC+03:00).Iran_Standard_Time—The time zone will be Iran Standard Time (UTC+03:30).Arabian_Standard_Time—The time zone will be Arabian Standard Time (UTC+04:00).Astrakhan_Standard_Time—The time zone will be Astrakhan Standard Time (UTC+04:00).Azerbaijan_Standard_Time—The time zone will be Azerbaijan Standard Time (UTC+04:00).Russia_Time_Zone_3—The time zone will be Russia Time Zone 3 (UTC+04:00).Mauritius_Standard_Time—The time zone will be Mauritius Standard Time (UTC+04:00).Saratov_Standard_Time—The time zone will be Saratov Standard Time (UTC+04:00).Georgian_Standard_Time—The time zone will be Georgian Standard Time (UTC+04:00).Caucasus_Standard_Time—The time zone will be Caucasus Standard Time (UTC+04:00).Afghanistan_Standard_Time—The time zone will be Afghanistan Standard Time (UTC+04:30).West_Asia_Standard_Time—The time zone will be West Asia Standard Time (UTC+05:00).Ekaterinburg_Standard_Time—The time zone will be Ekaterinburg Standard Time (UTC+05:00).Pakistan_Standard_Time—The time zone will be Pakistan Standard Time (UTC+05:00).Qyzylorda_Standard_Time—The time zone will be Qyzylorda Standard Time (UTC+05:00).India_Standard_Time—The time zone will be India Standard Time (UTC+05:30).Sri_Lanka_Standard_Time—The time zone will be Sri Lanka Standard Time (UTC+05:30).Nepal_Standard_Time—The time zone will be Nepal Standard Time (UTC+05:45).Central_Asia_Standard_Time—The time zone will be Central Asia Standard Time (UTC+06:00).Bangladesh_Standard_Time—The time zone will be Bangladesh Standard Time (UTC+06:00).Omsk_Standard_Time—The time zone will be Omsk Standard Time (UTC+06:00).Myanmar_Standard_Time—The time zone will be Myanmar Standard Time (UTC+06:30).SE_Asia_Standard_Time—The time zone will be SE Asia Standard Time (UTC+07:00).Altai_Standard_Time—The time zone will be Altai Standard Time (UTC+07:00).W._Mongolia_Standard_Time—The time zone will be W. Mongolia Standard Time (UTC+07:00).North_Asia_Standard_Time—The time zone will be North Asia Standard Time (UTC+07:00).N._Central_Asia_Standard_Time—The time zone will be N. Central Asia Standard Time (UTC+07:00).Tomsk_Standard_Time—The time zone will be Tomsk Standard Time (UTC+07:00).China_Standard_Time—The time zone will be China Standard Time (UTC+08:00).North_Asia_East_Standard_Time—The time zone will be North Asia East Standard Time (UTC+08:00).Singapore_Standard_Time—The time zone will be Singapore Standard Time (UTC+08:00).W._Australia_Standard_Time—The time zone will be W. Australia Standard Time (UTC+08:00).Taipei_Standard_Time—The time zone will be Taipei Standard Time (UTC+08:00).Ulaanbaatar_Standard_Time—The time zone will be Ulaanbaatar Standard Time (UTC+08:00).Aus_Central_W._Standard_Time—The time zone will be Aus Central W. Standard Time (UTC+08:45).Transbaikal_Standard_Time—The time zone will be Transbaikal Standard Time (UTC+09:00).Tokyo_Standard_Time—The time zone will be Tokyo Standard Time (UTC+09:00).North_Korea_Standard_Time—The time zone will be North Korea Standard Time (UTC+09:00).Korea_Standard_Time—The time zone will be Korea Standard Time (UTC+09:00).Yakutsk_Standard_Time—The time zone will be Yakutsk Standard Time (UTC+09:00).Cen._Australia_Standard_Time—The time zone will be Cen. Australia Standard Time (UTC+09:30).AUS_Central_Standard_Time—The time zone will be AUS Central Standard Time (UTC+09:30).E._Australia_Standard_Time—The time zone will be E. Australia Standard Time (UTC+10:00).AUS_Eastern_Standard_Time—The time zone will be AUS Eastern Standard Time (UTC+10:00).West_Pacific_Standard_Time—The time zone will be West Pacific Standard Time (UTC+10:00).Tasmania_Standard_Time—The time zone will be Tasmania Standard Time (UTC+10:00).Vladivostok_Standard_Time—The time zone will be Vladivostok Standard Time (UTC+10:00).Lord_Howe_Standard_Time—The time zone will be Lord Howe Standard Time (UTC+10:30).Bougainville_Standard_Time—The time zone will be Bougainville Standard Time (UTC+11:00).Russia_Time_Zone_10—The time zone will be Russia Time Zone 10 (UTC+11:00).Magadan_Standard_Time—The time zone will be Magadan Standard Time (UTC+11:00).Norfolk_Standard_Time—The time zone will be Norfolk Standard Time (UTC+11:00).Sakhalin_Standard_Time—The time zone will be Sakhalin Standard Time (UTC+11:00).Central_Pacific_Standard_Time—The time zone will be Central Pacific Standard Time (UTC+11:00).Russia_Time_Zone_11—The time zone will be Russia Time Zone 11 (UTC+11:00).New_Zealand_Standard_Time—The time zone will be New Zealand Standard Time (UTC+12:00).UTC+12—The time zone will be UTC+12 (UTC+12:00).Fiji_Standard_Time—The time zone will be Fiji Standard Time (UTC+12:00).Kamchatka_Standard_Time—The time zone will be Kamchatka Standard Time (UTC+12:00).Chatham_Islands_Standard_Time—The time zone will be Chatham Islands Standard Time (UTC+12:45).UTC+13—The time zone will be UTC+13 (UTC+13:00).Tonga_Standard_Time—The time zone will be Tonga Standard Time (UTC+13:00).Samoa_Standard_Time—The time zone will be Samoa Standard Time (UTC+13:00).Line_Islands_Standard_Time—The time zone will be Line Islands Standard Time (UTC+14:00). | String |
| input_dst(Optional) | Specifies whether the time stamps were collected while observing Daylight Saving Time rules in the input time zone. When reading the time values to convert the time zone, the time values will be adjusted to account for the shift in time during Daylight Saving Time.By default, the input time values are adjusted to account for the shift in time due to the Daylight Saving Time rules in the input time zone.INPUT_ADJUSTED_FOR_DST—The input time values are adjusted for Daylight Saving Time.INPUT_NOT_ADJUSTED_FOR_DST—The input time values are not adjusted for Daylight Saving Time. | Boolean |
| output_dst(Optional) | Indicates whether the output time values will account for the shift in time due to the Daylight Saving Time rules observed in the output time zone. By default, the output time values are adjusted to account for the shift in time due to the Daylight Saving Time rules observed in the output time zone.OUTPUT_ADJUSTED_FOR_DST—The output time values will be adjusted for Daylight Saving Time in the output time zone.OUTPUT_NOT_ADJUSTED_FOR_DST—The output time values will not be adjusted for Daylight Saving Time in the output time zone. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ConvertTimeZone(in_table, input_time_field, input_time_zone, output_time_field, output_time_zone, {input_dst}, {output_dst})
```

### Example 2

```python
import arcpy
arcpy.ConvertTimeZone_management("C:/Data/TemporalData.gdb/InputData","Input_Time","Pacific_Standard_Time","Output_Time","Eastern_Standard_Time","INPUT_ADJUSTED_FOR_DST","OUTPUT_ADJUSTED_FOR_DST")
```

### Example 3

```python
import arcpy
arcpy.ConvertTimeZone_management("C:/Data/TemporalData.gdb/InputData","Input_Time","Pacific_Standard_Time","Output_Time","Eastern_Standard_Time","INPUT_ADJUSTED_FOR_DST","OUTPUT_ADJUSTED_FOR_DST")
```

### Example 4

```python
# Name: ConvertTimeZone_Ex02.py
# Description: Convert a time field to another time zone
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/TemporalData.gdb/InputData"
inputTimeField = "Input_Time"
inputTimeZone = "Pacific_Standard_Time"

outputTimeField = "Output_Time"
onputTimeZone = "Eastern_Standard_Time"
inputUseDaylightSaving = "INPUT_ADJUSTED_FOR_DST"
outputUseDaylightSaving = "OUTPUT_ADJUSTED_FOR_DST"

# Execute CalculateEndDate
arcpy.ConvertTimeZone_management(inTable, inputTimeField, inputTimeZone, outputTimeField, onputTimeZone, inputUseDaylightSaving, outputUseDaylightSaving)
```

### Example 5

```python
# Name: ConvertTimeZone_Ex02.py
# Description: Convert a time field to another time zone
# Requirements: None

# Import system modules
import arcpy

# Set local variables
inTable = "C:/Data/TemporalData.gdb/InputData"
inputTimeField = "Input_Time"
inputTimeZone = "Pacific_Standard_Time"

outputTimeField = "Output_Time"
onputTimeZone = "Eastern_Standard_Time"
inputUseDaylightSaving = "INPUT_ADJUSTED_FOR_DST"
outputUseDaylightSaving = "OUTPUT_ADJUSTED_FOR_DST"

# Execute CalculateEndDate
arcpy.ConvertTimeZone_management(inTable, inputTimeField, inputTimeZone, outputTimeField, onputTimeZone, inputUseDaylightSaving, outputUseDaylightSaving)
```

---

## Copy Features (Data Management)

## Summary

Copies features from the input feature class or layer to a new feature class.

## Usage

- If the input is a layer and has a selection, only the selected features are copied to the output feature class.
- Both the geometry and attributes of the Input Features will be copied to the output feature class.
- This tool can be used for data conversion as it can read many feature formats (any you can add to a map) and write these to shapefile or geodatabase.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The features to be copied. | Feature Layer |
| Output Feature Class | The feature class which will be created and to which the features will be copied. | Feature Class |
| Configuration Keyword(Optional) | Geodatabase configuration keyword to be applied if the output is a geodatabase. | String |
| Output Spatial Grid 1(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |
| Output Spatial Grid 2(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |
| Output Spatial Grid 3(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |
| in_features | The features to be copied. | Feature Layer |
| out_feature_class | The feature class which will be created and to which the features will be copied. | Feature Class |
| config_keyword(Optional) | Geodatabase configuration keyword to be applied if the output is a geodatabase. | String |
| spatial_grid_1(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |
| spatial_grid_2(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |
| spatial_grid_3(Optional) | This parameter has been deprecated in ArcGIS Pro. Any value you enter is ignored. | Double |

## Code Samples

### Example 1

```python
arcpy.management.CopyFeatures(in_features, out_feature_class, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("climate.shp", "C:/output/output.gdb/climate")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("climate.shp", "C:/output/output.gdb/climate")
```

### Example 4

```python
# Name: CopyFeatures_Example2.py
# Description: Convert all shapefiles in a folder to geodatabase feature classes
 
# Import system modules
import arcpy
import os
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
out_workspace = "c:/output/output.gdb"
 
# Use ListFeatureClasses to generate a list of shapefiles in the workspace 
# shown above.
fc_list = arcpy.ListFeatureClasses()
 
# Run CopyFeatures for each input shapefile
for shapefile in fc_list:
    # Determine the new output feature class path and name
    out_featureclass = os.path.join(out_workspace, 
                                    os.path.splitext(shapefile)[0])
    arcpy.management.CopyFeatures(shapefile, out_featureclass)
```

### Example 5

```python
# Name: CopyFeatures_Example2.py
# Description: Convert all shapefiles in a folder to geodatabase feature classes
 
# Import system modules
import arcpy
import os
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
out_workspace = "c:/output/output.gdb"
 
# Use ListFeatureClasses to generate a list of shapefiles in the workspace 
# shown above.
fc_list = arcpy.ListFeatureClasses()
 
# Run CopyFeatures for each input shapefile
for shapefile in fc_list:
    # Determine the new output feature class path and name
    out_featureclass = os.path.join(out_workspace, 
                                    os.path.splitext(shapefile)[0])
    arcpy.management.CopyFeatures(shapefile, out_featureclass)
```

---

## Copy Raster (Data Management)

## Summary

Saves a copy of a raster dataset or converts a mosaic dataset into a single raster dataset.

## Usage

- You can save the output to BIL, BIP, BMP, BSQ, COG, CRF, ENVI DAT, ERDAS IMAGINE, GIF, JPEG, JPEG 2000, MRF, NetCDF, PNG, TIFF, or Esri Grid format or to any geodatabase raster dataset.
- When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing the raster dataset in a file format, specify the file extension as follows: .bil for Esri BIL.bip for Esri BIP.bmp for BMP.bsq for Esri BSQ.crf for CRF.dat for ENVI DAT.img for ERDAS IMAGINE.gif for GIF.jpg for JPEG.jp2 for JPEG 2000.mrf for MRF.nc for NetCDF.png for PNG.tif for TIFF and Cloud Optimized GeoTIFF.zarr for ZarrNo extension for Esri Grid
- .bil for Esri BIL
- .bip for Esri BIP
- .bmp for BMP
- .bsq for Esri BSQ
- .crf for CRF
- .dat for ENVI DAT
- .img for ERDAS IMAGINE
- .gif for GIF
- .jpg for JPEG
- .jp2 for JPEG 2000
- .mrf for MRF
- .nc for NetCDF
- .png for PNG
- .tif for TIFF and Cloud Optimized GeoTIFF
- .zarr for Zarr
- No extension for Esri Grid
- This tool can be used to scale the pixel type from one bit depth to another. When you scale the pixel depth, the raster will display the same, but the values will be scaled to the new bit depth that was specified.
- The output of this tool is always a raster dataset. This tool will accept a mosaic dataset as the input, but the output will be a raster dataset; the contents of the mosaic dataset will be mosaicked to create a raster dataset.
- If you check Use world file to define the coordinates of the raster in the Raster and Imagery options, a world file will be written out. If a world file exists, it will be overwritten. There may also be a half-pixel shift in the output spatial reference.
- For file-based rasters, the Ignore Background Value parameter must be set to the same value as the NoData Value parameter for the background value to be ignored. File geodatabase rasters and enterprise geodatabase rasters will work without this extra step.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- The GIF format only supports single-band raster datasets.
- Once created, an .mrf file cannot be copied, renamed, or deleted in ArcGIS Pro. An .mrf file can point to data that resides elsewhere, and determining the read or write status of the files is not always possible.
- The Pixel Type parameter determines the bit depth of the output raster dataset. Rescaling of the raster values occurs when a different pixel type is chosen. If the pixel type is demoted (lowered), the raster values outside the valid range for that pixel depth will be truncated and lost. To learn about the bit depth capacity for supported export formats, see List of supported sensors.
- The creation of a Cloud Optimized GeoTIFF (COG) format file from any raster by the tool is a two-step process as it involves the proper ordering of the GeoTIFF internal structure with COG specifications. The first step is to generate a temporary GeoTIFF format file from the input raster with pyramids, statistics, and other metadata. This temporary GeoTIFF format file is then copied to generate a COG format file.The disk requirements for COG format generation will be at least twice the space of the source image, which also depends on the compression of the input raster and the output COG format file.
- The type of NetCDF format supported for output follows the NetCDF Climate and Forecast (CF) metadata conventions.
- NetCDF format only supports single-band data. For multidimensional and multiband data, such as satellite data, the first band will be used.
- The Build Multidimensional Transpose parameter is for data access optimization. By default, a .crf file stores each multidimensional slice in a separate folder and each slice is chunked into tiles. When you perform a transpose, the data will be chunked along dimensions rather than by slice and tile, making analysis such as temporal profiling faster.
- When you specify an extent outside the boundaries of the raster dataset, the area that does not contain data is filled with NoData values.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset or mosaic dataset to be copied. | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; File; Image Service |
| Output Raster Dataset | The name and format for the raster dataset being created..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.crf—CRF.dat—ENVI DAT.img—ERDAS IMAGINE.gif—GIF.jpg—JPEG.jp2—JPEG 2000.mrf—MRF.nc—NetCDF.png—PNG.tif—TIFF and Cloud Optimized GeoTIFF.zarr—ZarrNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to JPEG, JPEG 2000, or TIFF format, or to a geodatabase, you can specify a compression type and compression quality. | Raster Dataset |
| Configuration Keyword (Optional) | The storage parameters (configuration) for a geodatabase. Configuration keywords are set up by your database administrator. | String |
| Ignore Background Value (Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset.For file-based rasters, Ignore Background Value must be set to the same value as NoData Value for the background value to be ignored. Enterprise and geodatabase rasters will work without this extra step. | Double |
| NoData Value (Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | String |
| Convert 1 bit data to 8 bit (Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.Unchecked—No conversion will occur. This is the default.Checked—The input raster will be converted. | Boolean |
| Colormap to RGB (Optional) | Specifies whether the input raster dataset will be converted to a three-band output raster dataset if the input raster dataset includes a color map. This is useful when mosaicking rasters with different color maps.Unchecked—No conversion will occur. This is the default.Checked—The input dataset will be converted. | Boolean |
| Pixel Type(Optional) | Specifies the bit depth, or radiometric resolution, that will be used for the raster or mosaic dataset. If not defined, the value from the first raster dataset will be used.1 bit—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2 bit—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4 bit—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8 bit unsigned—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8 bit signed—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16 bit unsigned—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16 bit signed—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32 bit unsigned—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32 bit signed—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32 bit float—The pixel type will be a 32-bit data type supporting decimals.64 bit—The pixel type will be a 64-bit data type supporting decimals. | String |
| Scale Pixel Value (Optional) | Specifies whether pixel values will be scaled. When the output is a pixel type other than the input (such as 16 bit to 8 bit), you can scale the values to fit into the new range; otherwise, the values that do not fit into the new pixel range will be discarded.If scaling up, such as 8 bit to 16 bit, the minimum and maximum of the 8-bit values will be scaled to the minimum and maximum in the 16-bit range. If scaling down, such as 16 bit to 8 bit, the minimum and maximum of the 16-bit values will be scaled to the minimum and maximum in the 8-bit range.Unchecked—The pixel values will remain the same and will not be scaled. Any values that do not fit within the value range will be discarded. This is the default.Checked—The pixel values will be scaled to the new pixel type. When you scale the pixel depth, the raster will display the same, but the values will be scaled to the new bit depth that was specified. | Boolean |
| RGB To Colormap (Optional) | Specifies whether an 8-bit, 3-band (RGB) raster dataset will be converted to a single-band raster dataset with a color map. This operation suppresses noise that is often found in scanned images and is ideal for screen captures, scanned maps, or scanned documents. This is not recommended for satellite or aerial imagery or thematic raster data.Unchecked—The RGB raster dataset will not be converted.Checked—The RGB raster dataset will be converted to a color map. | Boolean |
| Format (Optional) | Specifies the output raster format.TIFF format—The output format will be TIFF.Cloud Optimized GeoTIFF—The output format will be Cloud Optimized GeoTIFF.ERDAS IMAGINE format—The output format will be ERDAS IMAGINE.BMP format—The output format will be BMP.GIF format—The output format will be GIF.PNG format—The output format will be PNG.JPEG format—The output format will be JPEG.JPEG 2000 format—The output format will be JPEG 2000.Esri Grid format—The output format will be Esri Grid.Esri BIL format—The output format will be Esri BIL.Esri BSQ format—The output format will be Esri BSQ.Esri BIP format—The output format will be Esri BIP.ENVI DAT format—The output format will be ENVI DAT.Cloud raster format—The output format will be CRF.Meta raster format—The output format will be MRF.NetCDF format—The output format will be NetCDF.Zarr format—The output format will be Zarr. | String |
| Apply Transformation (Optional) | Specifies whether a transformation associated with the input raster will be applied to the output. The input raster can have a transformation associated with it that is not saved in the input, such as a world file or a geometric function. Unchecked—No associated transformation will be applied to the output.Checked—Any associated transformation will be applied to the output.Do not apply transformation—No associated transformation will be applied to the output.Apply transformation—Any associated transformation will be applied to the output. | Boolean |
| Process as Multidimensional (Optional) | Specifies whether the input mosaic dataset will be processed as a multidimensional raster dataset.Unchecked—The input will not be processed as a multidimensional raster dataset. If the input is multidimensional, only the slice that is currently displayed will be processed. This is the default. Checked—The input will be processed as a multidimensional raster dataset and all slices will be processed to produce a new multidimensional raster dataset. Set the Format parameter to Cloud raster format to use this option. | Boolean |
| Build Multidimensional Transpose(Optional) | Specifies whether the transpose for the input multidimensional raster dataset will be built to optimize data access. The transpose will chunk the multidimensional data along each dimension to optimize performance when accessing pixel values across all slices.Unchecked—No transpose will be built. This is the default. Checked—The input multidimensional raster dataset will be transposed. The Process as multidimensional parameter must be checked to use this option. | Boolean |
| in_raster | The raster dataset or mosaic dataset to be copied. | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; File; Image Service |
| out_rasterdataset | The name and format for the raster dataset being created..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.crf—CRF.dat—ENVI DAT.img—ERDAS IMAGINE.gif—GIF.jpg—JPEG.jp2—JPEG 2000.mrf—MRF.nc—NetCDF.png—PNG.tif—TIFF and Cloud Optimized GeoTIFF.zarr—ZarrNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to JPEG, JPEG 2000, or TIFF format, or to a geodatabase, you can specify a compression type and compression quality. | Raster Dataset |
| config_keyword(Optional) | The storage parameters (configuration) for a geodatabase. Configuration keywords are set up by your database administrator. | String |
| background_value(Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset.For file-based rasters, Ignore Background Value must be set to the same value as NoData Value for the background value to be ignored. Enterprise and geodatabase rasters will work without this extra step. | Double |
| nodata_value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | String |
| onebit_to_eightbit(Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.NONE—No conversion will occur. This is the default.OneBitTo8Bit—The input raster will be converted. | Boolean |
| colormap_to_RGB(Optional) | Specifies whether the input raster dataset will be converted to a three-band output raster dataset if the input raster dataset includes a color map. This is useful when mosaicking rasters with different color maps.NONE—No conversion will occur. This is the default.ColormapToRGB—The input dataset will be converted. | Boolean |
| pixel_type(Optional) | Specifies the bit depth, or radiometric resolution, that will be used for the raster or mosaic dataset. If not defined, the value from the first raster dataset will be used.1_BIT—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2_BIT—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4_BIT—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8_BIT_UNSIGNED—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8_BIT_SIGNED—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16_BIT_UNSIGNED—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16_BIT_SIGNED—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32_BIT_UNSIGNED—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32_BIT_SIGNED—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32_BIT_FLOAT—The pixel type will be a 32-bit data type supporting decimals.64_BIT—The pixel type will be a 64-bit data type supporting decimals. | String |
| scale_pixel_value(Optional) | Specifies whether pixel values will be scaled. When the output is a pixel type other than the input (such as 16 bit to 8 bit), you can scale the values to fit into the new range; otherwise, the values that do not fit into the new pixel range will be discarded. If scaling up, such as 8 bit to 16 bit, the minimum and maximum of the 8-bit values will be scaled to the minimum and maximum in the 16-bit range. If scaling down, such as 16 bit to 8 bit, the minimum and maximum of the 16-bit values will be scaled to the minimum and maximum in the 8-bit range.NONE—The pixel values will remain the same and will not be scaled. Any values that do not fit within the value range will be discarded. This is the default.ScalePixelValue—The pixel values will be scaled to the new pixel type. When you scale the pixel depth, the raster will display the same, but the values will be scaled to the new bit depth that was specified. | Boolean |
| RGB_to_Colormap(Optional) | Specifies whether an 8-bit, 3-band (RGB) raster dataset will be converted to a single-band raster dataset with a color map. This operation suppresses noise that is often found in scanned images and is ideal for screen captures, scanned maps, or scanned documents. This is not recommended for satellite or aerial imagery or thematic raster data.NONE—The RGB raster dataset will not be converted.RGBToColormap—The RGB raster dataset will be converted to a color map. | Boolean |
| format(Optional) | Specifies the output raster format.TIFF—The output format will be TIFF.COG—The output format will be Cloud Optimized GeoTIFF.IMAGINE Image—The output format will be ERDAS IMAGINE.BMP—The output format will be BMP.GIF—The output format will be GIF.PNG—The output format will be PNG.JPEG—The output format will be JPEG.JP2—The output format will be JPEG 2000.GRID—The output format will be Esri Grid.BIL—The output format will be Esri BIL.BSQ—The output format will be Esri BSQ.BIP—The output format will be Esri BIP.ENVI—The output format will be ENVI DAT.CRF—The output format will be CRF.MRF—The output format will be MRF.NetCDF—The output format will be NetCDF.ZARR—The output format will be Zarr. | String |
| transform(Optional) | Specifies whether a transformation associated with the input raster will be applied to the output. The input raster can have a transformation associated with it that is not saved in the input, such as a world file or a geometric function. NONE—No associated transformation will be applied to the output.Transform—Any associated transformation will be applied to the output. | Boolean |
| process_as_multidimensional(Optional) | Specifies whether the input mosaic dataset will be processed as a multidimensional raster dataset.CURRENT_SLICE—The input will not be processed as a multidimensional raster dataset. If the input is multidimensional, only the slice that is currently displayed will be processed. This is the default.ALL_SLICES—The input will be processed as a multidimensional raster dataset and all slices will be processed to produce a new multidimensional raster dataset. Set the format parameter to CRF to use this option. | Boolean |
| build_multidimensional_transpose(Optional) | Specifies whether the transpose for the input multidimensional raster dataset will be built, which will chunk the data along each dimension to optimize performance when accessing pixel values across all slices.NO_TRANSPOSE—No transpose will be built. This is the default.TRANSPOSE—The input multidimensional raster dataset will be transposed. The process_as_multidimensional parameter must be set to ALL_SLICES to use this option. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CopyRaster(in_raster, out_rasterdataset, {config_keyword}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {colormap_to_RGB}, {pixel_type}, {scale_pixel_value}, {RGB_to_Colormap}, {format}, {transform}, {process_as_multidimensional}, {build_multidimensional_transpose})
```

### Example 2

```python
##====================================
##Copy Raster
##Usage: CopyRaster_management(
##			in_raster, out_rasterdataset, {config_keyword}, {background_value}, 
##			{nodata_value}, {NONE | OneBitTo8Bit}, {NONE | ColormapToRGB}, 
##			{1_BIT | 2_BIT | 4_BIT | 8_BIT_UNSIGNED | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 
##			16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT | 64_BIT}, 
##			{NONE | ScalePixelValue}, {NONE | RGBToColormap}, {TIFF | IMAGINE Image | 
##			BMP | GIF | PNG | JPEG | JPEG2000 | Esri Grid | Esri BIL | Esri BSQ | 
##			Esri BIP | ENVI | CRF | MRF}, {NONE | Transform}, {CURRENT_SLICE | ALL_SLICES}, {NO_TRANSPOSE | TRANSPOSE})


try:
    import arcpy
    arcpy.env.workspace = r"C:\PrjWorkspace"
    
    ##Copy Multidimensional Raster Dataset to a new multidimensional dataset in Cloud raster format and with transpose for faster data access
    arcpy.management.CopyRaster('SeaSurfaceTemp.nc',"https://s3.amazonaws.com/S3Storage/seasurfacetemp","","","","","","","","", format = "CRF",'NONE',process_as_multidimensional = 'ALL_SLICES', build_multidimensional_transpose='TRANSPOSE')
    
    ##Copy 1 BIT 
    arcpy.management.CopyRaster("1bit.tif","SDE94.sde\\bit8","DEFAULTS","","","OneBitTo8Bit","","")
    
    ##Copy File RasterDataset to GDB Dataset with Background and Nodata setting
    arcpy.management.CopyRaster("background.tif","CpRaster.gdb\\background","DEFAULTS","0","9","","","8_BIT_UNSIGNED")
except:
    print "Copy Raster example failed."
    print arcpy.GetMessages()
```

### Example 3

```python
##====================================
##Copy Raster
##Usage: CopyRaster_management(
##			in_raster, out_rasterdataset, {config_keyword}, {background_value}, 
##			{nodata_value}, {NONE | OneBitTo8Bit}, {NONE | ColormapToRGB}, 
##			{1_BIT | 2_BIT | 4_BIT | 8_BIT_UNSIGNED | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 
##			16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT | 64_BIT}, 
##			{NONE | ScalePixelValue}, {NONE | RGBToColormap}, {TIFF | IMAGINE Image | 
##			BMP | GIF | PNG | JPEG | JPEG2000 | Esri Grid | Esri BIL | Esri BSQ | 
##			Esri BIP | ENVI | CRF | MRF}, {NONE | Transform}, {CURRENT_SLICE | ALL_SLICES}, {NO_TRANSPOSE | TRANSPOSE})


try:
    import arcpy
    arcpy.env.workspace = r"C:\PrjWorkspace"
    
    ##Copy Multidimensional Raster Dataset to a new multidimensional dataset in Cloud raster format and with transpose for faster data access
    arcpy.management.CopyRaster('SeaSurfaceTemp.nc',"https://s3.amazonaws.com/S3Storage/seasurfacetemp","","","","","","","","", format = "CRF",'NONE',process_as_multidimensional = 'ALL_SLICES', build_multidimensional_transpose='TRANSPOSE')
    
    ##Copy 1 BIT 
    arcpy.management.CopyRaster("1bit.tif","SDE94.sde\\bit8","DEFAULTS","","","OneBitTo8Bit","","")
    
    ##Copy File RasterDataset to GDB Dataset with Background and Nodata setting
    arcpy.management.CopyRaster("background.tif","CpRaster.gdb\\background","DEFAULTS","0","9","","","8_BIT_UNSIGNED")
except:
    print "Copy Raster example failed."
    print arcpy.GetMessages()
```

### Example 4

```python
##====================================
##Usage: CopyRaster_management(
##			in_raster, out_rasterdataset, {config_keyword}, {background_value}, 
##			{nodata_value}, {NONE | OneBitTo8Bit}, {NONE | ColormapToRGB}, 
##			{1_BIT | 2_BIT | 4_BIT | 8_BIT_UNSIGNED | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 
##			16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT | 64_BIT}, 
##			{NONE | ScalePixelValue}, {NONE | RGBToColormap}, {TIFF | IMAGINE Image | 
##			BMP | GIF | PNG | JPEG | JPEG2000 | Esri Grid | Esri BIL | Esri BSQ | 
##			Esri BIP | ENVI | CRF | MRF}, {NONE | Transform}, {CURRENT_SLICE | ALL_SLICES}, {NO_TRANSPOSE | TRANSPOSE})

import arcpy
arcpy.env.workspace = r"C:\PrjWorkspace"

##Copy to new multidimensional dataset in cloud raster format and with transpose for faster data access
arcpy.management.CopyRaster(
	"SeaSurfaceTemp.nc", "SST_Transpose.crf","","",-3.402823e+38,"NONE","NONE","","NONE","NONE", "CRF", "NONE", "ALL_SLICES", "TRANSPOSE")
```

### Example 5

```python
##====================================
##Usage: CopyRaster_management(
##			in_raster, out_rasterdataset, {config_keyword}, {background_value}, 
##			{nodata_value}, {NONE | OneBitTo8Bit}, {NONE | ColormapToRGB}, 
##			{1_BIT | 2_BIT | 4_BIT | 8_BIT_UNSIGNED | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 
##			16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT | 64_BIT}, 
##			{NONE | ScalePixelValue}, {NONE | RGBToColormap}, {TIFF | IMAGINE Image | 
##			BMP | GIF | PNG | JPEG | JPEG2000 | Esri Grid | Esri BIL | Esri BSQ | 
##			Esri BIP | ENVI | CRF | MRF}, {NONE | Transform}, {CURRENT_SLICE | ALL_SLICES}, {NO_TRANSPOSE | TRANSPOSE})

import arcpy
arcpy.env.workspace = r"C:\PrjWorkspace"

##Copy to new multidimensional dataset in cloud raster format and with transpose for faster data access
arcpy.management.CopyRaster(
	"SeaSurfaceTemp.nc", "SST_Transpose.crf","","",-3.402823e+38,"NONE","NONE","","NONE","NONE", "CRF", "NONE", "ALL_SLICES", "TRANSPOSE")
```

---

## Copy Rows (Data Management)

## Summary

Copies the rows of a table to a different table.

## Usage

- The tool copies the rows of a table, table view, feature class, feature layer, delimited file, or raster with an attribute table to a new geodatabase or dBASE table or a delimited file.
- This tool supports the following table formats as input: GeodatabasedBASE (.dbf)Microsoft Excel worksheets (.xls and .xlsx)Memory-based tablesDelimited filesComma-delimited files (.csv, .txt, and .asc)Tab-delimited files (.tsv and .tab)Pipe-delimited files (.psv)For delimited files, the first row of the input file is used as the field names on the output table. Field names cannot contain spaces or special characters (such as $ or *), and an error will occur if the first row of the input file contains spaces or special characters.
- Geodatabase
- dBASE (.dbf)
- Microsoft Excel worksheets (.xls and .xlsx)
- Memory-based tables
- Delimited filesComma-delimited files (.csv, .txt, and .asc)Tab-delimited files (.tsv and .tab)Pipe-delimited files (.psv)
- Comma-delimited files (.csv, .txt, and .asc)
- Tab-delimited files (.tsv and .tab)
- Pipe-delimited files (.psv)
- The tool can be used to output a delimited file by adding one of the following file extensions to the output name in a folder workspace:Comma-delimited files (.csv, .txt, or .asc)Tab-delimited files (.tsv or .tab)Pipe-delimited files (.psv)
- Comma-delimited files (.csv, .txt, or .asc)
- Tab-delimited files (.tsv or .tab)
- Pipe-delimited files (.psv)
- If the input is a table view or feature layer and has a selection, only the selected rows will be copied to the output table.
- All rows will be copied if the input is a feature class or table. If the input rows are from a layer or table view that has a selection, only the selected features or rows will be used.
- If the input rows are a feature class, only the attributes, not the geometry, will be copied to the output table.
- To add or append the copied rows to an existing table, use the Append tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rows | The input rows to be copied to a new table. | Table View; Raster Layer |
| Output Table | The table that will be created and to which rows from the input will be copied. If the output table is in a folder, include an extension such as .csv, .txt, or .dbf to make the table the specified format. If the output table is in a geodatabase, do not specify an extension. | Table |
| Configuration Keyword(Optional) | The default storage parameters for an enterprise geodatabase. | String |
| in_rows | The input rows to be copied to a new table. | Table View; Raster Layer |
| out_table | The table that will be created and to which rows from the input will be copied. If the output table is in a folder, include an extension such as .csv, .txt, or .dbf to make the table the specified format. If the output table is in a geodatabase, do not specify an extension. | Table |
| config_keyword(Optional) | The default storage parameters for an enterprise geodatabase. | String |

## Code Samples

### Example 1

```python
arcpy.management.CopyRows(in_rows, out_table, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyRows("vegtable.dbf", "C:/output/output.gdb/vegtable")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyRows("vegtable.dbf", "C:/output/output.gdb/vegtable")
```

### Example 4

```python
# Description: Convert all dBASE tables in a folder to geodatabase tables
# Requirement: os module

# Import system modules
import arcpy
import os
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
outWorkspace = "c:/output/output.gdb"
 
# Use ListTables to generate a list of dBASE tables in the
#  workspace shown above.
tableList = arcpy.ListTables()
 
# Run CopyRows for each input table
for dbaseTable in tableList:
    # Determine the new output feature class path and name
    outTable = os.path.join(outWorkspace, os.path.splitext(dbaseTable)[0])
    arcpy.management.CopyRows(dbaseTable, outTable)
```

### Example 5

```python
# Description: Convert all dBASE tables in a folder to geodatabase tables
# Requirement: os module

# Import system modules
import arcpy
import os
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
outWorkspace = "c:/output/output.gdb"
 
# Use ListTables to generate a list of dBASE tables in the
#  workspace shown above.
tableList = arcpy.ListTables()
 
# Run CopyRows for each input table
for dbaseTable in tableList:
    # Determine the new output feature class path and name
    outTable = os.path.join(outWorkspace, os.path.splitext(dbaseTable)[0])
    arcpy.management.CopyRows(dbaseTable, outTable)
```

---

## Copy (Data Management)

## Summary

Copies the input data to an output workspace of the same data type as the input workspace.

## Usage

- This tool copies only between workspaces of the same data type: from a folder to a folder and from any geodatabase type to any geodatabase type (file, enterprise, or mobile). To copy data between different workspace types, use the Copy Features tool or other tools in the Conversion toolbox.This tool does not support copying to or from a memory workspace.
- If a feature class is copied to a feature dataset, the spatial reference of the feature class and the feature dataset must match; otherwise, the tool will fail.
- Any data that is dependent on the input is also copied. For example, copying a feature class or table that is part of a relationship class also copies the relationship class. This also applies to a feature class that has feature-linked annotation, domains, subtypes, and indices—all are copied along with the feature class. Copying geometric networks, network datasets, and topologies also copies the participating feature classes.
- This tool does not copy layers, since a layer is only a reference to a feature class.
- Copying a mosaic dataset copies the mosaic dataset to the designated location; the images referenced by the mosaic dataset are not copied.
- This tool does not support copying a network dataset back into the same geodatabase.
- This tool does not support the MRF image format. To copy an .mrf file, use the Copy Raster tool.
- When the input has associated data, the Associated Data parameter value is displayed so the associated output data's name and config keyword can be controlled.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data | The data that will be copied. | Data Element |
| Output Data | The location and name of the output data. | Data Element |
| Data type(Optional) | The type of the data on disk that will be copied. This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |
| Associated Data (Optional) | When the input has associated data, this parameter can be used to control the associated output data's name and config keyword.From Name—The data associated with the input data, which will also be copied.Data Type—The type of the data on disk that will be copied. The only time you need to provide a value is when a geodatabase contains a feature dataset and a feature class with the same name. In this case, select the correct data type, FeatureDataset or FeatureClass, of the item you want to copy.To Name—The name of the copied data in the Output Data parameter value.Config Keyword—The geodatabase storage parameters (configuration).The From Name and To Name column names will be identical if the To Name value is not used in the Output Data parameter value. If the name exists in the Output Data value, a unique To Name value will be automatically created by appending an underscore and a number (for example, rivers_1) to the From Name value. | Value Table |
| in_data | The data that will be copied. | Data Element |
| out_data | The location and name of the output data. The file name extension of the output data must match the extension of the input data. For example, if you are copying a file geodatabase, the output data element must have .gdb as a suffix. | Data Element |
| data_type(Optional) | The type of the data on disk that will be copied. This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |
| associated_data[[from_name, data_type, to_name, config_keyword],...](Optional) | When the input has associated data, this parameter can be used to control the associated output data's name and config keyword.from_name—The data associated with the input data, which will also be copied. data_type—The type of the data on disk that will be copied. The only time you need to provide a value is when a geodatabase contains a feature dataset and a feature class with the same name. In this case, use the correct data type, FeatureDataset or FeatureClass, of the item you want to copy. to_name—The name of the copied data in the out_data parameter value.config_keyword—The geodatabase storage parameters (configuration). The from_name and to_name column names will be identical if the to_name value is not used in the out_data parameter value. If the name exists in the out_data value, a unique to_name value will be created by appending an underscore plus a number (for example, rivers_1) to the from_name value. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.Copy(in_data, out_data, {data_type}, {associated_data})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Copy("majorrds.shp", "C:/output/majorrdsCopy.shp")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Copy("majorrds.shp", "C:/output/majorrdsCopy.shp")
```

### Example 4

```python
# Name: Copy_Example2.py
# Description: Copy major roads dataset to preserve the original data

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_data =  "majorrds.shp"
out_data = "C:/output/majorrdsCopy.shp"

# Run Copy
arcpy.management.Copy(in_data, out_data)
```

### Example 5

```python
# Name: Copy_Example2.py
# Description: Copy major roads dataset to preserve the original data

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_data =  "majorrds.shp"
out_data = "C:/output/majorrdsCopy.shp"

# Run Copy
arcpy.management.Copy(in_data, out_data)
```

### Example 6

```python
# Name: Copy_Example3.py
# Description: Copy a feature dataset and specify associated_data

# Import system modules
import arcpy

# The input is a feature dataset containing 3 feature classes: lakes, cities, rivers
in_data =  "C:/data/proj.gdb/mexico" 
out_data = "C:/data/proj.sde/mexico"

associated_data = ";".join(["lakes FeatureClass mexico_lakes #",
                            "cities FeatureClass mexico_cities #",
                            "rivers FeatureClass mexico_rivers #"])

# Rename each feature class during the copy operation using the associated_data parameter
arcpy.management.Copy(in_data, out_data, associated_data=associated_data)
```

### Example 7

```python
# Name: Copy_Example3.py
# Description: Copy a feature dataset and specify associated_data

# Import system modules
import arcpy

# The input is a feature dataset containing 3 feature classes: lakes, cities, rivers
in_data =  "C:/data/proj.gdb/mexico" 
out_data = "C:/data/proj.sde/mexico"

associated_data = ";".join(["lakes FeatureClass mexico_lakes #",
                            "cities FeatureClass mexico_cities #",
                            "rivers FeatureClass mexico_rivers #"])

# Rename each feature class during the copy operation using the associated_data parameter
arcpy.management.Copy(in_data, out_data, associated_data=associated_data)
```

### Example 8

```python
import arcpy
arcpy.management.Copy( 
   in_data=r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Mexico", 
   out_data=r"C:\Users\GIS\SQLServerDatabase.sde\DBO.PyCmd_Mexico", 
   data_type="FeatureDataset", 
   associated_data="DBO.Rivers FeatureClass DBO.PyCmd_Rivers #;DBO.Lakes FeatureClass DBO.PyCmd_Lakes #;DBO.Cities FeatureClass DBO.PyCmd_Cities #" 
)
```

### Example 9

```python
import arcpy
arcpy.management.Copy( 
   in_data=r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Mexico", 
   out_data=r"C:\Users\GIS\SQLServerDatabase.sde\DBO.PyCmd_Mexico", 
   data_type="FeatureDataset", 
   associated_data="DBO.Rivers FeatureClass DBO.PyCmd_Rivers #;DBO.Lakes FeatureClass DBO.PyCmd_Lakes #;DBO.Cities FeatureClass DBO.PyCmd_Cities #" 
)
```

### Example 10

```python
# Description: Copy a feature dataset and specify associated_data within an
#              Enterprise geodatabase environment 

# Import system modules
import arcpy

# The input is a feature dataset containing 3 feature classes: lakes, cities,
# rivers.
in_data = r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Mexico"

# The output is a new feature dataset that the feature classes from in_data will
# be copied to
out_data = r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Py_Mexico"

# Define schema of the from_name and to_name values when preparing data to be
# created in an enterprise geodatabase
associated_data = ";".join(["DBO.Lakes FeatureClass DBO.Py_Lakes #",
                            "DBO.Cities FeatureClass DBO.Py_Cities #",
                            "DBO.Rivers FeatureClass DBO.Py_Rivers #"])

# Rename each feature class during the copy operation using the associated_data
# parameter
arcpy.management.Copy(in_data, out_data, associated_data=associated_data)
```

### Example 11

```python
# Description: Copy a feature dataset and specify associated_data within an
#              Enterprise geodatabase environment 

# Import system modules
import arcpy

# The input is a feature dataset containing 3 feature classes: lakes, cities,
# rivers.
in_data = r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Mexico"

# The output is a new feature dataset that the feature classes from in_data will
# be copied to
out_data = r"C:\Users\GIS\SQLServerDatabase.sde\DBO.Py_Mexico"

# Define schema of the from_name and to_name values when preparing data to be
# created in an enterprise geodatabase
associated_data = ";".join(["DBO.Lakes FeatureClass DBO.Py_Lakes #",
                            "DBO.Cities FeatureClass DBO.Py_Cities #",
                            "DBO.Rivers FeatureClass DBO.Py_Rivers #"])

# Rename each feature class during the copy operation using the associated_data
# parameter
arcpy.management.Copy(in_data, out_data, associated_data=associated_data)
```

---

## Create 3D Object Scene Layer Content (Data Management)

## Summary

Creates a scene layer package (.slpk) or scene layer content (.i3sREST) from a multipatch or 3D object feature layer input.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input multipatch or 3D object feature layer. | Layer File; Feature Layer |
| Output Scene Layer Package | The output scene layer package (.slpk). | File |
| Output Coordinate System(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| Geographic Transformation | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| Target Cloud Connection (Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| in_dataset | The input multipatch or 3D object feature layer. | Layer File; Feature Layer |
| out_slpk | The output scene layer package (.slpk). | File |
| out_coor_system(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| transform_method[transform_method,...] | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the DESKTOP or NONE options. ALL—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.DESKTOP—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.MOBILE—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.NONE—JPEG textures will be optimized for use in desktop and web platforms. | String |
| target_cloud_connection(Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.Create3DObjectSceneLayerPackage(in_dataset, out_slpk, {out_coor_system}, transform_method, {texture_optimization}, {target_cloud_connection})
```

### Example 2

```python
import arcpy
arcpy.management.Create3DObjectSceneLayerPackage(
    r'c:\temp\buildings.lyrx', r'c:\temp\output.slpk', 
    arcpy.SpatialReference(4326), None, 'DESKTOP')
```

### Example 3

```python
import arcpy
arcpy.management.Create3DObjectSceneLayerPackage(
    r'c:\temp\buildings.lyrx', r'c:\temp\output.slpk', 
    arcpy.SpatialReference(4326), None, 'DESKTOP')
```

### Example 4

```python
import arcpy
arcpy.management.Create3DObjectSceneLayerPackage(
    r'c:\temp\buildings.lyrx', None, arcpy.SpatialReference(4326), None, 
    'DESKTOP', r'c:\cloudConnections\AWS.acs')
```

### Example 5

```python
import arcpy
arcpy.management.Create3DObjectSceneLayerPackage(
    r'c:\temp\buildings.lyrx', None, arcpy.SpatialReference(4326), None, 
    'DESKTOP', r'c:\cloudConnections\AWS.acs')
```

---

## Create Building Scene Layer Content (Data Management)

## Summary

Creates a scene layer package (.slpk) or scene layer content (.i3sREST) from a building layer input.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input building layer or layer file (.lyrx). | Building Layer; Layer File |
| Output Scene Layer Package | The output scene layer package (.slpk). | File |
| Output Coordinate System(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| Geographic Transformation(Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note: The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| Target Cloud Connection (Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| in_dataset | The input building layer or layer file (.lyrx). | Building Layer; Layer File |
| out_slpk | The output scene layer package (.slpk). | File |
| out_coor_system(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| transform_method[transform_method,...](Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note: The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the DESKTOP or NONE options. ALL—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.DESKTOP—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.MOBILE—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.NONE—JPEG textures will be optimized for use in desktop and web platforms. | String |
| target_cloud_connection(Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.CreateBuildingSceneLayerPackage(in_dataset, out_slpk, {out_coor_system}, {transform_method}, {texture_optimization}, {target_cloud_connection})
```

### Example 2

```python
import arcpy
arcpy.management.CreateBuildingSceneLayerPackage(
    r'c:\temp\buildings.lyrx', r'c:\temp\output.slpk', 
    arcpy.SpatialReference(4326), 'DESKTOP')
```

### Example 3

```python
import arcpy
arcpy.management.CreateBuildingSceneLayerPackage(
    r'c:\temp\buildings.lyrx', r'c:\temp\output.slpk', 
    arcpy.SpatialReference(4326), 'DESKTOP')
```

### Example 4

```python
import arcpy
arcpy.management.CreateBuildingSceneLayerPackage(
    r'c:\temp\buildings.lyrx', None, arcpy.SpatialReference(4326),
    'DESKTOP', r'c:\cloudConnections\AWS.acs')
```

### Example 5

```python
import arcpy
arcpy.management.CreateBuildingSceneLayerPackage(
    r'c:\temp\buildings.lyrx', None, arcpy.SpatialReference(4326),
    'DESKTOP', r'c:\cloudConnections\AWS.acs')
```

---

## Create Catalog Dataset (Data Management)

## Summary

Creates a catalog dataset to which collections of layers, rasters, datasets, and other items can be added.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Catalog Dataset Location | The enterprise, mobile, or file geodatabase in which the output catalog dataset will be created. | Workspace; Feature Dataset |
| Catalog Dataset Name | The name of the catalog dataset that will be created. | String |
| Coordinate System (Optional) | The spatial reference of the catalog dataset. | Spatial Reference |
| Template Datasets (Optional) | The feature class or table that will be used as a template to define the attribute fields of the new catalog dataset. | Table View |
| Has Z (Optional) | Specifies whether the catalog dataset will contain elevation values (z-values).Disabled—The output catalog dataset will not contain z-values. This is the default.Enabled—The output catalog dataset will contain z-values.Same as Template—The output catalog dataset will contain z-values if the dataset specified in the Template Datasets parameter contains z-values. | String |
| Catalog Dataset Alias (Optional) | The alias name of the catalog dataset. | String |
| Configuration Keyword (Optional) | The configuration keyword determines the storage parameters of the database table. The configuration keyword applies to enterprise data only. | String |
| out_path | The enterprise, mobile, or file geodatabase in which the output catalog dataset will be created. | Workspace; Feature Dataset |
| out_name | The name of the catalog dataset that will be created. | String |
| spatial_reference(Optional) | The spatial reference of the catalog dataset. | Spatial Reference |
| template[template,...](Optional) | The feature class or table that will be used as a template to define the attribute fields of the new catalog dataset. | Table View |
| has_z(Optional) | Specifies whether the catalog dataset will contain elevation values (z-values).DISABLED—The output catalog dataset will not contain z-values. This is the default.ENABLED—The output catalog dataset will contain z-values.SAME_AS_TEMPLATE—The output catalog dataset will contain z-values if the dataset specified in the template parameter contains z-values. | String |
| out_alias(Optional) | The alias name of the catalog dataset. | String |
| config_keyword(Optional) | The configuration keyword determines the storage parameters of the database table. The configuration keyword applies to enterprise data only. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateCatalogDataset(out_path, out_name, {spatial_reference}, {template}, {has_z}, {out_alias}, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Dataspace"
dataset = "C:/Dataspace/studyarea.gdb/StudyAreaDataset"
spatial_ref = arcpy.Describe(dataset).spatialReference
arcpy.management.CreateCatalogDataset("C:/Dataspace/studyarea.gdb", 
                                      "SampleCatalog", "SampleFeatureClass", 
                                      "DISABLED", spatial_ref, 
                                      "NewCatalogDataset", "DEFAULTS")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Dataspace"
dataset = "C:/Dataspace/studyarea.gdb/StudyAreaDataset"
spatial_ref = arcpy.Describe(dataset).spatialReference
arcpy.management.CreateCatalogDataset("C:/Dataspace/studyarea.gdb", 
                                      "SampleCatalog", "SampleFeatureClass", 
                                      "DISABLED", spatial_ref, 
                                      "NewCatalogDataset", "DEFAULTS")
```

### Example 4

```python
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/output.gdb"

# Set local variables
catalogName = "MyCatalogDataset"
outCatalogDataset = "C:/output/output.gdb/catalogds1"
zValuesPresent = "ENABLED"

# Run CreateCatalogDataset
arcpy.management.CreateCatalogDataset(outCatalogDataset, catalogName, " ",
                                      zValuesPresent)
```

### Example 5

```python
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/output.gdb"

# Set local variables
catalogName = "MyCatalogDataset"
outCatalogDataset = "C:/output/output.gdb/catalogds1"
zValuesPresent = "ENABLED"

# Run CreateCatalogDataset
arcpy.management.CreateCatalogDataset(outCatalogDataset, catalogName, " ",
                                      zValuesPresent)
```

---

## Create Cloud Storage Connection File (Data Management)

## Summary

Creates a connection file for ArcGIS-supported cloud storage. This tool allows existing raster geoprocessing tools to write cloud raster format (CRF) datasets into the cloud storage bucket or read raster datasets (not limited to CRF) stored in the cloud storage as input.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Connection File Location | The folder path where the connection file will be created. | Folder |
| Connection File Name | The name of the cloud storage connection file. | String |
| Service Provider | Specifies the cloud storage service provider that will be used.Azure—The service provider will be Microsoft Azure.Amazon—The service provider will be Amazon S3.Google—The service provider will be Google Cloud Storage.Alibaba—The service provider will be Alibaba Cloud Storage.WebHDFS—The service provider will be WebHDFS.MinIO—The service provider will be MinIO.Azure Data Lake—The service provider will be Microsoft Azure Data Lake Storage.Ozone—The service provider will be Ozone. | String |
| Bucket (Container) Name | The name of the cloud storage container where the raster dataset will be stored. Many cloud providers also call it a bucket. | String |
| Access Key ID (Account Name) (Optional) | The access key ID string for the specific cloud storage type. It can also be the account name, as is the case with Azure. | String |
| Secret Access Key (Account Key)(Optional) | The secret access key string to authenticate the connection to cloud storage. | Encrypted String |
| Region (Environment)(Optional) | The region string for the cloud storage. If provided, the value must use the format defined by the cloud storage choice. The default is the selected cloud provider's default account. | String |
| Service End Point(Optional) | The service endpoint (URI) of the cloud storage, such as oss-us-west-1.aliyuncs.com. If no value is provided, the default endpoint for the selected cloud storage type will be used. The CNAME redirected endpoint can also be used if needed. | String |
| Provider Options(Optional) | The configuration options pertaining to the specific type of cloud service. Some services offer options, some do not. You only need to set this parameter if you want to turn on the options.Azure and Microsoft Azure Data Lake StorageAZURE_STORAGE_SAS_TOKEN—Specify a shared access signature. Ensure that its value is URL encoded and does not contain leading '?' or '&' characters. When using this option, the Secret Access Key (Account Key) parameter must be empty.AZURE_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that don't require authenticated access. When using this option, the Secret Access Key (Account Key) parameter must be empty. The default value is FalseAZURE_STORAGE_CONNECTION_STRING—Specify an Azure Storage connection string. This string embeds the account name, key, and endpoint. When using this option, the Access Key ID (Account Name) and Secret Access Key (Account Key) parameters must be empty. CPL_AZURE_USE_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.AZURE_IMDS_OBJECT_ID—Specify the Object ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set. AZURE_IMDS_CLIENT_ID—Specify the Client ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set. AZURE_IMDS_MSI_RES_ID—Specify the Resource ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set.Amazon and MinIOAWS_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that don't require authenticated access. The default value is False.AWS_SESSION_TOKEN—Specify temporary credentials.AWS_DEFAULT_PROFILE—AWS credential profiles are automatically used when the access key or ID is missing. This option can be used to specify the profile to use.AWS_REQUEST_PAYER—Requester Pays buckets can be accessed by setting this option to requester.AWS_Virtual_Hosting—If you use Amazon S3 or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE—Some older S3-compatible implementations do not support the Bulk Delete operation. Set this option to False for these providers. The default value is True.AWS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.GoogleGS_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that do not require authenticated access. The default value is TrueGS_USER_PROJECT—Requester Pays buckets can be accessed by setting OAuth2 keys and a project for billing. Set the project using this option and set OAuth2 keys using other options and not HMAC keys as a secret access key or ID. GS_OAUTH2_REFRESH_TOKEN—Specify OAuth2 Refresh Access Token. Set OAuth2 client credentials using GS_OAUTH2_CLIENT_ID and GS_OAUTH2_CLIENT_SECRET. GOOGLE_APPLICATION_CREDENTIALS—Specify Service Account OAuth2 credentials using a .json file containing a private key and client email address. GS_OAUTH2_ PRIVATE_KEY—Specify Service Account OAuth2 credentials using a private key string. GS_AUTH2_CLIENT_EMAIL must be set. GS_OAUTH2_ PRIVATE_KEY_FILE—Specify Service Account OAuth2 credentials using a private key from a file. GS_AUTH2_CLIENT_EMAIL must be set.GS_AUTH2_CLIENT_EMAIL—Specify Service Account OAuth2 credentials using a client email address.GS_AUTH2_SCOPE—Specify Service Account OAuth2 scope. Valid values are https://www.googleapis.com/auth/devstorage.read_write (the default) and https://www.googleapis.com/auth/devstorage.read_only. GDAL_HTTP_HEADER_FILE—Specify bearer authentication credentials stored in an external file.AlibabaOSS_Virtual_Hosting—If you use Alibaba or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.OSS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.WebHDFSWEBHDFS_REPLICATION (integer)—The replication value is used when creating a fileWEBHDFS_PERMISSION (decimal)—A permission mask is used when creating a file.If multiple authentication parameters are provided, precedence is as follows:Azure—AZURE_STORAGE_CONNECTION_STRING, account name or key, AZURE_STORAGE_SAS_TOKEN, AZURE_NO_SIGN_REQUEST, or RBAC. Amazon—AWS_NO_SIGN_REQUEST, access ID or key or AWS_SESSION_TOKEN, AWS Credential Profile, or IAM Role. Google—GS_NO_SIGN_REQUEST, access ID or key, GDAL_HTTP_HEADER_FILE, (GS_OAUTH2_REFRESH_TOKEN or GS_OAUTH2_CLIENT_ID and GS_OAUTH2_CLIENT_SECRET), GOOGLE_APPLICATION_CREDENTIALS, (GS_OAUTH2_PRIVATE_KEY or GS_OAUTH2_CLIENT_EMAIL), (GS_OAUTH2_PRIVATE_KEY_FILE or GS_OAUTH2_CLIENT_EMAIL), or IAM Role.OzoneAWS_DEFAULT_PROFILE—AWS credential profiles are automatically used when the access key or ID is missing. This option can be used to specify the profile to use.AWS_Virtual_Hosting—If you use Amazon S3 or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.AWS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE—Some older S3-compatible implementations do not support the Bulk Delete operation. Set this option to False for these providers. The default value is True.x-amz-storage-class—Specify REDUCED_REDUNDANCY for writing to a single container ozone as it has a single data node.In addition to the provider options listed above, the ARC_DEEP_CRAWL option can be used with all the service providers. If True, it is used to identify CRFs with no extension and cloud-enabled raster products in the cloud. This is operation intensive and it is recommended that you set this option to False for faster catalog browsing and crawling. The default value is False.Custom token vending services—such as Planetary Computer's data collection, for example—can be authenticated using the ARC_TOKEN_SERVICE_API (URL of the token vendor) and ARC_TOKEN_OPTION_NAME (type of token from the service provider) provider options.Note: The GDAL_DISABLE_READDIR_ON_OPEN option is available with all the service providers. To improve the performance of loading cloud-based rasters, this option is set to NO by default. If the raster resides in a folder that contains more than 30,000 items, set this option to YES. | Value Table |
| Folder (Optional) | The folder in the Bucket (Container) Name parameter value where the raster dataset will be stored. | String |
| Authentication(Optional) | The connection name of OAuth 2.0 authentication. A valid connection needs to be configured in the Options dialog box on the Authentication tab. | String |
| out_folder_path | The folder path where the connection file will be created. | Folder |
| out_name | The name of the cloud storage connection file. | String |
| service_provider | Specifies the cloud storage service provider that will be used.AZURE—The service provider will be Microsoft Azure.AMAZON—The service provider will be Amazon S3.GOOGLE—The service provider will be Google Cloud Storage.ALIBABA—The service provider will be Alibaba Cloud Storage.WEBHDFS—The service provider will be WebHDFS.MINIO—The service provider will be MinIO.AZUREDATALAKE—The service provider will be Microsoft Azure Data Lake Storage.OZONE—The service provider will be Ozone. | String |
| bucket_name | The name of the cloud storage container where the raster dataset will be stored. Many cloud providers also call it a bucket. | String |
| access_key_id(Optional) | The access key ID string for the specific cloud storage type. It can also be the account name, as is the case with Azure. | String |
| secret_access_key(Optional) | The secret access key string to authenticate the connection to cloud storage. | Encrypted String |
| region(Optional) | The region string for the cloud storage. If provided, the value must use the format defined by the cloud storage choice. The default is the selected cloud provider's default account. | String |
| end_point(Optional) | The service endpoint (URI) of the cloud storage, such as oss-us-west-1.aliyuncs.com. If no value is provided, the default endpoint for the selected cloud storage type will be used. The CNAME redirected endpoint can also be used if needed. | String |
| config_options[config_options,...](Optional) | The configuration options pertaining to the specific type of cloud service. Some services offer options, some do not. You only need to set this parameter if you want to turn on the options.Azure and Microsoft Azure Data Lake StorageAZURE_STORAGE_SAS_TOKEN—Specify a shared access signature. Ensure that its value is URL encoded and does not contain leading '?' or '&' characters. When using this option, the Secret Access Key (Account Key) parameter must be empty.AZURE_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that don't require authenticated access. When using this option, the Secret Access Key (Account Key) parameter must be empty. The default value is FalseAZURE_STORAGE_CONNECTION_STRING—Specify an Azure Storage connection string. This string embeds the account name, key, and endpoint. When using this option, the Access Key ID (Account Name) and Secret Access Key (Account Key) parameters must be empty. CPL_AZURE_USE_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.AZURE_IMDS_OBJECT_ID—Specify the Object ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set. AZURE_IMDS_CLIENT_ID—Specify the Client ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set. AZURE_IMDS_MSI_RES_ID—Specify the Resource ID of the managed identity authenticated using Azure Instance Metadata Service (IMDS) if your Azure VM has multiple user-assigned managed identities set.Amazon and MinIOAWS_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that don't require authenticated access. The default value is False.AWS_SESSION_TOKEN—Specify temporary credentials.AWS_DEFAULT_PROFILE—AWS credential profiles are automatically used when the access key or ID is missing. This option can be used to specify the profile to use.AWS_REQUEST_PAYER—Requester Pays buckets can be accessed by setting this option to requester.AWS_Virtual_Hosting—If you use Amazon S3 or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE—Some older S3-compatible implementations do not support the Bulk Delete operation. Set this option to False for these providers. The default value is True.AWS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.GoogleGS_NO_SIGN_REQUEST—Anonymously connect to buckets (containers) that do not require authenticated access. The default value is TrueGS_USER_PROJECT—Requester Pays buckets can be accessed by setting OAuth2 keys and a project for billing. Set the project using this option and set OAuth2 keys using other options and not HMAC keys as a secret access key or ID. GS_OAUTH2_REFRESH_TOKEN—Specify OAuth2 Refresh Access Token. Set OAuth2 client credentials using GS_OAUTH2_CLIENT_ID and GS_OAUTH2_CLIENT_SECRET. GOOGLE_APPLICATION_CREDENTIALS—Specify Service Account OAuth2 credentials using a .json file containing a private key and client email address. GS_OAUTH2_ PRIVATE_KEY—Specify Service Account OAuth2 credentials using a private key string. GS_AUTH2_CLIENT_EMAIL must be set. GS_OAUTH2_ PRIVATE_KEY_FILE—Specify Service Account OAuth2 credentials using a private key from a file. GS_AUTH2_CLIENT_EMAIL must be set.GS_AUTH2_CLIENT_EMAIL—Specify Service Account OAuth2 credentials using a client email address.GS_AUTH2_SCOPE—Specify Service Account OAuth2 scope. Valid values are https://www.googleapis.com/auth/devstorage.read_write (the default) and https://www.googleapis.com/auth/devstorage.read_only. GDAL_HTTP_HEADER_FILE—Specify bearer authentication credentials stored in an external file.AlibabaOSS_Virtual_Hosting—If you use Alibaba or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.OSS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.WebHDFSWEBHDFS_REPLICATION (integer)—The replication value is used when creating a fileWEBHDFS_PERMISSION (decimal)—A permission mask is used when creating a file.If multiple authentication parameters are provided, precedence is as follows:Azure—AZURE_STORAGE_CONNECTION_STRING, account name or key, AZURE_STORAGE_SAS_TOKEN, AZURE_NO_SIGN_REQUEST, or RBAC. Amazon—AWS_NO_SIGN_REQUEST, access ID or key or AWS_SESSION_TOKEN, AWS Credential Profile, or IAM Role. Google—GS_NO_SIGN_REQUEST, access ID or key, GDAL_HTTP_HEADER_FILE, (GS_OAUTH2_REFRESH_TOKEN or GS_OAUTH2_CLIENT_ID and GS_OAUTH2_CLIENT_SECRET), GOOGLE_APPLICATION_CREDENTIALS, (GS_OAUTH2_PRIVATE_KEY or GS_OAUTH2_CLIENT_EMAIL), (GS_OAUTH2_PRIVATE_KEY_FILE or GS_OAUTH2_CLIENT_EMAIL), or IAM Role.OzoneAWS_DEFAULT_PROFILE—AWS credential profiles are automatically used when the access key or ID is missing. This option can be used to specify the profile to use.AWS_Virtual_Hosting—If you use Amazon S3 or S3-compatible cloud providers that support only path-style requests, set this option to True. It is recommended that you use virtual hosting if it's supported. The default value is True.AWS_HTTPS—Set to False to use HTTP requests. Some servers may be configured to only support HTTPS requests. The default value is True.CPL_VSIS3_USE_BASE_RMDIR_RECURSIVE—Some older S3-compatible implementations do not support the Bulk Delete operation. Set this option to False for these providers. The default value is True.x-amz-storage-class—Specify REDUCED_REDUNDANCY for writing to a single container ozone as it has a single data node.In addition to the provider options listed above, the ARC_DEEP_CRAWL option can be used with all the service providers. If True, it is used to identify CRFs with no extension and cloud-enabled raster products in the cloud. This is operation intensive and it is recommended that you set this option to False for faster catalog browsing and crawling. The default value is False.Custom token vending services—such as Planetary Computer's data collection, for example—can be authenticated using the ARC_TOKEN_SERVICE_API (URL of the token vendor) and ARC_TOKEN_OPTION_NAME (type of token from the service provider) provider options.Note: The GDAL_DISABLE_READDIR_ON_OPEN option is available with all the service providers. To improve the performance of loading cloud-based rasters, this option is set to NO by default. If the raster resides in a folder that contains more than 30,000 items, set this option to YES. | Value Table |
| folder(Optional) | The folder in the bucket_name parameter value where the raster dataset will be stored. | String |
| authentication(Optional) | The connection name of OAuth 2.0 authentication | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateCloudStorageConnectionFile(out_folder_path, out_name, service_provider, bucket_name, {access_key_id}, {secret_access_key}, {region}, {end_point}, {config_options}, {folder}, {authentication})
```

### Example 2

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

# Create connection to open public bucket with requester pay option
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "awss3storage.acs", "AMAZON", "publicrasterstore",
    config_options="AWS_REQUEST_PAYER requester")

# Create connection to secured Azure bucket
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "azurestorage.acs", "AZURE", "rasterstore", "imageaccount",
    "NOGEOU1238987OUOUNOQEWQWEIO")

# Create Alibaba connection with end points
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "aliyun.acs", "ALIBABA", "rasterstore", "AYOUER9273PJJNY",
"NOGEOU1238987OUOUNOQEWQWEIO", end_point="rasterstore.oss-us-west-1.aliyuncs.com")
```

### Example 3

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

# Create connection to open public bucket with requester pay option
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "awss3storage.acs", "AMAZON", "publicrasterstore",
    config_options="AWS_REQUEST_PAYER requester")

# Create connection to secured Azure bucket
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "azurestorage.acs", "AZURE", "rasterstore", "imageaccount",
    "NOGEOU1238987OUOUNOQEWQWEIO")

# Create Alibaba connection with end points
arcpy.management.CreateCloudStorageConnectionFile(
    "C:/Workspace/connections", "aliyun.acs", "ALIBABA", "rasterstore", "AYOUER9273PJJNY",
"NOGEOU1238987OUOUNOQEWQWEIO", end_point="rasterstore.oss-us-west-1.aliyuncs.com")
```

### Example 4

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

outfolder = "C:/Workspace/connections"
connectname = "googlecloudos.acs"
provider = "GOOGLE"
accesskey = "AYOUER9273PJJNY"
secretkey = "NOGEOU1238987OUOUNOQEWQWEIO"
bucketname = "rasterstore"

# Create connection to Google cloud object storage
arcpy.management.CreateCloudStorageConnectionFile(
outfolder, connectname, provider, bucketname, accesskey, secretkey)
```

### Example 5

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

outfolder = "C:/Workspace/connections"
connectname = "googlecloudos.acs"
provider = "GOOGLE"
accesskey = "AYOUER9273PJJNY"
secretkey = "NOGEOU1238987OUOUNOQEWQWEIO"
bucketname = "rasterstore"

# Create connection to Google cloud object storage
arcpy.management.CreateCloudStorageConnectionFile(
outfolder, connectname, provider, bucketname, accesskey, secretkey)
```

### Example 6

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

outfolder = "C:/Workspace/connections"
connectname = "planetary_landsat.acs"
provider = "Azure"
accesskey = "landsateuwest"
secretkey = ""
bucketname = "landsat-c2"
folder = ""
region = ""
endpoint = ""
config_options= "ARC_TOKEN_OPTION_NAME AZURE_STORAGE_SAS_TOKEN; ARC_TOKEN_SERVICE_API https://planetarycomputer.microsoft.com/api/sas/v1/token/landsateuwest/landsat-c2"

# Create connection to planetary computer landsat data collection

print(arcpy.CreateCloudStorageConnectionFile_management(outfolder, connectname, provider, bucketname, accesskey, secretkey, region, endpoint, config_options, folder))
print(arcpy.GetMessages())
```

### Example 7

```python
#====================================
# CreateCloudStorageConnectionFile
# Usage:
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     { {Name} {Value}; {Name} {Value}...})
# arcpy.management.CreateCloudStorageConnectionFile(
#     out_folder_path, out_name, AZURE | AMAZON | GOOGLE | ALIBABA, bucket_name,
#     {access_key_id}, {secret_access_key}, {region}, {end_point},
#     {config_options})

import arcpy

outfolder = "C:/Workspace/connections"
connectname = "planetary_landsat.acs"
provider = "Azure"
accesskey = "landsateuwest"
secretkey = ""
bucketname = "landsat-c2"
folder = ""
region = ""
endpoint = ""
config_options= "ARC_TOKEN_OPTION_NAME AZURE_STORAGE_SAS_TOKEN; ARC_TOKEN_SERVICE_API https://planetarycomputer.microsoft.com/api/sas/v1/token/landsateuwest/landsat-c2"

# Create connection to planetary computer landsat data collection

print(arcpy.CreateCloudStorageConnectionFile_management(outfolder, connectname, provider, bucketname, accesskey, secretkey, region, endpoint, config_options, folder))
print(arcpy.GetMessages())
```

---

## Create Color Composite (Data Management)

## Summary

Creates a three-band raster dataset from a multiband raster dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input multiband raster data. | Raster Dataset; Raster Layer |
| Output Raster | The output three-band composite raster. | Raster Dataset |
| Method | Specifies the method that will be used to extract bands.Band names—The band name representing the wavelength interval on the electromagnetic spectrum (such as Red, Near Infrared, or Thermal Infrared) or the polarization (such as VH, VV, HH, or HV) will be used. This is the default.Band IDs— The band number (such as B1, B2, or B3) will be used. | String |
| Red Expression | The calculation that will be assigned to the first band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |
| Green Expression | The calculation that will be assigned to the second band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |
| Blue Expression | The calculation that will be assigned to the third band. A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |
| in_raster | The input multiband raster data. | Raster Dataset; Raster Layer |
| out_raster | The output three-band composite raster. | Raster Dataset |
| method | Specifies the method that will be used to extract bands.BAND_NAMES—The band name representing the wavelength interval on the electromagnetic spectrum (such as Red, Near Infrared, or Thermal Infrared) or the polarization (such as VH, VV, HH, or HV) will be used. This is the default.BAND_IDS— The band number (such as B1, B2, or B3) will be used. | String |
| red_expression | The calculation that will be assigned to the first band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |
| green_expression | The calculation that will be assigned to the second band.A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |
| blue_expression | The calculation that will be assigned to the third band. A band name, band ID, or an algebraic expression using the bands.The supported operators are unary: plus (+), minus (-), times (*), and divide (/). | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateColorComposite(in_raster, out_raster, method, red_expression, green_expression, blue_expression)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "D:\Data\SAR\S1\20181014" 
arcpy.management.CreateColorComposite(
     "IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC.crf", 
     "IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_RGB.crf", "BAND_NAMES", "VV", 
     "VH", "VV/VH")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "D:\Data\SAR\S1\20181014" 
arcpy.management.CreateColorComposite(
     "IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC.crf", 
     "IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_RGB.crf", "BAND_NAMES", "VV", 
     "VH", "VV/VH")
```

### Example 4

```python
# Import system modules 
import arcpy 

# Define input parameters 
in_raster = "D:\Data\SAR\S1\20181014\IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_dB.crf"
out_raster = "D:\Data\SAR\S1\20181014\IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_dB_RGB.crf"
method = "BAND_NAMES"
redExp = "VV"
greenExp = "VH"
blueExp = "VV-VH" 

arcpy.management.CreateColorComposite(in_raster, out_raster, method, 
    redExp, greenExp, blueExp)
```

### Example 5

```python
# Import system modules 
import arcpy 

# Define input parameters 
in_raster = "D:\Data\SAR\S1\20181014\IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_dB.crf"
out_raster = "D:\Data\SAR\S1\20181014\IW_manifest_TNR_CalB0_Dspk_RTFG0_GTC_dB_RGB.crf"
method = "BAND_NAMES"
redExp = "VV"
greenExp = "VH"
blueExp = "VV-VH" 

arcpy.management.CreateColorComposite(in_raster, out_raster, method, 
    redExp, greenExp, blueExp)
```

---

## Create Custom Geographic Transformation (Data Management)

## Summary

Creates a transformation definition for converting data between two geographic coordinate systems or datums. The output of this tool can be used as a transformation for any tool with a parameter that requires a geographic transformation.

## Usage

- All custom geographic transformation files are saved with the .gtf extension and stored in the Esri\ArcGISPro\ArcToolbox\CustomTransformations folder in your application data folder. In all Windows operating systems, the AppData folder is in %appdata% and the temp folder is in %temp%. Entering %appdata% in a command window returns the AppData folder location. Entering %temp% returns the temp folder location.In UNIX systems, the tmp and application data folders are located in your home directory, under $TMP and $HOME, respectively. Typing /tmp in a terminal returns the location.
- In all Windows operating systems, the AppData folder is in %appdata% and the temp folder is in %temp%. Entering %appdata% in a command window returns the AppData folder location. Entering %temp% returns the temp folder location.
- In UNIX systems, the tmp and application data folders are located in your home directory, under $TMP and $HOME, respectively. Typing /tmp in a terminal returns the location.
- Any geoprocessing tool that uses geographic transformations will inspect all custom transformations in the default storage location and present them as options in the transformation parameter's drop-down list. The transformations will also be available to the Geographic Transformations environment.
- You cannot edit custom transformation files. They are binary files that store version and string length information that may be corrupted if modified. To update a transformation file, create a new custom geographic transformation and overwrite the existing file.
- Transformation methods are divided into two groups: equation based and file based. Equation-based methods do not need external information. File-based methods require a file or files that are stored on disk to calculate the coordinate offset values. The files are similar to rasters— with regularly spaced points—and an offset for a location is calculated using an interpolation method and values from surrounding points.
- Files must be placed in one of following locations:If ArcGIS Coordinate Systems Data is installed on a per-machine (for everyone) basis, copy the file or files to the C:\Program Files (x86)\ArcGIS\CoordinateSystemsData\pedata\Geographic folder.If ArcGIS Coordinate Systems Data is installed on a per-user basis, copy the file or files to the C:\Users\username\AppData\Local\Programs\ArcGIS\CoordinateSystemsData\pedata\Geographic folder.If ArcGIS Coordinate Systems Data is not installed, copy the file or files to the ArcGIS Pro pedata folder at C:\Program Files\ArcGIS\Pro\Resources\pedata if ArcGIS Pro is installed on a per-machine (for everyone) basis.If ArcGIS Pro is installed on a per-user basis, copy the file or files to the ArcGIS Pro pedate folder at C:\Users\username\AppData\Local\Programs\ArcGIS\Pro\Resources\pedata. If you didn't use the default installation locations for ArcGIS Pro or the ArcGIS Coordinate Systems Data, copy the files to the equivalent pedata folder. If these locations are not available because of permission restrictions or are hidden, ask your system administrator to help you.
- If ArcGIS Coordinate Systems Data is installed on a per-machine (for everyone) basis, copy the file or files to the C:\Program Files (x86)\ArcGIS\CoordinateSystemsData\pedata\Geographic folder.
- If ArcGIS Coordinate Systems Data is installed on a per-user basis, copy the file or files to the C:\Users\username\AppData\Local\Programs\ArcGIS\CoordinateSystemsData\pedata\Geographic folder.
- If ArcGIS Coordinate Systems Data is not installed, copy the file or files to the ArcGIS Pro pedata folder at C:\Program Files\ArcGIS\Pro\Resources\pedata if ArcGIS Pro is installed on a per-machine (for everyone) basis.
- If ArcGIS Pro is installed on a per-user basis, copy the file or files to the ArcGIS Pro pedate folder at C:\Users\username\AppData\Local\Programs\ArcGIS\Pro\Resources\pedata.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Geographic Transformation Name | The name of the custom transformation definition. | String |
| Input Geographic Coordinate System | The starting geographic coordinate system.Provide a well known ID (WKID) or use the Select Coordinate System button to select a coordinate system. | Coordinate System |
| Output Geographic Coordinate System | The final geographic coordinate system.Provide a WKID or use the Select Coordinate System button to select a coordinate system. | Coordinate System |
| Custom Geographic Transformation | The custom transformation method that will be used.A list of the methods and parameters is available in the Geographic and Vertical Transformations pdf.From the drop-down list, choose the transformation method that will be used to transform the data from the input geographic coordinate system to the output geographic coordinate system. Once chosen, additional options will appear below the transformation method. | String |
| Extent (Optional) | The geographic area where the transformation is applicable. Transformed data within the provided extent is guaranteed to be converted with the specified level of accuracy Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Accuracy (Optional) | The expected difference between transformed (output) coordinates and their true values. Because any transformation in general degrades the quality of the dataset, the accuracy value is always the maximum absolute expected error in meters. | Double |
| geot_name | The name of the custom transformation definition. | String |
| in_coor_system | The starting geographic coordinate system. | Coordinate System |
| out_coor_system | The final geographic coordinate system. | Coordinate System |
| custom_geot | The custom transformation method that will be used.A list of the methods and parameters is available in the Geographic and Vertical Transformations pdf.Set the METHOD and PARAMETER values wrapped in a string for custom transformation GEOGTRAN. Set the name of the method from the available methods of Geocentric_Translation, Molodensky, Molodensky_Abridged, Position_Vector, Coordinate_Frame, Molodensky_Badekas, NADCON, HARN, NTV2, Longitude_Rotation, Unit_Change, and Geographic_2D_Offset. Each method has a set of parameters. You can edit the option values by entering text next to the name of the parameter within the whole string representation of the custom geographic transformation. See examples in the Python sample below. | String |
| extent(Optional) | The geographic area where the transformation is applicable. Transformed data within the provided extent is guaranteed to be converted with the specified level of accuracy | Extent |
| accuracy(Optional) | The expected difference between transformed (output) coordinates and their true values. Because any transformation in general degrades the quality of the dataset, the accuracy value is always the maximum absolute expected error in meters. | Double |

## Code Samples

### Example 1

```python
arcpy.management.CreateCustomGeoTransformation(geot_name, in_coor_system, out_coor_system, custom_geot, {extent}, {accuracy})
```

### Example 2

```python
# Name: CreateCustomGeographicTransformation.py
# Description: Create a custom geographic transformation in the default directory.

# import system modules
import arcpy

# set the variables
geoTransfmName = "cgt_geocentric2"

# create a spatial reference object for GCS_Tokyo
inGCS = arcpy.SpatialReference("Tokyo")

# create a spatial reference object for GCS_WGS_1984
outGCS = arcpy.SpatialReference("WGS 1984")

customGeoTransfm = "GEOGTRAN[METHOD['Geocentric_Translation'],PARAMETER['X_Axis_Translation',''],PARAMETER['Y_Axis_Translation',''],PARAMETER['Z_Axis_Translation','']]"

arcpy.management.CreateCustomGeoTransformation(geoTransfmName, inGCS, outGCS, customGeoTransfm)
```

### Example 3

```python
# Name: CreateCustomGeographicTransformation.py
# Description: Create a custom geographic transformation in the default directory.

# import system modules
import arcpy

# set the variables
geoTransfmName = "cgt_geocentric2"

# create a spatial reference object for GCS_Tokyo
inGCS = arcpy.SpatialReference("Tokyo")

# create a spatial reference object for GCS_WGS_1984
outGCS = arcpy.SpatialReference("WGS 1984")

customGeoTransfm = "GEOGTRAN[METHOD['Geocentric_Translation'],PARAMETER['X_Axis_Translation',''],PARAMETER['Y_Axis_Translation',''],PARAMETER['Z_Axis_Translation','']]"

arcpy.management.CreateCustomGeoTransformation(geoTransfmName, inGCS, outGCS, customGeoTransfm)
```

---

## Create Custom Vertical Transformation (Data Management)

## Summary

Creates a transformation definition for converting data between two vertical coordinate systems or datums. The output of this tool can be used as a transformation object for any tool with a parameter that requires a vertical transformation.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Vertical Transformation Name | The name of the custom transformation definition. | String |
| Source Vertical Coordinate System | The starting vertical coordinate system.Provide a well known ID (WKID) or use the Select Coordinate System button to select a coordinate system. | String |
| Target Vertical Coordinate System | The final vertical coordinate system.Provide a WKID or use the Select Coordinate System button to select a coordinate system. | String |
| Interpolation Geographic Coordinate System (Optional) | The interpolation geographic coordinate system.This parameter is only active if a vertical transformation method requires it.The geographic coordinate system is used when interpolating the offset values from a file, or latitude and longitude coordinates are required by the method. The methods that do not require an interpolation geographic coordinate system are Null, Unit Change, Height Depth Reversal, and Vertical Offset. Provide a WKID or use the Select Coordinate System button to select a coordinate system. | Coordinate System |
| Vertical Transformation Method (Optional) | The vertical transformation method that will be used.From the drop-down list, choose the transformation method that will be used to transform the data from the input vertical coordinate system to the output vertical coordinate system. Once chosen, its parameters will appear in the table for editing.A list of the methods and parameters is available in the Geographic and Vertical Transformations pdf.If using a method that requires a file, see the Usage Notes for where to put the file or files. | String |
| Extent (Optional) | The area where the transformation is applicable. Use WGS84 (WKID: 4326) or another GNSS-based geographic coordinate system such as NAD 1983 or GDA2020 for the extent coordinate system. If a projected coordinate system or a layer that has a projected coordinate system is provided, the values will be converted to latitude and longitude.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Accuracy (Optional) | A general statement of accuracy in meters. | Double |
| vt_name | The name of the custom transformation definition. | String |
| source_vt_coor_system | The starting vertical coordinate system. | String |
| target_vt_coor_system | The final vertical coordinate system. | String |
| interpolation_gcs(Optional) | The interpolation geographic coordinate system.This parameter is only active if a vertical transformation method requires it.The geographic coordinate system is used when interpolating the offset values from a file, or latitude and longitude coordinates are required by the method. The methods that do not require an interpolation geographic coordinate system are Null, Unit Change, Height Depth Reversal, and Vertical Offset. | Coordinate System |
| custom_vt(Optional) | The vertical transformation method that will be used.From the drop-down list, choose the transformation method that will be used to transform the data from the input vertical coordinate system to the output vertical coordinate system. Once chosen, its parameters will appear in the table for editing.A list of the methods and parameters is available in the Geographic and Vertical Transformations pdf.If using a method that requires a file, see the Usage Notes for where to put the file or files. | String |
| extent(Optional) | The area where the transformation is applicable. Use WGS84 (WKID: 4326) or another GNSS-based geographic coordinate system such as NAD 1983 or GDA2020 for the extent coordinate system. If a projected coordinate system or a layer that has a projected coordinate system is provided, the values will be converted to latitude and longitude.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| accuracy(Optional) | A general statement of accuracy in meters. | Double |

## Code Samples

### Example 1

```python
15.000000000000    291.00000000000    0.1666666666667E-01    0.1666666666665E-01    361   301  1
  -29.2936  -29.3314  -29.3710  -29.4121  -29.4540  -29.4965  -29.5382  -29.5807
  -29.6233  -29.6660  -29.7090  -29.7522  -29.7956  -29.8397  -29.8846  -29.9301
```

### Example 2

```python
15.000000000000    291.00000000000    0.1666666666667E-01    0.1666666666665E-01    361   301  1
  -29.2936  -29.3314  -29.3710  -29.4121  -29.4540  -29.4965  -29.5382  -29.5807
  -29.6233  -29.6660  -29.7090  -29.7522  -29.7956  -29.8397  -29.8846  -29.9301
```

### Example 3

```python
arcpy.management.CreateCustomVerticalTransformation(vt_name, source_vt_coor_system, target_vt_coor_system, {interpolation_gcs}, {custom_vt}, {extent}, {accuracy})
```

### Example 4

```python
import arcpy
arcpy.management.CreateCustomVerticalTransformation(
    vt_name="NAD_1983_2011_ellipsoid_to_GEOID18b",
    source_vt_coor_system='VERTCS["NAD_1983_2011",DATUM["D_NAD_1983_2011",SPHEROID["GRS_1980",6378137.0,298.257222101]],PARAMETER["Vertical_Shift",0.0],PARAMETER["Direction",1.0],UNIT["Meter",1.0]]',
    target_vt_coor_system='VERTCS["NAVD88_height_(ftIntl)",VDATUM["North_American_Vertical_Datum_1988"],PARAMETER["Vertical_Shift",0.0],PARAMETER["Direction",1.0],UNIT["Foot",0.3048]]',
    interpolation_gcs='GEOGCS["GCS_NAD_1983_2011",DATUM["D_NAD_1983_2011",SPHEROID["GRS_1980",6378137.0,298.257222101]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]',
    custom_vt="GEOID|g2018u0.bin|Bilinear",
    extent='-110 30 -80 40 GEOGCS["GCS_North_American_1983",DATUM["D_North_American_1983",SPHEROID["GRS_1980",6378137.0,298.257222101]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]',
    accuracy=0.5
)
```

### Example 5

```python
import arcpy
arcpy.management.CreateCustomVerticalTransformation(
    vt_name="NAD_1983_2011_ellipsoid_to_GEOID18b",
    source_vt_coor_system='VERTCS["NAD_1983_2011",DATUM["D_NAD_1983_2011",SPHEROID["GRS_1980",6378137.0,298.257222101]],PARAMETER["Vertical_Shift",0.0],PARAMETER["Direction",1.0],UNIT["Meter",1.0]]',
    target_vt_coor_system='VERTCS["NAVD88_height_(ftIntl)",VDATUM["North_American_Vertical_Datum_1988"],PARAMETER["Vertical_Shift",0.0],PARAMETER["Direction",1.0],UNIT["Foot",0.3048]]',
    interpolation_gcs='GEOGCS["GCS_NAD_1983_2011",DATUM["D_NAD_1983_2011",SPHEROID["GRS_1980",6378137.0,298.257222101]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]',
    custom_vt="GEOID|g2018u0.bin|Bilinear",
    extent='-110 30 -80 40 GEOGCS["GCS_North_American_1983",DATUM["D_North_American_1983",SPHEROID["GRS_1980",6378137.0,298.257222101]],PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]',
    accuracy=0.5
)
```

---

## Create Data Loading Workspace (Data Management)

## Summary

Creates a data loading workspace that can be used for data loading. The output workspace contains a collection of Microsoft Excel workbooks. These workbooks can be used to configure the source and target schema mapping.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Source to Target Mapping | Defines how source data will be mapped to the target schema. Both workspaces and individual classes are supported as source or target inputs. When workspaces are used, name similarity is used to match objects in the source and target schemas. | Value Table |
| Output Folder | The output folder where the data loading workspace will be created. | Folder |
| Predictive Field Matching Options(Optional) | Specifies whether field names or domain value descriptions will be matched.Field Name Similarity—Field names will be matched based on similarity between the source and target fields.Domain Coded Value Description Similarity—Attribute domain value descriptions will be matched based on similarity between the source and target fields. When this option is specified, fields will not be matched by name if the source or target field has a domain. | String |
| Mapping Table(Optional) | A table that will be used to perform substring matching for datasets, values, and attribute domain coded value descriptions. Use the table to create matches or block them. | Record Set |
| Calculate Row Count Statistics(Optional) | Specifies whether the count and percentage of filled-in values will be calculated for fields in the source schema.Checked—The count and percentage of filled-in values will be calculated.Unchecked—No calculations will be performed on the field values. This is the default. | Boolean |
| Create Matches by Subtype (Optional) | Specifies whether separate data mapping workbooks will be created by subtype if they exist. Checked—Separate data mapping workbooks will be created for each match if they exist. The class name will not be used to match candidates if subtypes exist. This is the default. Unchecked—Dataset matching will only be attempted at the class level. If classes contain subtypes, a subtype sheet will be created in the data mapping workbook. | Boolean |
| source_target_mapping[source_target_mapping,...] | Defines how source data will be mapped to the target schema. Both workspaces and individual classes are supported as source or target inputs. When workspaces are used, name similarity is used to match objects in the source and target schemas. | Value Table |
| out_folder | The output folder where the data loading workspace will be created. | Folder |
| match_options[match_options,...](Optional) | Specifies whether field names or domain value descriptions will be matched.MATCH_FIELDS—Field names will be matched based on similarity between the source and target fields.MATCH_VALUES—Attribute domain value descriptions will be matched based on similarity between the source and target fields. When this option is specified, fields will not be matched by name if the source or target field has a domain. | String |
| mapping_table(Optional) | A table that will be used to perform substring matching for datasets, values, and attribute domain coded value descriptions. Use the table to create matches or block them. | Record Set |
| calc_stats(Optional) | Specifies whether the count and percentage of filled-in values will be calculated for fields in the source schema.CALC_STATS—The count and percentage of filled-in values will be calculated.NO_STATS—No calculations will be performed on the field values. This is the default. | Boolean |
| match_subtypes(Optional) | Specifies whether separate data mapping workbooks will be created by subtype if they exist.MATCH_SUBTYPES—Separate data mapping workbooks will be created for each match if they exist. The class name will not be used to match candidates if subtypes exist. This is the default.NO_MATCH_SUBTYPES—Dataset matching will only be attempted at the class level. If classes contain subtypes, a subtype sheet will be created in the data mapping workbook. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateDataLoadingWorkspace(source_target_mapping, out_folder, {match_options}, {mapping_table}, {calc_stats}, {match_subtypes})
```

### Example 2

```python
import arcpy

arcpy.management.CreateDataLoadingWorkspace(
    [["C:/data/WaterUtilities.gdb/wControlValue", "C:/data/Water_AssetPackage.gdb/WaterDevice"]],
    "C:/data",
    "MATCH_FIELDS;MATCH_VALUES",
    None,
    "CALC_STATS",
    "MATCH_SUBTYPES",
)
```

### Example 3

```python
import arcpy

arcpy.management.CreateDataLoadingWorkspace(
    [["C:/data/WaterUtilities.gdb/wControlValue", "C:/data/Water_AssetPackage.gdb/WaterDevice"]],
    "C:/data",
    "MATCH_FIELDS;MATCH_VALUES",
    None,
    "CALC_STATS",
    "MATCH_SUBTYPES",
)
```

### Example 4

```python
# Name: CreateDataLoadingWorkspace.py
# Description: Create a new Data Loading Workspace

# Import required modules
import os
import arcpy

# Source and target workspaces with the mapping of table name to table name.
source_workspace = "C:/data/WaterUtilities.gdb/WaterDistribution"
target_workspace = "C:/data/Water_AssetPackage.gdb/UtilityNetwork"
mapping = [
    ("wControlValve", "WaterDevice"),
    ("wHydrant", "WaterJunction"),
    ("wFitting", "WaterJunction"),
    ("wMain", "WaterLine"),
]

# Fully qualify the table names.
source_target = [(os.path.join(source_workspace, a), os.path.join(target_workspace, b)) for a, b in mapping]

# Set local variables.
output_folder = "C:/data"
mapping_table = "C:/temp/Default.gdb/DataReference_GenerateMappingTable"

arcpy.management.CreateDataLoadingWorkspace(
    source_target_mapping=source_target,
    out_folder=output_folder,
    match_options="MATCH_FIELDS;MATCH_VALUES",
    mapping_table=mapping_table,
    calc_stats=True,
    match_subtypes=True,
)
```

### Example 5

```python
# Name: CreateDataLoadingWorkspace.py
# Description: Create a new Data Loading Workspace

# Import required modules
import os
import arcpy

# Source and target workspaces with the mapping of table name to table name.
source_workspace = "C:/data/WaterUtilities.gdb/WaterDistribution"
target_workspace = "C:/data/Water_AssetPackage.gdb/UtilityNetwork"
mapping = [
    ("wControlValve", "WaterDevice"),
    ("wHydrant", "WaterJunction"),
    ("wFitting", "WaterJunction"),
    ("wMain", "WaterLine"),
]

# Fully qualify the table names.
source_target = [(os.path.join(source_workspace, a), os.path.join(target_workspace, b)) for a, b in mapping]

# Set local variables.
output_folder = "C:/data"
mapping_table = "C:/temp/Default.gdb/DataReference_GenerateMappingTable"

arcpy.management.CreateDataLoadingWorkspace(
    source_target_mapping=source_target,
    out_folder=output_folder,
    match_options="MATCH_FIELDS;MATCH_VALUES",
    mapping_table=mapping_table,
    calc_stats=True,
    match_subtypes=True,
)
```

---

## Create Database Connection String (Data Management)

## Summary

Creates a connection string that geoprocessing tools can use to connect to a database or an enterprise geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Database Platform | Specifies the database platform to which the connection will be made.SQL Server—Connect to Microsoft SQL Server or Microsoft Azure SQL Database.Oracle—Connect to Oracle.DB2—Connect to IBM DB2 for Linux, UNIX, or Windows.PostgreSQL—Connect to PostgreSQL.Teradata—Connect to Teradata Data Warehouse Appliance.SAP HANA—Connect to SAP HANA.Dameng—Connect to Dameng. | String |
| Instance | The database server or instance to which the connection will be made.This parameter value depends on the Database Platform parameter value chosen. | String |
| Database Authentication (Optional) | Specifies the type of authentication that will be used.Database authentication—Database authentication will be used. An internal database user name and password are used to connect to the database. You aren't required to type your user name and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. This is the default.Note:If the connection file you are creating will provide ArcGIS services with access to the database or geodatabase, or if you want to use the Catalog search to locate data accessed through this connection file, you must type a user name and password.Operating system authentication—Operating system authentication will be used. You do not need to type a user name and password. The connection will be made with the user name and password that were used to log in to the operating system. If the login used for the operating system is not a valid geodatabase login, the connection will fail. | Boolean |
| Username (Optional) | The database user name that will be used when using database authentication. | String |
| Password (Optional) | The database user password that will be used when using database authentication. | Encrypted String |
| Database (Optional) | The name of the database to which you will connect. This parameter only applies to PostgreSQL and SQL Server platforms. | String |
| Dataset Object Name (Optional) | The name of the dataset or object in the database to which the connection string will point. This connection string can be used as a path to the specified dataset. | String |
| Data type (Optional) | The type of dataset or object referred to in the dataset object name. If there are multiple objects with the same name in the database, you may need to specify the data type of the object for which you want to make a connection string. | String |
| Target Feature Dataset (Optional) | The name of the feature dataset containing the dataset or object for which you want to make a connection string. If the dataset is not in a feature dataset (for example, if it's at the root of the database), do not specify a target feature dataset. | String |
| Schema (Oracle user schema geodatabases only) (Optional) | The user schema geodatabase to which you will connect. The tool will determine if it is connecting to an Oracle database that contains a user-schema geodatabase. If the Oracle database contains a user schema, this option is active; otherwise, it remains inactive. The default option for this parameter is to use the sde schema (master) geodatabase. | String |
| Version Type (Optional) | Specifies the type of version to which you will connect. This parameter only applies when connecting to a geodatabase.Note:If Historical is selected and a name is not provided, the default transactional version is used. If Point in time is selected and a date is not provided in the Date and Time parameter, the default transactional version is used.Transactional—Connect to a transactional version. If Transactional is selected, the The following version will be used parameter will be populated with a list of transactional versions, and the Date and Time parameter will be inactive. This is the Default.Historical—Connect to an historical marker. If Historical is selected, the The following version will be used parameter will be populated with a list of historical markers, and the Date and Time parameter will be inactive. Point in time—Connect to a specific point in time. If Point in time is selected, the The following version will be used parameter will be inactive, and the Date and Time parameter will become active.Branch—Connect to the default branch version. | String |
| The following version will be used (Optional) | The geodatabase transactional version or historical marker to connect to. The default option uses the default transactional version.If you choose a branch version type, the connection is always to the default branch version. | String |
| Date and Time (Optional) | The value representing the date and time that will be used to connect to the database. This parameter is used with archive-enabled data. Use the time picker to choose the appropriate date.If manually entering a date, the following formats can be used: 6/9/2011 4:20:15 PM6/9/2011 16:20:156/9/2011 4:20:15 PM16:20:15Note:If a time is entered without a date, the default date of December 30, 1899, will be used.If a date is entered without a time, the default time of 12:00:00 AM will be used. | Date |
| database_platform | Specifies the database platform to which the connection will be made.SQL_SERVER—Connect to Microsoft SQL Server or Microsoft Azure SQL Database.ORACLE—Connect to Oracle.DB2—Connect to IBM DB2 for Linux, UNIX, or Windows.POSTGRESQL—Connect to PostgreSQL.TERADATA—Connect to Teradata Data Warehouse Appliance.SAP HANA—Connect to SAP HANA.DAMENG—Connect to Dameng. | String |
| instance | The database server or instance to which the connection will be made.This parameter value depends on the Database Platform parameter value chosen. | String |
| account_authentication(Optional) | Specifies the type of authentication that will be used.DATABASE_AUTH—Database authentication will be used. An internal database user name and password are used to connect to the database. You aren't required to type your user name and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. This is the default.Note:If the connection file you are creating will provide ArcGIS services with access to the database or geodatabase, or if you want to use the Catalog search to locate data accessed through this connection file, you must type a user name and password.OPERATING_SYSTEM_AUTH—Operating system authentication will be used. You do not need to type a user name and password. The connection will be made with the user name and password that were used to log in to the operating system. If the login used for the operating system is not a valid geodatabase login, the connection will fail. | Boolean |
| username(Optional) | The database user name that will be used when using database authentication. | String |
| password(Optional) | The database user password that will be used when using database authentication. | Encrypted String |
| database(Optional) | The name of the database to which you will connect. This parameter only applies to PostgreSQL and SQL Server platforms. | String |
| object_name(Optional) | The name of the dataset or object in the database to which the connection string will point. This connection string can be used as a path to the specified dataset. | String |
| data_type(Optional) | The type of dataset or object referred to in the dataset object name. If there are multiple objects with the same name in the database, you may need to specify the data type of the object for which you want to make a connection string. | String |
| feature_dataset(Optional) | The name of the feature dataset containing the dataset or object for which you want to make a connection string. If the dataset is not in a feature dataset (for example, if it's at the root of the database), do not specify a target feature dataset. | String |
| schema(Optional) | The user schema geodatabase to which you will connect. This option only applies to Oracle databases that contain at least one user-schema geodatabase. The default value for this parameter is to use the sde schema (master) geodatabase. | String |
| version_type(Optional) | Specifies the type of version to which you will connect. This parameter only applies when connecting to a geodatabase.TRANSACTIONAL—Connect to a transactional version. If Transactional is selected, the The following version will be used parameter will be populated with a list of transactional versions, and the Date and Time parameter will be inactive. This is the Default.HISTORICAL—Connect to an historical marker. If Historical is selected, the The following version will be used parameter will be populated with a list of historical markers, and the Date and Time parameter will be inactive. POINT_IN_TIME—Connect to a specific point in time. If Point in time is selected, the The following version will be used parameter will be inactive, and the Date and Time parameter will become active.BRANCH—Connect to the default branch version.Note:If Historical is selected and a name is not provided, the default transactional version is used. If Point in time is selected and a date is not provided in the Date and Time parameter, the default transactional version is used. | String |
| version(Optional) | The geodatabase transactional version or historical marker to connect to. The default option uses the default transactional version.If you choose a branch version type, the connection is always to the default branch version. | String |
| date(Optional) | The value representing the date and time that will be used to connect to the database when working with archive-enabled data. Dates can be entered in the following formats: 6/9/2011 4:20:15 PM 6/9/2011 16:20:15 6/9/2011 4:20:15 PM16:20:15Note:If a time is entered without a date, the default date of December 30, 1899, will be used.If a date is entered without a time, the default time of 12:00:00 AM will be used. | Date |

## Code Samples

### Example 1

```python
arcpy.management.CreateDatabaseConnectionString(database_platform, instance, {account_authentication}, {username}, {password}, {database}, {object_name}, {data_type}, {feature_dataset}, {schema}, {version_type}, {version}, {date})
```

### Example 2

```python
import arcpy
data_conn_str = arcpy.CreateDatabaseConnectionString_management("SQL_SERVER",
                                          "utah",
                                          "DATABASE_AUTH",
                                          "gdb",
                                          "gdb", 
                                          "",
                                          "gdb.roads")
arcpy.Buffer_analysis(data_conn_str, r"c:\temp\Buffers.shp", "10 Miles")
```

### Example 3

```python
import arcpy
data_conn_str = arcpy.CreateDatabaseConnectionString_management("SQL_SERVER",
                                          "utah",
                                          "DATABASE_AUTH",
                                          "gdb",
                                          "gdb", 
                                          "",
                                          "gdb.roads")
arcpy.Buffer_analysis(data_conn_str, r"c:\temp\Buffers.shp", "10 Miles")
```

---

## Create Database Connection (Data Management)

## Summary

Creates a file that ArcGIS uses to connect to a database or an enterprise geodatabase.

## Usage

- After valid connection information is provided on the tool dialog box to establish a connection, the tool will connect to the database to determine if it contains the geodatabase schema.For databases, all the parameters in the Geodatabase Connection Properties parameter category in the Geoprocessing pane will be ignored.For geodatabases, the tool automatically populates the Geodatabase Connection Properties parameter category. The Schema parameter is set with the sde schema (for Oracle user schema geodatabases only), the default Version Type value for the geodatabase, and the Default version.
- For databases, all the parameters in the Geodatabase Connection Properties parameter category in the Geoprocessing pane will be ignored.
- For geodatabases, the tool automatically populates the Geodatabase Connection Properties parameter category. The Schema parameter is set with the sde schema (for Oracle user schema geodatabases only), the default Version Type value for the geodatabase, and the Default version.
- If you don't want to save the connection information in the Results window or don't want to store it in the geoprocessing history log files, disable all logging options for geoprocessing history and save the connection file without saving the connection information.
- Connections from ArcGIS to Altibase and Netezza are no longer supported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Connection File Location | The folder path where the database connection file (.sde) will be stored. | Folder |
| Connection File Name | The name of the database connection file. The output file will have the .sde extension. | String |
| Database Platform | Specifies the database management system platform to which the connection will be made. The following are valid options:BigQuery—The connection will be made to Google BigQuery.Dameng—The connection will be made to Dameng.Db2—The connection will be made to IBM Db2 for Linux, UNIX, or Windows.Oracle—The connection will be made to Oracle, Amazon Relational Database Service (RDS) for Oracle, or Autonomous Transaction Processing.PostgreSQL—The connection will be made to PostgreSQL, Amazon Aurora (PostgreSQL-compatible edition), Amazon Relational Database Service (RDS) for PostgreSQL, Google Cloud SQL for PostgreSQL, Microsoft Azure Database for PostgreSQL, or Microsoft Azure Cosmos DB for PostgreSQL.Redshift—The connection will be made to Amazon Redshift.Teradata—The connection will be made to Teradata Vantage.SAP HANA—The connection will be made to SAP HANA or SAP HANA Cloud.Snowflake—The connection will be made to Snowflake.SQL Server—The connection will be made to Microsoft SQL Server, Microsoft Azure SQL Database, Microsoft Azure SQL Managed Instance, Amazon Relational Database Service (RDS) for SQL Server, or Google Cloud SQL for SQL Server. | String |
| Instance/Server (Optional) | The database server or instance to which the connection will be made.The value you choose from the Database Platform drop-down list indicates the type of database or cloud data warehouse to which the connection will be made. The information you provide for the Instance/Server parameter will vary, depending on the connection type you choose. See below for information about what to provide for each platform.Dameng—The name of the server where the Dameng database is installedDb2—The name of the cataloged Db2 databaseOracle—Either the TNS name or the Oracle Easy Connection string to connect to the Oracle database or database servicePostgreSQL—The name of the server where PostgreSQL is installed or the name of the PostgreSQL database service instanceRedshift—The URL for the Redshift serverSAP HANA—The Open Database Connectivity (ODBC) data source name for the SAP HANA database or database serviceSnowflake—The URL of the Snowflake serverSQL Server—The name of the SQL Server database instance or the name of the database service instanceTeradata—The ODBC data source name for the Teradata database | String |
| Database Authentication (Optional) | Specifies the type of authentication that will be used.Database authentication—Database authentication will be used. An internal database username and a password will be used to connect to the database. You aren't required to provide your username and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. Note:If the connection file you are creating will provide ArcGIS services with access to the database or geodatabase, or if you want to use the Catalog search to locate data accessed through this connection file, you must include a username and password.Operating system authentication—Operating system authentication will be used. You do not need to provide a username and password. The connection will be made with the username and password that were used to log in to the operating system. If the login used for the operating system is not a valid database login, the connection will fail. | Boolean |
| Username(Optional) | The database username that will be used for database authentication. | String |
| Password (Optional) | The database user password that will be used for database authentication. | Encrypted String |
| Save username and password (Optional) | Specifies whether the username and password will be saved.Checked—The username and password will be saved in the connection file. This is the default. If the connection file you are creating will provide ArcGIS services with access to the database, geodatabase, or cloud data warehouse, if you need to use the Catalog search to locate data accessed through this connection file, or if you will use the connection file in a script, you must save the username and password.Unchecked—The username and password will not be saved in the connection file. Every time you attempt to connect using the file, you will be prompted for the username and password. Note:ArcGIS Pro caches credentials for the first successful database connection for which you do not save the username and password. If you create a second connection to the same database, geodatabase, or cloud data warehouse instance in the same ArcGIS Pro session and do not save credentials, the cached credentials from the first connection are used even if you provide different credentials in the second connection file. To connect with different credentials without saving the username and password, start a new ArcGIS Pro session. | Boolean |
| Database (Optional) | The name of the database to which the connection will be made. This parameter applies to PostgreSQL, Redshift, Snowflake, and SQL Server platforms. | String |
| Schema (Oracle user schema geodatabases only) (Optional) | The user schema geodatabase to which the connection will be made. The tool will determine if it is connecting to an Oracle database that contains a user–schema geodatabase. If the Oracle database contains a user schema, this option is active; otherwise, it remains inactive. The default option for this parameter is to use the sde schema geodatabase. | String |
| Version Type (Optional) | Specifies the type of version to which the connection will be made. This parameter only applies when connecting to a geodatabase.Note:If Historical is selected and a name is not provided, the default transactional version will be used. If Point in time is selected and a date is not provided for the Date and Time parameter, the default transactional version will be used.Transactional—The connection will be made to a transactional version. If Transactional is selected, the The following version will be used parameter will be populated with a list of transactional versions, and the Date and Time parameter will be inactive. This is the default.Historical—The connection will be made to an historical marker. If Historical is selected, the The following version will be used parameter will be populated with a list of historical markers, and the Date and Time parameter will be inactive.Point in time—The connection will be made to a specific point in time. If Point in time is selected, the The following version will be used parameter will be inactive, and the Date and Time parameter will become active.Branch—The connection will be made to the default branch version. | String |
| The following version will be used (Optional) | The geodatabase transactional version or historical marker to which the connection will be made. The default option uses the default transactional version.If you choose a branch version type, the connection is always to the default branch version. | String |
| Date and Time(Optional) | The value representing the date and time that will be used to connect to the database. This option is used with archive-enabled data. Use the time picker to choose the appropriate date.If manually entering a date, the following formats can be used: 6/9/2011 4:20:15 PM6/9/2011 16:20:156/9/2011 4:20:15 PM16:20:15Note:If a time is entered without a date, the default date of December 30, 1899, will be used.If a date is entered without a time, the default time of 12:00:00 AM will be used. | Date |
| Advanced Authentication Type (Optional) | Specifies the advanced authentication type that will be used when connecting to a cloud data warehouse, Microsoft Azure SQL Database, or Azure SQL Managed Instance. Microsoft Entra MFA—The Microsoft Entra multifactor authentication (MFA) username authentication type will be used, but not the password. When you connect, a code is sent to you in a text message, email, or MFA device, or it can use a fingerprint scan for authentication. This second part of the authentication process varies depending on how your network and authentication protocols are configured. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.Microsoft Entra Password—The Microsoft Entra Password authentication type, which requires username and password parameter values, will be used. Usernames can be a maximum of 30 characters. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.Microsoft Entra Integrated—The Microsoft Entra Integrated authentication type will be used. You do not need to provide a username and password. The connection will be made with the username and password that were used to log in to the operation system. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.Service Authentication—The service authentication type when connecting to Google BigQuery will be used. See Google BigQuery documentation about authentication for information.Standard—The standard authentication type when connecting to Amazon Redshift will be used. See the Amazon Redshift ODBC Data Connector Installation and Configuration Guide for information about standard authentication.User—An authentication method that requires a username and password when connecting to Snowflake will be used.User Authentication—The user authentication type when connecting to Google BigQuery will be used. See Google BigQuery documentation about authentication for information. | String |
| Project ID(Optional) | The project ID for the Google BigQuery connection. | String |
| Default Dataset(Optional) | The default dataset for the Google BigQuery connection. | String |
| Refresh Token(Optional) | The refresh token value.This parameter is only applicable for Google BigQuery connections when the advanced authentication type is user authentication. | Encrypted String |
| Key File(Optional) | The key file value.This parameter is only applicable for Google BigQuery connections when the advanced authentication type is server authentication. | File |
| Role (Optional) | The role value for a cloud data warehouse connection. This parameter is only applicable for connections to Snowflake. | String |
| Warehouse(Optional) | The warehouse value for the connection.This parameter is only applicable for connections to Snowflake. | String |
| Advanced Options(Optional) | The advanced options for the connection. This is optional connection information that is specific to the cloud data warehouse platform (Google BigQuery, Amazon Redshift, or Snowflake) to which you connect. Provide advanced options using Option=<value> separated by semicolons. For example, option1=value1;option2=value2;. Consult the cloud data warehouse documentation for information about optional connection options. | String |
| out_folder_path | The folder path where the database connection file (.sde) will be stored. | Folder |
| out_name | The name of the database connection file. The output file will have the .sde extension. | String |
| database_platform | Specifies the database management system platform to which the connection will be made. The following are valid options:BIGQUERY—The connection will be made to Google BigQuery.DAMENG—The connection will be made to Dameng.DB2—The connection will be made to IBM Db2 for Linux, UNIX, or Windows.ORACLE—The connection will be made to Oracle, Amazon Relational Database Service (RDS) for Oracle, or Autonomous Transaction Processing.POSTGRESQL—The connection will be made to PostgreSQL, Amazon Aurora (PostgreSQL-compatible edition), Amazon Relational Database Service (RDS) for PostgreSQL, Google Cloud SQL for PostgreSQL, Microsoft Azure Database for PostgreSQL, or Microsoft Azure Cosmos DB for PostgreSQL.REDSHIFT—The connection will be made to Amazon Redshift.SAP HANA—The connection will be made to SAP HANA or SAP HANA Cloud.SNOWFLAKE—The connection will be made to Snowflake.SQL_SERVER—The connection will be made to Microsoft SQL Server, Microsoft Azure SQL Database, Microsoft Azure SQL Managed Instance, Amazon Relational Database Service (RDS) for SQL Server, or Google Cloud SQL for SQL Server.TERADATA—The connection will be made to Teradata Vantage. | String |
| instance(Optional) | The database server or instance to which the connection will be made.The value you specify for the database_platform parameter indicates the type of database or cloud data warehouse to which the connection will be made. The information you provide for the instance parameter will vary, depending on the platform you specified. See below for information about what to provide for each platform.Dameng—The name of the server where the Dameng database is installedDb2—The name of the cataloged Db2 databaseOracle—Either the TNS name or the Oracle Easy Connection string to connect to the Oracle database or database servicePostgreSQL—The name of the server where PostgreSQL is installed or the name of the PostgreSQL database service instanceRedshift—The URL for the Redshift serverSAP HANA—The Open Database Connectivity (ODBC) data source name for the SAP HANA database or database serviceSnowflake—The URL of the Snowflake serverSQL Server—The name of the SQL Server database instance or the name of the database service instanceTeradata—The ODBC data source name for the Teradata database | String |
| account_authentication(Optional) | Specifies the type of authentication that will be used.DATABASE_AUTH—Database authentication will be used. An internal database username and a password will be used to connect to the database. You aren't required to provide your username and password to create a connection; however, if you don't, you will be prompted to enter them when a connection is established. Note:If the connection file you are creating will provide ArcGIS services with access to the database or geodatabase, or if you want to use the Catalog search to locate data accessed through this connection file, you must include a username and password.OPERATING_SYSTEM_AUTH—Operating system authentication will be used. You do not need to provide a username and password. The connection will be made with the username and password that were used to log in to the operating system. If the login used for the operating system is not a valid database login, the connection will fail. | Boolean |
| username(Optional) | The database username that will be used for database authentication. | String |
| password(Optional) | The database user password that will be used for database authentication. | Encrypted String |
| save_user_pass(Optional) | Specifies whether the username and password will be saved.SAVE_USERNAME—The username and password will be saved in the connection file. This is the default. If the connection file you are creating will provide ArcGIS services with access to the database, geodatabase, or cloud data warehouse, or if you want to use the Catalog search to locate data accessed through this connection file, you must save the username and password.DO_NOT_SAVE_USERNAME—The username and password will not be saved in the connection file. Every time you attempt to connect using the file, you will be prompted for the username and password.Note:ArcGIS Pro caches credentials for the first successful database connection for which you do not save the username and password. If you create a second connection to the same database, geodatabase, or cloud data warehouse instance in the same ArcGIS Pro session and do not save credentials, the cached credentials from the first connection are used even if you provide different credentials in the second connection file. To connect with different credentials without saving the username and password, start a new ArcGIS Pro session. | Boolean |
| database(Optional) | The name of the database to which the connection will be made. This parameter applies to PostgreSQL, Redshift, Snowflake, and SQL Server platforms. | String |
| schema(Optional) | The user schema geodatabase to which the connection will be made. This option only applies to Oracle databases that contain at least one user–schema geodatabase. The default value for this parameter is to use the sde schema geodatabase. | String |
| version_type(Optional) | Specifies the type of version to which the connection will be made.TRANSACTIONAL—The connection will be made to a traditional transactional version.Note:This option does not apply to geodatabases in SAP HANA.HISTORICAL—The connection will be made to an historical marker.POINT_IN_TIME—The connection will be made to a specific point in time. If POINT_IN_TIME is used, the version parameter will be ignored.BRANCH—The connection will be made to the default branch version.Note: If TRANSACTIONAL or HISTORICAL is used, the date parameter will be ignored. If HISTORICAL is used and a name is not provided for the version parameter, the default transactional version will be used. If POINT_IN_TIME is used and a date is not provided for the date parameter, the default transactional version will be used. | String |
| version(Optional) | The geodatabase transactional version or historical marker to which the connection will be made. The default option uses the default transactional version.If you choose a branch version type, the connection is always to the default branch version. | String |
| date(Optional) | The value representing the date and time that will be used to connect to the database when working with archive-enabled data. Dates can be entered in the following formats: 6/9/2011 4:20:15 PM 6/9/2011 16:20:15 6/9/2011 4:20:15 PM16:20:15Note:If a time is entered without a date, the default date of December 30, 1899, will be used.If a date is entered without a time, the default time of 12:00:00 AM will be used. | Date |
| auth_type(Optional) | Specifies the advanced authentication type that will be used when connecting to a cloud data warehouse, Microsoft Azure SQL Database, or Azure SQL Managed Instance. AZURE_ACTIVE_DIRECTORY_UNIVERSAL_WITH_MFA—The Microsoft Entra multifactor authentication (MFA) username authentication type will be used, but not the password. When you connect, a code is sent to you in a text message, email, or MFA device, or it can use a fingerprint scan for authentication. This second part of the authentication process varies depending on how your network and authentication protocols are configured. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.AZURE_ACTIVE_DIRECTORY_PASSWORD—The Microsoft Entra Password authentication type, which requires username and password parameter values, will be used. Usernames can be a maximum of 30 characters. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.AZURE_ACTIVE_DIRECTORY_INTEGRATEDSERVICE_AUTHENTICATION—The Microsoft Entra Integrated authentication type will be used. You do not need to provide a username and password. The connection will be made with the username and password that were used to log in to the operation system. This option is supported for Azure SQL Database and Azure SQL Managed Instance only.SERVICE_AUTHENTICATION—The service authentication type when connecting to Google BigQuery will be used. See Google BigQuery documentation about authentication for information.STANDARD—The standard authentication type when connecting to Amazon Redshift will be used. See the Amazon Redshift ODBC Data Connector Installation and Configuration Guide for information about standard authentication.USER—An authentication method that requires a username and password when connecting to Snowflake will be used.USER_AUTHENTICATION—The user authentication type when connecting to Google BigQuery will be used. See Google BigQuery documentation about authentication for information. | String |
| project_id(Optional) | The project ID for the Google BigQuery connection. | String |
| default_dataset(Optional) | The default dataset for the Google BigQuery connection. | String |
| refresh_token(Optional) | The refresh token value.This parameter is only applicable for Google BigQuery connections when the advanced authentication type is user authentication. | Encrypted String |
| key_file(Optional) | The key file value.This parameter is only applicable for Google BigQuery connections when the advanced authentication type is server authentication. | File |
| role(Optional) | The role value for a cloud data warehouse connection. This parameter is only applicable for connections to Snowflake. | String |
| warehouse(Optional) | The warehouse value for the connection.This parameter is only applicable for connections to Snowflake. | String |
| advanced_options(Optional) | The advanced options for the connection. This is optional connection information that is specific to the cloud data warehouse platform (Google BigQuery, Amazon Redshift, or Snowflake) to which you connect. Provide advanced options using Option=<value> separated by semicolons. For example, option1=value1;option2=value2;. Consult the cloud data warehouse documentation for information about optional connection options. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateDatabaseConnection(out_folder_path, out_name, database_platform, {instance}, {account_authentication}, {username}, {password}, {save_user_pass}, {database}, {schema}, {version_type}, {version}, {date}, {auth_type}, {project_id}, {default_dataset}, {refresh_token}, {key_file}, {role}, {warehouse}, {advanced_options})
```

### Example 2

```python
import arcpy
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "utah.sde",
                                          "SQL_SERVER",
                                          "utah",
                                          "DATABASE_AUTH",
                                          "gdb",
                                          "gdb",
                                          "SAVE_USERNAME",
                                          "garfield",
                                          "#",
                                          "TRANSACTIONAL",
                                          "sde.DEFAULT")
```

### Example 3

```python
import arcpy
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "utah.sde",
                                          "SQL_SERVER",
                                          "utah",
                                          "DATABASE_AUTH",
                                          "gdb",
                                          "gdb",
                                          "SAVE_USERNAME",
                                          "garfield",
                                          "#",
                                          "TRANSACTIONAL",
                                          "sde.DEFAULT")
```

### Example 4

```python
# Name: CreateDatabaseConnection2.py
# Description: Connects to a database using Easy Connect string
#              and operating system authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "zion.sde",
                                          "ORACLE",
                                          "zionserver/ORCL",
                                          "OPERATING_SYSTEM_AUTH")
```

### Example 5

```python
# Name: CreateDatabaseConnection2.py
# Description: Connects to a database using Easy Connect string
#              and operating system authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "zion.sde",
                                          "ORACLE",
                                          "zionserver/ORCL",
                                          "OPERATING_SYSTEM_AUTH")
```

### Example 6

```python
# Name: CreateDatabaseConnection3.py
# Description: Connects to a geodatabase historical marker using a
#              cataloged DB2 database and database authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "history.sde",
                                          "DB2",
                                          "DB2_DS",
                                          "DATABASE_AUTH",
                                          "butch",
                                          "sundance",
                                          "SAVE_USERNAME",
                                          "#",
                                          "#",
                                          "HISTORICAL",
                                          "June 9, 2010",
                                          "#")
```

### Example 7

```python
# Name: CreateDatabaseConnection3.py
# Description: Connects to a geodatabase historical marker using a
#              cataloged DB2 database and database authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "history.sde",
                                          "DB2",
                                          "DB2_DS",
                                          "DATABASE_AUTH",
                                          "butch",
                                          "sundance",
                                          "SAVE_USERNAME",
                                          "#",
                                          "#",
                                          "HISTORICAL",
                                          "June 9, 2010",
                                          "#")
```

### Example 8

```python
# Name: CreateDatabaseConnection4.py
# Description: Connects to a point in time in the geodatabase in
#              PostgreSQL using database authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "history.sde",
                                          "POSTGRESQL",
                                          "dbserver",
                                          "DATABASE_AUTH",
                                          "stevie",
                                          "smith",
                                          "SAVE_USERNAME",
                                          "archivedb",
                                          "#",
                                          "POINT_IN_TIME",
                                          "#",
                                          "5/19/2011 8:43:41 AM")
```

### Example 9

```python
# Name: CreateDatabaseConnection4.py
# Description: Connects to a point in time in the geodatabase in
#              PostgreSQL using database authentication.

# Import system modules
import arcpy

# Run the tool
arcpy.CreateDatabaseConnection_management("C:\\MyProject",
                                          "history.sde",
                                          "POSTGRESQL",
                                          "dbserver",
                                          "DATABASE_AUTH",
                                          "stevie",
                                          "smith",
                                          "SAVE_USERNAME",
                                          "archivedb",
                                          "#",
                                          "POINT_IN_TIME",
                                          "#",
                                          "5/19/2011 8:43:41 AM")
```

---

## Create Database Sequence (Data Management)

## Summary

Creates a database sequence in a geodatabase. You can use the sequences in custom applications that access the geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The file, mobile, or enterprise geodatabase in which the sequence will be created. For an enterprise geodatabase, the workspace is the database connection file (.sde) to connect to the enterprise geodatabase in which you want to create a sequence or the path to the file geodatabase (including the file geodatabase name). For database connections, the user specified in the database connection will be the owner of the sequence and must have the following permissions in the database:Db2—CREATEIN privilege on their schemaOracle—CREATE SEQUENCE system privilegePostgreSQL—Authority on their schemaSAP HANA—Must be a standard userSQL Server—CREATE SEQUENCE privilege and ALTER OR CONTROL permission on their schema | Workspace |
| Sequence Name | The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. It is important that you remember this name, as it is the name you use in your custom applications and expressions to invoke the sequence. | String |
| Sequence Start ID (Optional) | The starting number of the sequence. If you do not provide a starting number, the sequence starts with 1. If you do provide a starting number, it must be greater than 0. | Long |
| Sequence Increment Value (Optional) | Describes how the sequence numbers will increment. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence is 15, and the next value after that is 20. If you do not specify an increment value, sequence values will increment by 1. | Long |
| in_workspace | The file, mobile, or enterprise geodatabase in which the sequence will be created. For an enterprise geodatabase, the workspace is the database connection file (.sde) to connect to the enterprise geodatabase in which you want to create a sequence or the path to the file geodatabase (including the file geodatabase name). For database connections, the user specified in the database connection will be the owner of the sequence and must have the following permissions in the database:Db2—CREATEIN privilege on their schemaOracle—CREATE SEQUENCE system privilegePostgreSQL—Authority on their schemaSAP HANA—Must be a standard userSQL Server—CREATE SEQUENCE privilege and ALTER OR CONTROL permission on their schema | Workspace |
| seq_name | The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. It is important that you remember this name, as it is the name you use in your custom applications and expressions to invoke the sequence. | String |
| seq_start_id(Optional) | The starting number of the sequence. If you do not provide a starting number, the sequence starts with 1. If you do provide a starting number, it must be greater than 0. | Long |
| seq_inc_value(Optional) | Describes how the sequence numbers will increment. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence is 15, and the next value after that is 20. If you do not specify an increment value, sequence values will increment by 1. | Long |

## Code Samples

### Example 1

```python
arcpy.management.CreateDatabaseSequence(in_workspace, seq_name, {seq_start_id}, {seq_inc_value})
```

### Example 2

```python
import arcpy
arcpy.management.CreateDatabaseSequence(r"C:/myconnections/mygdb.sde", 
                                        "custom_sequence", 1, 1)
```

### Example 3

```python
import arcpy
arcpy.management.CreateDatabaseSequence(r"C:/myconnections/mygdb.sde", 
                                        "custom_sequence", 1, 1)
```

### Example 4

```python
import arcpy
arcpy.management.CreateDatabaseSequence(r"C:/geodatabases/myfilegdb.gdb", 
                                        "my_ids", 1, 1)
```

### Example 5

```python
import arcpy
arcpy.management.CreateDatabaseSequence(r"C:/geodatabases/myfilegdb.gdb", 
                                        "my_ids", 1, 1)
```

---

## Create Database User (Data Management)

## Summary

Creates a database user with privileges sufficient to create data in the database.

## Usage

- This tool can be used with Oracle, Microsoft SQL Server, or PostgreSQL. This tool is not supported with cloud-based database services.
- For Oracle and SQL Server, if an operating system login exists, this tool can add that login as a user to the specified database, but the login name and username must be the same.
- You cannot create a database user for a Microsoft Windows group.
- This tool creates log file tables for the user when run on a geodatabase in Oracle.
- If the login does not exist in the SQL Server instance or PostgreSQL database cluster, this tool adds the login, creates a user in the database provided as the Input Database Connection parameter value, and creates a schema for the user in the database. The database provided is set as the user's default database in SQL Server.
- If the login exists in the SQL Server instance, this tool adds the user to the database provided as the Input Database Connection parameter value and creates a matching schema. The user's default database is not changed in SQL Server.
- If the login exists in the PostgreSQL database cluster, this tool creates a matching schema in the database provided as the Input Database Connection parameter value.
- You cannot create a user named sde with this tool. The sde user is the geodatabase administrator user and requires more privileges than this tool grants.
- You cannot use delimiters, such as double quotation marks, when specifying a username. The username can only contain characters supported by the underlying database management system when provided without delimiters.
- Users created in the database have the following privileges granted to them:DBMSPrivilegesOracleCREATE PROCEDURECREATE SESSIONCREATE SEQUENCECREATE TABLECREATE TRIGGERCREATE VIEWSELECT ON DBA_ROLESPostgreSQLCONNECTTEMPORARYUSAGE on the sde schema if the user is created in a geodatabase or a database that has the ST_Geometry type installedSELECT, INSERT, UPDATE, and DELETE on the geometry_columns and geography_columns views and SELECT on the spatial_ref_sys view if PostGIS is installed in the databaseSQL ServerCREATE TABLECREATE PROCEDURECREATE VIEW

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The connection file to an Oracle, PostgreSQL, or SQL Server database or an enterprise geodatabase in those databases. Ensure that the connection is made as a user with privileges to create users in the database. When connecting to Oracle, you must connect as the sys user. | Workspace |
| Create Operating System Authenticated User (Optional) | Specifies the authentication type for the user. Use this parameter only if an operating system login exists for which you want to create a database user. This parameter is only supported for SQL Server and Oracle databases.Checked—An operating system-authenticated user will be created. The corresponding login must already exist.Unchecked—A database-authenticated user will be created. This is the default. | Boolean |
| Database User | The name of the new database user.If you create a database user for an operating system login, the username must be the same as the login name. | String |
| Database User Password (Optional) | The password for the new user. The password policy of the underlying database is enforced.If you create a database user for an operating system login, no input is required. | Encrypted String |
| Role (Optional) | The name of the existing database role to which the new user will be added. | String |
| Tablespace Name (Optional) | The name of the tablespace that will be used as the default tablespace for the new user in an Oracle database. You can specify a preconfigured tablespace, or, if the tablespace does not exist, it will be created in the Oracle default storage location with its size set to 400 MB. If no tablespace is provided, the user's default tablespace will be set to the Oracle default tablespace. | String |
| input_database | The connection file to an Oracle, PostgreSQL, or SQL Server database or an enterprise geodatabase in those databases. Ensure that the connection is made as a user with privileges to create users in the database. When connecting to Oracle, you must connect as the sys user. | Workspace |
| user_authentication_type(Optional) | Specifies the authentication type for the user. If you specify OPERATING_SYSTEM_USER, an operating system login must already exist for the user you will create. Operating system users are only supported for SQL Server and Oracle databases.DATABASE_USER—A database-authenticated user will be created. This is the default. If your database management system is not configured to allow database authentication, do not use this option.OPERATING_SYSTEM_USER—An operating system-authenticated user will be created. The corresponding login must already exist. If your database management system is not configured to allow operating system authentication, do not use this option. | Boolean |
| user_name | The name of the new database user.If you create a database user for an operating system login, the username must be the same as the login name. | String |
| user_password(Optional) | The password for the new user. The password policy of the underlying database is enforced.If you create a database user for an operating system login, no input is required. | Encrypted String |
| role(Optional) | The name of the existing database role to which the new user will be added. | String |
| tablespace_name(Optional) | The name of the tablespace that will be used as the default tablespace for the new user in an Oracle database. You can specify a preconfigured tablespace, or, if the tablespace does not exist, it will be created in the Oracle default storage location with its size set to 400 MB. If no tablespace is provided, the user's default tablespace will be set to the Oracle default tablespace. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateDatabaseUser(input_database, {user_authentication_type}, user_name, {user_password}, {role}, {tablespace_name})
```

### Example 2

```python
import arcpy
arcpy.management.CreateDatabaseUser("C:/myconnections/oracledb1.sde", 
                                    "DATABASE_USER", "map", "Pam987", "sdetbs")
```

### Example 3

```python
import arcpy
arcpy.management.CreateDatabaseUser("C:/myconnections/oracledb1.sde", 
                                    "DATABASE_USER", "map", "Pam987", "sdetbs")
```

### Example 4

```python
import arcpy
arcpy.management.CreateDatabaseConnection("C:/connections", "pgconn.sde", 
                                          "POSTGRESQL", myserver, mypgdb, 
                                          "DATABASE_AUTH", "ela", "3L@pwd", 
                                          "SAVE_USERNAME")
arcpy.management.CreateDatabaseUser("C:/connections/pgconn.sde", 
                                    "DATABASE_USER", "dataowner", "N0look")
```

### Example 5

```python
import arcpy
arcpy.management.CreateDatabaseConnection("C:/connections", "pgconn.sde", 
                                          "POSTGRESQL", myserver, mypgdb, 
                                          "DATABASE_AUTH", "ela", "3L@pwd", 
                                          "SAVE_USERNAME")
arcpy.management.CreateDatabaseUser("C:/connections/pgconn.sde", 
                                    "DATABASE_USER", "dataowner", "N0look")
```

### Example 6

```python
import arcpy
arcpy.management.CreateDatabaseUser("C:/gdbconnections/connection_ssi.sde", 
                                    "OPERATING_SYSTEM_USER", "mynet\\vorhoos")
```

### Example 7

```python
import arcpy
arcpy.management.CreateDatabaseUser("C:/gdbconnections/connection_ssi.sde", 
                                    "OPERATING_SYSTEM_USER", "mynet\\vorhoos")
```

### Example 8

```python
"""
Name: create_database_user.py
Description: Provide connection information to a database user.
Type create_database_user.py -h or create_database_user.py --help for usage
"""

# Import system modules
import arcpy
import os
import optparse
import sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 release")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")                   
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("--utype", dest="user_type", type ="choice", choices=['DATABASE_USER', 'OPERATING_SYSTEM_USER'], default='DATABASE_USER', help="Authentication type options (case-sensitive):  DATABASE_USER, OPERATING_SYSTEM_USER.  Default=DATABASE_USER")
parser.add_option ("-u", dest="dbuser", type="string", default="", help="database user name")
parser.add_option ("-p", dest="dbuser_pwd", type="string", default="", help="database user password")
parser.add_option ("-r", dest="role", type="string", default="", help="role to be granted to the user")
parser.add_option ("-t", dest="Tablespace", type="string", default="", help="Tablespace name")
# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print "%s: error: %s\n" % (sys.argv[0], "No command options given")
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	dbuser = options.dbuser
	dbuser_pwd = options.dbuser_pwd	
	tablespace = options.Tablespace
	user_type = options.user_type
	role = options.role

	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified."))
		parser.print_help()
		sys.exit(3)		
	
	if(database_type == "SQL_SERVER"):
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication"))
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication..."))
	else:		
		if( dbuser.lower() == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "Database user must be specified."))
			sys.exit(3)		
		if( dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!"))
			sys.exit(3)

	if ( user_type == "DATABASE_USER" and (dbuser =="" or dbuser_pwd =="")):
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "To create database authenticated user, user name and password must be specified!"))
		parser.print_help()
		sys.exit(3)	

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + "  Creating a user in an enterprise geodatabase or database requires an ArcGIS Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS Server license.")
		sys.exit("Re-authorize ArcGIS before creating a database user.")
	else:
		print("\n" + product_license + " license available!  Continuing to create...")
		arcpy.AddMessage("+++++++++")

	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + dbms_admin   

	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  

	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)

	try:
		print("\nCreating Database Connection File...\n")
		# Process: Create Database Connection File...
		# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, database, account_authentication, username, password, save_username_password(must be true)
		#arcpy.CreateDatabaseConnection_management(temp , Connection_File_Name, database_type, instance, database, account_authentication, dbms_admin, dbms_admin_pwd, "TRUE")
		arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=dbms_admin, password=dbms_admin_pwd, save_user_pass="TRUE")
	        for i in range(arcpy.GetMessageCount()):
			if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("\n+++++++++")
				arcpy.AddMessage("Exiting!!")
				arcpy.AddMessage("+++++++++\n")
				sys.exit(3)            
			else:
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("+++++++++\n")

		print("Creating database user...\n")
		arcpy.CreateDatabaseUser_management(input_workspace=Connection_File_Name_full_path, user_authentication_type=user_type, user_name=dbuser, user_password=dbuser_pwd, role=role, tablespace_name=tablespace)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()   
		parser.exit(2)
```

### Example 9

```python
"""
Name: create_database_user.py
Description: Provide connection information to a database user.
Type create_database_user.py -h or create_database_user.py --help for usage
"""

# Import system modules
import arcpy
import os
import optparse
import sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 release")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")                   
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("--utype", dest="user_type", type ="choice", choices=['DATABASE_USER', 'OPERATING_SYSTEM_USER'], default='DATABASE_USER', help="Authentication type options (case-sensitive):  DATABASE_USER, OPERATING_SYSTEM_USER.  Default=DATABASE_USER")
parser.add_option ("-u", dest="dbuser", type="string", default="", help="database user name")
parser.add_option ("-p", dest="dbuser_pwd", type="string", default="", help="database user password")
parser.add_option ("-r", dest="role", type="string", default="", help="role to be granted to the user")
parser.add_option ("-t", dest="Tablespace", type="string", default="", help="Tablespace name")
# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print "%s: error: %s\n" % (sys.argv[0], "No command options given")
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	dbuser = options.dbuser
	dbuser_pwd = options.dbuser_pwd	
	tablespace = options.Tablespace
	user_type = options.user_type
	role = options.role

	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified."))
		parser.print_help()
		sys.exit(3)		
	
	if(database_type == "SQL_SERVER"):
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication"))
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication..."))
	else:		
		if( dbuser.lower() == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "Database user must be specified."))
			sys.exit(3)		
		if( dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!"))
			sys.exit(3)

	if ( user_type == "DATABASE_USER" and (dbuser =="" or dbuser_pwd =="")):
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "To create database authenticated user, user name and password must be specified!"))
		parser.print_help()
		sys.exit(3)	

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + "  Creating a user in an enterprise geodatabase or database requires an ArcGIS Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS Server license.")
		sys.exit("Re-authorize ArcGIS before creating a database user.")
	else:
		print("\n" + product_license + " license available!  Continuing to create...")
		arcpy.AddMessage("+++++++++")

	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + dbms_admin   

	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  

	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)

	try:
		print("\nCreating Database Connection File...\n")
		# Process: Create Database Connection File...
		# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, database, account_authentication, username, password, save_username_password(must be true)
		#arcpy.CreateDatabaseConnection_management(temp , Connection_File_Name, database_type, instance, database, account_authentication, dbms_admin, dbms_admin_pwd, "TRUE")
		arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=dbms_admin, password=dbms_admin_pwd, save_user_pass="TRUE")
	        for i in range(arcpy.GetMessageCount()):
			if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("\n+++++++++")
				arcpy.AddMessage("Exiting!!")
				arcpy.AddMessage("+++++++++\n")
				sys.exit(3)            
			else:
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("+++++++++\n")

		print("Creating database user...\n")
		arcpy.CreateDatabaseUser_management(input_workspace=Connection_File_Name_full_path, user_authentication_type=user_type, user_name=dbuser, user_password=dbuser_pwd, role=role, tablespace_name=tablespace)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()   
		parser.exit(2)
```

---

## Create Database View (Data Management)

## Summary

Creates a view in a database based on an SQL expression.

## Usage

- The SQL expression used to define the view is validated by the database when you run the tool. Valid syntax for the view definition is determined by the underlying database. If the syntax is incorrect, an error message is returned.
- Views created in a geodatabase using this tool do not support a forward slash (/) character in the view definition. For example, if you have a column in the create view definition that is based on an expression such as Population1990 / 10, you need to create the view using SQL tools.
- This tool is supported for databases, Open Geospatial Consortium (OGC) GeoPackage files, enterprise geodatabases, mobile geodatabases, and file geodatabases.
- Views created in a geodatabase using this tool are not registered with the geodatabase.
- If a spatial column is included in the view, the geometry storage type of the column must be an SQL data type (such as ST_Geometry or Geometry), not a binary geometry storage type.
- Learn more about constructing a query and see examples of SQL expressions.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The database that contains the tables or feature classes used to construct the view. This database is also where the view will be created. | Workspace |
| Output View Name | The name of the view that will be created in the database. | String |
| View Definition | An SQL statement that will be used to construct the view. | String |
| input_database | The database that contains the tables or feature classes used to construct the view. This database is also where the view will be created. | Workspace |
| view_name | The name of the view that will be created in the database. | String |
| view_definition | An SQL statement that will be used to construct the view. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateDatabaseView(input_database, view_name, view_definition)
```

### Example 2

```python
import arcpy
arcpy.CreateDatabaseView_management("c:/Connections/city_data.sde","trees","select objectid, owner, parcel from inventory where type = trees")
```

### Example 3

```python
import arcpy
arcpy.CreateDatabaseView_management("c:/Connections/city_data.sde","trees","select objectid, owner, parcel from inventory where type = trees")
```

### Example 4

```python
import arcpy 
arcpy.management.CreateDatabaseView("C:\\Connections\\city_data.gdb",
                                    "trees",
                                    "select objectid, owner, parcel from inventory where type = trees")
```

### Example 5

```python
import arcpy 
arcpy.management.CreateDatabaseView("C:\\Connections\\city_data.gdb",
                                    "trees",
                                    "select objectid, owner, parcel from inventory where type = trees")
```

### Example 6

```python
import arcpy
arcpy.management.CreateDatabaseView("C:\\mymgdbs\\facilities.geodatabase",
                                    "dog_parks",
                                    "select objectid, park_name, park_address, park_hours from parks where park_type = dog")
```

### Example 7

```python
import arcpy
arcpy.management.CreateDatabaseView("C:\\mymgdbs\\facilities.geodatabase",
                                    "dog_parks",
                                    "select objectid, park_name, park_address, park_hours from parks where park_type = dog")
```

---

## Create Domain (Data Management)

## Summary

Creates an attribute domain in the specified workspace.

## Usage

- Domain management involves the following: Create the domain using this tool.Add values to or set the range of values for the domain using the Add Coded Value to Domain or Set Value For Range Domain tool.Associate the domain with a feature class using the Assign Domain To Field tool.
- Create the domain using this tool.
- Add values to or set the range of values for the domain using the Add Coded Value to Domain or Set Value For Range Domain tool.
- Associate the domain with a feature class using the Assign Domain To Field tool.
- Coded value domains only support default value and duplicate split policies and default value merge policies.
- Range domains support all split and merge policies. After a split or merge operation, the attribute values of output features are calculated based on the numeric values of the input features and the specified split or merge policy.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase that will contain the new domain. | Workspace |
| Domain Name | The name of the domain that will be created. | String |
| Domain Description(Optional) | The description of the domain that will be created. | String |
| Field Type(Optional) | Specifies the type of attribute domain that will be created. Attribute domains are rules that describe the accepted values of a field type. Specify a field type that matches the data type of the field to which the attribute domain will be assigned.Short (16-bit integer)—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.Long (32-bit integer)—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.Big integer (64-bit integer)—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.Float (32-bit floating point)—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.Double (64-bit floating point)—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.Text—The field type will be text. Text fields support a string of characters.Date—The field type will be date. Date fields support date and time values. Date only—The field type will be date only. Date only fields support date values with no time values.Time only—The field type will be time only. Time only fields support time values with no date values. | String |
| Domain Type(Optional) | Specifies the domain type that will be created.Coded value domain—A coded type domain will be created that contains a valid set of values for an attribute. This is the default. For example, a coded value domain can specify valid pipe material values such as CL—cast iron pipe, DL—ductile iron pipe, or ACP—asbestos concrete pipe.Range domain—A range type domain will be created that contains a valid range of values for a numeric attribute. For example, if distribution water mains have a pressure between 50 and 75 psi, a range domain specifies these minimum and maximum values. | String |
| Split Policy(Optional) | Specifies the split policy that will be used for the created domain. The behavior of an attribute's values when a feature that is split is controlled by its split policy.Use the attribute's default value—The attributes of the two resulting features will use the default value of the attribute of the given feature class or subtype. Duplicate attribute values—The attribute of the two resulting features will use a copy of the original object's attribute value. Use geometric ratio—The attributes of resulting features will be a ratio of the original feature's value. The ratio is based on the proportion into which the original geometry is divided. If the geometry is divided equally, each new feature's attribute gets one-half the value of the original object's attribute. The geometry ratio policy only applies to range domains. | String |
| Merge Policy(Optional) | Specifies the merge policy that will be used for the created domain. When two features are merged into a single feature, merge policies control attribute values in the new feature.Use the attribute's default value—The attribute of the resulting feature will use the default value of the attribute of the given feature class or subtype. This is the only merge policy that applies to nonnumeric fields and coded value domains. Sum of the values—The attribute of the resulting feature will use the sum of the values from the original feature's attribute. The sum values policy only applies to range domains. Area weighted average—The attribute of the resulting feature will be the weighted average of the attribute values of the original features. This average is based on the original feature's geometry. The area weighted policy only applies to range domains. | String |
| in_workspace | The geodatabase that will contain the new domain. | Workspace |
| domain_name | The name of the domain that will be created. | String |
| domain_description(Optional) | The description of the domain that will be created. | String |
| field_type(Optional) | Specifies the type of attribute domain that will be created. Attribute domains are rules that describe the accepted values of a field type. Specify a field type that matches the data type of the field to which the attribute domain will be assigned. SHORT—The field type will be short. Short fields support whole numbers between -32,768 and 32,767.LONG—The field type will be long. Long fields support whole numbers between -2,147,483,648 and 2,147,483,647.BIGINTEGER—The field type will be big integer. Big integer fields support whole numbers between -(253) and 253.FLOAT—The field type will be float. Float fields support fractional numbers between -3.4E38 and 1.2E38.DOUBLE—The field type will be double. Double fields support fractional numbers between -2.2E308 and 1.8E308.TEXT—The field type will be text. Text fields support a string of characters.DATE—The field type will be date. Date fields support date and time values. DATEONLY—The field type will be date only. Date only fields support date values with no time values.TIMEONLY—The field type will be time only. Time only fields support time values with no date values. | String |
| domain_type(Optional) | Specifies the domain type that will be created.CODED—A coded type domain will be created that contains a valid set of values for an attribute. This is the default. For example, a coded value domain can specify valid pipe material values such as CL—cast iron pipe, DL—ductile iron pipe, or ACP—asbestos concrete pipe.RANGE—A range type domain will be created that contains a valid range of values for a numeric attribute. For example, if distribution water mains have a pressure between 50 and 75 psi, a range domain specifies these minimum and maximum values. | String |
| split_policy(Optional) | Specifies the split policy that will be used for the created domain. The behavior of an attribute's values when a feature that is split is controlled by its split policy.DEFAULT—The attributes of the two resulting features will use the default value of the attribute of the given feature class or subtype. DUPLICATE—The attribute of the two resulting features will use a copy of the original object's attribute value. GEOMETRY_RATIO—The attributes of resulting features will be a ratio of the original feature's value. The ratio is based on the proportion into which the original geometry is divided. If the geometry is divided equally, each new feature's attribute gets one-half the value of the original object's attribute. The geometry ratio policy only applies to range domains. | String |
| merge_policy(Optional) | Specifies the merge policy that will be used for the created domain. When two features are merged into a single feature, merge policies control attribute values in the new feature.DEFAULT—The attribute of the resulting feature will use the default value of the attribute of the given feature class or subtype. This is the only merge policy that applies to nonnumeric fields and coded value domains. SUM_VALUES—The attribute of the resulting feature will use the sum of the values from the original feature's attribute. The sum values policy only applies to range domains. AREA_WEIGHTED—The attribute of the resulting feature will be the weighted average of the attribute values of the original features. This average is based on the original feature's geometry. The area weighted policy only applies to range domains. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateDomain(in_workspace, domain_name, {domain_description}, {field_type}, {domain_type}, {split_policy}, {merge_policy})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateDomain("montgomery.gdb", "Materials", 
                              "Valid pipe materials", "TEXT", "CODED")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateDomain("montgomery.gdb", "Materials", 
                              "Valid pipe materials", "TEXT", "CODED")
```

### Example 4

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data 
# every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"

# Process: Create the coded value domain
arcpy.management.CreateDomain("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")

# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", 
           "ACP": "Asbestos concrete", "COP": "Copper"}
    
# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.management.AddCodedValueToDomain(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.management.AssignDomainToField(inFeatures, inField, domName)
```

### Example 5

```python
# Name: MakeDomain.py
# Description: Create an attribute domain to constrain pipe material values
 
# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data 
# every time)
arcpy.env.workspace = "C:/data"
 
# Set local parameters
domName = "Material4"
gdb = "montgomery.gdb"
inFeatures = "Montgomery.gdb/Water/Distribmains"
inField = "Material"

# Process: Create the coded value domain
arcpy.management.CreateDomain("montgomery.gdb", domName, "Valid pipe materials", 
                              "TEXT", "CODED")

# Store all the domain values in a dictionary with the domain code as the "key" 
# and the domain description as the "value" (domDict[code])
domDict = {"CI":"Cast iron", "DI": "Ductile iron", "PVC": "PVC", 
           "ACP": "Asbestos concrete", "COP": "Copper"}
    
# Process: Add valid material types to the domain
# use a for loop to cycle through all the domain codes in the dictionary
for code in domDict:        
    arcpy.management.AddCodedValueToDomain(gdb, domName, code, domDict[code])
    
# Process: Constrain the material value of distribution mains
arcpy.management.AssignDomainToField(inFeatures, inField, domName)
```

---

## Create Enterprise Geodatabase (Data Management)

## Summary

Creates a database, storage locations, and a database user to act as the geodatabase administrator and owner of the geodatabase. Functionality varies depending on the database management system used. The tool grants the geodatabase administrator the privileges required to create a geodatabase; it then creates a geodatabase in the database.

## Usage

- The following table indicates the tool functionality for each type of database management system:FunctionDatabaseCreates a databasePostgreSQL and Microsoft SQL ServerCreates a tablespaceOracleCreates a geodatabase administrator user in the databaseOracle, PostgreSQL, and SQL ServerThe tool only creates a user in SQL Server if you create an sde-schema geodatabase.Grants the geodatabase administrator the privileges required to create a geodatabase, upgrade a geodatabase, and remove database connectionsOracle and PostgreSQLGrants the geodatabase administrator the privileges required to create a geodatabase and remove database connectionsSQL Server (if creating an sde-schema geodatabase)Grants the geodatabase administrator the privileges required to import data using Oracle Data PumpOracleCreates a geodatabase in the specified databaseOracle, PostgreSQL, and SQL Server
- You must have ArcGIS Desktop (Standard or Advanced), ArcGIS Pro (Standard or Advanced), or ArcGIS Server installed on the computer where you'll create the geodatabase. If you're using Oracle or SQL Server, you must also install and configure a database management system client on the computer where the ArcGIS client is installed.
- Before you can create a geodatabase in Oracle, you must download the DatabaseSupport.zip (Windows) or DatabaseSupport.tar (Linux) file from My Esri to obtain the st_shapelib (Windows) or libst_shapelib (Linux) library. Place the library on the Oracle server. Be sure to copy the correct library for the operating system where Oracle is installed.
- You must make a spatial type available to the PostgreSQL database cluster before you create a geodatabase. To use the ST_Geometry type, you must download the DatabaseSupport.zip (Windows) or DatabaseSupport.tar (Linux) file from My Esri to obtain the st_geometry library. Place the library in the %PostgreSQL%\lib directory (Windows) or $PKGLIBDIR directory (Linux) on the PostgreSQL server. Be sure to copy the correct library for the version of PostgreSQL you're using and the operating system where PostgreSQL is installed. To use a PostGIS spatial type, you must install PostGIS in the PostgreSQL database cluster.
- This tool is not supported with database services.
- Once the geodatabase is created in PostgreSQL or Oracle, you cannot rename the database in which it is stored. The database name is stored in geodatabase system tables and is used to fully qualify table and feature class names; changing the database name using database tools makes the geodatabase data inaccessible.
- To generate a license file for an enterprise geodatabases licensed with ArcGIS Enterprise on Kubernetes, use the exportGeodatabaseLicense REST operation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Database Platform | Specifies the type of database management system to which a connection to create a geodatabase will be made.Oracle—Connection to an Oracle instance will be made.PostgreSQL—Connection to a PostgreSQL database cluster will be made.SQL Server— Connection to a Microsoft SQL Server instance will be made. | String |
| Instance | The name of the instance.For SQL Server, provide the SQL Server instance name. Case-sensitive or binary collation SQL Server instances are not supported.For Oracle, provide either the TNS name or the Oracle Easy Connection string.For PostgreSQL, provide the name of the server where PostgreSQL is installed. | String |
| Database (Optional) | The name of the database.This parameter is valid for PostgreSQL and SQL Server. You can provide either the name of an existing, preconfigured database or a name for a database that the tool will create.If the tool creates the database in SQL Server, the file sizes will be either the same as defined for the SQL Server model database or 500 MB for the .mdf file and 125 MB for the .ldf file, whichever is greater. Both the .mdf and .ldf files are created in the default SQL Server location on the database server. Do not name the database sde.If the tool creates the database in PostgreSQL, it uses the template1 database as the template for the database. If you need to use a different template—for example, you require a template that is enabled for PostGIS—you must create the database before running this tool and provide the name of the existing database. Always use lowercase characters for the database name. If you use uppercase letters, the tool will convert them to lowercase. | String |
| Operating System Authentication (Optional) | Specifies the type of authentication that will be used for the database connection.Checked—Operating system authentication will be used. The login information that you provide for the computer where you run the tool is the login that will be used to authenticate the database connection. If the database management system is not configured to allow operating system authentication, authentication will fail.Unchecked—Database authentication will be used. You must provide a valid database username and password for authentication in the database. This is the default. If the database management system is not configured to allow database authentication, authentication will fail. | Boolean |
| Database Administrator (Optional) | The database administrator user for database authentication. For Oracle, use the sys user. For PostgreSQL, specify a user with superuser status. For SQL Server, specify any member of the sysadmin fixed server role. | String |
| Database Administrator Password (Optional) | The database administrator password for database authentication. | Encrypted String |
| Sde Owned Schema(Optional) | This parameter is only active for SQL Server and specifies whether the geodatabase will be created in the schema of the sde user or in the dbo schema in the database. .Checked—The geodatabase will be created in the schema of the sde user.Unchecked—You must be logged in to the SQL Server instance as a user who is dbo in the instance, and the geodatabase will be created in the dbo schema in the database. | Boolean |
| Geodatabase Administrator (Optional) | The name of the geodatabase administrator user.If you are using PostgreSQL, this value must be sde. If the sde login role does not exist, this tool will create it and grant it superuser status in the database cluster. If the sde login role exists, this tool will grant it superuser status if it does not already have it. The tool also creates an sde schema in the database and grants usage on the schema to public.If you are using Oracle, the value is sde. If the sde user does not exist in the Oracle database, the tool will create it and grant it the privileges required to create and upgrade a geodatabase and disconnect users from the database. The tool also grants privileges to allow data imports using Oracle Data Pump. If the sde user exists, the tool will grant these same privileges to the existing user.Note:Creating or upgrading user-schema geodatabases in Oracle is no longer supported. If you are using SQL Server and specified an sde-schema geodatabase, this value must be sde. The tool will create an sde login, database user, and schema and grant it privileges to create a geodatabase and remove connections from the SQL Server instance. If you specified a dbo schema, do not provide a value for this parameter. | String |
| Geodatabase Administrator Password (Optional) | The password for the geodatabase administrator user. If the geodatabase administrator user exists in the database management system, the password you provide must match the existing password. If the geodatabase administrator user does not exist, provide a valid database password for the new user. The password must meet the password policy enforced by the database.The password is an encrypted string. | Encrypted String |
| Tablespace Name (Optional) | The name of the tablespace.This parameter is only valid for Oracle and PostgreSQL database management system types. For Oracle, do one of the following: Provide the name of an existing tablespace. This tablespace will be used as the default tablespace for the geodatabase administrator user.Provide a valid name for a new tablespace. The tool will create a 400 MB tablespace in the Oracle default storage location and set it as the geodatabase administrator's default tablespace.Leave the tablespace blank. The tool will create a 400 MB tablespace named SDE_TBS in the Oracle default storage location. The SDE_TBS tablespace will be set as the geodatabase administrator's default tablespace.This tool does not create a tablespace in PostgreSQL. You must either provide the name of an existing tablespace to be used as the database's default tablespace or leave this parameter blank. If you leave the parameter blank, the tool will create a database in the pg_default tablespace. | String |
| Authorization File | The keycodes file that was created when ArcGIS Server was authorized. If you have not done so, authorize ArcGIS Server to create this file.This file is in the <drive>\Program Files\ESRI\License<release#>\sysgen folder on Windows or the /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. Tip:The /.wine directory is a hidden directory.You may need to copy the keycodes file from the ArcGIS Server machine to a location that is accessible to the tool. | File |
| Spatial Type (Optional) | Specifies the spatial type that will be used. This is only applicable to PostgreSQL databases.ST_Geometry—The ST_Geometry spatial type will be used. This is the default.PostGIS—The PostGIS spatial type will be used. | String |
| database_platform | Specifies the type of database management system to which a connection to create a geodatabase will be made.Oracle—Connection to an Oracle instance will be made.PostgreSQL—Connection to a PostgreSQL database cluster will be made.SQL_Server— Connection to a Microsoft SQL Server instance will be made. | String |
| instance_name | The name of the instance.For SQL Server, provide the SQL Server instance name. Case-sensitive or binary collation SQL Server instances are not supported.For Oracle, provide either the TNS name or the Oracle Easy Connection string.For PostgreSQL, provide the name of the server where PostgreSQL is installed. | String |
| database_name(Optional) | The name of the database.This parameter is valid for PostgreSQL and SQL Server. You can provide either the name of an existing, preconfigured database or a name for a database that the tool will create.If the tool creates the database in SQL Server, the file sizes will be either the same as defined for the SQL Server model database or 500 MB for the .mdf file and 125 MB for the .ldf file, whichever is greater. Both the .mdf and .ldf files are created in the default SQL Server location on the database server. Do not name the database sde.If the tool creates the database in PostgreSQL, it uses the template1 database as the template for the database. If you need to use a different template—for example, you require a template that is enabled for PostGIS—you must create the database before running this tool and provide the name of the existing database. Always use lowercase characters for the database name. If you use uppercase letters, the tool will convert them to lowercase. | String |
| account_authentication(Optional) | Specifies the type of authentication that will be used for the database connection.OPERATING_SYSTEM_AUTH—Operating system authentication will be used. The login information that you provide for the computer where you run the tool is the login that will be used to authenticate the database connection. If the database management system is not configured to allow operating system authentication, authentication will fail.DATABASE_AUTH—Database authentication will be used. You must provide a valid database username and password for authentication in the database. This is the default. If the database management system is not configured to allow database authentication, authentication will fail. | Boolean |
| database_admin(Optional) | The database administrator user for database authentication. For Oracle, use the sys user. For PostgreSQL, specify a user with superuser status. For SQL Server, specify any member of the sysadmin fixed server role. | String |
| database_admin_password(Optional) | The database administrator password for database authentication. | Encrypted String |
| sde_schema(Optional) | This parameter is only relevant to SQL Server and specifies whether the geodatabase will be created in the schema of a user named sde or in the dbo schema in the database. If creating a dbo-schema geodatabase, connect as a user who is dbo in the SQL Server instance.SDE_SCHEMA—The geodatabase repository is owned by and will be stored in the schema of a user named sde. This is the default.DBO_SCHEMA—The geodatabase repository will be stored in the dbo schema in the database. | Boolean |
| gdb_admin_name(Optional) | The name of the geodatabase administrator user.If you are using PostgreSQL, this value must be sde. If the sde login role does not exist, this tool will create it and grant it superuser status in the database cluster. If the sde login role exists, this tool will grant it superuser status if it does not already have it. The tool also creates an sde schema in the database and grants usage on the schema to public.If you are using Oracle, the value is sde. If the sde user does not exist in the Oracle database, the tool will create it and grant it the privileges required to create and upgrade a geodatabase and disconnect users from the database. The tool also grants privileges to allow data imports using Oracle Data Pump. If the sde user exists, the tool will grant these same privileges to the existing user.Note:Creating or upgrading user-schema geodatabases in Oracle is no longer supported. If you are using SQL Server and specified an sde-schema geodatabase, this value must be sde. The tool will create an sde login, database user, and schema and grant it privileges to create a geodatabase and remove connections from the SQL Server instance. If you specified a dbo schema, do not provide a value for this parameter. | String |
| gdb_admin_password(Optional) | The password for the geodatabase administrator user. If the geodatabase administrator user exists in the database management system, the password you provide must match the existing password. If the geodatabase administrator user does not exist, provide a valid database password for the new user. The password must meet the password policy enforced by the database.The password is an encrypted string. | Encrypted String |
| tablespace_name(Optional) | The name of the tablespace.This parameter is only valid for Oracle and PostgreSQL database management system types. For Oracle, do one of the following: Provide the name of an existing tablespace. This tablespace will be used as the default tablespace for the geodatabase administrator user.Provide a valid name for a new tablespace. The tool will create a 400 MB tablespace in the Oracle default storage location and set it as the geodatabase administrator's default tablespace.Leave the tablespace blank. The tool will create a 400 MB tablespace named SDE_TBS in the Oracle default storage location. The SDE_TBS tablespace will be set as the geodatabase administrator's default tablespace.This tool does not create a tablespace in PostgreSQL. You must either provide the name of an existing tablespace to be used as the database's default tablespace or leave this parameter blank. If you leave the parameter blank, the tool will create a database in the pg_default tablespace. | String |
| authorization_file | The keycodes file that was created when ArcGIS Server was authorized. If you have not done so, authorize ArcGIS Server to create this file.This file is in the <drive>\Program Files\ESRI\License<release#>\sysgen folder on Windows or the /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. Tip:The /.wine directory is a hidden directory.You may need to copy the keycodes file from the ArcGIS Server machine to a location that is accessible to the tool. | File |
| spatial_type(Optional) | Specifies the spatial type that will be used. This is only applicable to PostgreSQL databases.ST_GEOMETRY—The ST_Geometry spatial type will be used. This is the default.POSTGIS—The PostGIS spatial type will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateEnterpriseGeodatabase(database_platform, instance_name, {database_name}, {account_authentication}, {database_admin}, {database_admin_password}, {sde_schema}, {gdb_admin_name}, {gdb_admin_password}, {tablespace_name}, authorization_file, {spatial_type})
```

### Example 2

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "ORACLE", "ora11g:1521/elf", "", "DATABASE_AUTH", "sys", 
    "manager", "", "sde", "supersecret", "sdetbs", 
    "//myserver/mymounteddrive/myaccessibledirectory/keycodes")
```

### Example 3

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "ORACLE", "ora11g:1521/elf", "", "DATABASE_AUTH", "sys", 
    "manager", "", "sde", "supersecret", "sdetbs", 
    "//myserver/mymounteddrive/myaccessibledirectory/keycodes")
```

### Example 4

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "SQL_SERVER", "tor\ssinstance1", "sp_data", "OPERATING_SYSTEM_AUTH", "", "", 
    "SDE_SCHEMA", "sde", "sde", "", "//myserver/myaccessibledirectory/keycodes")
```

### Example 5

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "SQL_SERVER", "tor\ssinstance1", "sp_data", "OPERATING_SYSTEM_AUTH", "", "", 
    "SDE_SCHEMA", "sde", "sde", "", "//myserver/myaccessibledirectory/keycodes")
```

### Example 6

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "POSTGRESQL", "feldspar", "pggdb", "DATABASE_AUTH", "postgres", "averturis", 
    "", "sde", "nomira", "gdbspace", 
    "//arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License/sysgen/keycodes" "ST_GEOMETRY")
```

### Example 7

```python
import arcpy
arcpy.management.CreateEnterpriseGeodatabase(
    "POSTGRESQL", "feldspar", "pggdb", "DATABASE_AUTH", "postgres", "averturis", 
    "", "sde", "nomira", "gdbspace", 
    "//arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License/sysgen/keycodes" "ST_GEOMETRY")
```

### Example 8

```python
"""
Name: create_enterprise_gdb.py
Description: Provide connection information to a DBMS instance and create an enterprise geodatabase.
Type  create_enterprise_gdb.py -h or create_enterprise_gdb.py --help for usage
Author: Esri
"""

# Import system modules
import arcpy, os, optparse, sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 and higher releases")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("--schema", dest="Schema_type", type="choice", choices=['SDE_SCHEMA', 'DBO_SCHEMA'], default="SDE_SCHEMA", help="Schema Type for SQL Server geodatabase, SDE or DBO. Default=SDE_SCHEMA")
parser.add_option ("-u", dest="Gdb_admin", type="string", default="", help="Geodatabase administrator user name")
parser.add_option ("-p", dest="Gdb_admin_pwd", type="string", default="", help="Geodatabase administrator password")
parser.add_option ("-t", dest="Tablespace", type="string", default="", help="Tablespace name")
parser.add_option ("-l", dest="Authorization_file", type="string", default="", help="Full path and name of authorization file")
parser.add_option ("--type", dest="Spatial_type", type="choice", choices=['ST_GEOMETRY', 'POSTGIS'], default="ST_GEOMETRY", help="Spatial Type for PostgreSQL geodatabase, ST_GEOMETRY or POSTGIS. Default=ST_GEOMETRY")

# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	
	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print("%s: error: %s\n" % (sys.argv[0], "No command options given"))
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	schema_type = options.Schema_type.upper()
	gdb_admin = options.Gdb_admin
	gdb_admin_pwd = options.Gdb_admin_pwd	
	tablespace = options.Tablespace
	license = options.Authorization_file
	spatial_type = options.Spatial_type.upper()
	
	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified."))
		parser.print_help()
		sys.exit(3)		
		
	if (license == ""):
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "Authorization file (-l) must be specified."))
		parser.print_help()
		sys.exit(3)			
	
	if(database_type == "SQL_SERVER"):
		if(schema_type == "SDE_SCHEMA" and gdb_admin.lower() != "sde"):
			print("\n%s: error: %s\n" % (sys.argv[0], "To create SDE schema on SQL Server, geodatabase administrator must be SDE."))
			sys.exit(3)
		if (schema_type == "DBO_SCHEMA" and gdb_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring geodatabase administrator specified when creating DBO schema..."))
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication"))
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication..."))	
	else:
		if (schema_type == "DBO_SCHEMA"):
			print("\nWarning: %s %s, %s\n" % ("Only SDE schema is supported on", database_type, "switching to SDE schema..." ))
			
		if( gdb_admin.lower() == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "Geodatabase administrator must be specified."))
			sys.exit(3)
	
		if( dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!"))
			sys.exit(3)

		if (account_authentication == "OPERATING_SYSTEM_AUTH"):
			print("Warning: %s %s, %s\n" % ("Only database authentication is supported on", database_type, "switching to database authentication..." ))

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + " Creating an enterprise geodatabase requires an ArcGIS for Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS for Server license.")
		sys.exit("Re-authorize ArcGIS before creating enterprise geodatabase.")
	else:
		print("\n" + product_license + " license available!  Continuing to create...")
		arcpy.AddMessage("+++++++++")
	
	
	try:
		print("Creating enterprise geodatabase...\n")
		arcpy.management.CreateEnterpriseGeodatabase(database_platform=database_type,instance_name=instance, database_name=database, account_authentication=account_authentication, database_admin=dbms_admin, database_admin_password=dbms_admin_pwd, sde_schema=schema_type, gdb_admin_name=gdb_admin, gdb_admin_password=gdb_admin_pwd, tablespace_name=tablespace, authorization_file=license, spatial_type=spatial_type)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()   
		parser.exit(2)
```

### Example 9

```python
"""
Name: create_enterprise_gdb.py
Description: Provide connection information to a DBMS instance and create an enterprise geodatabase.
Type  create_enterprise_gdb.py -h or create_enterprise_gdb.py --help for usage
Author: Esri
"""

# Import system modules
import arcpy, os, optparse, sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 and higher releases")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("--schema", dest="Schema_type", type="choice", choices=['SDE_SCHEMA', 'DBO_SCHEMA'], default="SDE_SCHEMA", help="Schema Type for SQL Server geodatabase, SDE or DBO. Default=SDE_SCHEMA")
parser.add_option ("-u", dest="Gdb_admin", type="string", default="", help="Geodatabase administrator user name")
parser.add_option ("-p", dest="Gdb_admin_pwd", type="string", default="", help="Geodatabase administrator password")
parser.add_option ("-t", dest="Tablespace", type="string", default="", help="Tablespace name")
parser.add_option ("-l", dest="Authorization_file", type="string", default="", help="Full path and name of authorization file")
parser.add_option ("--type", dest="Spatial_type", type="choice", choices=['ST_GEOMETRY', 'POSTGIS'], default="ST_GEOMETRY", help="Spatial Type for PostgreSQL geodatabase, ST_GEOMETRY or POSTGIS. Default=ST_GEOMETRY")

# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	
	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print("%s: error: %s\n" % (sys.argv[0], "No command options given"))
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	schema_type = options.Schema_type.upper()
	gdb_admin = options.Gdb_admin
	gdb_admin_pwd = options.Gdb_admin_pwd	
	tablespace = options.Tablespace
	license = options.Authorization_file
	spatial_type = options.Spatial_type.upper()
	
	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified."))
		parser.print_help()
		sys.exit(3)		
		
	if (license == ""):
		print(" \n%s: error: \n%s\n" % (sys.argv[0], "Authorization file (-l) must be specified."))
		parser.print_help()
		sys.exit(3)			
	
	if(database_type == "SQL_SERVER"):
		if(schema_type == "SDE_SCHEMA" and gdb_admin.lower() != "sde"):
			print("\n%s: error: %s\n" % (sys.argv[0], "To create SDE schema on SQL Server, geodatabase administrator must be SDE."))
			sys.exit(3)
		if (schema_type == "DBO_SCHEMA" and gdb_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring geodatabase administrator specified when creating DBO schema..."))
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication"))
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print("\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication..."))	
	else:
		if (schema_type == "DBO_SCHEMA"):
			print("\nWarning: %s %s, %s\n" % ("Only SDE schema is supported on", database_type, "switching to SDE schema..." ))
			
		if( gdb_admin.lower() == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "Geodatabase administrator must be specified."))
			sys.exit(3)
	
		if( dbms_admin == ""):
			print("\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!"))
			sys.exit(3)

		if (account_authentication == "OPERATING_SYSTEM_AUTH"):
			print("Warning: %s %s, %s\n" % ("Only database authentication is supported on", database_type, "switching to database authentication..." ))

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + " Creating an enterprise geodatabase requires an ArcGIS for Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS for Server license.")
		sys.exit("Re-authorize ArcGIS before creating enterprise geodatabase.")
	else:
		print("\n" + product_license + " license available!  Continuing to create...")
		arcpy.AddMessage("+++++++++")
	
	
	try:
		print("Creating enterprise geodatabase...\n")
		arcpy.management.CreateEnterpriseGeodatabase(database_platform=database_type,instance_name=instance, database_name=database, account_authentication=account_authentication, database_admin=dbms_admin, database_admin_password=dbms_admin_pwd, sde_schema=schema_type, gdb_admin_name=gdb_admin, gdb_admin_password=gdb_admin_pwd, tablespace_name=tablespace, authorization_file=license, spatial_type=spatial_type)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()   
		parser.exit(2)
```

---

## Create Feature Class (Data Management)

## Summary

Creates an empty feature class in a geodatabase or a shapefile in a folder.

## Usage

- The Feature Class Location parameter value (geodatabase or folder) must already exist.
- This tool creates only simple feature classes such as point, multipoint, polygon, and polyline.
- A shapefile created by this tool has an integer field named ID. The ID field is not created when you provide a Template Dataset parameter value.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Feature Class Location | The enterprise or file geodatabase or the folder in which the output feature class will be created. This workspace must already exist. | Workspace; Feature Dataset |
| Feature Class Name | The name of the feature class to be created. | String |
| Geometry Type(Optional) | Specifies the geometry type of the output feature class.Point—The geometry type will be point. Multipoint—The geometry type will be multipoint. Polygon—The geometry type will be polygon. Polyline—The geometry type will be polyline.Multipatch—The geometry type will be multipatch. | String |
| Template Datasets(Optional) | An existing dataset or a list of datasets used as templates to define the attribute fields of the new feature class. | Table View |
| Has M(Optional) | Specifies whether the feature class will have linear measurement values (m-values).No—The output feature class will not have m-values. This is the default.Yes—The output feature class will have m-values.Same as the template feature class—The output feature class will have m-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has m-values. | String |
| Has Z(Optional) | Specifies whether the feature class will have elevation values (z-values).No—The output feature class will not have z-values. This is the default.Yes—The output feature class will have z-values.Same as the template feature class—The output feature class will have z-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has z-values. | String |
| Coordinate System (Optional) | The spatial reference of the output feature dataset. On the Spatial Reference Properties dialog box, you can select, import, or create a new coordinate system. To set aspects of the spatial reference, such as the x,y-, z-, or m-domain, resolution, or tolerance, use the Environments dialog box. Note:This parameter is optional, but providing a value is recommended. If no spatial reference is provided, the output will have an undefined spatial reference.Note:The spatial reference of the Template Feature Class value has no effect on the output spatial reference. If you want the output to be in the coordinate system of the Template Feature Class value, set the Coordinate System parameter to the spatial reference of the Template Feature Class value. | Spatial Reference |
| Configuration Keyword(Optional) | The configuration keyword applies to enterprise geodatabase data only. It determines the storage parameters of the database table. | String |
| Output Spatial Grid 1(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| Output Spatial Grid 2(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| Output Spatial Grid 3(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| Feature Class Alias(Optional) | The alternate name for the output feature class that will be created. | String |
| OID Type(Optional) | Specifies whether the output Object ID field will be 32 bit or 64 bit.Same as template—The output Object ID field type (32 bit or 64 bit) will be the same as the Object ID field of the first template dataset. This is the default.64-bit—The output Object ID field will be 64 bit. 32-bit—The output Object ID field will be 32 bit. | String |
| out_path | The enterprise or file geodatabase or the folder in which the output feature class will be created. This workspace must already exist. | Workspace; Feature Dataset |
| out_name | The name of the feature class to be created. | String |
| geometry_type(Optional) | Specifies the geometry type of the output feature class.POINT—The geometry type will be point. MULTIPOINT—The geometry type will be multipoint. POLYGON—The geometry type will be polygon. POLYLINE—The geometry type will be polyline.MULTIPATCH—The geometry type will be multipatch. | String |
| template[template,...](Optional) | An existing dataset or a list of datasets used as templates to define the attribute fields of the new feature class. | Table View |
| has_m(Optional) | Specifies whether the feature class will have linear measurement values (m-values).DISABLED—The output feature class will not have m-values. This is the default.ENABLED—The output feature class will have m-values.SAME_AS_TEMPLATE—The output feature class will have m-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has m-values. | String |
| has_z(Optional) | Specifies whether the feature class will have elevation values (z-values).DISABLED—The output feature class will not have z-values. This is the default.ENABLED—The output feature class will have z-values.SAME_AS_TEMPLATE—The output feature class will have z-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has z-values. | String |
| spatial_reference(Optional) | The spatial reference of the output feature dataset. You can specify the spatial reference in the following ways: Enter the path to a .prj file, such as C:/workspace/watershed.prj. Reference a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland.Define a spatial reference object before using this tool, such as sr = arcpy.SpatialReference("Sinusoidal (Africa)"), which you then use as the spatial reference parameter. Note:This parameter is optional, but providing a value is recommended. If no spatial reference is provided, the output will have an undefined spatial reference.Note:The spatial reference of the Template Feature Class value has no effect on the output spatial reference. If you want the output to be in the coordinate system of the Template Feature Class value, set the Coordinate System parameter to the spatial reference of the Template Feature Class value. | Spatial Reference |
| config_keyword(Optional) | The configuration keyword applies to enterprise geodatabase data only. It determines the storage parameters of the database table. | String |
| spatial_grid_1(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| spatial_grid_2(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| spatial_grid_3(Optional) | This parameter is not supported. Any value provided will be ignored. | Double |
| out_alias(Optional) | The alternate name for the output feature class that will be created. | String |
| oid_type(Optional) | Specifies whether the output Object ID field will be 32 bit or 64 bit.SAME_AS_TEMPLATE—The output Object ID field type (32 bit or 64 bit) will be the same as the Object ID field of the first template dataset. This is the default.64_BIT—The output Object ID field will be 64 bit. 32_BIT—The output Object ID field will be 32 bit. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateFeatureclass(out_path, out_name, {geometry_type}, {template}, {has_m}, {has_z}, {spatial_reference}, {config_keyword}, {spatial_grid_1}, {spatial_grid_2}, {spatial_grid_3}, {out_alias}, {oid_type})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateFeatureclass("C:/output", "habitatareas.shp", "POLYGON", 
                                    "study_quads.shp", "DISABLED", "DISABLED", 
                                    "C:/workspace/landuse.shp")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateFeatureclass("C:/output", "habitatareas.shp", "POLYGON", 
                                    "study_quads.shp", "DISABLED", "DISABLED", 
                                    "C:/workspace/landuse.shp")
```

### Example 4

```python
# Description: Create a feature class to store the gnatcatcher habitat zones

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
out_path = "C:/output"
out_name = "habitatareas.shp"
geometry_type = "POLYGON"
template = "study_quads.shp"
has_m = "DISABLED"
has_z = "DISABLED"

# Use Describe to get a SpatialReference object
spatial_ref = arcpy.Describe("C:/workspace/studyarea.shp").spatialReference

# Run CreateFeatureclass
arcpy.management.CreateFeatureclass(out_path, out_name, geometry_type, template, 
                                    has_m, has_z, spatial_ref)
```

### Example 5

```python
# Description: Create a feature class to store the gnatcatcher habitat zones

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
out_path = "C:/output"
out_name = "habitatareas.shp"
geometry_type = "POLYGON"
template = "study_quads.shp"
has_m = "DISABLED"
has_z = "DISABLED"

# Use Describe to get a SpatialReference object
spatial_ref = arcpy.Describe("C:/workspace/studyarea.shp").spatialReference

# Run CreateFeatureclass
arcpy.management.CreateFeatureclass(out_path, out_name, geometry_type, template, 
                                    has_m, has_z, spatial_ref)
```

---

## Create Feature Dataset (Data Management)

## Summary

Creates a feature dataset in the output location: an existing enterprise, file, or mobile geodatabase.

## Usage

- A feature dataset is a collection of related feature classes that share a common coordinate system. Feature datasets are used to organize related feature classes into a common container for building a topology, network dataset, terrain, utility network, trace network, or parcel fabric.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Geodatabase | The enterprise, file, or mobile geodatabase where the output feature dataset will be created. | Workspace |
| Feature Dataset Name | The name of the feature dataset to be created. | String |
| Coordinate System(Optional) | The spatial reference of the output feature dataset. On the Spatial Reference Properties dialog box, you can select, import, or create a new coordinate system. To set aspects of the spatial reference, such as the x,y-, z-, or m-domain, resolution, or tolerance, use the Environments dialog box. Note:This parameter is optional, but providing a value is recommended. To edit the data in a feature dataset, a spatial reference is required. | Spatial Reference |
| out_dataset_path | The enterprise, file, or mobile geodatabase where the output feature dataset will be created. | Workspace |
| out_name | The name of the feature dataset to be created. | String |
| spatial_reference(Optional) | The spatial reference of the output feature dataset. You can specify the spatial reference in the following ways: Enter the path to a .prj file, such as C:/workspace/watershed.prj. Reference a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland.Define a spatial reference object before using this tool, such as sr = arcpy.SpatialReference("Sinusoidal (Africa)"), which you then use as the spatial reference parameter. Note:This parameter is optional, but providing a value is recommended. To edit the data in a feature dataset, a spatial reference is required. | Spatial Reference |

## Code Samples

### Example 1

```python
arcpy.management.CreateFeatureDataset(out_dataset_path, out_name, {spatial_reference})
```

### Example 2

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "HabitatAnalysis.gdb")
arcpy.management.CreateFeatureDataset("C:/output/HabitatAnalysis.gdb", 
                                      "analysisresults", 
                                      "C:/workspace/landuse.prj")
```

### Example 3

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "HabitatAnalysis.gdb")
arcpy.management.CreateFeatureDataset("C:/output/HabitatAnalysis.gdb", 
                                      "analysisresults", 
                                      "C:/workspace/landuse.prj")
```

### Example 4

```python
# Name: CreateFeatureDataset_Example2.py
# Description: Create a feature dataset 

# Import system modules
import arcpy

# Set local variables
out_dataset_path = "C:/output/HabitatAnalysis.gdb" 
out_name = "analysisresults"

# Create a spatial reference object
sr = arcpy.SpatialReference("C:/data/studyarea.prj")

# Create a file geodatabase for the feature dataset
arcpy.management.CreateFileGDB("C:/output", "HabitatAnalysis.gdb")

# Run CreateFeatureDataset 
arcpy.management.CreateFeatureDataset(out_dataset_path, out_name, sr)
```

### Example 5

```python
# Name: CreateFeatureDataset_Example2.py
# Description: Create a feature dataset 

# Import system modules
import arcpy

# Set local variables
out_dataset_path = "C:/output/HabitatAnalysis.gdb" 
out_name = "analysisresults"

# Create a spatial reference object
sr = arcpy.SpatialReference("C:/data/studyarea.prj")

# Create a file geodatabase for the feature dataset
arcpy.management.CreateFileGDB("C:/output", "HabitatAnalysis.gdb")

# Run CreateFeatureDataset 
arcpy.management.CreateFeatureDataset(out_dataset_path, out_name, sr)
```

---

## Create Field Group (Data Management)

## Summary

Create a field group for a feature class or table. Field groups are used when creating contingent values.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase table or feature class in which the field group will be created. | Table View |
| Field Group Name | The name of the field group that will be created. This name must be unique to the feature class or table that will contain the field group. | String |
| Fields | The names of the fields in the field group. | String |
| Is Restrictive (Optional) | Specifies if the field group is restrictive. This parameter allows you to control the editing experience when using contingent values. Checked—The field group is restrictive. Values entered on a field in the field group are restricted to those specified as contingent values. This is the default.Unchecked—The field group is not restrictive. Values can be committed to a field in a field group even if they are not specified as contingent values. | Boolean |
| target_table | The input geodatabase table or feature class in which the field group will be created. | Table View |
| name | The name of the field group that will be created. This name must be unique to the feature class or table that will contain the field group. | String |
| fields[fields,...] | The names of the fields in the field group. | String |
| is_restrictive(Optional) | Specifies if the field group is restrictive. This parameter allows you to control the editing experience when using contingent values.RESTRICT—The field group is restrictive. Values entered on a field in the field group are restricted to those specified as contingent values. This is the default.DO_NOT_RESTRICT—The field group is not restrictive. Values can be committed to a field in a field group even if they are not specified as contingent values. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateFieldGroup(target_table, name, fields, {is_restrictive})
```

### Example 2

```python
import arcpy
arcpy.management.CreateFieldGroup("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC",
                                  "MyFieldGroup", 
                                  ["Field1", "Field2", "Field3"],
                                  "RESTRICT")
```

### Example 3

```python
import arcpy
arcpy.management.CreateFieldGroup("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC",
                                  "MyFieldGroup", 
                                  ["Field1", "Field2", "Field3"],
                                  "RESTRICT")
```

---

## Create File Geodatabase (Data Management)

## Summary

Creates a file geodatabase in a folder.

## Usage

- The output path must already exist.
- If the output geodatabase name does not include the .gdb extension, it will be added.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| File Geodatabase Location | The folder where the file geodatabase will be created. | Folder |
| File GDB Name | The name of the file geodatabase to be created. | String |
| File Geodatabase Version(Optional) | Specifies the ArcGIS version for the new geodatabase.Current—A geodatabase compatible with the currently installed version of ArcGIS will be created. This is the default.10.0—A geodatabase compatible with ArcGIS version 10 will be created.9.3—A geodatabase compatible with ArcGIS version 9.3 will be created.9.2—A geodatabase compatible with ArcGIS version 9.2 will be created. | String |
| out_folder_path | The folder where the file geodatabase will be created. | Folder |
| out_name | The name of the file geodatabase to be created. | String |
| out_version(Optional) | Specifies the ArcGIS version for the new geodatabase.CURRENT—A geodatabase compatible with the currently installed version of ArcGIS will be created. This is the default.10.0—A geodatabase compatible with ArcGIS version 10 will be created.9.3—A geodatabase compatible with ArcGIS version 9.3 will be created.9.2—A geodatabase compatible with ArcGIS version 9.2 will be created. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateFileGDB(out_folder_path, out_name, {out_version})
```

### Example 2

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "fGDB.gdb")
```

### Example 3

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "fGDB.gdb")
```

### Example 4

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "fGDB.gdb", "9.2")
```

### Example 5

```python
import arcpy
arcpy.management.CreateFileGDB("C:/output", "fGDB.gdb", "9.2")
```

### Example 6

```python
# Name: CreateFileGDB_Example2.py
# Description: Create a file GDB

# Import system modules
import arcpy

# Set local variables
out_folder_path = "C:/output" 
out_name = "fGDB.gdb"

# Run CreateFileGDB
arcpy.management.CreateFileGDB(out_folder_path, out_name)
```

### Example 7

```python
# Name: CreateFileGDB_Example2.py
# Description: Create a file GDB

# Import system modules
import arcpy

# Set local variables
out_folder_path = "C:/output" 
out_name = "fGDB.gdb"

# Run CreateFileGDB
arcpy.management.CreateFileGDB(out_folder_path, out_name)
```

---

## Create Fishnet (Data Management)

## Summary

Creates a fishnet of rectangular cells. The output can be polyline or polygon features.

## Usage

- The coordinate system of the output can be set by either entering a feature class or layer in the Template Extent parameter or setting the Output Coordinate System environment variable.
- In addition to creating the output fishnet, a new point feature class is created with label points at the center of each fishnet cell if the Create Label Points parameter is checked. The name of this feature class is the same as the output feature class with a suffix of _label and is created in the same location.
- Using the Geometry Type parameter, you can create output polyline (default) or polygon cells. Creating a polygon fishnet may be slower, depending on the number of rows and columns.
- The Cell Size Width and the Cell Size Height parameter values are in the same units as defined by the output feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Feature Class | The output feature class containing the fishnet of rectangular cells. | Feature Class |
| Fishnet Origin Coordinate | The starting pivot point of the fishnet. | Point |
| Y-Axis Coordinate | The y-axis coordinate is used to orient the fishnet. The fishnet is rotated by the same angle as defined by the line connecting the origin and the y-axis coordinate. | Point |
| Cell Size Width | The width of each cell. To calculate the cell width using the Number of Rows parameter value, leave this parameter unspecified or set the value to zero; the width will be calculated when the tool is run. | Double |
| Cell Size Height | The height of each cell. To calculate the cell height using the Number of Columns parameter value, leave this parameter unspecified or set the value to zero; the height will be calculated when the tool is run. | Double |
| Number of Rows | The number of rows the fishnet will have. To calculate the number of rows using the Cell Size Width parameter value, leave this parameter unspecified or set the value to zero; the number of rows will be calculated when the tool is run. | Long |
| Number of Columns | The number of columns the fishnet will have. To calculate the number of columns using the Cell Size Height parameter value, leave this parameter unspecified or set the value to zero; the number of columns will be calculated when the tool is run. | Long |
| Opposite corner of Fishnet(Optional) | The opposite corner of the fishnet set by the Fishnet Origin Coordinate parameter. The values for opposite corner are automatically set if the Template Extent parameter is specified.This parameter is inactive if the Fishnet Origin Coordinate, Y-Axis Coordinate, Cell Size Width, Cell Size Height, Number of Rows and Number of Columns parameters are specified. | Point |
| Create Label Points(Optional) | Specifies whether a point feature class will be created containing label points at the center of each fishnet cell.Checked—A point feature class will be created. This is the default.Unchecked—A point feature class will not be created. | Boolean |
| Template Extent(Optional) | The extent of the fishnet. The extent can be entered by specifying the coordinates or using a template dataset. Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Geometry Type(Optional) | Specifies whether the output fishnet cells will be polyline or polygon features.Polyline—Output will be a polyline feature class. Each cell is defined by four line features.Polygon—Output will be a polygon feature class. Each cell is defined by one polygon feature. | String |
| out_feature_class | The output feature class containing the fishnet of rectangular cells. | Feature Class |
| origin_coord | The starting pivot point of the fishnet. | Point |
| y_axis_coord | The y-axis coordinate is used to orient the fishnet. The fishnet is rotated by the same angle as defined by the line connecting the origin and the y-axis coordinate. | Point |
| cell_width | The width of each cell. To calculate the cell width using the number_rows parameter value, leave this parameter unspecified or set the value to zero; the width will be calculated when the tool is run. | Double |
| cell_height | The height of each cell. To calculate the cell height using the number_columns parameter value, leave this parameter unspecified or set the value to zero; the height will be calculated when the tool is run. | Double |
| number_rows | The number of rows the fishnet will have. To calculate the number of rows using the cell_width parameter value, leave this parameter unspecified or set the value to zero; the number of rows will be calculated when the tool is run. | Long |
| number_columns | The number of columns the fishnet will have. To calculate the number of columns using the cell_height parameter value, leave this parameter unspecified or set the value to zero; the number of columns will be calculated when the tool is run. | Long |
| corner_coord(Optional) | The opposite corner of the fishnet set by the origin_coord parameter.This parameter is disabled if the origin_coord, y_axis_coord, cell_width, cell_height, number_rows and number_columns parameters are specified. | Point |
| labels(Optional) | Specifies whether a point feature class will be created containing label points at the center of each fishnet cell.LABELS—A point feature class will be created. This is the default.NO_LABELS—A point feature class will not be created. | Boolean |
| template(Optional) | The extent of the fishnet. The extent can be entered by specifying the coordinates or using a template dataset. MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| geometry_type(Optional) | Specifies whether the output fishnet cells will be polyline or polygon features.POLYLINE—Output will be a polyline feature class. Each cell is defined by four line features.POLYGON—Output will be a polygon feature class. Each cell is defined by one polygon feature. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateFishnet(out_feature_class, origin_coord, y_axis_coord, cell_width, cell_height, number_rows, number_columns, {corner_coord}, {labels}, {template}, {geometry_type})
```

### Example 2

```python
import arcpy

# Create a fishnet with 9 columns and 9 rows
# with origin at (1, 1) and output geometry is set to default (POLYLINE)
arcpy.CreateFishnet_management("C:/data/output/fishnet1.shp", "1 1", "1 9", "1", "1", "9", "9", "#", "NO_LABELS")
```

### Example 3

```python
import arcpy

# Create a fishnet with 9 columns and 9 rows
# with origin at (1, 1) and output geometry is set to default (POLYLINE)
arcpy.CreateFishnet_management("C:/data/output/fishnet1.shp", "1 1", "1 9", "1", "1", "9", "9", "#", "NO_LABELS")
```

### Example 4

```python
# Name: CreateFishnet.py
# Description: Creates rectangular cells

# import system module
import arcpy
from arcpy import env

# set workspace environment
env.workspace = "C:/data/output"

# Set coordinate system of the output fishnet
env.outputCoordinateSystem = arcpy.SpatialReference("NAD 1983 UTM Zone 11N")

outFeatureClass = "fishnet10by10.shp"

# Set the origin of the fishnet
originCoordinate = '1037.26 4145.81'

# Set the orientation
yAxisCoordinate = '1037.26 4155.81'

# Enter 0 for width and height - these values will be calcualted by the tool
cellSizeWidth = '0'
cellSizeHeight = '0'

# Number of rows and columns together with origin and opposite corner 
# determine the size of each cell 
numRows =  '10'
numColumns = '10'

oppositeCoorner = '19273.61 18471.17'

# Create a point label feature class 
labels = 'LABELS'

# Extent is set by origin and opposite corner - no need to use a template fc
templateExtent = '#'

# Each output cell will be a polygon
geometryType = 'POLYGON'

arcpy.CreateFishnet_management(outFeatureClass, originCoordinate, yAxisCoordinate, cellSizeWidth, cellSizeHeight, numRows, numColumns, oppositeCoorner, labels, templateExtent, geometryType)
```

### Example 5

```python
# Name: CreateFishnet.py
# Description: Creates rectangular cells

# import system module
import arcpy
from arcpy import env

# set workspace environment
env.workspace = "C:/data/output"

# Set coordinate system of the output fishnet
env.outputCoordinateSystem = arcpy.SpatialReference("NAD 1983 UTM Zone 11N")

outFeatureClass = "fishnet10by10.shp"

# Set the origin of the fishnet
originCoordinate = '1037.26 4145.81'

# Set the orientation
yAxisCoordinate = '1037.26 4155.81'

# Enter 0 for width and height - these values will be calcualted by the tool
cellSizeWidth = '0'
cellSizeHeight = '0'

# Number of rows and columns together with origin and opposite corner 
# determine the size of each cell 
numRows =  '10'
numColumns = '10'

oppositeCoorner = '19273.61 18471.17'

# Create a point label feature class 
labels = 'LABELS'

# Extent is set by origin and opposite corner - no need to use a template fc
templateExtent = '#'

# Each output cell will be a polygon
geometryType = 'POLYGON'

arcpy.CreateFishnet_management(outFeatureClass, originCoordinate, yAxisCoordinate, cellSizeWidth, cellSizeHeight, numRows, numColumns, oppositeCoorner, labels, templateExtent, geometryType)
```

---

## Create Folder (Data Management)

## Summary

Creates a folder in the specified location.

## Usage

- The output folder name must be unique.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Folder Location | The disk location where the folder is created. | Folder |
| Folder Name | The folder to be created. | String |
| out_folder_path | The disk location where the folder is created. | Folder |
| out_name | The folder to be created. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateFolder(out_folder_path, out_name)
```

### Example 2

```python
import arcpy
arcpy.CreateFolder_management("C:/output", "folder1")
```

### Example 3

```python
import arcpy
arcpy.CreateFolder_management("C:/output", "folder1")
```

### Example 4

```python
# Name: CreateFolder_Example2.py
# Description: Create a folder

# Import system modules
import arcpy

# Set local variables
out_folder_path = "C:/output" 
out_name = "folder1"

# Execute CreateFolder
arcpy.CreateFolder_management(out_folder_path, out_name)
```

### Example 5

```python
# Name: CreateFolder_Example2.py
# Description: Create a folder

# Import system modules
import arcpy

# Set local variables
out_folder_path = "C:/output" 
out_name = "folder1"

# Execute CreateFolder
arcpy.CreateFolder_management(out_folder_path, out_name)
```

---

## Create Integrated Mesh Scene Layer Content (Data Management)

## Summary

Creates scene layer content (.slpk or .i3sREST) from OpenSceneGraph binary (OSGB) data.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The OSGB format files, or folders containing OSGB format files, that will be imported into the integrated mesh scene layer package. This parameter allows a selection of multiple OSGB format files or a selection of multiple folders containing OSGB format files. | File; Folder |
| Output Scene Layer Package (Optional) | The integrated mesh scene layer package that will be created. This parameter is required if the Target Cloud Connection parameter value is not specified. | File |
| Anchor Point (Optional) | The point feature or .3mx, .xml, or .wld3 file that will be used to position the center of the OSGB model. If there are multiple points in the feature class, only the first point will be used to georeference the data. | Feature Layer; File |
| File Suffix(Optional) | Specifies the files that will be processed for the input dataset.All supported files—All binary files, regardless of their extension, will be processed to determine if they are in the OSGB format.Files with *.osgb extension—Only files with the .osgb extension will be processed. | String |
| Output Coordinate System(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| Maximum Texture Size(Optional) | The maximum texture size in pixels for each scene layer node. | Long |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| Target Cloud Connection (Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| Output Name (Optional) | The output name of the scene layer content when output to a cloud store. This parameter is only available when the Target Cloud Connection parameter value is specified. | String |
| in_dataset[in_dataset,...] | The OSGB format files, or folders containing OSGB format files, that will be imported into the integrated mesh scene layer package. This parameter allows a selection of multiple OSGB format files or a selection of multiple folders containing OSGB format files. | File; Folder |
| out_slpk(Optional) | The integrated mesh scene layer package that will be created. This parameter is required if the Target Cloud Connection parameter value is not specified. | File |
| anchor_point(Optional) | The point feature or .3mx, .xml, or .wld3 file that will be used to position the center of the OSGB model. If there are multiple points in the feature class, only the first point will be used to georeference the data. | Feature Layer; File |
| file_suffix(Optional) | Specifies the files that will be processed for the input dataset.*—All binary files, regardless of their extension, will be processed to determine if they are in the OSGB format.osgb—Only files with the .osgb extension will be processed. | String |
| out_coor_system(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| max_texture_size(Optional) | The maximum texture size in pixels for each scene layer node. | Long |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| target_cloud_connection(Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| out_name(Optional) | The output name of the scene layer content when output to a cloud store. This parameter is only available when the target_cloud_connection parameter value is specified. | String |

## Code Samples

### Example 1

```python
0, 0, 0 -117.17222, 34.0392512, 0
```

### Example 2

```python
0, 0, 0 -117.17222, 34.0392512, 0
```

### Example 3

```python
arcpy.management.CreateIntegratedMeshSceneLayerPackage(in_dataset, {out_slpk}, {anchor_point}, {file_suffix}, {out_coor_system}, {max_texture_size}, {texture_optimization}, {target_cloud_connection}, {out_name})
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    "terrain_osgb", "mesh.slpk", "anchor.wld3", "OSGB", arcpy.SpatialReference(4326))
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    "terrain_osgb", "mesh.slpk", "anchor.wld3", "OSGB", arcpy.SpatialReference(4326))
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    ["Tile_+001_+001", "Tile_+001_+002", "Tile_+002_+001"], "mesh.slpk", 
    "anchor.shp", "OSGB", arcpy.SpatialReference(4326), 2048, "DESKTOP")
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    ["Tile_+001_+001", "Tile_+001_+002", "Tile_+002_+001"], "mesh.slpk", 
    "anchor.shp", "OSGB", arcpy.SpatialReference(4326), 2048, "DESKTOP")
```

### Example 8

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    ["Tile_+001_+001", "Tile_+001_+002", "Tile_+002_+001"], "mesh.slpk", 
    "anchor.shp", "OSGB", arcpy.SpatialReference(4326), 2048, "DESKTOP",
    'AWS.acs', 'mySceneLayer.i3srest')
```

### Example 9

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.managementCreateIntegratedMeshSceneLayerPackage(
    ["Tile_+001_+001", "Tile_+001_+002", "Tile_+002_+001"], "mesh.slpk", 
    "anchor.shp", "OSGB", arcpy.SpatialReference(4326), 2048, "DESKTOP",
    'AWS.acs', 'mySceneLayer.i3srest')
```

---

## Create LAS Dataset (Data Management)

## Summary

Creates a LAS dataset referencing one or more .las files and optional surface constraint features.

## Usage

- The LAS dataset is designed for use with point cloud data stored in the LAS format using file versions 1.0–1.4. LAS files stored in the compressed ZLAS format are also supported. ZLAS files can be generated by any tool that creates new LAS files, such as Convert LAS, Extract LAS, Thin LAS, or Tile LAS. Additionally, the LAS Optimizer 1.2 stand-alone application can also be used to compress .las files to .zlas or uncompress them back to .las files.
- Each .las file is examined to determine if its internal structure is consistent with the LAS specifications. If any .las file fails to load into the LAS dataset, this may indicate that the file is corrupted or has erroneous information in its header. You can use Esri's CheckLAS utility to determine whether commonly encountered issues may exist in the data.
- Surface constraint features can be used to enforce feature-derived elevation values that represent surface characteristics in the LAS dataset.
- Each .las file typically contains spatial reference information in its header that is read by the LAS dataset. If this information is missing or improperly defined, the .las file will not be placed in its correct location. If the correct spatial reference is known, you can use the Create PRJ For LAS parameter to create a .prj file that properly georeferences the lidar data. The .prj file will share the .las file's name, reside in the same folder, and contain the well-known text representation of the .las file's coordinate system, similar to the .prj file associated with a shapefile.
- ArcGIS uses the LAS classification scheme defined by the American Society for Photogrammetry and Remote Sensing (ASPRS). Learn more about lidar point classification
- In the Geoprocessing pane, a folder can also be specified as an input by selecting the folder in File Explorer and dragging it onto the parameter's input box.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Files | The .las files, .zlas files, LAS datasets, and folders containing .las files that will be referenced by the output LAS dataset. When a LAS dataset is specified as input, all the .las and .zlas files that have a valid path reference will be added to the input LAS dataset. In the Geoprocessing pane, a folder can also be specified as an input by selecting the folder in File Explorer and dragging it onto the parameter's input box. | LAS Dataset Layer; File; Folder |
| Output LAS Dataset | The LAS dataset that will be created. | LAS Dataset |
| Include subfolders(Optional) | Specifies whether .las files residing in the subdirectories of an input folder will be referenced by the LAS dataset. Unchecked—Only .las files residing in an input folder will be added to the LAS dataset. This is the default. Checked—All .las files residing in the subdirectories of an input folder will be added to the LAS dataset. | Boolean |
| Surface Constraints(Optional) | The features that will contribute to the definition of the triangulated surface generated from the LAS dataset. Input Features—The features with geometry that will be incorporated into the LAS dataset's triangulated surface. Height Field—The feature's elevation source can be derived from any numeric field in the feature's attribute table or the geometry by selecting Shape.Z. If no height is necessary, specify the keyword <None> to create z-less features with elevation that will be interpolated from the surface. Type—Defines the feature's role in the triangulated surface generated from the LAS dataset. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change.Surface Feature Type—The surface feature type that defines how the feature geometry will be incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints—Elevation points that will not be thinned away. This option is only available for single-point feature geometry. hardline or softline—Breaklines that enforce a height value. hardclip or softclip—Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase—Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace—Polygon dataset that defines areas of constant height. | Value Table |
| Coordinate System(Optional) | The spatial reference of the LAS dataset. If no spatial reference is explicitly assigned, the LAS dataset will use the coordinate system of the first input .las file. If the input files do not contain spatial reference information and the coordinate system is not set, the coordinate system of the LAS dataset will be listed as unknown. | Coordinate System |
| Compute Statistics (Optional) | Specifies whether statistics for the .las files will be computed and a spatial index generated for the LAS dataset. The presence of statistics allows the LAS dataset layer's filtering and symbology options to only show LAS attribute values that exist in the .las files. A .lasx auxiliary file is created for each .las file. Unchecked—Statistics will not be computed. This is the default. Checked—Statistics will be computed. | Boolean |
| Store Relative Paths (Optional) | Specifies whether lidar files and surface constraint features will be referenced by the LAS dataset through relative or absolute paths. Using relative paths may be convenient for cases in which the LAS dataset and its associated data will be relocated in the file system using the same relative location to one another. Unchecked—Absolute paths will be used for the data referenced by the LAS dataset. This is the default. Checked—Relative paths will be used for the data referenced by the LAS dataset. | Boolean |
| Create PRJ For LAS Files | Specifies whether .prj files will be created for the .las files referenced by the LAS dataset.No LAS Files—No .prj files will be created. This is the default.Files with Missing Spatial References—Corresponding .prj files will be created for .las files with no spatial reference.All LAS Files—Corresponding .prj files will be created for all .las files. | String |
| Processing Extent (Optional) | The processing extent will be used to select a subset of .las files from the list of files and folders in the Input Files parameter value. Any .las files that fall entirely outside of this extent will be excluded from the resulting LAS dataset. Additionally, .las files that fall partially outside the extent will be excluded if the Add only entirely contained files parameter is checked. | Extent |
| Processing Boundary (Optional) | The polygon features whose boundary will be used to select a subset of .las files from the list of files and folders in the Input Files parameter. Any .las files that fall entirely outside of the polygon will be excluded from the resulting LAS dataset. Additionally, .las files that fall partially outside the polygons will be excluded if the Add only entirely contained files parameter is checked. | Feature Layer |
| Add only entirely contained files (Optional) | Specifies whether the .las files that will be added to the LAS dataset must be fully or partially contained by either the processing extent, the processing boundary polygon, or the intersection of both. Unchecked—All files that intersect the processing extent, processing boundary, or the intersection of both will be added to the LAS dataset. This is the default. Checked—Only files that are entirely contained by the processing extent, processing boundary, or the intersection of both will be added to the LAS dataset. | Boolean |
| input[input,...] | The .las files, LAS datasets, and folders containing .las files that will be referenced by the LAS dataset. This information can be supplied as a string containing all the input data or a list of strings containing specific data elements (for example, "lidar1.las; lidar2.las; folder1; folder2" or ["lidar1.las", "lidar2.las", "folder1", "folder2"]). | LAS Dataset Layer; File; Folder |
| out_las_dataset | The LAS dataset that will be created. | LAS Dataset |
| folder_recursion(Optional) | Specifies whether lidar files residing in the subdirectories of an input folder will be added to the LAS dataset. NO_RECURSION—Only lidar files residing in an input folder will be added to the LAS dataset. This is the default.RECURSION—All lidar files residing in the subdirectories of an input folder will be added to the LAS dataset. | Boolean |
| in_surface_constraints[[in_feature_class, height_field, SF_type],...](Optional) | The features that will be referenced by the LAS dataset when generating a triangulated surface. Each feature must have the following properties defined: in_feature_class—The feature to be referenced by the LAS dataset. height_field—Any numeric field in the feature's attribute table can be used to define the height source. If the feature's geometry contains z-values, it can be selected by specifying Shape.Z. If no height is necessary, specify the keyword <None> to create z-less features with elevation that will be interpolated from the surface. SF_type—The surface feature type that defines how the feature geometry will be incorporated into the triangulation for the surface. Options with hard or soft designation refer to whether the feature edges represent distinct breaks in slope or a gradual change. anchorpoints—Elevation points that will not be thinned away. This option is only available for single-point feature geometry. hardline or softline—Breaklines that enforce a height value. hardclip or softclip—Polygon dataset that defines the boundary of the LAS dataset. harderase or softerase—Polygon dataset that defines holes in the LAS dataset. hardreplace or softreplace—Polygon dataset that defines areas of constant height. | Value Table |
| spatial_reference(Optional) | The spatial reference of the LAS dataset. If no spatial reference is explicitly assigned, the LAS dataset will use the coordinate system of the first input .las file. If the input files do not contain spatial reference information and the coordinate system is not set, the coordinate system of the LAS dataset will be listed as unknown. | Coordinate System |
| compute_stats(Optional) | Specifies whether statistics for the .las files will be computed and a spatial index generated for the LAS dataset. The presence of statistics allows the LAS dataset layer's filtering and symbology options to only show LAS attribute values that exist in the .las files. A .lasx auxiliary file is created for each .las file. COMPUTE_STATS—Statistics will be computed.NO_COMPUTE_STATS—Statistics will not be computed. This is the default. | Boolean |
| relative_paths(Optional) | Specifies whether lidar files and surface constraint features will be referenced by the LAS dataset through relative or absolute paths. Using relative paths may be convenient for cases in which the LAS dataset and its associated data will be relocated in the file system using the same relative location to one another. ABSOLUTE_PATHS—Absolute paths will be used for the data referenced by the LAS dataset. This is the default.RELATIVE_PATHS—Relative paths will be used for the data referenced by the LAS dataset. | Boolean |
| create_las_prj | Specifies whether .prj files will be created for the .las files referenced by the LAS dataset. NO_FILES—No .prj files will be created. This is the default.FILES_MISSING_PROJECTION—Corresponding .prj files will be created for .las files with no spatial reference.ALL_FILES—Corresponding .prj files will be created for all .las files. | String |
| extent(Optional) | The processing extent will be used to select a subset of .las files from the list of files and folders in the input parameter value. Any .las files that fall entirely outside of this extent will be excluded from the resulting LAS dataset. Additionally, .las files that fall partially outside the extent will be excluded if the add_only_contained_files parameter is set to INTERSECTED_FILES. | Extent |
| boundary(Optional) | The polygon features whose boundary will be used to select a subset of .las files from the list of files and folders in the input parameter. Any .las files that fall entirely outside of the polygon features will be excluded from the resulting LAS dataset. Additionally, .las files that fall partially outside the polygons will be excluded if the add_only_contained_files parameter is set to INTERSECTED_FILES. | Feature Layer |
| add_only_contained_files(Optional) | Specifies whether the .las files that will be added to the LAS dataset must be fully or partially contained by either the processing extent, the processing boundary polygon, or the intersection of both.CONTAINED_FILES—All files that intersect the processing extent, processing boundary, or the intersection of both will be added to the LAS dataset. This is the default.INTERSECTED_FILES—Only files that are entirely contained by the processing extent, processing boundary, or the intersection of both will be added to the LAS dataset. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateLasDataset(input, out_las_dataset, {folder_recursion}, {in_surface_constraints}, {spatial_reference}, {compute_stats}, {relative_paths}, create_las_prj, {extent}, {boundary}, {add_only_contained_files})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.CreateLasDataset_management("folder_a; folder_b/5S4E.las", 
                                "test.lasd", "RECURSION",
                                "LA/boundary.shp <None> Softclip;"\
                                "LA/ridges.shp Elevation hardline", "", 
                                "COMPUTE_STATS", "RELATIVE_PATHS")
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.CreateLasDataset_management("folder_a; folder_b/5S4E.las", 
                                "test.lasd", "RECURSION",
                                "LA/boundary.shp <None> Softclip;"\
                                "LA/ridges.shp Elevation hardline", "", 
                                "COMPUTE_STATS", "RELATIVE_PATHS")
```

### Example 4

```python
'''*********************************************************************
Name: Export Elevation Raster from Ground LAS Measurements
Description: This script demonstrates how to export
             ground measurements from LAS files to a raster using a
             LAS dataset. This sample is designed to be used as a script
             tool.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Set Local Variables
    inLas = arcpy.GetParameterAsText(0)
    recursion = arcpy.GetParameterAsText(1)
    surfCons = arcpy.GetParameterAsText(2)
    classCode = arcpy.GetParameterAsText(3)
    returnValue = arcpy.GetParameterAsText(4)
    spatialRef = arcpy.GetParameterAsText(5)
    lasD = arcpy.GetParameterAsText(6)
    outRaster = arcpy.GetParameterAsText(7)
    cellSize = arcpy.GetParameter(8)
    zFactor = arcpy.GetParameter(9)

    # Execute CreateLasDataset
    arcpy.management.CreateLasDataset(inLas, lasD, recursion, surfCons, sr)
    # Execute MakeLasDatasetLayer
    lasLyr = arcpy.CreateUniqueName('Baltimore')
    arcpy.management.MakeLasDatasetLayer(lasD, lasLyr, classCode, returnValue)
    # Execute LasDatasetToRaster
    arcpy.conversion.LasDatasetToRaster(lasLyr, outRaster, 'ELEVATION',
                              'TRIANGULATION LINEAR WINDOW_SIZE 10', 'FLOAT',
                              'CELLSIZE', cellSize, zFactor)
    print(arcpy.GetMessages())

except arcpy.ExecuteError:
    print(arcpy.GetMessages())

except Exception as err:
    print(err.args[0])

finally:
    arcpy.management.Delete(lasLyr)
```

### Example 5

```python
'''*********************************************************************
Name: Export Elevation Raster from Ground LAS Measurements
Description: This script demonstrates how to export
             ground measurements from LAS files to a raster using a
             LAS dataset. This sample is designed to be used as a script
             tool.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Set Local Variables
    inLas = arcpy.GetParameterAsText(0)
    recursion = arcpy.GetParameterAsText(1)
    surfCons = arcpy.GetParameterAsText(2)
    classCode = arcpy.GetParameterAsText(3)
    returnValue = arcpy.GetParameterAsText(4)
    spatialRef = arcpy.GetParameterAsText(5)
    lasD = arcpy.GetParameterAsText(6)
    outRaster = arcpy.GetParameterAsText(7)
    cellSize = arcpy.GetParameter(8)
    zFactor = arcpy.GetParameter(9)

    # Execute CreateLasDataset
    arcpy.management.CreateLasDataset(inLas, lasD, recursion, surfCons, sr)
    # Execute MakeLasDatasetLayer
    lasLyr = arcpy.CreateUniqueName('Baltimore')
    arcpy.management.MakeLasDatasetLayer(lasD, lasLyr, classCode, returnValue)
    # Execute LasDatasetToRaster
    arcpy.conversion.LasDatasetToRaster(lasLyr, outRaster, 'ELEVATION',
                              'TRIANGULATION LINEAR WINDOW_SIZE 10', 'FLOAT',
                              'CELLSIZE', cellSize, zFactor)
    print(arcpy.GetMessages())

except arcpy.ExecuteError:
    print(arcpy.GetMessages())

except Exception as err:
    print(err.args[0])

finally:
    arcpy.management.Delete(lasLyr)
```

---

## Create Map Tile Package (Data Management)

## Summary

Generates tiles from a map and packages them as a single tile package or multiple smaller tile packages.

## Usage

- This tool honors the Parallel Processing Factor environment variable. When the Create Multiple Packages parameter is checked, parallel processing will generate cache content across multiple processes to use the available CPU and generate tile packages when the default threshold (1 GB size limit) is reached.
- Use the Create Multiple Packages parameter when you are working with large volumes of data. With this parameter checked, multiple small tile packages will be created instead of a single large one. This allows you to generate tile content greater than 500 GB in single job and share it or upload and publish it to ArcGIS Online as a hosted tile layer. Using this approach, you can generate large tile content into small tile packages for sharing without subdividing cache extents and levels into multiple jobs.When this parameter is checked, you must provide a path to an empty folder on the file system in the Ouptut Folder parameter to save the output packages. You can create multipart packages only when the Package type parameter is set to tpkx and the Parallel Processing Factor environment variable is not 0.
- When the Tiling Format parameter is set to PNG, the tool will automatically use the correct format (PNG8, PNG24, or PNG32) based on the value specified for the Maximum Level Of Detail parameter.
- The input map must include a description and tags for the tool to run. To add a description and tags, right-click the map name in the Contents pane, and select Properties. On the Map Properties dialog box, on the Metadata tab, fill in the Tags and Description text boxes.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | The map from which tiles will be generated and packaged. | Map |
| Package for ArcGIS Online \| Bing Maps \| Google Maps | Specifies whether the tiling scheme will be generated from an existing map service or the map tiles will be generated for ArcGIS Online, Bing Maps, and Google Maps. Checked—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. This is the default.The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Desktop includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this tiling scheme, the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system.The ArcGIS Online/Bing Maps/Google Maps tiling scheme is required if you'll be overlaying the package with ArcGIS Online, Bing Maps, or Google Maps. One advantage of the ArcGIS Online/Bing Maps/Google Maps tiling scheme is that it is widely known in the web mapping world, so the tiles will match those of other organizations that have used this tiling scheme. Even if you don't plan to overlay any of these well-known map services, you may choose the tiling scheme for its interoperability potential.The ArcGIS Online/Bing Maps/Google Maps tiling scheme may contain scales that will be zoomed in too far to be of use in your map. Packaging for large scales can take up time and disk storage space. For example, the largest scale in the tiling scheme is approximately 1:1,000. Packaging the entire continental United States at this scale can take weeks and require hundreds of gigabytes of storage. If you aren't prepared to package at this scale level, remove this scale level when you create the tile package.Unchecked—A tiling scheme from an existing map service will be used.Choose this option if your organization has created a tiling scheme for an existing service on the server and you want to match it. Matching tiling schemes ensures that the tiles will overlay correctly in your Maps SDKs application.If you choose this option, use the same coordinate system for the source map as the map with the tiling scheme you're importing. | Boolean |
| Output File | The output path and file name for the map tile package. When the Create Multiple Packages parameter is checked, this parameter is replaced by the Output Folder parameter to specify where the tile packages will be generated. | File |
| Tiling Format | Specifies the format that will be used for the generated tiles.PNG—The correct format (PNG 8, PNG 24, or PNG 32) will be used based on the specified Maximum Level Of Detail parameter value. This is the default.PNG 8 bit—PNG8 format will be used. Use this format for overlay services that need to have a transparent background, such as roads and boundaries. PNG8 creates tiles of very small size on disk with no loss of information. Do not use PNG8 if the map contains more than 256 colors. Imagery, hillshades, gradient fills, transparency, and antialiasing can use more than 256 colors in a map. Even symbols such as highway shields may have subtle antialiasing around the edges that unexpectedly adds colors to a map.PNG 24 bit—PNG24 format will be used. Use this format for overlay services, such as roads and boundaries, that have more than 256 colors (if fewer than 256 colors, use PNG8). PNG 32 bit—PNG32 format will be used. Use this format for overlay services, such as roads and boundaries, that have more than 256 colors. PNG32 works well for overlay services that have antialiasing enabled on lines or text. PNG32 creates larger tiles on disk than PNG24.JPEG—JPEG format will be used. Use this format for basemap services that have large color variation and do not need a transparent background. For example, raster imagery and detailed vector basemaps work well with JPEG. JPEG is a lossy image format. It attempts to selectively remove data without affecting the appearance of the image. This can cause very small tile sizes on disk, but if a map contains vector line work or labels, it may produce too much noise or blurry areas around the lines. If this is the case, you can raise the compression value from the default of 75. A higher value, such as 90, may balance an acceptable quality of line work with the small tile size benefit of the JPEG.If you are willing to accept a minor amount of noise in the images, you may save large amounts of disk space with JPEG. The smaller tile size also means the application can download the tiles faster.Mixed—JPEG format will be used in the center of the package and PNG32 will be used on the edge of the package. Use mixed mode when you want to cleanly overlay raster packages on other layers.When a mixed package is created, PNG32 tiles are created where transparency is detected (in other words, where the map background is visible). The rest of the tiles are built using JPEG. This keeps the average file size down while providing a clean overlay on top of other packages. If you do not use the mixed mode package in this scenario, a nontransparent collar around the periphery of the image where it overlaps the other package will be visible. | String |
| Maximum Level Of Detail | The integer representation corresponding to the number of scales used to define a cache tiling scheme. This scale value defines the maximum level up to which the cache tiles will be generated in the tile package. Larger values reflect larger scales that show more detail but require more storage space. Smaller values reflect smaller scales that show less detail and require less storage space. Possible values are from 1 to 23. The default value is 1. The maximum level of detail value must be greater than the minimum level of detail value. | Long |
| Service(Optional) | The name of the map service or the .xml files that will be used for the tiling scheme. This parameter is required only when the Package for ArcGIS Online \| Bing Maps \| Google Maps parameter is unchecked. | Map Server; File |
| Summary(Optional) | The summary information that will be added to the properties of the package. | String |
| Tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added, separated by a comma or semicolon. | String |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Compression Quality(Optional) | A value between 1 and 100 for the JPEG compression quality. The default value is 75 for JPEG tile format and zero for other formats.Compression is supported only for JPEG and mixed formats. A higher value will result in a larger file size with a higher-quality image. A lower value will result in a smaller file size with a lower-quality image. | Long |
| Package type (Optional) | Specifies the type of tile package that will be created. tpk—A .tpk file will be created. Tiles will be stored using Compact storage format. This format is supported across ArcGIS.tpkx—A .tpkx file will be created. Tiles will be stored using CompactV2 storage format, which provides better performance on network shares and cloud store directories. This is the default. | String |
| Minimum Level Of Detail (Optional) | The integer representation corresponding to the number of scales used to define a cache tiling scheme. This scale value defines the level at which the cache tiles begin to be available and generated in the tile package. Possible values are from 0 to 23. The default value is 0. The minimum level of detail value must be less than or equal to the maximum level of detail value. | Long |
| Area of Interest (Optional) | A feature set that constrains where tiles will be created. Use an area of interest to create tiles for irregularly shaped areas or multipart features. The areas outside the bounding box of area of interest features will not be cached. If no value is provided for this parameter, the area of interest will be the full extent of the input map. | Feature Set |
| Create Multiple Packages (Optional) | Specifies whether a single large tile package or multiple small tile packages will be generated. This parameter is not available when the Parallel Processing Factor environment variable is 0 or when the Package type parameter is set to tpk.Checked—Multiple tile packages (each approximately 1 GB in size) will be generated in the location defined in the Output Folder parameter.Unchecked—A single tile package will be generated in the location defined in the Output File parameter. This is the default. | Boolean |
| Output Folder | The location where the multiple tile packages will be generated. If the output folder is not empty, a subfolder will be created in the output folder to store the tiles. An automatically generated GUID will be used as the folder name.When the Create Multiple Packages parameter is unchecked, this parameter is replaced by the Output File parameter to specify the name of the single tile package that will be generated. | Folder |
| in_map | The map from which tiles will be generated and packaged. | Map |
| service_type | Specifies whether the tiling scheme will be generated from an existing map service or whether map tiles will be generated for ArcGIS Online, Bing Maps, and Google Maps. EXISTING—A tiling scheme from an existing map service will be used. You must specify a map service in the service_file parameter.Choose this option if your organization has created a tiling scheme for an existing service on the server and you want to match it. Matching tiling schemes ensures that the tiles will overlay correctly in your Maps SDKs application.If you choose this option, use the same coordinate system for the source map as the map with the tiling scheme you're importing.ONLINE—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. This is the default.The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Desktop includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this tiling scheme, the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system.The ArcGIS Online/Bing Maps/Google Maps tiling scheme is required if you'll be overlaying the package with ArcGIS Online, Bing Maps, or Google Maps. One advantage of the ArcGIS Online/Bing Maps/Google Maps tiling scheme is that it is widely known in the web mapping world, so the tiles will match those of other organizations that have used this tiling scheme. Even if you don't plan to overlay any of these well-known map services, you may choose the tiling scheme for its interoperability potential.The ArcGIS Online/Bing Maps/Google Maps tiling scheme may contain scales that will be zoomed in too far to be of use in your map. Packaging for large scales can take up time and disk storage space. For example, the largest scale in the tiling scheme is approximately 1:1,000. Packaging the entire continental United States at this scale can take weeks and require hundreds of gigabytes of storage. If you aren't prepared to package at this scale level, remove this scale level when you create the tile package. | Boolean |
| output_file | The output path and file name for the map tile package. | File |
| format_type | Specifies the format that will be used for the generated tiles.PNG—The correct format (PNG 8, PNG 24, or PNG 32) will be used based on the specified Maximum Level Of Detail parameter value. This is the default.PNG8—PNG8 format will be used. Use this format for overlay services that need to have a transparent background, such as roads and boundaries. PNG8 creates tiles of very small size on disk with no loss of information. Do not use PNG8 if the map contains more than 256 colors. Imagery, hillshades, gradient fills, transparency, and antialiasing can use more than 256 colors in a map. Even symbols such as highway shields may have subtle antialiasing around the edges that unexpectedly adds colors to a map.PNG24—PNG24 format will be used. Use this format for overlay services, such as roads and boundaries, that have more than 256 colors (if fewer than 256 colors, use PNG8). PNG32—PNG32 format will be used. Use this format for overlay services, such as roads and boundaries, that have more than 256 colors. PNG32 works well for overlay services that have antialiasing enabled on lines or text. PNG32 creates larger tiles on disk than PNG24.JPEG—JPEG format will be used. Use this format for basemap services that have large color variation and do not need a transparent background. For example, raster imagery and detailed vector basemaps work well with JPEG. JPEG is a lossy image format. It attempts to selectively remove data without affecting the appearance of the image. This can cause very small tile sizes on disk, but if a map contains vector line work or labels, it may produce too much noise or blurry areas around the lines. If this is the case, you can raise the compression value from the default of 75. A higher value, such as 90, may balance an acceptable quality of line work with the small tile size benefit of the JPEG.If you are willing to accept a minor amount of noise in the images, you may save large amounts of disk space with JPEG. The smaller tile size also means the application can download the tiles faster.MIXED—JPEG format will be used in the center of the package and PNG32 will be used on the edge of the package. Use mixed mode when you want to cleanly overlay raster packages on other layers.When a mixed package is created, PNG32 tiles are created where transparency is detected (in other words, where the map background is visible). The rest of the tiles are built using JPEG. This keeps the average file size down while providing a clean overlay on top of other packages. If you do not use the mixed mode package in this scenario, a nontransparent collar around the periphery of the image where it overlaps the other package will be visible. | String |
| level_of_detail | The integer representation corresponding to the number of scales used to define a cache tiling scheme. This scale value defines the maximum level up to which the cache tiles will be generated in the tile package. Larger values reflect larger scales that show more detail but require more storage space. Smaller values reflect smaller scales that show less detail and require less storage space. Possible values are from 1 to 23. The default value is 1. The maximum level of detail value must be greater than the minimum level of detail value. | Long |
| service_file(Optional) | The name of the map service or the .xml files that will be used for the tiling scheme. This parameter is required only when the service_type parameter is set to EXISTING. | Map Server; File |
| summary(Optional) | The summary information that will be added to the properties of the package. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added, separated by a comma or semicolon. | String |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| compression_quality(Optional) | A value between 1 and 100 for the JPEG compression quality. The default value is 75 for JPEG tile format and zero for other formats.Compression is supported only for JPEG and mixed formats. A higher value will result in a larger file size with a higher-quality image. A lower value will result in a smaller file size with a lower-quality image. | Long |
| package_type(Optional) | Specifies the type of tile package that will be created. tpk—A .tpk file will be created. Tiles will be stored using Compact storage format. This format is supported across ArcGIS.tpkx—A .tpkx file will be created. Tiles will be stored using CompactV2 storage format, which provides better performance on network shares and cloud store directories. This is the default. | String |
| min_level_of_detail(Optional) | The integer representation corresponding to the number of scales used to define a cache tiling scheme. This scale value defines the level at which the cache tiles begin to be available and generated in the tile package. Possible values are from 0 to 23. The default value is 0. The minimum level of detail value must be less than or equal to the maximum level of detail value. | Long |
| area_of_interest(Optional) | A feature set that constrains where tiles will be created. Use an area of interest to create tiles for irregularly shaped areas or multipart features. The areas outside the bounding box of area of interest features will not be cached. If no value is provided for this parameter, the area of interest will be the full extent of the input map. | Feature Set |
| create_multiple_packages(Optional) | Specifies whether a single large tile package or multiple small tile packages will be generated. This parameter is not available when the parallelProcessingFactor environment variable is 0 or when the package_type parameter is set to tpk.CREATE_MULTIPLE_PACKAGES—Multiple tile packages (each approximately 1 GB in size) will be generated in the location defined in the output_folder parameter.CREATE_SINGLE_PACKAGE—A single tile package will be generated in the location defined in the output_file parameter. This is the default. | Boolean |
| output_folder | The output folder where the multiple tile packages will be generated. If the output folder is not empty, a subfolder will be created in the output folder to store the tiles. An automatically generated GUID will be used as the folder name. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.CreateMapTilePackage(in_map, service_type, output_file, format_type, level_of_detail, {service_file}, {summary}, {tags}, {extent}, {compression_quality}, {package_type}, {min_level_of_detail}, {area_of_interest}, {create_multiple_packages}, output_folder)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"
aprx = arcpy.mp.ArcGISProject(r"C:\Data\MinMaxLOD\states73K\states73K_1.aprx")
map1 = aprx.listMaps()[0]
arcpy.management.CreateMapTilePackage(map1, "ONLINE", "Example.tpkx", "PNG", "5")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"
aprx = arcpy.mp.ArcGISProject(r"C:\Data\MinMaxLOD\states73K\states73K_1.aprx")
map1 = aprx.listMaps()[0]
arcpy.management.CreateMapTilePackage(map1, "ONLINE", "Example.tpkx", "PNG", "5")
```

### Example 4

```python
# Name: CreateMapTilePackage.py
# Description: Find all the maps in the project and
#   create a single map tile package for each map

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"

# Loop through the project, find all the maps, and
#   create a single map tile package for each map,
#   using the same name as the map
p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
extent = ""
aoi = ""

for m in p.listMaps():
    print("Packaging " + m.name)
    arcpy.management.CreateMapTilePackage(m, "ONLINE", "{}.tpkx".format(m.name), 
                                            "PNG", 9, None, "MapSummary", "MapTag", extent, "", "tpkx", 5, aoi)
```

### Example 5

```python
# Name: CreateMapTilePackage.py
# Description: Find all the maps in the project and
#   create a single map tile package for each map

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"

# Loop through the project, find all the maps, and
#   create a single map tile package for each map,
#   using the same name as the map
p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
extent = ""
aoi = ""

for m in p.listMaps():
    print("Packaging " + m.name)
    arcpy.management.CreateMapTilePackage(m, "ONLINE", "{}.tpkx".format(m.name), 
                                            "PNG", 9, None, "MapSummary", "MapTag", extent, "", "tpkx", 5, aoi)
```

### Example 6

```python
# Name: CreateMapTilePackage.py
# Description: Create multiple map tile packages for a given map

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"

# Create multiple map tile packages for given map,

aprx = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
map1 = aprx.listMaps()[0]
extent = ""
aoi = ""
createMultiplePackages = "create_multiple_packages"
outputFolder = r"C:\11\multi"

arcpy.management.CreateMapTilePackage(map1, "ONLINE", "", "PNG", 9, None, "MapSummary", "MapTag",
                                      extent, "", "tpkx", 5, aoi,createMultiplePackages, outputFolder )
```

### Example 7

```python
# Name: CreateMapTilePackage.py
# Description: Create multiple map tile packages for a given map

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = r"C:\Data\MinMaxLOD\states73K"

# Create multiple map tile packages for given map,

aprx = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
map1 = aprx.listMaps()[0]
extent = ""
aoi = ""
createMultiplePackages = "create_multiple_packages"
outputFolder = r"C:\11\multi"

arcpy.management.CreateMapTilePackage(map1, "ONLINE", "", "PNG", 9, None, "MapSummary", "MapTag",
                                      extent, "", "tpkx", 5, aoi,createMultiplePackages, outputFolder )
```

---

## Create Mobile Geodatabase (Data Management)

## Summary

Creates a mobile geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mobile Geodatabase Location | The folder where the mobile geodatabase will be created. | Folder |
| Mobile Geodatabase Name | The name of the mobile geodatabase to be created. | String |
| out_folder_path | The folder where the mobile geodatabase will be created. | Folder |
| out_name | The name of the mobile geodatabase to be created. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateMobileGDB(out_folder_path, out_name)
```

### Example 2

```python
import arcpy
arcpy.CreateMobileGDB_management("C:/MyProject/MyData", "MyMobileGDB")
```

### Example 3

```python
import arcpy
arcpy.CreateMobileGDB_management("C:/MyProject/MyData", "MyMobileGDB")
```

---

## Create Mobile Map Package (Data Management)

## Summary

Packages maps and basemaps along with all referenced data sources into a single .mmpk file.

## Usage

- Mobile map packages can be used by ArcGIS Pro, ArcGIS Navigator, and with ArcGIS Maps SDKs for Native Apps.
- Mobile map packages can be shared with others by uploading them to your organization in ArcGIS Online or by sharing an .mmpk file through common file sharing methods.
- Mobile map packages support simple features, raster datasets, tables, relationship classes, locators, network datasets, and maps that contain map tile (.tpk) or vector tile (.vtpk) packages. Mobile map packages created in ArcGIS Pro for use with ArcGIS Maps SDKs for Native Apps support the following layer types: feature layers, raster layers, image tile layers from previously created tile packages, and vector tile layers from previously created vector tile packages.
- If no extent is specified, the visible extent of the map will be used to define the area of interest (AOI) and consolidate only those features that intersect that extent.
- When creating a mobile map package, all maps, basemaps, and data layers will be projected to a common coordinate system. The coordinate system that will be used is the coordinate system of the first input map specified.
- If the map contains a network layer that references a network dataset, it will be included in the mobile map package as a transportation network for routing and driving directions. All feature classes and tables that the network references will become part of the mobile map package. This may include layers that are not part of the map. For example, if there is only a network layer in the map, the package will include the streets, junctions, and turns feature classes associated with the network.For a network dataset to support mobile map packaging, keep the following restrictions in mind:The network dataset must be part of a geodatabase. This excludes networks that are in SDC or shapefile format.The network dataset must be from ArcGIS 10.0 or later. If the network is from an earlier version, upgrade the geodatabase and network.The network dataset cannot have an unknown coordinate system.The network dataset cannot use any Visual Basic or Python script evaluators.The network dataset cannot use any custom COM evaluators.A network dataset that uses live traffic will be packaged, but the live traffic components will not be used since they are not supported.
- The network dataset must be part of a geodatabase. This excludes networks that are in SDC or shapefile format.
- The network dataset must be from ArcGIS 10.0 or later. If the network is from an earlier version, upgrade the geodatabase and network.
- The network dataset cannot have an unknown coordinate system.
- The network dataset cannot use any Visual Basic or Python script evaluators.
- The network dataset cannot use any custom COM evaluators.
- A network dataset that uses live traffic will be packaged, but the live traffic components will not be used since they are not supported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | One or more maps or basemaps that will be packaged into a single .mmpk file. | Map |
| Output File | The output mobile map package (.mmpk). | File |
| Input Locator (Optional) | One or more locators (.loc) that will be included in the mobile map package.Note:Locators have the following restrictions:The locator cannot have an unknown coordinate system.The locator or any participating locator in a composite locator cannot be a geocoding service, including services from ArcGIS Enterprise or ArcGIS Online. | Address Locator |
| Area of Interest(Optional) | A polygon layer that defines the area of interest. Only those features that intersect this value will be included in the mobile map package. | Feature Layer |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Clip Features(Optional) | Specifies whether the geometry of the output features will be clipped to the specified Area of Interest or Extent parameter value, or remain unaltered.Checked—The geometry of the features will be clipped to the specified Area of Interest or Extent parameter value.Unchecked—Features in the map will be selected and their geometry will remain unaltered. This is the default. | Boolean |
| Title(Optional) | The title information that will be added to the properties of the package. | String |
| Summary(Optional) | The text that will be used as the output package's summary property. | String |
| Description(Optional) | The description information that will be added to the properties of the package. | String |
| Tags (Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| Credits(Optional) | The credit information that will be added to the properties of the package. | String |
| Use Limitations(Optional) | The use limitations that will be added to the properties of the package. | String |
| Enable Anonymous Use (Optional) | Specifies whether the mobile map can be used without an Esri Named User account.Checked—Anyone with access to the package can use the mobile map without signing in with an Esri Named User account.Unchecked—Anyone with access to the package must be signed in with an Esri Named User account to use the mobile map. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| Enable Map Expiration (Optional) | Specifies whether a time-out will be enabled on the mobile map package.Checked—A time-out will be enabled on the mobile map package.Unchecked—A time-out will not be enabled on the mobile map package. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| Map Expiration Type (Optional) | Specifies the type of access a user will have to the expired mobile map package.Allow to open—A user of the package will be warned that the map has expired but will be allowed to open it. This is the default.Do not allow to open—A user of the package will be warned that the map has expired and will not be allowed to open it.License:This optional parameter is only available with the Publisher extension. | String |
| Expiration Date (Optional) | The date the mobile map package will expire.License:This optional parameter is only available with the Publisher extension. | Date |
| Expiration Message (Optional) | A text message that will display when an expired map is accessed. License:This optional parameter is only available with the Publisher extension. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Reference online content(Optional) | Specifies whether service layers will be referenced in the package.Unchecked—Service layers will not be referenced in the mobile package. This is the default.Checked—Service layers will be referenced in the mobile package. | Boolean |
| in_map[in_map,...] | One or more maps or basemaps that will be packaged into a single .mmpk file. | Map |
| output_file | The output mobile map package (.mmpk). | File |
| in_locator[in_locator,...](Optional) | One or more locators (.loc) that will be included in the mobile map package.Note:Locators have the following restrictions:The locator cannot have an unknown coordinate system.The locator or any participating locator in a composite locator cannot be a geocoding service, including services from ArcGIS Enterprise or ArcGIS Online. | Address Locator |
| area_of_interest(Optional) | A polygon layer that defines the area of interest. Only those features that intersect this value will be included in the mobile map package. | Feature Layer |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| clip_features(Optional) | Specifies whether the geometry of the output features will be clipped to the specified area of interest or extent, or remain unaltered.CLIP—The geometry of the features will be clipped to the specified area_of_interest or extent parameter value.SELECT— Features in the map will be selected and their geometry will remain unaltered. This is the default. | Boolean |
| title(Optional) | The title information that will be added to the properties of the package. | String |
| summary(Optional) | The text that will be used as the output package's summary property. | String |
| description(Optional) | The description information that will be added to the properties of the package. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| credits(Optional) | The credit information that will be added to the properties of the package. | String |
| use_limitations(Optional) | The use limitations that will be added to the properties of the package. | String |
| anonymous_use(Optional) | Specifies whether the mobile map can be used without an Esri Named User account.ANONYMOUS_USE—Anyone with access to the package can use the mobile map without signing in with an Esri Named User account.STANDARD—Anyone with access to the package must be signed in with an Esri Named User account to use the mobile map. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| enable_map_expiration(Optional) | Specifies whether a time-out will be enabled on the mobile map package.ENABLE_MAP_EXPIRATION—A time-out will be enabled on the mobile map package.DISABLE_MAP_EXPIRATION—A time-out will not be enabled on the mobile map package. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| map_expiration_type(Optional) | Specifies the type of access a user will have to the expired mobile map package.ALLOW_TO_OPEN—A user of the package will be warned that the map has expired but will be allowed to open it. This is the default.DONOT_ALLOW_TO_OPEN—A user of the package will be warned that the map has expired and will not be allowed to open it.License:This optional parameter is only available with the Publisher extension. | String |
| expiration_date(Optional) | The date the mobile map package will expire.License:This optional parameter is only available with the Publisher extension. | Date |
| expiration_message(Optional) | A text message that will display when an expired map is accessed. License:This optional parameter is only available with the Publisher extension. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| reference_online_content(Optional) | Specifies whether service layers will be referenced in the package.INCLUDE_SERVICE_LAYERS—Service layers will be referenced in the mobile package.EXCLUDE_SERVICE_LAYERS—Service layers will not be referenced in the mobile package. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateMobileMapPackage(in_map, output_file, {in_locator}, {area_of_interest}, {extent}, {clip_features}, {title}, {summary}, {description}, {tags}, {credits}, {use_limitations}, {anonymous_use}, {enable_map_expiration}, {map_expiration_type}, {expiration_date}, {expiration_message}, {select_related_rows}, {reference_online_content})
```

### Example 2

```python
import arcpy

arcpy.management.CreateMobileMapPackage(
    ["Basemap1", "Basemap2", "Map1"], r"d:\temp\MobileMapPackage1.mmpk", 
    r"d:\data\MyLocators\AddressLocator.loc", None, "DEFAULT", "SELECT", 
    "Title", "Summary", "description", "Tag", "Credit information", 
    "Usage_Limitations")
```

### Example 3

```python
import arcpy

arcpy.management.CreateMobileMapPackage(
    ["Basemap1", "Basemap2", "Map1"], r"d:\temp\MobileMapPackage1.mmpk", 
    r"d:\data\MyLocators\AddressLocator.loc", None, "DEFAULT", "SELECT", 
    "Title", "Summary", "description", "Tag", "Credit information", 
    "Usage_Limitations")
```

### Example 4

```python
import arcpy
import datetime

days_valid = 14
expiration_date = (datetime.date.today() + datetime.timedelta(days=days_valid)).strftime("%x")
outputfile = "d:/Data/Output/sandiego.mmpk"

arcpy.management.CreateMobileMapPackage(
    [r"C:\data\Basemap1.mapx", r"C:\data\Map1.mapx"], outputfile, None, 
    r"\\share\layers\AreaOfInterest.lyrx", "DEFAULT", "CLIP", "Title", 
    "Summary", "description", "Tag", "Credits", "Use",
    "STANDARD", "ENABLE_MAP_EXPIRATION",
    "DONOT_ALLOW_TO_OPEN", expiration_date,
    "This map is expired.  Contact the map publisher for an updated map.")
```

### Example 5

```python
import arcpy
import datetime

days_valid = 14
expiration_date = (datetime.date.today() + datetime.timedelta(days=days_valid)).strftime("%x")
outputfile = "d:/Data/Output/sandiego.mmpk"

arcpy.management.CreateMobileMapPackage(
    [r"C:\data\Basemap1.mapx", r"C:\data\Map1.mapx"], outputfile, None, 
    r"\\share\layers\AreaOfInterest.lyrx", "DEFAULT", "CLIP", "Title", 
    "Summary", "description", "Tag", "Credits", "Use",
    "STANDARD", "ENABLE_MAP_EXPIRATION",
    "DONOT_ALLOW_TO_OPEN", expiration_date,
    "This map is expired.  Contact the map publisher for an updated map.")
```

---

## Create Mobile Scene Package (Data Management)

## Summary

Creates a mobile scene package file (.mspk) from one or more scenes for use across the ArcGIS system.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Scene | One or more local or global scenes that will be packaged into a single .mspk file. Active scenes and .mapx files can be added as input. | Map |
| Output File | The output mobile scene package .mspk file. | File |
| Input Locator (Optional) | One or more locators (.loc file) that will be included in the mobile scene package. Note:Locators have the following restrictions:The locator cannot have an unknown coordinate system.The locator or any participating locator in a composite locator cannot be a geocoding service, including services from ArcGIS Enterprise or ArcGIS Online. | Address Locator |
| Area of Interest (Optional) | A polygon layer that defines the area of interest. Only those features that intersect the area of interest will be included in the mobile scene package. | Feature Layer |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Clip Features (Optional) | Specifies whether the output features will be clipped to the given area of interest or extent. Checked—The geometry of the features will be clipped to the given area of interest or extent.Unchecked—Features in the scene will be selected and their geometry will remain unaltered. This is the default.Note:Multipatch feature layers, 3D point feature layers, LAS dataset layers, service layers, and tile packages cannot be clipped and will be completely copied to the mobile scene package. | Boolean |
| Title (Optional) | Title information that will be added to the properties of the package. | String |
| Summary(Optional) | Summary information that will be added to the properties of the package. | String |
| Description(Optional) | Description information that will be added to the properties of the package. | String |
| Tags(Optional) | Tag information that will be added to the properties of the package. Multiple tags can be added, separated by a comma or semicolon. | String |
| Credits(Optional) | Credit information that will be added to the properties of the package. | String |
| Use Limitations(Optional) | Use limitations that will be added to the properties of the package. | String |
| Enable Anonymous Use(Optional) | Specifies whether the mobile scenes can be used by anyone or only those with an ArcGIS account. Checked—Anyone with access to the package can use the mobile scene without signing in with an Esri named user account.Unchecked—Anyone with access to the package must be signed in with a named user account to use the mobile scene. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| Enable Scene Expiration (Optional) | Specifies whether the mobile scene package will time out.Checked—Time-out functionality will be enabled on the mobile scene package.Unchecked—Time-out functionality will not be enabled on the mobile scene package. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| Scene Expiration Type (Optional) | Specifies the type of scene access that will be used for the expired mobile scene package.Allow to open—The user of the package will be warned that the scene has expired and allowed to open the scene. This is the default.Do not allow to open—The user of the package will be warned that the scene has expired and will not be allowed to open the package.License:This optional parameter is only available with the Publisher extension. | String |
| Expiration Date (Optional) | The date the mobile scene package will expire.License:This optional parameter is only available with the Publisher extension. | Date |
| Expiration Message (Optional) | The text message that will appear when an expired scene is accessed. License:This optional parameter is only available with the Publisher extension. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Reference online content(Optional) | Specifies whether service layers will be referenced in the package.Unchecked—Service layers will not be referenced in the mobile package. This is the default.Checked—Service layers will be referenced in the mobile package. | Boolean |
| in_scene[in_scene,...] | One or more local or global scenes that will be packaged into a single .mspk file. Active scenes and .mapx files can be added as input. | Map |
| output_file | The output mobile scene package .mspk file. | File |
| in_locator[in_locator,...](Optional) | One or more locators (.loc file) that will be included in the mobile scene package. Note:Locators have the following restrictions:The locator cannot have an unknown coordinate system.The locator or any participating locator in a composite locator cannot be a geocoding service, including services from ArcGIS Enterprise or ArcGIS Online. | Address Locator |
| area_of_interest(Optional) | A polygon layer that defines the area of interest. Only those features that intersect the area of interest will be included in the mobile scene package. | Feature Layer |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| clip_features(Optional) | Specifies whether the output features will be clipped to the given area of interest or extent. Checked—The geometry of the features will be clipped to the given area of interest or extent.Unchecked—Features in the scene will be selected and their geometry will remain unaltered. This is the default.Note:Multipatch feature layers, 3D point feature layers, LAS dataset layers, service layers, and tile packages cannot be clipped and will be completely copied to the mobile scene package.Specifies whether the output features will be clipped to the given area of interest or extent.CLIP—The geometry of the features will be clipped to the given area of interest or extent.SELECT— Features in the map will be selected and their geometry will remain unaltered. This is the default.Note:Multipatch feature layers, 3D point feature layers, LAS dataset layers, and tile packages cannot be clipped and will be completely copied to the mobile scene package. | Boolean |
| title(Optional) | Title information that will be added to the properties of the package. | String |
| summary(Optional) | Summary information that will be added to the properties of the package. | String |
| description(Optional) | Description information that will be added to the properties of the package. | String |
| tags(Optional) | Tag information that will be added to the properties of the package. Multiple tags can be added, separated by a comma or semicolon. | String |
| credits(Optional) | Credit information that will be added to the properties of the package. | String |
| use_limitations(Optional) | Use limitations that will be added to the properties of the package. | String |
| anonymous_use(Optional) | Specifies whether the mobile scenes can be used by anyone or only those with an ArcGIS account.ANONYMOUS_USE—Anyone with access to the package can use the mobile scene without signing in with an Esri named user account.STANDARD—Anyone with access to the package must be signed in with a named user account to use the mobile scene. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the DESKTOP or NONE options. ALL—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.DESKTOP—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.MOBILE—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.NONE—JPEG textures will be optimized for use in desktop and web platforms. | String |
| enable_scene_expiration(Optional) | Specifies whether the mobile scene package will time out.ENABLE_SCENE_EXPIRATION—Time-out functionality will be enabled on the mobile scene package.DISABLE_SCENE_EXPIRATION—Time-out functionality will not be enabled on the mobile scene package. This is the default.License:This optional parameter is only available with the Publisher extension. | Boolean |
| scene_expiration_type(Optional) | Specifies the type of scene access that will be used for the expired mobile scene package.ALLOW_TO_OPEN—The user of the package will be warned that the scene has expired and allowed to open the scene. This is the default.DONOT_ALLOW_TO_OPEN—The user of the package will be warned that the scene has expired and will not be allowed to open the package.License:This optional parameter is only available with the Publisher extension. | String |
| expiration_date(Optional) | The date the mobile scene package will expire.License:This optional parameter is only available with the Publisher extension. | Date |
| expiration_message(Optional) | The text message that will appear when an expired scene is accessed. License:This optional parameter is only available with the Publisher extension. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| reference_online_content(Optional) | Specifies whether service layers will be referenced in the package.INCLUDE_SERVICE_LAYERS—Service layers will be referenced in the mobile package.EXCLUDE_SERVICE_LAYERS—Service layers will not be referenced in the mobile package. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateMobileScenePackage(in_scene, output_file, {in_locator}, {area_of_interest}, {extent}, {clip_features}, {title}, {summary}, {description}, {tags}, {credits}, {use_limitations}, {anonymous_use}, {texture_optimization}, {enable_scene_expiration}, {scene_expiration_type}, {expiration_date}, {expiration_message}, {select_related_rows}, {reference_online_content})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r'c:\data'
arcpy.management.CreateMobileScenePackage(
    'loma_linda','LomaLindaBuilding.mspk', None, None, 'DEFAULT', 'SELECT', 
    'Loma Linda Proposed Building', 
    'Offline mobile scene package for planning department', None, 'mspk', None, 
    None, 'STANDARD', 'DESKTOP', 'ENABLE_SCENE_EXPIRATION', 'ALLOW_TO_OPEN',
    '12/31/2019 9:00:00 AM', 'This scene is expired. Contact admin@email.com',
    'EXCLUDE_SERVICE_LAYERS')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r'c:\data'
arcpy.management.CreateMobileScenePackage(
    'loma_linda','LomaLindaBuilding.mspk', None, None, 'DEFAULT', 'SELECT', 
    'Loma Linda Proposed Building', 
    'Offline mobile scene package for planning department', None, 'mspk', None, 
    None, 'STANDARD', 'DESKTOP', 'ENABLE_SCENE_EXPIRATION', 'ALLOW_TO_OPEN',
    '12/31/2019 9:00:00 AM', 'This scene is expired. Contact admin@email.com',
    'EXCLUDE_SERVICE_LAYERS')
```

### Example 4

```python
import arcpy

arcpy.env.workspace = r'c:\data'
arcpy.management.CreateMobileScenePackage(
    'Yosemite.mapx','YosemiteOffline.mspk', None, None, 'DEFAULT', 'SELECT', 
    'YosemiteOfflineScene', 
    'Offline mobile scene package for Yosemite National Park', None, 
    'mspk, yosemite, offline', None, None, 'STANDARD', 'DESKTOP', 
    'DISABLE_SCENE_EXPIRATION', 'ALLOW_TO_OPEN')
```

### Example 5

```python
import arcpy

arcpy.env.workspace = r'c:\data'
arcpy.management.CreateMobileScenePackage(
    'Yosemite.mapx','YosemiteOffline.mspk', None, None, 'DEFAULT', 'SELECT', 
    'YosemiteOfflineScene', 
    'Offline mobile scene package for Yosemite National Park', None, 
    'mspk, yosemite, offline', None, None, 'STANDARD', 'DESKTOP', 
    'DISABLE_SCENE_EXPIRATION', 'ALLOW_TO_OPEN')
```

---

## Create Mosaic Dataset (Data Management)

## Summary

Creates an empty mosaic dataset in a geodatabase.

## Usage

- The mosaic dataset must be created in a geodatabase.
- Once the mosaic dataset is created, you can use the Add Rasters To Mosaic Dataset tool to populate it with rasters.
- Starting at ArcGIS Pro 1.4, mosaic datasets created in Oracle, PostgreSQL, and SQL Server geodatabases will be created with the RASTERBLOB keyword. The RASTERBLOB keyword implements a transfer of the mosaic dataset catalog items to the DBMS.
- Mosaic datasets created with the RASTERBLOB keyword cannot be opened with earlier versions of the software. To create mosaic datasets that are backward compatible with earlier versions, alter the configuration keyword for RASTER_STORAGE to one of the following compatible keywords: BINARY for PostgreSQL and SQL ServerBLOB for Oracle
- BINARY for PostgreSQL and SQL Server
- BLOB for Oracle
- The name of the mosaic dataset must be in accordance with the limits of the geodatabase or underlying database; for example, the name cannot start with a number.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Location | The path to the geodatabase.Starting at ArcGIS Pro 1.4, mosaic datasets created in Oracle, PostgreSQL, and SQL Server geodatabases will be created with the RASTERBLOB keyword. The RASTERBLOB keyword implements a transfer of the mosaic dataset catalog items to the DBMS.Mosaic datasets created with the RASTERBLOB keyword cannot be opened with earlier versions of the software. To create mosaic datasets that are backward compatible with earlier versions, alter the configuration keyword for RASTER_STORAGE to one of the following compatible keywords: BINARY for PostgreSQL and SQL ServerBLOB for Oracle | Workspace |
| Mosaic Dataset Name | The name of the new mosaic dataset. | String |
| Coordinate System | The coordinate system that will be used for all of the items in the mosaic dataset. | Coordinate System |
| Number of Bands(Optional) | The number of bands the raster datasets will have in the mosaic dataset. | Long |
| Pixel Type (Optional) | Specifies the bit depth, or radiometric resolution, that will be used for the mosaic dataset. If not defined, the pixel type of the first raster dataset will be used.1 bit—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2 bit—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4 bit—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8-bit unsigned—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8-bit signed—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16-bit unsigned—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16-bit signed—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32-bit unsigned—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32-bit signed—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32-bit floating point—The pixel type will be a 32-bit data type supporting decimals.64 bit—The pixel type will be a 64-bit data type supporting decimals. | String |
| Product Definition(Optional) | Specifies whether a template is specific to the type of imagery you are working with or is generic. The generic options include the following standard raster data types:None—No band ordering is specified for the mosaic dataset. This is the default.Natural color—A 3-band mosaic dataset, with red, green, and blue wavelength ranges will be created. This is designed for natural color imagery.Natural color and infrared—A 4-band mosaic dataset, with red, green, blue, and near infrared wavelength ranges will be created.U and V—A mosaic dataset displaying two variables will be created.Magnitude and Direction—A mosaic dataset displaying magnitude and direction will be created.Color infrared—A 3-band mosaic dataset, with near infrared, red, and green wavelength ranges will be created.BlackSky—A 3-band mosaic dataset using the BlackSky wavelength ranges will be createdDMCii—A 3-band mosaic dataset using the DMCii wavelength ranges will be created.Deimos-2—A 4-band mosaic dataset using the Deimos-2 wavelength ranges will be created.DubaiSat-2—A 4-band mosaic dataset using the DubaiSat-2 wavelength ranges will be created.FORMOSAT-2—A 4-band mosaic dataset using the FORMOSAT-2 wavelength ranges will be created.GeoEye-1—A 4-band mosaic dataset using the GeoEye-1 wavelength ranges will be created.GF-1 Panchromatic/Multispectral (PMS)—A 4-band mosaic dataset using the Gaofen-1 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-1 Wide Field of View (WFV)—A 4-band mosaic dataset using the Gaofen-1 Wide Field of View Sensor wavelength ranges will be created.GF-2 Panchromatic/Multispectral (PMS)—A 4-band mosaic dataset using the Gaofen-2 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-4 Panchromatic/Multispectral Imagery (PMI)—A 4-band mosaic dataset using the Gaofen-4 panchromatic and multispectral wavelength ranges will be created.HJ 1A/1B Multispectral/Hyperspectral—A 4-band mosaic dataset using the Huan Jing-1 CCD Multispectral or Hyperspectral Sensor wavelength ranges will be created.IKONOS—A 4-band mosaic dataset using the IKONOS wavelength ranges will be created.Jilin-1—A 3-band mosaic dataset using the Jilin-1 wavelength ranges will be created.KOMPSAT-2—A 4-band mosaic dataset using the KOMPSAT-2 wavelength ranges will be created.KOMPSAT-3—A 4-band mosaic dataset using the KOMPSAT-3 wavelength ranges will be created.Landsat TM and ETM+—A 6-band mosaic dataset using the Landsat 5 and 7 wavelength ranges from the TM and ETM+ sensors will be created.Landsat OLI—An 8-band mosaic dataset using the Landsat 8 wavelength ranges will be created.Landsat 9—An 8-band mosaic dataset using the Landsat 9 wavelength ranges will be created.Landsat MSS—A 4-band mosaic dataset using the Landsat wavelength ranges from the MSS sensor will be created.PlanetScope—A 5-band mosaic dataset using the PlanetScope wavelength ranges will be created.Pleiades 1—A 4-band mosaic dataset using the Pleiades 1 wavelength ranges will be created.Pleiades Neo—A 6-band mosaic dataset using the Pleiades Neo wavelength ranges will be created.QuickBird—A 4-band mosaic dataset using the QuickBird wavelength ranges will be created.RapidEye—A 5-band mosaic dataset using the RapidEye wavelength ranges will be created.Sentinel 2 MSI—A 13-band mosaic dataset using the Sentinel 2 MSI wavelength ranges will be created.SkySat-C—A 4-band mosaic dataset using the SkySat-C MSI wavelength ranges will be created.SPOT-5—A 4-band mosaic dataset using the SPOT-5 wavelength ranges will be created.SPOT-6—A 4-band mosaic dataset using the SPOT-6 wavelength ranges will be created.SPOT-7—A 4-band mosaic dataset using the SPOT-7 wavelength ranges will be created.SuperView-1—A 4-band mosaic dataset using the SuperView-1 wavelength ranges will be created.TH-01—A 4-band mosaic dataset using the Tian Hui-1 wavelength ranges will be created.Vision-1—A 4-band mosaic dataset using the Vision-1 wavelength ranges will be created.WorldView-2—An 8-band mosaic dataset using the WorldView-2 wavelength ranges will be created.WorldView-3—An 8-band mosaic dataset using the WorldView-3 wavelength ranges will be created.WorldView-4—A 4-band mosaic dataset using the WorldView-4 wavelength ranges will be created.ZY-1 Panchromatic/Multispectral—A 3-band mosaic dataset using the ZiYuan-1 panchromatic/multispectral wavelength ranges will be created.ZY-3 CRESDA—A 4-band mosaic dataset using the ZiYuan-3 CRESDA wavelength ranges will be created.ZY3 SASMAC—A 4-band mosaic dataset using the ZiYuan-3 SASMAC wavelength ranges will be created.Custom—The number of bands and the average wavelength for each band are defined using the Product Band Definitions parameter (product_band_definitions in Python). | String |
| Product Band Definitions (Optional) | The definitions of the bands. Edit Product Definition by adjusting the wavelength ranges, changing the band order, and adding new bands. | Value Table |
| in_workspace | The path to the geodatabase.Starting at ArcGIS Pro 1.4, mosaic datasets created in Oracle, PostgreSQL, and SQL Server geodatabases will be created with the RASTERBLOB keyword. The RASTERBLOB keyword implements a transfer of the mosaic dataset catalog items to the DBMS.Mosaic datasets created with the RASTERBLOB keyword cannot be opened with earlier versions of the software. To create mosaic datasets that are backward compatible with earlier versions, alter the configuration keyword for RASTER_STORAGE to one of the following compatible keywords: BINARY for PostgreSQL and SQL ServerBLOB for Oracle | Workspace |
| in_mosaicdataset_name | The name of the new mosaic dataset. | String |
| coordinate_system | The coordinate system that will be used for all of the items in the mosaic dataset. | Coordinate System |
| num_bands(Optional) | The number of bands the raster datasets will have in the mosaic dataset. | Long |
| pixel_type(Optional) | Specifies the bit depth, or radiometric resolution, that will be used for the mosaic dataset. If not defined, the pixel type of the first raster dataset will be used.1_BIT—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2_BIT—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4_BIT—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8_BIT_UNSIGNED—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8_BIT_SIGNED—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16_BIT_UNSIGNED—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16_BIT_SIGNED—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32_BIT_UNSIGNED—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32_BIT_SIGNED—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32_BIT_FLOAT—The pixel type will be a 32-bit data type supporting decimals.64_BIT—The pixel type will be a 64-bit data type supporting decimals. | String |
| product_definition(Optional) | Specifies whether a template is specific to the type of imagery you are working with or is generic. The generic options include the following standard raster data types:NONE—No band ordering is specified for the mosaic dataset. This is the default.NATURAL_COLOR_RGB—A 3-band mosaic dataset, with red, green, and blue wavelength ranges will be created. This is designed for natural color imagery.NATURAL_COLOR_RGBI—A 4-band mosaic dataset, with red, green, blue, and near infrared wavelength ranges will be created.VECTOR_FIELD_UV—A mosaic dataset displaying two variables will be created.VECTOR_FIELD_MAGNITUDE_DIRECTION—A mosaic dataset displaying magnitude and direction will be created.FALSE_COLOR_IRG—A 3-band mosaic dataset, with near infrared, red, and green wavelength ranges will be created.BLACKSKY—A 3-band mosaic dataset using the BlackSky wavelength ranges will be createdDMCII_3BANDS—A 3-band mosaic dataset using the DMCii wavelength ranges will be created.DEIMOS2_4BANDS—A 4-band mosaic dataset using the Deimos-2 wavelength ranges will be created.DUBAISAT-2_4BANDS—A 4-band mosaic dataset using the DubaiSat-2 wavelength ranges will be created.FORMOSAT-2_4BANDS—A 4-band mosaic dataset using the FORMOSAT-2 wavelength ranges will be created.GEOEYE-1_4BANDS—A 4-band mosaic dataset using the GeoEye-1 wavelength ranges will be created.GF-1 PMS_4BANDS—A 4-band mosaic dataset using the Gaofen-1 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-1 WFV_4BANDS—A 4-band mosaic dataset using the Gaofen-1 Wide Field of View Sensor wavelength ranges will be created.GF-2 PMS_4BANDS—A 4-band mosaic dataset using the Gaofen-2 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-4 PMI_4BANDS—A 4-band mosaic dataset using the Gaofen-4 panchromatic and multispectral wavelength ranges will be created.HJ 1A/1B CCD_4BANDS—A 4-band mosaic dataset using the Huan Jing-1 CCD Multispectral or Hyperspectral Sensor wavelength ranges will be created.IKONOS_4BANDS—A 4-band mosaic dataset using the IKONOS wavelength ranges will be created.JILIN-1_3BANDS—A 3-band mosaic dataset using the Jilin-1 wavelength ranges will be created.KOMPSAT-2_4BANDS—A 4-band mosaic dataset using the KOMPSAT-2 wavelength ranges will be created.KOMPSAT-3_4BANDS—A 4-band mosaic dataset using the KOMPSAT-3 wavelength ranges will be created.LANDSAT_6BANDS—A 6-band mosaic dataset using the Landsat 5 and 7 wavelength ranges from the TM and ETM+ sensors will be created.LANDSAT_8BANDS—An 8-band mosaic dataset using the Landsat 8 wavelength ranges will be created.LANDSAT_9BANDS—An 8-band mosaic dataset using the Landsat 9 wavelength ranges will be created.LANDSAT_MSS_4BANDS—A 4-band mosaic dataset using the Landsat wavelength ranges from the MSS sensor will be created.PLANETSCOPE—A 5-band mosaic dataset using the PlanetScope wavelength ranges will be created.PLEIADES-1_4BANDS—A 4-band mosaic dataset using the Pleiades 1 wavelength ranges will be created.PLEIADES_NEO_6BANDS—A 6-band mosaic dataset using the Pleiades Neo wavelength ranges will be created.QUICKBIRD_4BANDS—A 4-band mosaic dataset using the QuickBird wavelength ranges will be created.RAPIDEYE_5BANDS—A 5-band mosaic dataset using the RapidEye wavelength ranges will be created.SENTINEL2_13BANDS—A 13-band mosaic dataset using the Sentinel 2 MSI wavelength ranges will be created.SKYSAT_4BANDS—A 4-band mosaic dataset using the SkySat-C MSI wavelength ranges will be created.SPOT-5_4BANDS—A 4-band mosaic dataset using the SPOT-5 wavelength ranges will be created.SPOT-6_4BANDS—A 4-band mosaic dataset using the SPOT-6 wavelength ranges will be created.SPOT-7_4BANDS—A 4-band mosaic dataset using the SPOT-7 wavelength ranges will be created.SUPERVIEW-1_4BANDS—A 4-band mosaic dataset using the SuperView-1 wavelength ranges will be created.TH-01_4BANDS—A 4-band mosaic dataset using the Tian Hui-1 wavelength ranges will be created.WORLDVIEW-2_8BANDS—An 8-band mosaic dataset using the WorldView-2 wavelength ranges will be created.WORLDVIEW-3_8BANDS—An 8-band mosaic dataset using the WorldView-3 wavelength ranges will be created.WORLDVIEW-4_4BANDS—A 4-band mosaic dataset using the WorldView-4 wavelength ranges will be created.VISION-1_4BANDS—A 4-band mosaic dataset using the Vision-1 wavelength ranges will be created.ZY1-02C PMS_3BANDS—A 3-band mosaic dataset using the ZiYuan-1 panchromatic/multispectral wavelength ranges will be created.ZY3-CRESDA_4BANDS—A 4-band mosaic dataset using the ZiYuan-3 CRESDA wavelength ranges will be created.ZY3-SASMAC_4BANDS—A 4-band mosaic dataset using the ZiYuan-3 SASMAC wavelength ranges will be created.CUSTOM—The number of bands and the average wavelength for each band are defined using the Product Band Definitions parameter (product_band_definitions in Python). | String |
| product_band_definitions[Band Name {Wavelength Minimum} {Wavelength Maximum},...](Optional) | The definitions of the bands. Edit product_definition when using the CUSTOM keyword by adjusting the wavelength ranges, changing the band order, and adding new bands. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.CreateMosaicDataset(in_workspace, in_mosaicdataset_name, coordinate_system, {num_bands}, {pixel_type}, {product_definition}, {product_band_definitions})
```

### Example 2

```python
import arcpy
arcpy.CreateMosaicDataset_management(
     "C:/workspace/CreateMD.gdb","mosaicds", 
     "C:/workspace/World_Mercator.prj", "3", 
     "8_BIT_UNSIGNED", "False Color Infrared")
```

### Example 3

```python
import arcpy
arcpy.CreateMosaicDataset_management(
     "C:/workspace/CreateMD.gdb","mosaicds", 
     "C:/workspace/World_Mercator.prj", "3", 
     "8_BIT_UNSIGNED", "False Color Infrared")
```

### Example 4

```python
#Create 3-Band FGDB Mosaic Dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"

gdbname = "CreateMD.gdb"
mdname = "mosaicds"
prjfile = "C:/Workspace/World_Mercator.prj"
noband = "3"
pixtype = "8_BIT_UNSIGNED"
pdef = "NONE"
wavelength = ""

arcpy.CreateMosaicDataset_management(gdbname, mdname, prjfile, noband, 
                                     pixtype, pdef, wavelength)
```

### Example 5

```python
#Create 3-Band FGDB Mosaic Dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"

gdbname = "CreateMD.gdb"
mdname = "mosaicds"
prjfile = "C:/Workspace/World_Mercator.prj"
noband = "3"
pixtype = "8_BIT_UNSIGNED"
pdef = "NONE"
wavelength = ""

arcpy.CreateMosaicDataset_management(gdbname, mdname, prjfile, noband, 
                                     pixtype, pdef, wavelength)
```

---

## Create Ortho Corrected Raster Dataset (Data Management)

## Summary

Creates an orthocorrected raster dataset using a digital elevation model (DEM) and control data to accurately align imagery.

## Usage

- For a more accurate result, use the DEM option for elevation. Use a DEM in the orthocorrection process to correct geometric errors caused by relief displacement.
- Using a constant elevation value for the Orthorectification Type parameter will not yield accurate results and should only be used when no DEM is available and approximate spatial accuracy is acceptable.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- If using satellite data, RPCs require a DEM referenced to ellipsoidal heights, but most elevation data (such as USGS NED and ArcGIS Online World Elevation) are referenced to sea level orthometric heights. Check the Geoid parameter (Geoid = "GEOID" in Python) to orthorectify with RPCs unless the DEM is referenced to an ellipsoidal height.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset or mosaic dataset that will be orthorectified. | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer |
| Output Raster Dataset | The name, location, and format of the dataset that will be created.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments. | Raster Dataset |
| Orthorectification Type | Specifies whether the orthorectification type will be a DEM or a specified value that represents the average elevation across the image. Constant elevation—A specified elevation value will be used.DEM—A specified digital elevation model raster will be used. | String |
| Constant Elevation (Meters) | The constant elevation value that will be used when the Orthorectification Type parameter is Constant elevation.If a DEM is used in the orthocorrection process, this parameter value is not used. | Double |
| DEM Raster(Optional) | The DEM raster that will be used for orthorectification when the Orthorectification Type parameter is DEM | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; Image Service |
| Z Factor(Optional) | The scaling factor that will be used to convert the elevation values in the DEM.If the vertical units are meters, set the parameter to 1. If the vertical units are feet, set the parameter to 0.3048. If any other vertical units are used, use this parameter to scale the units to meters. | Double |
| Z Offset(Optional) | The base value that will be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level. | Double |
| Geoid(Optional) | Specifies whether the geoid correction required by RPCs that reference ellipsoidal heights will be made. Most elevation datasets are referenced to sea level orthometric heights, so this correction is required in these cases to convert to ellipsoidal heights. Unchecked—No geoid correction will be made. Use this option only if the DEM is already expressed in ellipsoidal heights. Checked—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |
| in_raster | The raster dataset or mosaic dataset that will be orthorectified. | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer |
| out_raster_dataset | The name, location, and format of the dataset that will be created.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments. | Raster Dataset |
| Ortho_type | Specifies whether the orthorectification type will be a DEM or a specified value that represents the average elevation across the image. CONSTANT_ELEVATION—A specified elevation value will be used.DEM—A specified digital elevation model raster will be used. | String |
| constant_elevation | The constant elevation value that will be used when the Ortho_type parameter is CONSTANT_ELEVATION.If a DEM is used in the orthocorrection process, this parameter value is not used. | Double |
| in_DEM_raster(Optional) | The DEM raster that will be used for orthorectification when the Ortho_type parameter is DEM. | Raster Dataset; Mosaic Dataset; Mosaic Layer; Raster Layer; Image Service |
| ZFactor(Optional) | The scaling factor that will be used to convert the elevation values in the DEM.If the vertical units are meters, set the parameter to 1. If the vertical units are feet, set the parameter to 0.3048. If any other vertical units are used, use this parameter to scale the units to meters. | Double |
| ZOffset(Optional) | The base value that will be added to the elevation value in the DEM. This can be used to offset elevation values that do not start at sea level. | Double |
| Geoid(Optional) | Specifies whether the geoid correction required by RPCs that reference ellipsoidal heights will be made. Most elevation datasets are referenced to sea level orthometric heights, so this correction is required in these cases to convert to ellipsoidal heights. NONE—No geoid correction will be made. Use NONE only if the DEM is already expressed in ellipsoidal heights. GEOID—A geoid correction will be made to convert orthometric heights to ellipsoidal heights (based on EGM96 geoid). | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateOrthoCorrectedRasterDataset(in_raster, out_raster_dataset, Ortho_type, constant_elevation, {in_DEM_raster}, {ZFactor}, {ZOffset}, {Geoid})
```

### Example 2

```python
import arcpy
arcpy.CreateOrthoCorrectedRasterDataset_management("c:/data/RPCdata.tif",
                                                   "c:/data/orthoready.tif",
                                                   "DEM", "#", "c:/data/DEM.img",
                                                   "#", "10", "GEOID")
```

### Example 3

```python
import arcpy
arcpy.CreateOrthoCorrectedRasterDataset_management("c:/data/RPCdata.tif",
                                                   "c:/data/orthoready.tif",
                                                   "DEM", "#", "c:/data/DEM.img",
                                                   "#", "10", "GEOID")
```

### Example 4

```python
##====================================
##Create Ortho Corrected Raster Dataset
##Usage: CreateOrthoCorrectedRasterDataset_management in_raster out_raster_dataset
##                                                    CONSTANT_ELEVATION | DEM constant_ elevation
##                                                    in_DEM_raster {ZFactor} {ZOffset} {NONE | GEOID}

import arcpy
arcpy.env.workspace = "C:/Workspace"

##Ortho correct with Constant elevation
arcpy.CreateOrthoCorrectedRasterDataset_management("ortho.img", "orthoready.tif",\
                                                   "CONSTANT_ELEVATION", "30", "#",\
                                                   "#", "#", "#")

##Ortho correct with DEM image and Z factors
arcpy.CreateOrthoCorrectedRasterDataset_management("ortho.img", "orthoready_dem.tif",\
                                                   "DEM", "#", "dem.img", "#", "10", "GEOID")
```

### Example 5

```python
##====================================
##Create Ortho Corrected Raster Dataset
##Usage: CreateOrthoCorrectedRasterDataset_management in_raster out_raster_dataset
##                                                    CONSTANT_ELEVATION | DEM constant_ elevation
##                                                    in_DEM_raster {ZFactor} {ZOffset} {NONE | GEOID}

import arcpy
arcpy.env.workspace = "C:/Workspace"

##Ortho correct with Constant elevation
arcpy.CreateOrthoCorrectedRasterDataset_management("ortho.img", "orthoready.tif",\
                                                   "CONSTANT_ELEVATION", "30", "#",\
                                                   "#", "#", "#")

##Ortho correct with DEM image and Z factors
arcpy.CreateOrthoCorrectedRasterDataset_management("ortho.img", "orthoready_dem.tif",\
                                                   "DEM", "#", "dem.img", "#", "10", "GEOID")
```

---

## Create Pansharpened Raster Dataset (Data Management)

## Summary

Combines a high-resolution panchromatic raster dataset with a lower-resolution multiband raster dataset to create a high-resolution multiband raster dataset for visual analysis.

## Usage

- Only the areas that fully overlap will be affected by this tool.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format or to any geodatabase raster dataset.
- The four weight values assigned to the blue, green, red, and infrared bands allow you to adjust the pan-sharpening algorithms.
- Pan sharpening performed on a three-band raster dataset will produce a raster dataset that has three bands. Pan sharpening performed on a four-band raster dataset will produce a raster dataset that has four bands.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset that will be pan sharpened. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| Red Channel | The input raster band that will display with the red color channel. | Long |
| Green Channel | The input raster band that will display with the green color channel. | Long |
| Blue Channel | The input raster band that will display with the blue color channel. | Long |
| Infrared Channel(Optional) | The input raster band that will display with the infrared color channel. | Long |
| Output Raster Dataset | The name, location, and format of the raster dataset that will be created.When storing the raster dataset in a file format, specify the file extension as follows:When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing the raster dataset in a file format, you must specify the file extension: .bil for Esri BIL.bip for Esri BIP.bmp for BMP.bsq for Esri BSQ.dat for ENVI DAT.gif for GIF.img for ERDAS IMAGINE.jpg for JPEG.jp2 for JPEG 2000.png for PNG.tif for TIFF.mrf for MRF.crf for CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| Panchromatic Image | The higher-resolution panchromatic image. | Raster Layer |
| Pan-sharpening Type | Specifies the algorithm that will be used to combine the panchromatic and multispectral bands.IHS—Intensity, Hue, and Saturation color space will be used.Brovey—The Brovey algorithm based on spectral modeling will be used.Esri—The Esri algorithm based on spectral modeling will be used.Simple mean—The averaged value between the red, green, and blue values and the panchromatic pixel value will be used.Gram-Schmidt—The Gram-Schmidt spectral-sharpening algorithm to sharpen multispectral data will be used. | String |
| Red Weight(Optional) | A value from 0 to 1 that will be used to weight the red band. | Double |
| Green Weight(Optional) | A value from 0 to 1 that will be used to weight the green band. | Double |
| Blue Weight(Optional) | A value from 0 to 1 that will be used to weight the blue band. | Double |
| Infrared Weight(Optional) | A value from 0 to 1 that will be used to weight the infrared band. | Double |
| Sensor (Optional) | Specifies the sensor of the multiband raster input.You can specify the sensor when the Pan-sharpening Type parameter is set to Gram-Schmidt. Specifying the sensor will set appropriate band weights.Unknown—The sensor is unknown or unlisted.BlackSky—The sensor is a BlackSky satellite sensor.DubaiSat-2—The sensor is a DubaiSat-2 satellite sensor.GeoEye-1—The sensor is a GeoEye-1 and OrbView-3 satellite sensor.GF-1 PMS—The sensor is a Gao Fen satellite 1, Panchromatic and Multispectral CCD Camera sensor.GF-2 PMS—The sensor is a Gao Fen 2 satellite, Panchromatic and Multispectral CCD Camera sensor.IKONOS—The sensor is an IKONOS satellite sensor.Jilin-1—The sensor is a Jilin-1 satellite sensor.KOMPSAT-2—The sensor is a KOMPSAT-2 satellite sensor.KOMPSAT-3—The sensor is a KOMPSAT-3 satellite sensor.Landsat 1-5 MSS—The sensor is a Landsat MSS satellite sensor.Landsat 7 ETM+—The sensor is a Landsat 7 satellite sensor.Landsat 8—The sensor is a Landsat 8 satellite sensor.Landsat 9—The sensor is a Landsat 9 satellite sensor.Pléiades-1—The sensor is a Pléiades satellite sensor.Pléiades Neo—The sensor is a Pléiades Neo satellite sensor.Quickbird—The sensor is a QuickBird satellite sensor.SkySat-C—The sensor is a SkySat-C satellite sensor.SPOT 5—The sensor is a SPOT 5 satellite sensor.SPOT 6—The sensor is a SPOT 6 satellite sensor.SPOT 7—The sensor is a SPOT 7 satellite sensor.SuperView-1—The sensor is a SuperView-1 satellite sensor.Tian Hui 1—The sensor is a Tian Hui 1 satellite sensor.Ultracam—The sensor is an UltraCam aerial sensor.Vision-1—The sensor is a Vision-1 satellite sensor.WorldView-2—The sensor is a WorldView-2 satellite sensor.WorldView-3—The sensor is a WorldView-3 satellite sensor.WorldView-4—The sensor is a WorldView-4 satellite sensor.ZY-1 PMS—The sensor is a Ziyuan High Panchromatic Multispectral Sensor.ZY-3 CRESDA—The sensor is a Ziyuan CRESDA satellite sensor.ZY-3 SASMAC—The sensor is a Ziyuan SASMAC satellite sensor. | String |
| in_raster | The raster dataset that will be pan sharpened. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| red_channel | The input raster band that will display with the red color channel. | Long |
| green_channel | The input raster band that will display with the green color channel. | Long |
| blue_channel | The input raster band that will display with the blue color channel. | Long |
| infrared_channel(Optional) | The input raster band that will display with the infrared color channel. | Long |
| out_raster_dataset | The name, location, and format of the raster dataset that will be created.When storing the raster dataset in a file format, specify the file extension as follows:When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing the raster dataset in a file format, you must specify the file extension: .bil for Esri BIL.bip for Esri BIP.bmp for BMP.bsq for Esri BSQ.dat for ENVI DAT.gif for GIF.img for ERDAS IMAGINE.jpg for JPEG.jp2 for JPEG 2000.png for PNG.tif for TIFF.mrf for MRF.crf for CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| in_panchromatic_image | The higher-resolution panchromatic image. | Raster Layer |
| pansharpening_type | Specifies the algorithm that will be used to combine the panchromatic and multispectral bands.IHS—Intensity, Hue, and Saturation color space will be used.BROVEY—The Brovey algorithm based on spectral modeling will be used.Esri—The Esri algorithm based on spectral modeling will be used.SIMPLE_MEAN—The averaged value between the red, green, and blue values and the panchromatic pixel value will be used.Gram-Schmidt—The Gram-Schmidt spectral-sharpening algorithm to sharpen multispectral data will be used. | String |
| red_weight(Optional) | A value from 0 to 1 that will be used to weight the red band. | Double |
| green_weight(Optional) | A value from 0 to 1 that will be used to weight the green band. | Double |
| blue_weight(Optional) | A value from 0 to 1 that will be used to weight the blue band. | Double |
| infrared_weight(Optional) | A value from 0 to 1 that will be used to weight the infrared band. | Double |
| sensor(Optional) | Specifies the sensor of the multiband raster input.You can specify the sensor when the pansharpening_type parameter is set to Gram-Schmidt. Specifying the sensor will set appropriate band weights.UNKNOWN—The sensor is unknown or unlisted.BlackSky—The sensor is a BlackSky satellite sensor.DubaiSat-2—The sensor is a DubaiSat-2 satellite sensor.GeoEye-1—The sensor is a GeoEye-1 and OrbView-3 satellite sensor.GF-1 PMS—The sensor is a Gao Fen satellite 1, Panchromatic and Multispectral CCD Camera sensor.GF-2 PMS—The sensor is a Gao Fen 2 satellite, Panchromatic and Multispectral CCD Camera sensor.IKONOS—The sensor is an IKONOS satellite sensor.Jilin-1—The sensor is a Jilin-1 satellite sensor.KOMPSAT-2—The sensor is a KOMPSAT-2 satellite sensor.KOMPSAT-3—The sensor is a KOMPSAT-3 satellite sensor.Landsat 1-5 MSS—The sensor is a Landsat MSS satellite sensor.Landsat 7 ETM+—The sensor is a Landsat 7 satellite sensor.Landsat 8—The sensor is a Landsat 8 satellite sensor.Landsat 9—The sensor is a Landsat 9 satellite sensor.Pleiades-1—The sensor is a Pléiades satellite sensor.Pleiades Neo—The sensor is a Pléiades Neo satellite sensor.QuickBird—The sensor is a QuickBird satellite sensor.SkySat—The sensor is a SkySat-C satellite sensor.SPOT 5—The sensor is a SPOT 5 satellite sensor.SPOT 6—The sensor is a SPOT 6 satellite sensor.SPOT 7—The sensor is a SPOT 7 satellite sensor.SuperView-1—The sensor is a SuperView-1 satellite sensor.TH-01—The sensor is a Tian Hui 1 satellite sensor.UltraCam—The sensor is an UltraCam aerial sensor.Vision-1—The sensor is a Vision-1 satellite sensor.WorldView-2—The sensor is a WorldView-2 satellite sensor.WorldView-3—The sensor is a WorldView-3 satellite sensor.WorldView-4—The sensor is a WorldView-4 satellite sensor.ZY1-02C PMS—The sensor is a Ziyuan High Panchromatic Multispectral Sensor.ZY3-CRESDA—The sensor is a Ziyuan CRESDA satellite sensor.ZY3-SASMAC—The sensor is a Ziyuan SASMAC satellite sensor. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreatePansharpenedRasterDataset(in_raster, red_channel, green_channel, blue_channel, {infrared_channel}, out_raster_dataset, in_panchromatic_image, pansharpening_type, {red_weight}, {green_weight}, {blue_weight}, {infrared_weight}, {sensor})
```

### Example 2

```python
import arcpy
arcpy.CreatePansharpenedRasterDataset_management(
     "c:/data/rgbn.tif","3","2","1","4", "c:/data/outpan.tif",
     "c:/data/in_pan.img","Gram-Schmidt","","","","","QuickBird")
```

### Example 3

```python
import arcpy
arcpy.CreatePansharpenedRasterDataset_management(
     "c:/data/rgbn.tif","3","2","1","4", "c:/data/outpan.tif",
     "c:/data/in_pan.img","Gram-Schmidt","","","","","QuickBird")
```

### Example 4

```python
#3 Band RGB Pansharpen with Brovey algorithm

import arcpy
arcpy.env.workspace = "C:/workspace"
    
arcpy.CreatePansharpenedRasterDataset_management(
     "rgb.img","3","2","1","1", "output\\rgb_pan.img","pan.img","Brovey")
```

### Example 5

```python
#3 Band RGB Pansharpen with Brovey algorithm

import arcpy
arcpy.env.workspace = "C:/workspace"
    
arcpy.CreatePansharpenedRasterDataset_management(
     "rgb.img","3","2","1","1", "output\\rgb_pan.img","pan.img","Brovey")
```

---

## Create Point Cloud Scene Layer Content (Data Management)

## Summary

Creates a point cloud scene layer package (.slpk) or scene layer content (.i3sREST) in the cloud from LAS, zLAS, LAZ, or LAS dataset input.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The lidar data (LAS, zLAS, LAZ, or LAS dataset) that will be used to create a scene layer package. The lidar data can also be specified by selecting the parent folder that contains the files. | Layer File; LAS Dataset Layer; Folder; File |
| Output Scene Layer Package(Optional) | The output scene layer package (.slpk). | File |
| Output Coordinate System(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| Geographic Transformation (Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS_1984_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| Attributes to cache(Optional) | Specifies the source data attributes that will be included in the scene layer package. These values will be accessible when the content is consumed in other viewers. Select attributes that are required for the desired rendering and filtering options (for example, intensity, returns, class codes, and RGB). To reduce storage, exclude unneeded attributes.Intensity— The return strength of the laser pulse for each lidar point will be included.RGB—RGB imagery information collected for each lidar point will be included.LAS flags—Classification and scan direction flags will be included.Classification code—Classification code values will be included.Return value—Discrete return numbers from the lidar pulse will be includedUser data—A customizable attribute that can be any number in the range of 0 through 255 will be included.Point source ID—For aerial lidar, this value typically identifies the flight path that collected a given lidar point, which will be included.GPS time— The GPS time stamp at which the laser point was emitted from the aircraft will be included. The time is in GPS seconds of the week in which the time stamp is between 0 and 604800 and resets at midnight on a Sunday.Scan angle—The angular direction of the laser scanner for a given lidar point will be included. The value range is -90 through 90.Near infrared—Near infrared records collected for each lidar point will be included. | String |
| Point Size (m)(Optional) | The point size of the lidar data. For airborne lidar data, the default of 0 or a value close to the average point spacing is usually best. For terrestrial lidar data, the point size should match the desired point spacing for the areas of interest. Values are expressed in meters. The default of 0 will automatically determine the best value for the input dataset. | Double |
| XY Max Error (m)(Optional) | The maximum x,y error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. Values are expressed in meters. The default is 0.001. | Double |
| Z Max Error (m)(Optional) | The maximum z-error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. Values are expressed in meters. The default is 0.001. | Double |
| Input Coordinate System(Optional) | The coordinate system of the input .laz files. This parameter is only used for .laz files that do not contain spatial reference information in their header or have a .prj file in the same location. | Coordinate System |
| Scene Layer Version(Optional) | The Indexed 3D Scene Layer (I3S) version of the resulting point cloud scene layer package. Specifying a version supports backward compatibility and allows scene layer packages to be shared with earlier versions of ArcGIS. 1.x—The point cloud scene layer package will be supported in all ArcGIS clients.2.x—The point cloud scene layer package will be supported in ArcGIS Pro 2.1.2 or later and can be published to ArcGIS Online and ArcGIS 10.6.1 or later. This is the default. | String |
| Target Cloud Connection (Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| Output Name (Optional) | The output name of the scene layer content when output to a cloud store. This parameter is only available when a Target Cloud Connection parameter value is specified. | String |
| in_dataset | The lidar data (LAS, zLAS, LAZ, or LAS dataset) that will be used to create a scene layer package. The lidar data can also be specified by selecting the parent folder that contains the files. | Layer File; LAS Dataset Layer; Folder; File |
| out_slpk(Optional) | The output scene layer package (.slpk). | File |
| out_coor_system(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| transform_method[transform_method,...](Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS_1984_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| attributes[attributes,...](Optional) | Specifies the source data attributes that will be included in the scene layer package. These values will be accessible when the content is consumed in other viewers. Select attributes that are required for the desired rendering and filtering options (for example, intensity, returns, class codes, and RGB). To reduce storage, exclude unneeded attributes.INTENSITY— The return strength of the laser pulse for each lidar point will be included.RGB—RGB imagery information collected for each lidar point will be included.FLAGS—Classification and scan direction flags will be included.CLASS_CODE—Classification code values will be included.RETURNS—Discrete return numbers from the lidar pulse will be includedUSER_DATA—A customizable attribute that can be any number in the range of 0 through 255 will be included.POINT_SRC_ID—For aerial lidar, this value typically identifies the flight path that collected a given lidar point, which will be included.GPS_TIME— The GPS time stamp at which the laser point was emitted from the aircraft will be included. The time is in GPS seconds of the week in which the time stamp is between 0 and 604800 and resets at midnight on a Sunday.SCAN_ANGLE—The angular direction of the laser scanner for a given lidar point will be included. The value range is -90 through 90.NEAR_INFRARED—Near infrared records collected for each lidar point will be included. | String |
| point_size_m(Optional) | The point size of the lidar data. For airborne lidar data, the default of 0 or a value close to the average point spacing is usually best. For terrestrial lidar data, the point size should match the desired point spacing for the areas of interest. Values are expressed in meters. The default of 0 will automatically determine the best value for the input dataset. | Double |
| xy_max_error_m(Optional) | The maximum x,y error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. Values are expressed in meters. The default is 0.001. | Double |
| z_max_error_m(Optional) | The maximum z-error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. Values are expressed in meters. The default is 0.001. | Double |
| in_coor_system(Optional) | The coordinate system of the input .laz files. This parameter is only used for .laz files that do not contain spatial reference information in their header or have a .prj file in the same location. | Coordinate System |
| scene_layer_version(Optional) | The Indexed 3D Scene Layer (I3S) version of the resulting point cloud scene layer package. Specifying a version supports backward compatibility and allows scene layer packages to be shared with earlier versions of ArcGIS. 1.X—The point cloud scene layer package will be supported in all ArcGIS clients.2.X—The point cloud scene layer package will be supported in ArcGIS Pro 2.1.2 or later and can be published to ArcGIS Online and ArcGIS 10.6.1 or later. This is the default. | String |
| target_cloud_connection(Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| out_name(Optional) | The output name of the scene layer content when output to a cloud store. This parameter is only available when a target_cloud_connection parameter value is specified. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreatePointCloudSceneLayerPackage(in_dataset, {out_slpk}, {out_coor_system}, {transform_method}, {attributes}, {point_size_m}, {xy_max_error_m}, {z_max_error_m}, {in_coor_system}, {scene_layer_version}, {target_cloud_connection}, {out_name})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/gis_data"
arcpy.management.CreatePointCloudSceneLayerPackage(
    "Milan.lyrx", "Milan.slpk", arcpy.SpatialReference(4326), 
    ["ITRF_2000_To_WGS_1984 + WGS_1984_To_WGS_1984_EGM2008_2.5x2.5_Height"],
    ["INTENSITY", "RGB", "CLASS_CODE", "FLAGS", "RETURNS"], 0, 0.1, 0.1, None, 
    "1.X", None, "")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/gis_data"
arcpy.management.CreatePointCloudSceneLayerPackage(
    "Milan.lyrx", "Milan.slpk", arcpy.SpatialReference(4326), 
    ["ITRF_2000_To_WGS_1984 + WGS_1984_To_WGS_1984_EGM2008_2.5x2.5_Height"],
    ["INTENSITY", "RGB", "CLASS_CODE", "FLAGS", "RETURNS"], 0, 0.1, 0.1, None, 
    "1.X", None, "")
```

---

## Create Point Scene Layer Content (Data Management)

## Summary

Creates a point scene layer package (.slpk) or scene layer content (.i3sREST) from a point feature layer.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input point feature layer. | Layer File; Feature Layer |
| Output Scene Layer Package | The output scene layer package (.slpk). | File |
| Output Coordinate System(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| Geographic Transformation(Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| Target Cloud Connection (Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| Support Symbol Referencing | Specifies whether Esri symbols will be referenced by the scene layer package or copied to it.Checked—Esri symbols will be referenced by the scene layer package. Using this option will improve the time to create the scene layer package as well as reduce the file size of the package.Unchecked—Esri symbols will be copied to the scene layer package. This is the default. | Boolean |
| in_dataset | The input point feature layer. | Layer File; Feature Layer |
| out_slpk | The output scene layer package (.slpk). | File |
| out_coor_system(Optional) | The coordinate system of the output scene layer package. It can be any projected or custom coordinate system. Supported geographic coordinate systems include WGS84 and China Geodetic Coordinate System 2000. WGS84 and EGM96 Geoid are the default horizontal and vertical coordinate systems, respectively. The coordinate system can be specified in any of the following ways:Specify the path to a .prj file.Reference a dataset with the correct coordinate system.Use an arcpy.SpatialReference object. | Spatial Reference |
| transform_method[transform_method,...](Optional) | The datum transformation method that will be used when the input layer's coordinate system uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS84_3 will work correctly even if the datum conversion is from WGS84 to NAD 1927. Note:The ArcGIS coordinate system data is required for vertical datum transformations between ellipsoidal and gravity-related and two gravity-related datums. | String |
| target_cloud_connection(Optional) | The target cloud connection file (.acs) where the scene layer content (.i3sREST) will be output. | Folder |
| support_symbol_referencing | Specifies whether Esri symbols will be referenced by the scene layer package or copied to it. SUPPORT_REFERENCING_SYMBOLS—Esri symbols will be referenced by the scene layer package. Using this option will improve the time to create the scene layer package as well as reduce the file size of the package.DO_NOT_SUPPORT_REFERENCING_SYMBOLS—Esri symbols will be copied to the scene layer package. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreatePointSceneLayerPackage(in_dataset, out_slpk, {out_coor_system}, {transform_method}, {target_cloud_connection}, support_symbol_referencing)
```

### Example 2

```python
import arcpy
arcpy.management.CreatePointSceneLayerPackage(
    r'c:\temp\points.lyrx', r'c:\temp\output.slpk', arcpy.SpatialReference(4326))
```

### Example 3

```python
import arcpy
arcpy.management.CreatePointSceneLayerPackage(
    r'c:\temp\points.lyrx', r'c:\temp\output.slpk', arcpy.SpatialReference(4326))
```

### Example 4

```python
import arcpy
arcpy.management.CreatePointSceneLayerPackage(
    r'c:\temp\points.lyrx', None, arcpy.SpatialReference(4326), 
    r'c:\cloudConnections\AWS.acs')
```

### Example 5

```python
import arcpy
arcpy.management.CreatePointSceneLayerPackage(
    r'c:\temp\points.lyrx', None, arcpy.SpatialReference(4326), 
    r'c:\cloudConnections\AWS.acs')
```

---

## Create Random Points (Data Management)

## Summary

Creates a specified number of random point features. Random points can be generated in an extent window, inside polygon features, on point features, or along line features.

## Usage

- The area in which random points will be generated can be defined either by constraining polygon, point, or line features or by a constraining extent window.
- The Number of Points parameter can be specified as a number or as a numeric field in the constraining feature class containing values for how many random points to place within each feature. The field option is only valid for polygon or line constraining features. If the number of points is supplied as a number, each feature in the constraining feature class will have that number of random points generated inside or along it.If you are using a constraining feature class that has more than one feature, and you wish to specify the total number of random points to be generated (as opposed to the number of random points to be placed inside each feature), first use the Dissolve tool so that the constraining feature class only contains a single feature; then use that dissolved feature class as the constraining feature class.
- The coordinate system of the output feature class will be the coordinate system of the Constraining feature class if one is specified Map data frame if a constraining extent is specified in the map using a layer in the map or the path to a feature classFeature class if a constraining extent is specified in Python by using the path to a feature classOutput Coordinate System geoprocessing environment if it is set (and overrides all other behaviors detailed above)Unknown if none of the above applies
- Constraining feature class if one is specified
- Map data frame if a constraining extent is specified in the map using a layer in the map or the path to a feature class
- Feature class if a constraining extent is specified in Python by using the path to a feature class
- Output Coordinate System geoprocessing environment if it is set (and overrides all other behaviors detailed above)
- Unknown if none of the above applies
- To assign random values to randomly placed points, first generate random points using this tool. Second, use the Add Field tool to create a new numeric field in the random points feature class. Suggested field types are long integer or float. Third, use the Calculate Field tool to assign random values to the empty field in the random points feature class. To generate a random integer between a and b (inclusively), use the Python expression random.randint(a,b). To generate a random float number between a and b (exclusively), use the Python expression random.uniform(a,b). In the code block, import the random module using the expression import random.
- The Constraining Extent parameter can be entered as a set of minimum and maximum x- and y-coordinates or as equal to the extent of a feature layer or feature class.
- If both a constraining feature class and constraining extent are specified, the constraining feature class value will be used and the constraining extent value will be ignored.
- When unable to place anymore random points within a constraining area without breaking the minimum allowed distance specified, the number of random points in the constraining area will be reduced to the maximum possible under the minimum allowed distance.
- The Minimum Allowed Distance parameter can be specified as a linear unit or a field from the constraining features containing numeric values. This value will determine the minimum allowed distance between random points within each input feature. The field option is only valid for polygon or line constraining features. Random points may be within the minimum allowed distance if they were generated inside or along different constraining feature parts.
- Using point features as the constraining feature class creates a random subset of the constraining point features. No new point locations are generated.
- Noninteger (whole) positive values for the Number of Points and Minimum Allowed Distance parameters will be rounded to the nearest whole number. Nonnumeric and negative values are set to 0.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Location | The location or workspace in which the random points feature class will be created. This location or workspace must already exist. | Feature Dataset;Workspace |
| Output Point Feature Class | The name of the random points feature class to be created. | String |
| Constraining Feature Class(Optional) | Random points will be generated inside or along the features in this feature class. The constraining feature class can be point, multipoint, line, or polygon. Points will be randomly placed inside polygon features, along line features, or at point feature locations. Each feature in this feature class will have the specified number of points generated inside it (for example, if you specify 100 points, and the constraining feature class has 5 features, 100 random points will be generated in each feature, totaling 500 points). | Feature Layer |
| Constraining Extent(Optional) | Random points will be generated inside the extent. The constraining extent will only be used if no constraining feature class is specified.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent; Feature Layer; Raster Layer |
| Number of Points [value or field](Optional) | The number of points to be randomly generated.The number of points can be specified as a long integer number or as a field from the constraining features containing numeric values for how many random points to place within each feature. The field option is only valid for polygon or line constraining features. If the number of points is supplied as a long integer number, each feature in the constraining feature class will have that number of random points generated inside or along it. | Field; Long |
| Minimum Allowed Distance [value or field](Optional) | The shortest distance allowed between any two randomly placed points. If a value of 1 Meter is specified, all random points will be farther than 1 meter away from the closest point. | Field; Linear Unit |
| Create Multipoint Output(Optional) | Determines if the output feature class will be a multipart or single-part feature.Unchecked—The output will be geometry type point (each point is a separate feature). This is the default.Checked—The output will be geometry type multipoint (all points are a single feature). | Boolean |
| Maximum Number of Points per Multipoint(Optional) | If Create Multipoint Output is checked, specify the number of random points to be placed in each multipoint geometry. | Long |
| out_path | The location or workspace in which the random points feature class will be created. This location or workspace must already exist. | Feature Dataset;Workspace |
| out_name | The name of the random points feature class to be created. | String |
| constraining_feature_class(Optional) | Random points will be generated inside or along the features in this feature class. The constraining feature class can be point, multipoint, line, or polygon. Points will be randomly placed inside polygon features, along line features, or at point feature locations. Each feature in this feature class will have the specified number of points generated inside it (for example, if you specify 100 points, and the constraining feature class has 5 features, 100 random points will be generated in each feature, totaling 500 points). | Feature Layer |
| constraining_extent(Optional) | Random points will be generated inside the extent. The constraining extent will only be used if no constraining feature class is specified. MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent; Feature Layer; Raster Layer |
| number_of_points_or_field(Optional) | The number of points to be randomly generated.The number of points can be specified as a long integer number or as a field from the constraining features containing numeric values for how many random points to place within each feature. The field option is only valid for polygon or line constraining features. If the number of points is supplied as a long integer number, each feature in the constraining feature class will have that number of random points generated inside or along it. | Field; Long |
| minimum_allowed_distance(Optional) | The shortest distance allowed between any two randomly placed points. If a value of 1 Meter is specified, all random points will be farther than 1 meter away from the closest point. | Field; Linear Unit |
| create_multipoint_output(Optional) | Determines if the output feature class will be a multipart or single-part feature.POINT—The output will be geometry type point (each point is a separate feature). This is the default.MULTIPOINT—The output will be geometry type multipoint (all points are a single feature). | Boolean |
| multipoint_size(Optional) | If create_multipoint_output is set to MULTIPOINT, specify the number of random points to be placed in each multipoint geometry. The default is 10. | Long |

## Code Samples

### Example 1

```python
arcpy.management.CreateRandomPoints(out_path, out_name, {constraining_feature_class}, {constraining_extent}, {number_of_points_or_field}, {minimum_allowed_distance}, {create_multipoint_output}, {multipoint_size})
```

### Example 2

```python
import arcpy
arcpy.management.CreateRandomPoints("c:/data/project", "samplepoints", 
                                    "c:/data/studyarea.shp", "", 500, "", 
                                    "POINT")
```

### Example 3

```python
import arcpy
arcpy.management.CreateRandomPoints("c:/data/project", "samplepoints", 
                                    "c:/data/studyarea.shp", "", 500, "", 
                                    "POINT")
```

### Example 4

```python
# Name: RandomPointsRandomValues.py
# Purpose: create random points with random values

# Import system modules
import arcpy

# Create random points in the features of a constraining feature class
# Number of points for each feature determined by the value in the field 
# specified
outGDB = "C:/data/county.gdb"
outName = "randpeople"
conFC = "C:/data/county.gdb/blocks"
numField = "POP2000"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numField)

# set workspace
arcpy.env.workspace = "C:/data/county.gdb"

# Create fields for random values
fieldInt = "fieldInt"
fieldFlt = "fieldFlt"
arcpy.management.AddField(outName, fieldInt, "LONG")  # add long integer field
arcpy.AddField_management(outName, fieldFlt, "FLOAT") # add float field

# Calculate random values between 1-100 in the new fields
arcpy.management.CalculateField(outName, fieldInt, "random.randint(1,100)", 
                                "PYTHON", "import random")
arcpy.management.CalculateField(outName, fieldFlt, "random.uniform(1,100)", 
                                "PYTHON", "import random")
```

### Example 5

```python
# Name: RandomPointsRandomValues.py
# Purpose: create random points with random values

# Import system modules
import arcpy

# Create random points in the features of a constraining feature class
# Number of points for each feature determined by the value in the field 
# specified
outGDB = "C:/data/county.gdb"
outName = "randpeople"
conFC = "C:/data/county.gdb/blocks"
numField = "POP2000"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numField)

# set workspace
arcpy.env.workspace = "C:/data/county.gdb"

# Create fields for random values
fieldInt = "fieldInt"
fieldFlt = "fieldFlt"
arcpy.management.AddField(outName, fieldInt, "LONG")  # add long integer field
arcpy.AddField_management(outName, fieldFlt, "FLOAT") # add float field

# Calculate random values between 1-100 in the new fields
arcpy.management.CalculateField(outName, fieldInt, "random.randint(1,100)", 
                                "PYTHON", "import random")
arcpy.management.CalculateField(outName, fieldFlt, "random.uniform(1,100)", 
                                "PYTHON", "import random")
```

### Example 6

```python
# Name: RandomPoints.py
# Purpose: create several types of random points feature classes

# Import system modules
import arcpy

# set environment settings
arcpy.env.overwriteOutput = True

# Create random points in an extent defined simply by numbers
outFolder = "C:/data"
numExtent = "0 0 1000 1000"
numPoints = 100
outName = "myRandPnts.shp"
arcpy.env.outputCoordinateSystem = "Coordinate Systems/Projected Coordinate Systems/World/Miller Cylindrical (world).prj"
arcpy.management.CreateRandomPoints(outFolder, outName, "", numExtent, numPoints)
arcpy.env.outputCoordinateSystem = ""
 
# Create random points in an extent defined by another feature class
outName = "testpoints.shp"
fcExtent = "C:/data/studyarea.shp"
arcpy.management.CreateRandomPoints(outFolder, outName, "", fcExtent, numPoints)
 
# Create random points in the features of a constraining feature class
# Number of points for each feature determined by the value in the field specified
outGDB = "C:/data/county.gdb"
outName = "randpeople"
conFC = "C:/data/county.gdb/blocks"
numField = "POP2000"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numField)

# Create random points in the features of a constraining 
# Feature class with a minimum allowed distance
outName = "constparcelpnts"
conFC = "C:/data/county.gdb/parcels"
numPoints = 10
minDistance = "5 Feet"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numPoints, 
                                    minDistance) 

# Create random points with a multipoint output
outName = "randomMPs"
fcExtent = "C:/data/county.gdb/county"
numPoints = 100
numMP = 10
arcpy.management.CreateRandomPoints(outGDB, outName, "", fcExtent, numPoints, 
                                    "", "MULTIPOINT", numMP)
```

### Example 7

```python
# Name: RandomPoints.py
# Purpose: create several types of random points feature classes

# Import system modules
import arcpy

# set environment settings
arcpy.env.overwriteOutput = True

# Create random points in an extent defined simply by numbers
outFolder = "C:/data"
numExtent = "0 0 1000 1000"
numPoints = 100
outName = "myRandPnts.shp"
arcpy.env.outputCoordinateSystem = "Coordinate Systems/Projected Coordinate Systems/World/Miller Cylindrical (world).prj"
arcpy.management.CreateRandomPoints(outFolder, outName, "", numExtent, numPoints)
arcpy.env.outputCoordinateSystem = ""
 
# Create random points in an extent defined by another feature class
outName = "testpoints.shp"
fcExtent = "C:/data/studyarea.shp"
arcpy.management.CreateRandomPoints(outFolder, outName, "", fcExtent, numPoints)
 
# Create random points in the features of a constraining feature class
# Number of points for each feature determined by the value in the field specified
outGDB = "C:/data/county.gdb"
outName = "randpeople"
conFC = "C:/data/county.gdb/blocks"
numField = "POP2000"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numField)

# Create random points in the features of a constraining 
# Feature class with a minimum allowed distance
outName = "constparcelpnts"
conFC = "C:/data/county.gdb/parcels"
numPoints = 10
minDistance = "5 Feet"
arcpy.management.CreateRandomPoints(outGDB, outName, conFC, "", numPoints, 
                                    minDistance) 

# Create random points with a multipoint output
outName = "randomMPs"
fcExtent = "C:/data/county.gdb/county"
numPoints = 100
numMP = 10
arcpy.management.CreateRandomPoints(outGDB, outName, "", fcExtent, numPoints, 
                                    "", "MULTIPOINT", numMP)
```

---

## Create Random Raster (Data Management)

## Summary

Creates a raster dataset of random values with a distribution you define.

## Usage

- You can save your output to Esri Grid, CRF, IMG, TIFF, or any geodatabase raster dataset.
- The values assigned to each cell in the output raster are derived from the random number generator and the selected distribution type. There are several random number generators available. Review the Random Number Generator environment to determine the one to use. The random number generator starts a stream of random numbers based on the generator type and a seed value. These numbers are randomly determined and the values fall between 0 and 1. Each value is independent of the other values.Multiple distribution types are available for the random number generators when assigning (or transforming) the values in the output raster. The distributions generally produce different results, and the distribution to use is determined by the end use of the raster. If the random raster is to model a natural phenomenon, the distribution selected should be the best representation of the process of the phenomenon.For a description of the distributions and how they are typically used, see Distributions for assigning random values.
- The Uniform, Integer, Normal, and Exponential distributions' processing times are independent of their arguments, while the Poisson, Gamma, Binomial, Geometric, and Pascal distributions' processing times can vary considerably when arguments are changed.
- A default value is calculated for the cell size parameter if no value is provided. This value is based on the size of the extent.
- On the tool dialog box, the values of the Output extent parameter are in the coordinate system of the map. During tool execution, the extent is projected to the Output Coordinate System if it is specified in the environment settings.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Location | The folder or geodatabase where the output raster dataset will be stored. | Workspace |
| Raster Dataset Name with Extension | The name and format of the raster dataset you are creating.To store the output as a raster dataset in a geodatabase, do not add a file extension to the raster dataset name.For file-based rasters, use the appropriate extension to specify the format to create as follows:.tif—TIFF raster.img—ERDAS IMAGINE raster.crf—CRF rasterNo extension—Esri Grid | String |
| Distribution(Optional) | Specifies the random value distribution method to use.Each type has one or two settings to control the distribution.Uniform—A uniform distribution with the defined range between the Minimum and Maximum values. The default values are 0.0 for Minimum and 1.0 for Maximum. This is the default.Integer—An integer distribution with the defined range between the Minimum and Maximum values. The default values are 1 for Minimum and 10 for Maximum.Normal—A normal distribution with defined Mean and Standard Deviation values. The default values are 0.0 for Mean and 1.0 for Standard Deviation.Exponential—An exponential distribution with a defined Mean value. The default value is 1.0.Poisson-—A Poisson distribution with a defined Mean value. The default value is 1.0.Gamma—A gamma distribution with defined Alpha and Beta values. The default values are 1.0 for Alpha and 1.0 for Beta.Binomial—A binomial distribution with defined N and Probability values. The default values are 10 for N and 0.5 for Probability.Geometric—A geometric distribution with a defined Probability value. The default value is 0.5.Negative Binomial—A Pascal distribution with defined r and Probability values. The default values are 10.0 for r and 0.5 for Probability. To edit the default value, click the value in the table and type the new value. | String |
| Output extent(Optional) | The extent of the output raster dataset.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Cellsize(Optional) | The spatial resolution of the output raster dataset. | Double |
| Build raster attribute table(Optional) | Specifies whether the tool will unconditionally build a raster attribute table for the output raster in which the selected distribution results in an integer output raster.This parameter has no effect if the output raster is floating point.Checked—A raster attribute table will be unconditionally built for integer output rasters. This is the default.Unchecked—A raster attribute table will not be built for integer output rasters if the number of unique values is greater than or equal to 65535. If the number of unique values is less than 65535, a raster attribute table will be built. | Boolean |
| out_path | The folder or geodatabase where the output raster dataset will be stored. | Workspace |
| out_name | The name and format of the raster dataset you are creating.To store the output as a raster dataset in a geodatabase, do not add a file extension to the raster dataset name.For file-based rasters, use the appropriate extension to specify the format to create as follows:.tif—TIFF raster.img—ERDAS IMAGINE raster.crf—CRF rasterNo extension—Esri Grid | String |
| distribution(Optional) | Specifies the random value distribution method to use.Each type has one or two settings to control the distribution. UNIFORM {Minimum}, {Maximum}—A uniform distribution with the defined range. The default values are 0.0 for {Minimum} and 1.0 for {Maximum}. Both values are of type double.INTEGER {Minimum}, {Maximum}—An integer distribution with the defined range. The default values are 1 for {Minimum} and 10 for {Maximum}. Both values are of type long.NORMAL {Mean}, {Standard Deviation}—A normal distribution with defined {Mean} and {Standard Deviation} values. The default values are 0.0 for {Mean} and 1.0 for {Standard Deviation}. Both values are of type double.EXPONENTIAL {Mean}—An exponential distribution with a defined {Mean} value. The default value is 1.0. The value is of type double.POISSON {Mean}—A Poisson distribution with a defined {Mean} value. The default value is 1.0. The value is of type double.GAMMA {Alpha}, {Beta}—A gamma distribution with defined {Alpha} and {Beta} values. The default values are 1.0 for {Alpha} and 1.0 for {Beta}. Both values are of type double.BINOMIAL {N}, {Probability}—A binomial distribution with defined {N} and {Probability} values. The {N} value is of type long with a default of 10. The {Probability} value is of type double with a default of 0.5.GEOMETRIC {Probability}—A geometric distribution with a defined {Probability} value. The default value is 0.5. The value is of type double.NEGATIVE BINOMIAL {r}, {Probability}—A Pascal distribution with defined {r} and {Probability} values. The {r} value is of type double with a default of 10.0. The {Probability} value is of type double with a default of 0.5. | String |
| raster_extent(Optional) | The extent of the output raster dataset.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| cellsize(Optional) | The spatial resolution of the output raster dataset. | Double |
| build_rat(Optional) | Specifies whether the tool will unconditionally build a raster attribute table for the output raster in which the selected distribution results in an integer output raster.This parameter has no effect if the output raster is floating point.BUILD—A raster attribute table will be unconditionally built for integer output rasters. This is the default.DO_NOT_BUILD—A raster attribute table will not be built for integer output rasters if the number of unique values is greater than or equal to 65535. If the number of unique values is less than 65535, a raster attribute table will be built. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateRandomRaster(out_path, out_name, {distribution}, {raster_extent}, {cellsize}, {build_rat})
```

### Example 2

```python
import arcpy
arcpy.CreateRandomRaster_management("c:/output", "randrast", 
                                    "NORMAL 3.0", "0 0 500 500", 50)
```

### Example 3

```python
import arcpy
arcpy.CreateRandomRaster_management("c:/output", "randrast", 
                                    "NORMAL 3.0", "0 0 500 500", 50)
```

### Example 4

```python
# Name: CreateRandomRaster_Ex_02.py
# Description: Creates a random raster dataset based on a 
#              user-specified distribution and extent.
# Requirements: None

# Import system modules
import arcpy

# Set local variables
outPath = "c:/output"
outFile = "randrast02"
distribution = "POISSON 6.4"
outExtent = "250 250 750 750"
cellSize = 25

# Execute CreateRandomRaster
arcpy.CreateRandomRaster_management(outPath, outFile, distribution, 
                                    outExtent, cellSize)
```

### Example 5

```python
# Name: CreateRandomRaster_Ex_02.py
# Description: Creates a random raster dataset based on a 
#              user-specified distribution and extent.
# Requirements: None

# Import system modules
import arcpy

# Set local variables
outPath = "c:/output"
outFile = "randrast02"
distribution = "POISSON 6.4"
outExtent = "250 250 750 750"
cellSize = 25

# Execute CreateRandomRaster
arcpy.CreateRandomRaster_management(outPath, outFile, distribution, 
                                    outExtent, cellSize)
```

---

## Create Raster Dataset (Data Management)

## Summary

Creates an empty raster dataset.

## Usage

- When you create a raster dataset, you are creating an empty location to contain a single raster dataset. You can then mosaic or load raster datasets into this location.
- You can save the output to BIL, BIP, BMP, BSQ, CRF, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, or TIFF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- The GIF format only supports single-band raster datasets.
- A raster dataset created in CRF format is expandable. The extent of an expandable CRF dataset is adjusted automatically by other tools that accept CRF as an input, such as the Mosaic, Copy Raster, Clip Rasters, and Resample tools.
- An expandable CRF dataset supports multidimensional rasters. An expandable multidimensional raster dataset is created when you add a multidimensional data to an empty raster dataset using the Mosaic tool. You can also define the multidimensional information for an empty raster dataset using ArcPy, as shown in the following example: Example of how to define the multidimensional information for an empty raster dataset. The definition of the multidimensional raster dataset is based on an existing multidimensional raster, represented by template.crf in the sample below.import arcpy arcpy.CheckOutExtension("ImageAnalyst") r = arcpy.Raster(r"\\location_to_a_mdim_raster\template.crf", True) r2 = arcpy.Raster(r"C:\Temp\expandable_mdim.crf") r2.mdinfo = r.mdinfo r2.mdinfo
- Building pyramids improves the display performance of raster datasets.
- Calculating statistics allows ArcGIS applications to properly stretch and symbolize raster data for display.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Location | The folder or geodatabase where the raster dataset will be stored. | Workspace |
| Raster Dataset Name with Extension | The name, location, and format for the newly created dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil for Esri BIL.bip for Esri BIP.bmp for BMP.bsq for Esri BSQ.crf for CRF.dat for ENVI DAT.gif for GIF.img for ERDAS IMAGINE.jpg for JPEG.jp2 for JPEG 2000.png for PNG.tif for TIFFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | String |
| Cellsize(Optional) | The pixel size that will be used for the new raster dataset. | Double |
| Pixel Type | The bit depth (radiometric resolution) of the output raster dataset. If this is not specified, the raster dataset will be created with a default pixel type of 8-bit unsigned integer.Not all data types are supported by all raster formats. Check the List of supported sensors help topic to ensure that the format you are using will support the necessary data type.1 bit—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2 bit—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4 bit—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8 bit unsigned—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8 bit signed—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16 bit unsigned—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16 bit signed—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32 bit unsigned—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32 bit signed—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32 bit float—The pixel type will be a 32-bit data type supporting decimals.64 bit—The pixel type will be a 64-bit data type supporting decimals. | String |
| Spatial Reference for Raster(Optional) | The coordinate system for the output raster dataset.If this is not specified, the coordinate system set in the environment settings will be used. | Coordinate System |
| Number of Bands | The number of bands of the output raster dataset. | Long |
| Configuration Keyword (Optional) | The storage parameters (configuration) for a file or enterprise geodatabase. Configuration keywords are set up by your database administrator. | String |
| Create pyramids(Optional) | Creates pyramids.For Pyramid Levels, specify a number of -1 or higher. A value of 0 will not create pyramids, and a value of -1 will automatically determine the correct number of pyramid layers to create.Pyramid Resampling Technique defines how the data will be resampled when creating the pyramids.NEAREST—Use nearest neighbor for nominal data or raster datasets with color maps, such as land-use or pseudo color images.BILINEAR—Use bilinear interpolation with continuous data, such as satellite imagery or aerial photography.CUBIC—Use cubic convolution continuous data, such as satellite imagery or aerial photography. It is similar to bilinear interpolation; however, it resamples the data using a larger matrix.Pyramid Compression Type defines the method used when compressing the pyramids.DEFAULT—The compression that is normally used by the raster dataset format will be used.LZ77—A lossless compression will be used. The values of the cells in the raster will not be changed.JPEG—A lossy compression will be used.NONE—No data compression will be used. | Pyramid |
| Tile size(Optional) | The size of the tiles.The tile width controls the number of pixels that can be stored in each tile. This is specified as a number of pixels in x. The default tile width is 128.The tile height controls the number of pixels that can be stored in each tile. This is specified as a number of pixels in y. The default tile height is 128.Only geodatabases and enterprise geodatabases use tile size. | Tile Size |
| Compression(Optional) | Specifies the type of compression that will be used to store the raster dataset.None—No compression will be used. This is the default.LZ77—Lossless compression that preserves all raster cell values will be used.Jpeg—Lossy compression that uses the public JPEG compression algorithm will be used. If you choose JPEG, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. This compression can be used for .jpg files and .tif files.Jpeg 2000—Lossy compression will be used. Lzw—Lossless compression that preserves all raster cell values will be used.Packbits—PackBits compression will be used for .tif files.Rle—Run-length encoding will be used for .img files.Ccitt Group 3—Lossless compression for 1-bit data will be used.Ccitt Group 4—Lossless compression for 1-bit data will be used.Ccitt 1D—Lossless compression for 1-bit data will be used. | Compression |
| Origin/Pyramid Reference Point(Optional) | The origination location of the raster pyramid. It is recommended that you specify this point if you plan to build large mosaics in a file geodatabase or enterprise geodatabase, especially if you plan to mosaic them over time (for example, when updating).Set the pyramid reference point at the upper left corner of the raster dataset.In setting this point for a file geodatabase or enterprise geodatabase, partial pyramiding will be used when updating with a new mosaicked raster dataset. Partial pyramiding updates the parts of the pyramid that do not exist due to the new mosaicked datasets. It is a good practice to set a pyramid reference point so that the entire raster mosaic will be below and to the right of this point. However, a pyramid reference point should not be set too large either. | Point |
| out_path | The folder or geodatabase where the raster dataset will be stored. | Workspace |
| out_name | The name, location, and format for the newly created dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil for Esri BIL.bip for Esri BIP.bmp for BMP.bsq for Esri BSQ.crf for CRF.dat for ENVI DAT.gif for GIF.img for ERDAS IMAGINE.jpg for JPEG.jp2 for JPEG 2000.png for PNG.tif for TIFFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | String |
| cellsize(Optional) | The pixel size that will be used for the new raster dataset. | Double |
| pixel_type | The bit depth (radiometric resolution) of the output raster dataset. If this is not specified, the raster dataset will be created with a default pixel type of 8-bit unsigned integer.Not all data types are supported by all raster formats. Check the List of supported sensors help topic to ensure that the format you are using will support the necessary data type.1_BIT—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2_BIT—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4_BIT—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8_BIT_UNSIGNED—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8_BIT_SIGNED—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16_BIT_UNSIGNED—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16_BIT_SIGNED—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32_BIT_UNSIGNED—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32_BIT_SIGNED—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32_BIT_FLOAT—The pixel type will be a 32-bit data type supporting decimals.64_BIT—The pixel type will be a 64-bit data type supporting decimals. | String |
| raster_spatial_reference(Optional) | The coordinate system for the output raster dataset.If this is not specified, the coordinate system set in the environment settings will be used. | Coordinate System |
| number_of_bands | The number of bands of the output raster dataset. | Long |
| config_keyword(Optional) | The storage parameters (configuration) for a file or enterprise geodatabase. Configuration keywords are set up by your database administrator. | String |
| pyramids(Optional) | Creates pyramids.For Pyramid Levels, specify a number of -1 or higher. A value of 0 will not create pyramids, and a value of -1 will automatically determine the correct number of pyramid layers to create.Pyramid Resampling Technique defines how the data will be resampled when creating the pyramids.NEAREST—Use nearest neighbor for nominal data or raster datasets with color maps, such as land-use or pseudo color images.BILINEAR—Use bilinear interpolation with continuous data, such as satellite imagery or aerial photography.CUBIC—Use cubic convolution with continuous data, such as satellite imagery or aerial photography. It is similar to bilinear interpolation; however, it resamples the data using a larger matrix.Pyramid Compression Type defines the method used when compressing the pyramids.DEFAULT—The compression that is normally used by the raster dataset format will be used.LZ77—A lossless compression will be used. The values of the cells in the raster will not be changed.JPEG—A lossy compression will be used.NONE—No data compression will be used. | Pyramid |
| tile_size(Optional) | The size of the tiles.The tile width controls the number of pixels that can be stored in each tile. This is specified as a number of pixels in x. The default tile width is 128.The tile height controls the number of pixels that can be stored in each tile. This is specified as a number of pixels in y. The default tile height is 128.Only geodatabases and enterprise geodatabases use tile size. | Tile Size |
| compression(Optional) | Specifies the type of compression that will be used to store the raster dataset.LZ77—Lossless compression that preserves all raster cell values will be used.JPEG—Lossy compression that uses the public JPEG compression algorithm will be used. If you choose JPEG, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. This compression can be used for .jpg files and .tif files.JPEG 2000—Lossy compression will be used. PACKBITS—PackBits compression will be used for .tif files.LZW—Lossless compression that preserves all raster cell values will be used.RLE—Run-length encoding will be used for .img files.CCITT GROUP 3—Lossless compression for 1-bit data will be used.CCITT GROUP 4—Lossless compression for 1-bit data will be used.CCITT_1D—Lossless compression for 1-bit data will be used.NONE—No compression will be used. This is the default. | Compression |
| pyramid_origin(Optional) | The origination location of the raster pyramid. It is recommended that you specify this point if you plan to build large mosaics in a file geodatabase or enterprise geodatabase, especially if you plan to mosaic them over time (for example, when updating).Set the pyramid reference point at the upper left corner of the raster dataset.In setting this point for a file geodatabase or enterprise geodatabase, partial pyramiding will be used when updating with a new mosaicked raster dataset. Partial pyramiding updates the parts of the pyramid that do not exist due to the new mosaicked datasets. It is a good practice to set a pyramid reference point so that the entire raster mosaic will be below and to the right of this point. However, a pyramid reference point should not be set too large either. | Point |

## Code Samples

### Example 1

```python
import arcpy
arcpy.CheckOutExtension("ImageAnalyst")
r = arcpy.Raster(r"\\location_to_a_mdim_raster\template.crf", True)
r2 = arcpy.Raster(r"C:\Temp\expandable_mdim.crf")
r2.mdinfo = r.mdinfo
r2.mdinfo
```

### Example 2

```python
import arcpy
arcpy.CheckOutExtension("ImageAnalyst")
r = arcpy.Raster(r"\\location_to_a_mdim_raster\template.crf", True)
r2 = arcpy.Raster(r"C:\Temp\expandable_mdim.crf")
r2.mdinfo = r.mdinfo
r2.mdinfo
```

### Example 3

```python
arcpy.management.CreateRasterDataset(out_path, out_name, {cellsize}, pixel_type, {raster_spatial_reference}, number_of_bands, {config_keyword}, {pyramids}, {tile_size}, {compression}, {pyramid_origin})
```

### Example 4

```python
import arcpy
arcpy.CreateRasterDataset_management("c:/data", "EmptyTIFF.tif", "2",
                                     "8_BIT_UNSIGNED", "World_Mercator.prj",
                                     "3", "", "PYRAMIDS -1 NEAREST JPEG",
                                     "128 128", "NONE", "")
```

### Example 5

```python
import arcpy
arcpy.CreateRasterDataset_management("c:/data", "EmptyTIFF.tif", "2",
                                     "8_BIT_UNSIGNED", "World_Mercator.prj",
                                     "3", "", "PYRAMIDS -1 NEAREST JPEG",
                                     "128 128", "NONE", "")
```

### Example 6

```python
##==================================
##Create Raster Dataset
##Usage: CreateRasterDataset_management out_path out_name {cellsize} 8_BIT_UNSIGNED | 1_BIT | 2_BIT | 4_BIT | 8_BIT_SIGNED 
##                                      | 16_BIT_UNSIGNED | 16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT 
##                                      | 64_BIT {raster_spatial_reference} number_of_bands {config_keyword} {pyramids} {tile_size} 
##                                      {compression} {pyramid_origin}

import arcpy
arcpy.env.workspace = r"\\workspace\PrjWorkspace\RasGP"
##Create a empty TIFF format Raster Dataset with the following parameters
##Cellsize: 2
##Pixel type: 8 Bit Unsigned Integer
##Number of Bands: 3
##Pyramid: Build full pyramids with NEAREST interpolation and JPEG compression
##Compression: NONE
##Projection: World_Mercator
##Tile size: 128 128
arcpy.CreateRasterDataset_management("CreateRD","EmptyTIFF.tif","2","8_BIT_UNSIGNED",\
                                     "World_Mercator.prj", "3", "", "PYRAMIDS -1 NEAREST JPEG",\
                                     "128 128", "NONE", "")

##Create a SDE Raster Dataset
##No Spatial Reference, with Pyramid Origin
arcpy.CreateRasterDataset_management("CreateRD\\CreateRD.gdb","NewRD","10","16_BIT_UNSIGNED",\
                                     "", "1", "MAX_FILE_SIZE_4GB", "PYRAMIDS 3 BILINEAR DEFAULT",\
                                     "128 128", "JPEG2000 80", "-20037508.34278775 30198185.16987658")
```

### Example 7

```python
##==================================
##Create Raster Dataset
##Usage: CreateRasterDataset_management out_path out_name {cellsize} 8_BIT_UNSIGNED | 1_BIT | 2_BIT | 4_BIT | 8_BIT_SIGNED 
##                                      | 16_BIT_UNSIGNED | 16_BIT_SIGNED | 32_BIT_UNSIGNED | 32_BIT_SIGNED | 32_BIT_FLOAT 
##                                      | 64_BIT {raster_spatial_reference} number_of_bands {config_keyword} {pyramids} {tile_size} 
##                                      {compression} {pyramid_origin}

import arcpy
arcpy.env.workspace = r"\\workspace\PrjWorkspace\RasGP"
##Create a empty TIFF format Raster Dataset with the following parameters
##Cellsize: 2
##Pixel type: 8 Bit Unsigned Integer
##Number of Bands: 3
##Pyramid: Build full pyramids with NEAREST interpolation and JPEG compression
##Compression: NONE
##Projection: World_Mercator
##Tile size: 128 128
arcpy.CreateRasterDataset_management("CreateRD","EmptyTIFF.tif","2","8_BIT_UNSIGNED",\
                                     "World_Mercator.prj", "3", "", "PYRAMIDS -1 NEAREST JPEG",\
                                     "128 128", "NONE", "")

##Create a SDE Raster Dataset
##No Spatial Reference, with Pyramid Origin
arcpy.CreateRasterDataset_management("CreateRD\\CreateRD.gdb","NewRD","10","16_BIT_UNSIGNED",\
                                     "", "1", "MAX_FILE_SIZE_4GB", "PYRAMIDS 3 BILINEAR DEFAULT",\
                                     "128 128", "JPEG2000 80", "-20037508.34278775 30198185.16987658")
```

---

## Create Raster Type (Data Management)

## Summary

The Create Raster Type tool is not supported beginning with ArcGIS 10.6 and ArcGIS Pro 2.1.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | Specify the database connection (.sde) file for the geodatabase in which you want to install the ST_Raster data type. You must connect as the geodatabase administrator. | Workspace |
| input_database | Specify the database connection (.sde) file for the geodatabase in which you want to install the ST_Raster data type. You must connect as the geodatabase administrator. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.CreateRasterType(input_database)
```

---

## Create Referenced Mosaic Dataset (Data Management)

## Summary

Creates a separate mosaic dataset from items in an existing mosaic dataset.

## Usage

- A referenced mosaic dataset can be created in a geodatabase or a folder. When the mosaic dataset is created in a folder, it will not include a boundary.
- Overviews cannot be created for a referenced mosaic dataset.
- Use this tool to create a mosaic dataset from another mosaic dataset when you want to create a mosaic dataset with a different output. For example, you can create one mosaic dataset with elevation data and create another that will be used to produce a derived product, such as slope or hillshade.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset from which items will be selected. | Mosaic Layer; Mosaic Dataset |
| Output Mosaic Dataset | The referenced mosaic dataset to be created. | Mosaic Dataset |
| Coordinate System(Optional) | The projection for the output mosaic dataset. | Coordinate System |
| Number of Bands(Optional) | The number of bands that the referenced mosaic dataset will have. | Long |
| Pixel Type (Optional) | The bit depth, or radiometric resolution, of the mosaic dataset. If this is not defined, it will be taken from the first raster dataset.1-bit—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2-bit—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4-bit—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8-bit unsigned—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8-bit signed—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16-bit unsigned—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16-bit signed—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32-bit unsigned—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32-bit signed—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32-bit floating point—The pixel type will be a 32-bit data type supporting decimals.64-bit—The pixel type will be a 64-bit data type supporting decimals. | String |
| Query Definition(Optional) | An SQL expression to select raster datasets that will be included in the output mosaic dataset. | SQL Expression |
| Extent from Dataset(Optional) | Select raster datasets based on the extent of another image or feature class. Raster datasets that lay along the defined extent will be included in the mosaic dataset. To manually input the minimum and maximum coordinates for the extent, use the Extent parameter. | Feature Layer; Raster Layer |
| Extent (Optional) | The minimum and maximum coordinates for the extent. If a dataset is selected in Extent from Dataset, those coordinates will automatically appear here. | Envelope |
| Using Input Geometry for Selection(Optional) | Limit the extent to the shape or envelope when a feature class is selected in the Extent from Dataset parameter.Checked—Select based on the shape of the feature. This is the default.Unchecked—Select based on the extent of the feature class. | Boolean |
| Scale Field(Optional) | Legacy:This parameter has been deprecated and is ignored in tool execution. It remains for backward compatibility reasons. | Field |
| Minimum Cell Size Field (Optional) | Specify a field from the footprint attribute table that defines the minimum cell size for displaying the mosaic dataset; otherwise, only a footprint will be displayed. | Field |
| Maximum Cell Size Field (Optional) | Specify a field from the footprint attribute table that defines the maximum cell size for displaying the mosaic dataset; otherwise, only a footprint will be displayed. | Field |
| Maximum Visible Cell Size(Optional) | Set a maximum cell size to display the mosaic instead of specifying a field. If you zoom out beyond this cell size, only the footprint will be displayed. | Double |
| Build Boundary (Optional) | Rebuild the boundary. If the selection covers a smaller area than the source mosaic dataset, this is recommended.This is only available if the mosaic dataset is created in a geodatabase.Checked—Generate the boundary. This is the default.Unchecked—Do not generate the boundary. | Boolean |
| in_dataset | The mosaic dataset from which items will be selected. | Mosaic Layer; Mosaic Dataset |
| out_mosaic_dataset | The referenced mosaic dataset to be created. | Mosaic Dataset |
| coordinate_system(Optional) | The projection for the output mosaic dataset. | Coordinate System |
| number_of_bands(Optional) | The number of bands that the referenced mosaic dataset will have. | Long |
| pixel_type(Optional) | The bit depth, or radiometric resolution, of the mosaic dataset. If this is not defined, it will be taken from the first raster dataset. 1_BIT—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2_BIT—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4_BIT—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8_BIT_UNSIGNED—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8_BIT_SIGNED—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16_BIT_UNSIGNED—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16_BIT_SIGNED—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32_BIT_UNSIGNED—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32_BIT_SIGNED—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32_BIT_FLOAT—The pixel type will be a 32-bit data type supporting decimals.64_BIT—The pixel type will be a 64-bit data type supporting decimals. | String |
| where_clause(Optional) | An SQL expression to select raster datasets that will be included in the output mosaic dataset. | SQL Expression |
| in_template_dataset(Optional) | Select raster datasets based on the extent of another image or feature class. Raster datasets that lay along the defined extent will be included in the mosaic dataset. To manually input the minimum and maximum coordinates for the extent, use the Extent parameter. | Feature Layer; Raster Layer |
| extent(Optional) | The minimum and maximum coordinates for the extent. | Envelope |
| select_using_features(Optional) | Limit the extent to the shape or envelope when a feature class is specified in the in_template_dataset parameter. SELECT_USING_FEATURES—Select using the shape of the feature. This is the default.NO_SELECT_USING_FEATURES—Select using the extent of the data in the feature class. | Boolean |
| lod_field(Optional) | Legacy:This parameter has been deprecated and is ignored in tool execution. It remains for backward compatibility reasons. | Field |
| minPS_field(Optional) | Specify a field from the footprint attribute table that defines the minimum cell size for displaying the mosaic dataset; otherwise, only a footprint will be displayed. | Field |
| maxPS_field(Optional) | Specify a field from the footprint attribute table that defines the maximum cell size for displaying the mosaic dataset; otherwise, only a footprint will be displayed. | Field |
| pixelSize(Optional) | Set a maximum cell size to display the mosaic instead of specifying a field. If you zoom out beyond this cell size, only the footprint will be displayed. | Double |
| build_boundary(Optional) | Rebuild the boundary. If the selection covers a smaller area than the source mosaic dataset, this is recommended.This is only available if the mosaic dataset is created in a geodatabase.BUILD_BOUNDARY—The boundary will be generated or updated. This is the default. NO_BOUNDARY— The boundary will not be generated. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateReferencedMosaicDataset(in_dataset, out_mosaic_dataset, {coordinate_system}, {number_of_bands}, {pixel_type}, {where_clause}, {in_template_dataset}, {extent}, {select_using_features}, {lod_field}, {minPS_field}, {maxPS_field}, {pixelSize}, {build_boundary})
```

### Example 2

```python
import arcpy
arcpy.CreateReferencedMosaicDataset_management(
     "C:/workspace/RefMD.gdb/md", "ref_md.amd", "GCS_WGS_1984.prj",
     "1", "#", "#", "ref_md.shp", "#", "SELECT_USING_FEATURES", "#", 
     "#", "#", "#", "NO_BOUNDARY")
```

### Example 3

```python
import arcpy
arcpy.CreateReferencedMosaicDataset_management(
     "C:/workspace/RefMD.gdb/md", "ref_md.amd", "GCS_WGS_1984.prj",
     "1", "#", "#", "ref_md.shp", "#", "SELECT_USING_FEATURES", "#", 
     "#", "#", "#", "NO_BOUNDARY")
```

### Example 4

```python
# Create Referenced Mosaic Dataset from existing Mosaic Dataset
# Use shape file to clip the source mosaic dataset


import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.CreateReferencedMosaicDataset_management(
     "RefMD.gdb/md", "ref_md.amd", "GCS_WGS_1984.prj", "1", "#", "#", 
     "ref_md.shp", "#", "SELECT_USING_FEATURES", "#", "#", "#", "#", 
     "NO_BOUNDARY")
```

### Example 5

```python
# Create Referenced Mosaic Dataset from existing Mosaic Dataset
# Use shape file to clip the source mosaic dataset


import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.CreateReferencedMosaicDataset_management(
     "RefMD.gdb/md", "ref_md.amd", "GCS_WGS_1984.prj", "1", "#", "#", 
     "ref_md.shp", "#", "SELECT_USING_FEATURES", "#", "#", "#", "#", 
     "NO_BOUNDARY")
```

---

## Create Relationship Class (Data Management)

## Summary

Creates a relationship class to store an association between fields or features in the origin table and the destination table.

## Usage

- Relationships can exist between spatial objects (features in feature classes), nonspatial objects (rows in a table), or spatial and nonspatial objects.
- Once created, a relationship class cannot be modified; you can only add, delete, or refine its rules.
- For many-to-many relationship classes, a new table is created in the database to store the foreign keys used to link the origin and destination classes. This table can also have other fields to store attributes of the relationship that are not attributed to either the origin class or the destination class. For example, in a parcel database, you might have a relationship class between parcels and owners in which owners own parcels and parcels are owned by owners. An attribute of that relationship might be percentage ownership. One-to-one and one-to-many relationship classes can also have attributes. In this case, a table is created to store the relationships.
- Simple or peer-to-peer relationships involve two or more objects in the database that exist independently of each other. For example, in a railroad network, you might have railroad crossings that have one or more related signal lamps. However, a railroad crossing can exist without a signal lamp, and signal lamps can exist on the railroad network where there are no railroad crossings. Simple relationships can have one-to-one, one-to-many, or many-to-many cardinality.
- A composite relationship is one in which the lifetime of one object controls the lifetime of its related objects. For example, power poles support transformers, and transformers are mounted on poles. Once a pole is deleted, a delete message is propagated to its related transformers, which are deleted from the transformers' feature class. Composite relationships are always one to many.
- Forward and backward path labels describe the relationship when navigating from one object to another. The forward path label describes the relationship navigated from the origin class to the destination class. In the pole-transformer example, a forward path label might be Poles support transformers. The backward path label describes the relationship navigated from the destination class to the origin class. In the pole-transformer example, a backward path label might be Transformers are mounted on poles.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Origin Table | The table or feature class that is associated with the destination table. | Table View |
| Destination Table | The table that is associated with the origin table. | Table View |
| Output Relationship Class | The relationship class that will be created. | Relationship Class |
| Relationship Type | Specifies the type of relationship that will be created between the origin and destination tables.Simple—The origin and destination tables will have a simple relationship. This is the default. Composite—The origin and destination tables will have a composite relationship. | String |
| Forward Path Label | A name to uniquely identify the relationship when navigating from the origin table to the destination table. | String |
| Backward Path label | A name to uniquely identify the relationship when navigating from the destination table to the origin table. | String |
| Message Direction | Specifies the direction in which messages will be passed between the origin and destination tables. For example, in a relationship between poles and transformers, when a pole is deleted, a message will be sent to its related transformer objects indicating that it was deleted.Forward (origin to destination)—Messages will be passed from the origin table to the destination table. Backward (destination to origin)—Messages will be passed from the destination table to the origin table. Both directions—Messages will be passed from the origin table to the destination table and from the destination table to the origin table. None (no messages propagated)—No messages will be passed. This is the default. | String |
| Cardinality | Specifies how many relationships will exist between rows or features in the origin table and rows or features in the destination table.One to one (1:1)—Each row or feature in the origin table can be related to zero or one row or feature in the destination table. This is the default. One to many (1:M)—Each row or feature in the origin table can be related to one or several rows or features in the destination table. Many to many (M:N)—Several rows or features in the origin table can be related to one or several rows or features in the destination table. | String |
| Relationship class is attributed | Specifies whether the relationship class will have attributes.Checked—The relationship class will have attributes.Unchecked—The relationship class will not have attributes. This is the default. | Boolean |
| Origin Primary Key | For many-to-many or attributed relationship classes, this is the field in the origin table that links to the Origin Foreign Key field in the relationship class table.For one-to-one or one-to-many relationship classes that are not attributed, this is the field in the origin table that links to the Origin Foreign Key field in the destination table. | String |
| Origin Foreign Key | For many-to-many or attributed relationship classes, this is the field in the relationship class table that links to the Origin Primary Key field in the origin table.For one-to-one or one-to-many relationship classes that are not attributed, this is the field in the destination table that links to the Origin Primary Key field in the origin table. | String |
| Destination Primary Key(Optional) | The field in the destination table that links to the Destination Foreign Key field in the relationship class table. This value is required for many-to-many or attributed relationship classes, but should be left empty for one-to-one or one-to-many relationship classes that are not attributed. | String |
| Destination Foreign Key(Optional) | The field in the relationship class table that links to the Destination Primary Key field in the destination table. This value is required for many-to-many or attributed relationship classes, but should be left empty for one-to-one or one-to-many relationship classes that are not attributed. | String |
| origin_table | The table or feature class that is associated with the destination table. | Table View |
| destination_table | The table that is associated with the origin table. | Table View |
| out_relationship_class | The relationship class that will be created. | Relationship Class |
| relationship_type | Specifies the type of relationship that will be created between the origin and destination tables.SIMPLE—The origin and destination tables will have a simple relationship. This is the default. COMPOSITE—The origin and destination tables will have a composite relationship. | String |
| forward_label | A name to uniquely identify the relationship when navigating from the origin table to the destination table. | String |
| backward_label | A name to uniquely identify the relationship when navigating from the destination table to the origin table. | String |
| message_direction | Specifies the direction in which messages will be passed between the origin and destination tables. For example, in a relationship between poles and transformers, when a pole is deleted, a message will be sent to its related transformer objects indicating that it was deleted.FORWARD—Messages will be passed from the origin table to the destination table. BACKWARD—Messages will be passed from the destination table to the origin table. BOTH—Messages will be passed from the origin table to the destination table and from the destination table to the origin table. NONE—No messages will be passed. This is the default. | String |
| cardinality | Specifies how many relationships will exist between rows or features in the origin table and rows or features in the destination table.ONE_TO_ONE—Each row or feature in the origin table can be related to zero or one row or feature in the destination table. This is the default. ONE_TO_MANY—Each row or feature in the origin table can be related to one or several rows or features in the destination table. MANY_TO_MANY—Several rows or features in the origin table can be related to one or several rows or features in the destination table. | String |
| attributed | Specifies whether the relationship class will have attributes.NONE—The relationship class will not have attributes. This is the default. ATTRIBUTED—The relationship class will have attributes. | Boolean |
| origin_primary_key | For many-to-many or attributed relationship classes, this is the field in the origin table that links to the origin_foreign_key field in the relationship class table.For one-to-one or one-to-many relationship classes that are not attributed, this is the field in the origin table that links to the origin_foreign_key field in the destination table. | String |
| origin_foreign_key | For many-to-many or attributed relationship classes, this is the field in the relationship class table that links to the origin_primary_key field in the origin table.For one-to-one or one-to-many relationship classes that are not attributed, this is the field in the destination table that links to the origin_primary_key field in the origin table. | String |
| destination_primary_key(Optional) | The field in the destination table that links to the destination_foreign_key field in the relationship class table. This value is required for many-to-many or attributed relationship classes, but should be left empty for one-to-one or one-to-many relationship classes that are not attributed. | String |
| destination_foreign_key(Optional) | The field in the relationship class table that links to the destination_primary_key field in the destination table. This value is required for many-to-many or attributed relationship classes, but should be left empty for one-to-one or one-to-many relationship classes that are not attributed. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateRelationshipClass(origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, attributed, origin_primary_key, origin_foreign_key, {destination_primary_key}, {destination_foreign_key})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
arcpy.CreateRelationshipClass_management("vegtype", "vegtable", "veg_RelClass", "SIMPLE",
                                         "Attributes from vegtable", "Attributes and Features from vegtype",
                                         "NONE", "ONE_TO_ONE", "NONE", "HOLLAND95", "HOLLAND95")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
arcpy.CreateRelationshipClass_management("vegtype", "vegtable", "veg_RelClass", "SIMPLE",
                                         "Attributes from vegtable", "Attributes and Features from vegtype",
                                         "NONE", "ONE_TO_ONE", "NONE", "HOLLAND95", "HOLLAND95")
```

### Example 4

```python
# Name: CreateRelationshipClass.py
# Description: Create a relationship class between vegetation feature
#              class and table with additional vegetation information
# Author: ESRI

# Import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy vegtable.dbf to file gdb table, since both tables to be related
# must be in the same database
vegDbf = "vegtable.dbf"
vegTbl = "Habitat_Analysis.gdb/vegtable"
arcpy.CopyRows_management(vegDbf, vegTbl)

# Create simple relationship class between 'vegtype' vegetation layer
# and 'vegtable' table with additional vegetation information
veg = "Habitat_Analysis.gdb/vegtype"
relClass = "Habitat_Analysis.gdb/veg_RelClass"
forLabel = "Attributes from vegtable"
backLabel = "Attributes and Features from vegtype"
primaryKey = "HOLLAND95"
foreignKey = "HOLLAND95"
arcpy.CreateRelationshipClass_management(veg,
                                         vegTbl,
                                         relClass,
                                         "SIMPLE",
                                         forLabel,
                                         backLabel,
                                         "NONE",
                                         "ONE_TO_ONE",
                                         "NONE",
                                         primaryKey,
                                         foreignKey)
```

### Example 5

```python
# Name: CreateRelationshipClass.py
# Description: Create a relationship class between vegetation feature
#              class and table with additional vegetation information
# Author: ESRI

# Import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy vegtable.dbf to file gdb table, since both tables to be related
# must be in the same database
vegDbf = "vegtable.dbf"
vegTbl = "Habitat_Analysis.gdb/vegtable"
arcpy.CopyRows_management(vegDbf, vegTbl)

# Create simple relationship class between 'vegtype' vegetation layer
# and 'vegtable' table with additional vegetation information
veg = "Habitat_Analysis.gdb/vegtype"
relClass = "Habitat_Analysis.gdb/veg_RelClass"
forLabel = "Attributes from vegtable"
backLabel = "Attributes and Features from vegtype"
primaryKey = "HOLLAND95"
foreignKey = "HOLLAND95"
arcpy.CreateRelationshipClass_management(veg,
                                         vegTbl,
                                         relClass,
                                         "SIMPLE",
                                         forLabel,
                                         backLabel,
                                         "NONE",
                                         "ONE_TO_ONE",
                                         "NONE",
                                         primaryKey,
                                         foreignKey)
```

### Example 6

```python
# Name: CreateRelationshipClass.py
# Description: Create a relationship class between parcels feature
#              class and table with owner information
# Author: ESRI

# Import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy owners.dat to file gdb table, since both tables to be related
# must be in the same database
ownerDat = "owners.dat"
ownerTbl = "Montgomery.gdb/owners"
arcpy.CopyRows_management(ownerDat, ownerTbl)

# Create simple relationship class between 'parcel' parcel layer
# and 'owner' table with additional parcel owner information
parcel = "Montgomery.gdb/Parcels"
relClass = "Montgomery.gdb/parcelowners_RelClass"
forLabel = "Owns"
backLabel = "Is Owned By"
primaryKey = "PROPERTY_ID"
foreignKey = "PROPERTY_ID"
arcpy.CreateRelationshipClass_management(ownerTbl,
                                         parcel,
                                         relClass,
                                         "SIMPLE",
                                         forLabel,
                                         backLabel,
                                         "BACKWARD",
                                         "ONE_TO_MANY",
                                         "NONE",
                                         primaryKey,
                                         foreignKey)
```

### Example 7

```python
# Name: CreateRelationshipClass.py
# Description: Create a relationship class between parcels feature
#              class and table with owner information
# Author: ESRI

# Import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy owners.dat to file gdb table, since both tables to be related
# must be in the same database
ownerDat = "owners.dat"
ownerTbl = "Montgomery.gdb/owners"
arcpy.CopyRows_management(ownerDat, ownerTbl)

# Create simple relationship class between 'parcel' parcel layer
# and 'owner' table with additional parcel owner information
parcel = "Montgomery.gdb/Parcels"
relClass = "Montgomery.gdb/parcelowners_RelClass"
forLabel = "Owns"
backLabel = "Is Owned By"
primaryKey = "PROPERTY_ID"
foreignKey = "PROPERTY_ID"
arcpy.CreateRelationshipClass_management(ownerTbl,
                                         parcel,
                                         relClass,
                                         "SIMPLE",
                                         forLabel,
                                         backLabel,
                                         "BACKWARD",
                                         "ONE_TO_MANY",
                                         "NONE",
                                         primaryKey,
                                         foreignKey)
```

---

## Create Replica From Server (Data Management)

## Summary

Creates a replica using a specified list of feature classes, layers, feature datasets, and tables from a remote geodatabase using a geodata service published on ArcGIS Server.

## Usage

- The source must be a geodata service representing a remote enterprise geodatabase. The destination can be either a local or a remote geodatabase.
- The data that will be replicated must be registered as traditional versioned but not with the option to move edits to base. The connected database user must also have write permissions to the data. For two-way and both types of one-way replicas, all datasets must have a globalid column and a high-precision spatial reference.
- For check-out and one-way replicas, the child replica geodatabase can be an enterprise or file geodatabase.
- For two-way and one-way child-to-parent replicas, the child geodatabase must be an enterprise geodatabase.
- To use archiving for one-way replicas, the parent workspace must be connected to the default version. For one-way child-to-parent replicas, the child workspace must be connected to the default version.
- The default for feature classes is to replicate all features. The default filter for tables is Schema Only; only the schema for the table will be replicated. If you set the Extent environment or specify replica geometry features, it will be applied as a spatial filter in which only features intersecting the extent will be replicated. Tables will also include rows that are related to rows that are part of the replica.
- The Replica Geometry Features parameter can be used to define the replica geometry. You can also use the Extent environment setting to define the replica geometry.If only the Replica Geometry Features parameter is set, only data intersecting the replica geometry features will be replicated. If only the Extent environment is set, only data intersecting the extent will be replicated.If both the Replica Geometry Features parameter and the Extent environment are set, the Replica Geometry Features parameter will be used. If neither the Replica Geometry Features parameter nor the Extent environment is specified, the full extent of the data will be used.
- If only the Replica Geometry Features parameter is set, only data intersecting the replica geometry features will be replicated.
- If only the Extent environment is set, only data intersecting the extent will be replicated.
- If both the Replica Geometry Features parameter and the Extent environment are set, the Replica Geometry Features parameter will be used.
- If neither the Replica Geometry Features parameter nor the Extent environment is specified, the full extent of the data will be used.
- The replica geometry features can be points, lines, or polygons.
- A feature layer used for the replica geometry features can contain one or more features. If there is more than one, the geometries are merged, and only data that intersects the merged geometries will be replicated.
- If filters (such as spatial, selection, or definition query) have been defined on the replica geometry features, only features that satisfy these filters will be used to define the replica geometry. See Preparing data for replication for more information.
- The Re-use Schema parameter options are only available for checkout replicas.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Geodata Service | The geodata service representing the geodatabase from which the replica will be created. The geodatabase referenced by the geodata service must be an enterprise geodatabase. | GeoDataServer |
| Datasets | The list of the feature datasets, stand-alone feature classes, tables, and stand-alone attributed relationship classes from the geodata service to be replicated. | String |
| Replica Type | Specifies the type of replica that will be created.Two way replica— Changes will be sent between child and parent replicas in both directions.One way replica—Changes will be sent from the parent replica to the child replica only.Check out replica—Data will be replicated, edited, and checked back in one time.One way child to parent replica—Changes will be sent from the child replica to the parent replica only. | String |
| Geodatabase to replicate data to | The local geodatabase that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an enterprise or file geodatabase. For two-way replicas, the child geodatabase must be an enterprise geodatabase. For one-way and check-out replicas, the geodatabase can be a file or enterprise geodatabase. File geodatabases must exist before running this tool. | Workspace ; GeoDataServer |
| Replica Name | The name that identifies the replica. | String |
| Replica Access Type(Optional) | Specifies the type of replica access.Full—Complex types such as topologies, are supported and the data must be versioned. This is the default.Simple—The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in topologies) will be converted to simple features (for example, point, line, and polygon feature classes). | String |
| Initial Data Sender(Optional) | Specifies which replica will send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica will not send updates until the changes are first received from the initial data sender.Child data sender—The child replica will be the initial data sender. This is the default.Parent data sender—The parent replica will be the initial data sender. | String |
| Expand Feature Classes and Tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.Use defaults—The expanded feature classes and tables related to the feature classes and tables in the replica will be added. The default for feature classes is to replicate all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to replicate the schema only. This is the default.Add with schema only—Only the schema for the expanded feature classes and tables will be added.All rows—All rows for expanded feature classes and tables will be added.Do not add—No expanded feature classes or tables will be added. | String |
| Re-use Schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be replicated will be reused. This reduces the amount of time required to replicate the data. This parameter is only available for checkout replicas.Do not reuse—Schema will not be reused. This is the default.Reuse—Schema will be used. | String |
| Replicate Related Data(Optional) | Specifies whether rows related to rows existing in the replica will be replicated. For example, a feature (f1) is inside the replication filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the replica if you choose to get related data.Do not get related—Related data will not be replicated.Get related—Related data will be replicated. This is the default. | String |
| Replica Geometry Features(Optional) | The features that will be used to define the area to replicate. | Feature Layer |
| Use archiving to track changes for 1 way replication (Optional) | Specifies whether the archive class will be used to track changes instead of the versioning delta tables. This is only available for one-way replicas.Archiving—Archiving will be used to track changes.Do not use archiving—Archiving will not be used to track changes. This is the default. | Boolean |
| All records for tables (Optional) | Specifies whether all records or only the schema will be copied to the child geodatabase for tables that do not have filters applied (such as selections or definition queries).Tables with applied filters will be honored. Checked—For tables with no applied filters, all records will be copied to the child geodatabase for tables. This option will override the Expand Feature Classes and Tables parameter value.Unchecked—For tables with no applied filters, only the schema will be copied to the child geodatabase for tables. Tables with applied filters will be honored. This is the default. | Boolean |
| in_geodataservice | The geodata service representing the geodatabase from which the replica will be created. The geodatabase referenced by the geodata service must be an enterprise geodatabase. | GeoDataServer |
| datasets[dataset_name,...] | The list of the feature datasets, stand-alone feature classes, tables, and stand-alone attributed relationship classes from the geodata service to be replicated. | String |
| in_type | Specifies the type of replica that will be created.TWO_WAY_REPLICA— Changes will be sent between child and parent replicas in both directions.ONE_WAY_REPLICA—Changes will be sent from the parent replica to the child replica only.CHECK_OUT—Data will be replicated, edited, and checked back in one time.ONE_WAY_CHILD_TO_PARENT_REPLICA—Changes will be sent from the child replica to the parent replica only. | String |
| out_geodatabase | The local geodatabase that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an enterprise or file geodatabase. For two-way replicas, the child geodatabase must be an enterprise geodatabase. For one-way and check-out replicas, the geodatabase can be a file or enterprise geodatabase. File geodatabases must exist before running this tool. | Workspace ; GeoDataServer |
| out_name | The name that identifies the replica. | String |
| access_type(Optional) | Specifies the type of replica access.FULL—Complex types such as topologies, are supported and the data must be versioned. This is the default.SIMPLE—The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in topologies) will be converted to simple features (for example, point, line, and polygon feature classes). | String |
| initial_data_sender(Optional) | Specifies which replica will send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica will not send updates until the changes are first received from the initial data sender.CHILD_DATA_SENDER—The child replica will be the initial data sender. This is the default.PARENT_DATA_SENDER—The parent replica will be the initial data sender. | String |
| expand_feature_classes_and_tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.USE_DEFAULTS—The expanded feature classes and tables related to the feature classes and tables in the replica will be added. The default for feature classes is to replicate all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to replicate the schema only. This is the default.ADD_WITH_SCHEMA_ONLY—Only the schema for the expanded feature classes and tables will be added.ALL_ROWS—All rows for expanded feature classes and tables will be added.DO_NOT_ADD—No expanded feature classes or tables will be added. | String |
| reuse_schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be replicated will be reused. This reduces the amount of time required to replicate the data. This parameter is only available for checkout replicas.DO_NOT_REUSE—Schema will not be reused. This is the default.REUSE—Schema will be used. | String |
| get_related_data(Optional) | Specifies whether rows related to rows existing in the replica will be replicated. For example, a feature (f1) is inside the replication filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the replica if you choose to get related data.DO_NOT_GET_RELATED—Related data will not be replicated.GET_RELATED—Related data will be replicated. This is the default. | String |
| geometry_features(Optional) | The features that will be used to define the area to replicate. | Feature Layer |
| archiving(Optional) | Specifies whether the archive class will be used to track changes instead of the versioning delta tables. This is only available for one-way replicas.ARCHIVING—Archiving will be used to track changes.DO_NOT_USE_ARCHIVING—Archiving will not be used to track changes. This is the default. | Boolean |
| all_records_for_tables(Optional) | Specifies whether all records or only the schema will be copied to the child geodatabase for tables that do not have filters applied (such as selections or definition queries).Tables with applied filters will be honored. ALL_RECORDS_FOR_TABLES—For tables with no applied filters, all records will be copied to the child geodatabase. This option will override the expand_feature_classes_and_tables parameter value.SCHEMA_ONLY_FOR_TABLES— For tables with no applied filters, only the schema will be copied to the child geodatabase. Tables with applied filters will be honored. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.CreateReplicaFromServer(in_geodataservice, datasets, in_type, out_geodatabase, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, {archiving}, {all_records_for_tables})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data/MySDEdata.sde"
arcpy.management.CreateReplicaFromServer(
        "C:/MyServerConn/RoadMap.GeoDataServer", "Roads", "TWO_WAY_REPLICA", 
        arcpy.env.workspace, "MajorRoads_replica", "FULL", "CHILD_DATA_SENDER", 
        "USE_DEFAULTS", "DO_NOT_REUSE", "GET_RELATED")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data/MySDEdata.sde"
arcpy.management.CreateReplicaFromServer(
        "C:/MyServerConn/RoadMap.GeoDataServer", "Roads", "TWO_WAY_REPLICA", 
        arcpy.env.workspace, "MajorRoads_replica", "FULL", "CHILD_DATA_SENDER", 
        "USE_DEFAULTS", "DO_NOT_REUSE", "GET_RELATED")
```

### Example 4

```python
# Name: CreateReplicaFromServer_Example2.py
# Description: Creates a two-way replica from a geodata service

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
gisServer = "C:/MyServerConn/RoadMap.GeodataServer"
in_datasets = "Roads; Streets"
replica_type = "TWO_WAY_REPLICA"
out_workspace = env.workspace
replica_name = "MajorRoads_replica"
access_type = "FULL"
initial_sender = "CHILD_DATA_SENDER"
expand = "USE_DEFAULTS"
reUse = "DO_NOT_REUSE"
related = "GET_RELATED"
replica_geometry = "LA_County"
archiving = "DO_NOT_USE_ARCHIVING"

# Run CreateReplicaFromServer
arcpy.management.CreateReplicaFromServer(
        gisServer, in_datasets, replica_type, out_workspace, replica_name, 
        access_type, initial_sender, expand, reUse, related, replica_geometry, 
        archiving)
```

### Example 5

```python
# Name: CreateReplicaFromServer_Example2.py
# Description: Creates a two-way replica from a geodata service

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MySDEdata.sde"

# Set local variables
gisServer = "C:/MyServerConn/RoadMap.GeodataServer"
in_datasets = "Roads; Streets"
replica_type = "TWO_WAY_REPLICA"
out_workspace = env.workspace
replica_name = "MajorRoads_replica"
access_type = "FULL"
initial_sender = "CHILD_DATA_SENDER"
expand = "USE_DEFAULTS"
reUse = "DO_NOT_REUSE"
related = "GET_RELATED"
replica_geometry = "LA_County"
archiving = "DO_NOT_USE_ARCHIVING"

# Run CreateReplicaFromServer
arcpy.management.CreateReplicaFromServer(
        gisServer, in_datasets, replica_type, out_workspace, replica_name, 
        access_type, initial_sender, expand, reUse, related, replica_geometry, 
        archiving)
```

---

## Create Replica (Data Management)

## Summary

Creates a replica in a geodatabase from a specified list of feature classes, layers, datasets, and tables in an enterprise geodatabase.

## Usage

- All datasets must be from the same enterprise geodatabase.
- The data that will be replicated must be registered as traditional versioned but not with the option to move edits to base.
- The connected database user must also have write permissions to the data.
- For two-way and both types of one-way replicas, all datasets must have a GlobalID column.
- For check-out and one-way replicas, the child replica geodatabase can be an enterprise or file geodatabase.
- For two-way and one-way child-to-parent replicas, the child geodatabase must be an enterprise geodatabase.
- To use archiving for one-way replicas, the parent workspace must be connected to the default version. For one-way child-to-parent replicas, the child workspace must be connected to the default version.
- You can replicate all of the data in the datasets or replicate subsets of the data. You can specify the subsets of data to be replicated in the following ways:Use definition queries on the data.Specify an extent using the geoprocessing Extent environment setting.Use the Replica Geometry Features parameter in this tool.
- Use definition queries on the data.
- Specify an extent using the geoprocessing Extent environment setting.
- Use the Replica Geometry Features parameter in this tool.
- For tables, the default filter is Schema Only; only the schema for the table will be replicated. To apply a filter to a table, first create a table view with the desired filters. Then use this as input to the Create Replica tool. See Make Table View for more information. For more information about filters and replication, see Prepare data for replication.
- You can use the Replica Geometry Features parameter or the Extent environment setting to define the replica geometry.If a value is provided for the Replica Geometry Features parameter, it will be used as the replica geometry.If no value is provided for the Replica Geometry Features parameter, the Extent environment will be used as the replica geometry. If neither the Replica Geometry Features parameter value nor the Extent environment is specified, the full extent of the data will be used.
- If a value is provided for the Replica Geometry Features parameter, it will be used as the replica geometry.
- If no value is provided for the Replica Geometry Features parameter, the Extent environment will be used as the replica geometry.
- If neither the Replica Geometry Features parameter value nor the Extent environment is specified, the full extent of the data will be used.
- The replica geometry features can be points, lines, or polygons.
- A feature layer used for the replica geometry features can contain one or more features. If there is more than one, the geometries are merged, and only data that intersects the merged geometries will be replicated.
- If filters (such as selection or definition query) have been defined on the replica geometry features, only features that satisfy these filters will be used to define the replica geometry. See Prepare data for replication for more information.
- The Re-use Schema parameter options are only available for checkout replicas.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Replica Datasets | The data to be replicated. This list consists of layers and tables referencing versioned, editable data from an enterprise geodatabase. | Table View; Dataset |
| Replica Type | Specifies the type of replica that will be created.Two way replica— Changes will be sent between child and parent replicas in both directions.One way replica—Changes will be sent from the parent replica to the child replica only.Check out replica—Data will be replicated, edited, and checked back in one time.One way child to parent replica—Changes will be sent from the child replica to the parent replica only. | String |
| Geodatabase to replicate data to(Optional) | The geodatabase that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an enterprise or file geodatabase. For two-way and one-way-child-to-parent replicas, the child geodatabase must be an enterprise geodatabase. For one-way and checkout replicas, the geodatabase can be a file or enterprise geodatabase. This parameter is required if the Output Type parameter is set to Geodatabase. | Workspace; GeoDataServer |
| Replica Name | The name that identifies the replica. | String |
| Replica Access Type(Optional) | Specifies the type of replica access.Full—Complex types such as topologies, are supported and the data must be versioned. This is the default.Simple—The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in topologies) will be converted to simple features (for example, point, line, and polygon feature classes). | String |
| Initial Data Sender(Optional) | Specifies which replica will send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica will not send updates until the changes are first received from the initial data sender.Child data sender—The child replica will be the initial data sender. This is the default.Parent data sender—The parent replica will be the initial data sender. | String |
| Expand Feature Classes and Tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.Use defaults—The expanded feature classes and tables related to the feature classes and tables in the replica will be added. The default for feature classes is to replicate all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to replicate the schema only. This is the default.Add with schema only—Only the schema for the expanded feature classes and tables will be added.All rows—All rows for expanded feature classes and tables will be added.Do not add—No expanded feature classes or tables will be added. | String |
| Re-use Schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be replicated will be reused. This reduces the amount of time required to replicate the data. This parameter is only available for checkout replicas.Do not reuse—Schema will not be reused. This is the default.Reuse—Schema will be used. | String |
| Replicate Related Data(Optional) | Specifies whether rows related to rows existing in the replica will be replicated. For example, a feature (f1) is inside the replication filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the replica if you choose to get related data.Do not get related—Related data will not be replicated.Get related—Related data will be replicated. This is the default. | String |
| Replica Geometry Features(Optional) | The features that will be used to define the area to replicate. | Feature Layer |
| Use archiving to track changes for 1 way replication (Optional) | Specifies whether the archive class will be used to track changes instead of the versioning delta tables. This is only available for one-way replicas.Archiving—Archiving will be used to track changes.Do not use archiving—Archiving will not be used to track changes. This is the default. | Boolean |
| Register existing data only (Optional) | Specifies whether existing data in the child geodatabase will be used to register the replica datasets. The datasets in the child geodatabase must have the same names as the datasets in the parent database and be owned by the user that is connected to the child geodatabase.Checked—Existing data in the child geodatabase will be used to register the replica.Unchecked—Data in the parent geodatabase will be copied to the child geodatabase. This is the default. | Boolean |
| Output Type (Optional) | Specifies the output type of the data that will be replicated. Geodatabase—The data will be replicated to an existing geodatabase. This is the default.Xml file—The data will be replicated to an XML workspace document.New file geodatabase—The data will be replicated to a new file geodatabase that will be created when the tool is run. Provide the location and name of the new file geodatabase in the File geodatabase location parameter. This option is only valid for one-way and checkout replicas. | String |
| XML file to replicate data to (Optional) | The name and location of the .xml file that will be created. This parameter is required if the Output Type parameter is set to Xml file. | File |
| All records for tables (Optional) | Specifies whether all records or only the schema will be copied to the child geodatabase for tables that do not have filters applied (such as selections or definition queries).Tables with applied filters will be honored. Checked—For tables with no applied filters, all records will be copied to the child geodatabase for tables. This option will override the Expand Feature Classes and Tables parameter value.Unchecked—For tables with no applied filters, only the schema will be copied to the child geodatabase for tables. Tables with applied filters will be honored. This is the default. | Boolean |
| File geodatabase location(Optional) | The location of the file geodatabase that will be created to host the child replica. This parameter is required if the Output Type parameter is set to New file geodatabase and is only valid for one-way and checkout replicas. | Folder |
| File geodatabase name(Optional) | The name of the file geodatabase that will be created to host the child replica. This parameter is required if the Output Type parameter is set to New file geodatabase and is only valid for one-way and checkout replicas. | String |
| in_data[in_data,...] | The data to be replicated. This list consists of layers and tables referencing versioned, editable data from an enterprise geodatabase. | Table View; Dataset |
| in_type | Specifies the type of replica that will be created.TWO_WAY_REPLICA— Changes will be sent between child and parent replicas in both directions.ONE_WAY_REPLICA—Changes will be sent from the parent replica to the child replica only.CHECK_OUT—Data will be replicated, edited, and checked back in one time.ONE_WAY_CHILD_TO_PARENT_REPLICA—Changes will be sent from the child replica to the parent replica only. | String |
| out_geodatabase(Optional) | The geodatabase that will host the child replica. Geodata services are used to represent remote geodatabases. The geodatabase can be an enterprise or file geodatabase. For two-way and one-way-child-to-parent replicas, the child geodatabase must be an enterprise geodatabase. For one-way and checkout replicas, the geodatabase can be a file or enterprise geodatabase. This parameter is required if the out_type parameter is set to GEODATABASE. | Workspace; GeoDataServer |
| out_name | The name that identifies the replica. | String |
| access_type(Optional) | Specifies the type of replica access.FULL—Complex types such as topologies, are supported and the data must be versioned. This is the default.SIMPLE—The data on the child is not versioned and must be simple. This allows the replica to be interoperable. Nonsimple features in the parent (for example, features in topologies) will be converted to simple features (for example, point, line, and polygon feature classes). | String |
| initial_data_sender(Optional) | Specifies which replica will send changes when in disconnected mode. If you are working in a connected mode, this parameter is inconsequential. This ensures that the relative replica will not send updates until the changes are first received from the initial data sender.CHILD_DATA_SENDER—The child replica will be the initial data sender. This is the default.PARENT_DATA_SENDER—The parent replica will be the initial data sender. | String |
| expand_feature_classes_and_tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.USE_DEFAULTS—The expanded feature classes and tables related to the feature classes and tables in the replica will be added. The default for feature classes is to replicate all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to replicate the schema only. This is the default.ADD_WITH_SCHEMA_ONLY—Only the schema for the expanded feature classes and tables will be added.ALL_ROWS—All rows for expanded feature classes and tables will be added.DO_NOT_ADD—No expanded feature classes or tables will be added. | String |
| reuse_schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be replicated will be reused. This reduces the amount of time required to replicate the data. This parameter is only available for checkout replicas.DO_NOT_REUSE—Schema will not be reused. This is the default.REUSE—Schema will be used. | String |
| get_related_data(Optional) | Specifies whether rows related to rows existing in the replica will be replicated. For example, a feature (f1) is inside the replication filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the replica if you choose to get related data.DO_NOT_GET_RELATED—Related data will not be replicated.GET_RELATED—Related data will be replicated. This is the default. | String |
| geometry_features(Optional) | The features that will be used to define the area to replicate. | Feature Layer |
| archiving(Optional) | Specifies whether the archive class will be used to track changes instead of the versioning delta tables. This is only available for one-way replicas.ARCHIVING—Archiving will be used to track changes.DO_NOT_USE_ARCHIVING—Archiving will not be used to track changes. This is the default. | Boolean |
| register_existing_data(Optional) | Specifies whether existing data in the child geodatabase will be used to register the replica datasets. The datasets in the child geodatabase must have the same names as the datasets in the parent geodatabase.REGISTER_EXISTING_DATA—Existing data in the child geodatabase will be used to register the replica.DO_NOT_USE_REGISTER_EXISTING_DATA—Data in the parent geodatabase will be copied to the child geodatabase. This is the default. | Boolean |
| out_type(Optional) | Specifies the output type of the data that will be replicated. GEODATABASE—The data will be replicated to an existing geodatabase. This is the default.XML_FILE—The data will be replicated to an XML workspace document.NEW_FILE_GEODATABASE—The data will be replicated to a new file geodatabase that will be created when the tool is run. Provide the location and name of the new file geodatabase in the out_filegdb_folder_path parameter. This option is only valid for one-way and checkout replicas. | String |
| out_xml(Optional) | The name and location of the .xml file that will be created. This parameter is required if the out_type parameter is set to XML_FILE. | File |
| all_records_for_tables(Optional) | Specifies whether all records or only the schema will be copied to the child geodatabase for tables that do not have filters applied (such as selections or definition queries).Tables with applied filters will be honored. ALL_RECORDS_FOR_TABLES—For tables with no applied filters, all records will be copied to the child geodatabase. This option will override the expand_feature_classes_and_tables parameter value.SCHEMA_ONLY_FOR_TABLES— For tables with no applied filters, only the schema will be copied to the child geodatabase. Tables with applied filters will be honored. This is the default. | Boolean |
| out_filegdb_folder_path(Optional) | The location of the file geodatabase that will be created to host the child replica. This parameter is required if the out_type parameter is set to NEW_FILE_GEODATABASE and is only valid for one-way and checkout replicas. | Folder |
| out_filegdb_name(Optional) | The name of the file geodatabase that will be created to host the child replica. This parameter is required if the out_type parameter is set to NEW_FILE_GEODATABASE and is only valid for one-way and checkout replicas. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateReplica(in_data, in_type, {out_geodatabase}, out_name, {access_type}, {initial_data_sender}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {geometry_features}, {archiving}, {register_existing_data}, {out_type}, {out_xml}, {all_records_for_tables}, {out_filegdb_folder_path}, {out_filegdb_name})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data/MyData.sde"
arcpy.management.CreateReplica("roads", "ONE_WAY_REPLICA", 
                               "C:\Data\MyTargetGDB.gdb", "MyReplica", "FULL", 
                               "PARENT_DATA_SENDER", "USE_DEFAULTS", 
                               "DO_NOT_REUSE", "GET_RELATED")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data/MyData.sde"
arcpy.management.CreateReplica("roads", "ONE_WAY_REPLICA", 
                               "C:\Data\MyTargetGDB.gdb", "MyReplica", "FULL", 
                               "PARENT_DATA_SENDER", "USE_DEFAULTS", 
                               "DO_NOT_REUSE", "GET_RELATED")
```

### Example 4

```python
# Name: CreateReplica_Example2.py
# Description: Create a one-way replica of a Feature Dataset to a file geodatabase. 

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MyData.sde"

# Set local variables
in_data = "Parks" # a feature dataset
replica_type = "ONE_WAY_REPLICA"
output_workspace = "C:\Data\MyTargetGDB.gdb"
replica_name = "MyReplica"
access_type = "FULL"
initial_sender = "PARENT_DATA_SENDER"
expand = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related = "GET_RELATED"
replica_geometry = "LA_County"
archiving = "DO_NOT_USE_ARCHIVING"

# Run CreateReplica
arcpy.management.CreateReplica(in_data, replica_type, output_workspace, 
                               replica_name, access_type, initial_sender, 
                               expand, reuse_schema, get_related, 
                               replica_geometry, archiving)
```

### Example 5

```python
# Name: CreateReplica_Example2.py
# Description: Create a one-way replica of a Feature Dataset to a file geodatabase. 

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data/MyData.sde"

# Set local variables
in_data = "Parks" # a feature dataset
replica_type = "ONE_WAY_REPLICA"
output_workspace = "C:\Data\MyTargetGDB.gdb"
replica_name = "MyReplica"
access_type = "FULL"
initial_sender = "PARENT_DATA_SENDER"
expand = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related = "GET_RELATED"
replica_geometry = "LA_County"
archiving = "DO_NOT_USE_ARCHIVING"

# Run CreateReplica
arcpy.management.CreateReplica(in_data, replica_type, output_workspace, 
                               replica_name, access_type, initial_sender, 
                               expand, reuse_schema, get_related, 
                               replica_geometry, archiving)
```

---

## Create Role (Data Management)

## Summary

Creates a database role, allowing you to add users to or remove them from the role.

## Usage

- This tool can be used with Oracle, Microsoft SQL Server, or PostgreSQL only.
- You cannot use delimiters, such as double quotation marks, when specifying the role. The role can only contain characters supported by the underlying database management system when provided without delimiters.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The connection file to a database or enterprise geodatabase. Connect as a database administrator user. | Workspace |
| Role | The name of the database role to create. If it's an existing role, type the name for the role you want to add users to or remove them from. | String |
| Grant To or Revoke From User(s) (Optional) | Specifies whether the role will be added to a user or list of users or a user or list of users will be removed from the role.Grant role—The role will be granted to the specified user or users, making them a member of the role. This is the default.Revoke role—The role will be revoked from the specified user or users, removing them from the role. | String |
| User Name(s) (Optional) | The name of the user whose role membership will change. To specify multiple users, type the user names separated by commas (no spaces). | String |
| input_database | The connection file to a database or enterprise geodatabase. Connect as a database administrator user. | Workspace |
| role | The name of the database role to create. If it's an existing role, type the name for the role you want to add users to or remove them from. | String |
| grant_revoke(Optional) | Specifies whether the role will be added to a user or list of users or a user or list of users will be removed from the role.GRANT—The role will be granted to the specified user or users, making them a member of the role. This is the default.REVOKE—The role will be revoked from the specified user or users, removing them from the role. | String |
| user_name(Optional) | The name of the user whose role membership will change. To specify multiple users, type the user names separated by commas (no spaces). | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateRole(input_database, role, {grant_revoke}, {user_name})
```

### Example 2

```python
import arcpy
arcpy.CreateRole_management("C:\\gdbconnections\\gdb_oracle.sde", "editors")
```

### Example 3

```python
import arcpy
arcpy.CreateRole_management("C:\\gdbconnections\\gdb_oracle.sde", "editors")
```

### Example 4

```python
import arcpy
arcpy.CreateRole_management("C:\\dbconnections\\db_postg.sde", "drafters", 
                            "GRANT", "eng1,eng2")
```

### Example 5

```python
import arcpy
arcpy.CreateRole_management("C:\\dbconnections\\db_postg.sde", "drafters", 
                            "GRANT", "eng1,eng2")
```

### Example 6

```python
import arcpy
arcpy.CreateRole_management("C:\\connectionfiles\\db_ss.sde", "readers", 
                            "GRANT", "auditor")
```

### Example 7

```python
import arcpy
arcpy.CreateRole_management("C:\\connectionfiles\\db_ss.sde", "readers", 
                            "GRANT", "auditor")
```

### Example 8

```python
import arcpy
arcpy.CreateRole_management("D:\\myconnectionfiles\\mygdb.sde", "creators", 
                            "REVOKE", "intern1,intern2,intern3")
```

### Example 9

```python
import arcpy
arcpy.CreateRole_management("D:\\myconnectionfiles\\mygdb.sde", "creators", 
                            "REVOKE", "intern1,intern2,intern3")
```

### Example 10

```python
"""
Name: create_database_role.py
Description: Provide connection information to a database user.
Type create_database_role.py -h or create_database_role.py --help for usage
Author: Esri
"""

# Import system modules
import arcpy, os, optparse, sys

# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 release")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")                   
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("-o", dest="operation", type ="choice", choices=['GRANT', 'REVOKE'], default='GRANT', help="Specify which operation to perform: grant to or revoke from the user(s). Options (case-sensitive):  GRANT, REVOKE.  Default=GRANT")
parser.add_option ("-r", dest="role", type="string", default="", help="role to be granted to the user")
parser.add_option ("-u", dest="userlist", type="string", default="", help="List of users to grant to or revoke from, separated by comma.")
# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print "%s: error: %s\n" % (sys.argv[0], "No command options given")
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	userlist = options.userlist
	role = options.role
	operation = options.operation

	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print " \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified.")
		parser.print_help()
		sys.exit(3)		
		
	if (role == ""):
		print " \n%s: error: \n%s\n" % (sys.argv[0], "Role name (-r) must be specified.")
		parser.print_help()
		sys.exit(3)			
	
	if(database_type == "SQL_SERVER"):
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print "\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication")
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print "\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication...")	
	else:				
		if( dbms_admin == ""):
			print "\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!")
			sys.exit(3)

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print "\n" + product_license + " license found!" + " Creating a role in an enterprise geodatabase or database requires an ArcGIS for Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS for Server license."
		sys.exit("Re-authorize ArcGIS before creating enterprise geodatabase.")
	else:
		print "\n" + product_license + " license available!  Continuing to create..."
		arcpy.AddMessage("+++++++++")

	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + dbms_admin   

	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  

	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)

	try:
		print "\nCreating Database Connection File...\n"	
		# Process: Create Database Connection File...
		# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, account_authentication, username, password, database, save_username_password(must be true)
		arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=dbms_admin, password=dbms_admin_pwd, save_user_pass="TRUE")
	        for i in range(arcpy.GetMessageCount()):
			if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("\n+++++++++")
				arcpy.AddMessage("Exiting!!")
				arcpy.AddMessage("+++++++++\n")
				sys.exit(3)            
			else:
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("+++++++++\n")

		print "Creating database role...\n"
		arcpy.CreateRole_management(input_database=Connection_File_Name_full_path, grant_revoke=operation, role=role,  user_name=userlist)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print "\n"
		parser.print_help()   
		parser.exit(2)
```

### Example 11

```python
"""
Name: create_database_role.py
Description: Provide connection information to a database user.
Type create_database_role.py -h or create_database_role.py --help for usage
Author: Esri
"""

# Import system modules
import arcpy, os, optparse, sys

# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 1.0 for 10.1 release")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', ''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")                   
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")
parser.add_option ("--auth", dest="Account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-U", dest="Dbms_admin", type="string", default="", help="DBMS administrator user")
parser.add_option ("-P", dest="Dbms_admin_pwd", type="string", default="", help="DBMS administrator password")
parser.add_option ("-o", dest="operation", type ="choice", choices=['GRANT', 'REVOKE'], default='GRANT', help="Specify which operation to perform: grant to or revoke from the user(s). Options (case-sensitive):  GRANT, REVOKE.  Default=GRANT")
parser.add_option ("-r", dest="role", type="string", default="", help="role to be granted to the user")
parser.add_option ("-u", dest="userlist", type="string", default="", help="List of users to grant to or revoke from, separated by comma.")
# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print "%s: error: %s\n" % (sys.argv[0], "No command options given")
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection
	database_type = options.Database_type.upper()
	instance = options.Instance
	database = options.Database.lower()	
	account_authentication = options.Account_authentication.upper()
	dbms_admin = options.Dbms_admin
	dbms_admin_pwd = options.Dbms_admin_pwd
	userlist = options.userlist
	role = options.role
	operation = options.operation

	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	if( database_type ==""):	
		print " \n%s: error: \n%s\n" % (sys.argv[0], "DBMS type (--DBMS) must be specified.")
		parser.print_help()
		sys.exit(3)		
		
	if (role == ""):
		print " \n%s: error: \n%s\n" % (sys.argv[0], "Role name (-r) must be specified.")
		parser.print_help()
		sys.exit(3)			
	
	if(database_type == "SQL_SERVER"):
		if( account_authentication == "DATABASE_AUTH" and dbms_admin == ""):
			print "\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified with database authentication")
			sys.exit(3)
		if( account_authentication == "OPERATING_SYSTEM_AUTH" and dbms_admin != ""):
			print "\nWarning: %s\n" % ("Ignoring DBMS administrator specified when using operating system authentication...")	
	else:				
		if( dbms_admin == ""):
			print "\n%s: error: %s\n" % (sys.argv[0], "DBMS administrator must be specified!")
			sys.exit(3)

	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print "\n" + product_license + " license found!" + " Creating a role in an enterprise geodatabase or database requires an ArcGIS for Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS for Server license."
		sys.exit("Re-authorize ArcGIS before creating enterprise geodatabase.")
	else:
		print "\n" + product_license + " license available!  Continuing to create..."
		arcpy.AddMessage("+++++++++")

	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + dbms_admin   

	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  

	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)

	try:
		print "\nCreating Database Connection File...\n"	
		# Process: Create Database Connection File...
		# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, account_authentication, username, password, database, save_username_password(must be true)
		arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=dbms_admin, password=dbms_admin_pwd, save_user_pass="TRUE")
	        for i in range(arcpy.GetMessageCount()):
			if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("\n+++++++++")
				arcpy.AddMessage("Exiting!!")
				arcpy.AddMessage("+++++++++\n")
				sys.exit(3)            
			else:
				arcpy.AddReturnMessage(i)
				arcpy.AddMessage("+++++++++\n")

		print "Creating database role...\n"
		arcpy.CreateRole_management(input_database=Connection_File_Name_full_path, grant_revoke=operation, role=role,  user_name=userlist)
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		arcpy.AddMessage("+++++++++\n")
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
			
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print "\n"
		parser.print_help()   
		parser.exit(2)
```

---

## Create Scene Layer Package (Data Management)

## Summary

Creates a scene layer package (.slpk file) from 3D points, multipatch features, or LAS data.

## Usage

- The output coordinate system should match the spatial reference of the web scene in which it will be displayed. If the intended display environment will be a global web scene view, the output coordinate system must use GCS WGS84.
- Note the following considerations for how z-coordinates are defined in the 3D data being processed:If the output scene layer package will have x,y coordinates in GCS WGS 84, the z-coordinate system can be defined using any ellipsoidal datum or EGM96 or EGM2008 through the Spatial Reference parameter. If the z-coordinate system is undefined, the vertical units will be in meters.If the output scene layer package will store x,y values in projected coordinates and the z-coordinate system is not defined, the vertical units will be the same as the x,y coordinates. For 3D features with undefined z-units that differ from the x,y units, consider using either the Adjust 3D Z tool to convert z-values to a matching linear unit or the Define Projection tool to define the correct z-datum.
- If the output scene layer package will have x,y coordinates in GCS WGS 84, the z-coordinate system can be defined using any ellipsoidal datum or EGM96 or EGM2008 through the Spatial Reference parameter. If the z-coordinate system is undefined, the vertical units will be in meters.
- If the output scene layer package will store x,y values in projected coordinates and the z-coordinate system is not defined, the vertical units will be the same as the x,y coordinates. For 3D features with undefined z-units that differ from the x,y units, consider using either the Adjust 3D Z tool to convert z-values to a matching linear unit or the Define Projection tool to define the correct z-datum.
- The elevation of multipatch layers must be defined with absolute heights, whereas point layers can be on the ground or offset from the ground.
- When using a feature layer as input, only the fields designated as visible in the layer properties will be preserved in the scene layer package. If certain fields are not needed in the resulting scene layer, consider hiding the unwanted columns.
- All LAS, ZLAS, or LAZ files being processed must have the same spatial reference. If multiple coordinate systems are used by the desired collection of LAS or ZLAS files, consider using the Extract LAS tool to reproject the data to a common spatial reference.
- A LAS, ZLAS, or LAZ file can have its spatial reference defined in its header or by a PRJ file with the same base name that is placed in the same location. The PRJ file will override the information in the header and can be used to rectify missing or invalid spatial reference information. If all of the LAS, ZLAS, or LAZ files being processed have unknown coordinates but use the same spatial reference, only one file will need to have a PRJ defined for the tool to process the collection.If the LAS, ZLAS or LAZ files being processed have missing or incorrect spatial reference information, a PRJ file with the same name can be placed in the same location as the file to override the information in the header. For LAS and ZLAS files, a PRJ file can be generated with the Create LAS Dataset tool.A LAZ file with missing coordinate system information can define one through the Input Coordinate System parameter.
- If the LAS, ZLAS or LAZ files being processed have missing or incorrect spatial reference information, a PRJ file with the same name can be placed in the same location as the file to override the information in the header. For LAS and ZLAS files, a PRJ file can be generated with the Create LAS Dataset tool.
- A LAZ file with missing coordinate system information can define one through the Input Coordinate System parameter.
- The output .slpk file will draw LAS data with undefined or unsupported renderer information with the stretch renderer using elevation values.
- A scene layer package with a file size of less than 1 GB can be uploaded directly in a browser to ArcGIS Online or Portal for ArcGIS. If it's larger than 1 GB, use the Share Package tool to upload it to ArcGIS Online or Portal for ArcGIS 10.3.1 or later.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The 3D points, multipatch feature layers, or LAS data (LAS, ZLAS, or LAZ) that will be used to create a scene layer package. LAS data can also be specified by selecting the parent folder that contains the desired files. | Layer File; Feature Layer; File; Folder; LAS Dataset Layer |
| Output Scene Layer Package | The output scene layer package (.slpk file). | File |
| Attributes to Cache (Optional) | The source data attributes to be included in the scene layer package. These values will be accessible when the content is consumed in other viewers. Select attributes are required for the desired rendering and filtering option (for example, intensity, returns, class codes, and RGB). To reduce storage, unneeded attributes should be excluded.Intensity— The return strength of the laser pulse for each lidar point.RGB—RGB imagery information collected for each lidar point.LAS flags—Classification and scan direction flags.Classification code—Classification code values.Return value—Discrete return number from the lidar pulse.User data—A customizable attribute that can be any number in the range from 0 through 255.Point source ID—For aerial lidar, this value typically identifies the flight path that collected a given lidar point.GPS time— The GPS time stamp at which the laser point was emitted from the aircraft. The time is in GPS seconds of the week.Scan angle—The angular direction of the laser scanner for a given lidar point. This value can range from -90 through 90.Near infrared—Near infrared records collected for each lidar point. | String |
| Output Coordinate System (Optional) | The spatial reference of the output scene layer package. It can be any projected coordinate system or GCS_WGS_1984 but not a custom coordinate system. If a z-datum is defined, the linear unit must match that of the horizontal coordinate system. If the horizontal coordinate system is expressed in geographic coordinates, the z-datum must use meters.A custom coordinate system is not supported for scene layers composed of points or 3D objects. | Spatial Reference |
| Point Size (m) (Optional) | The point size of the lidar data. For airborne lidar data, the default of 0 or a value close to the average point spacing is usually best. For terrestrial lidar data, the point size should match the desired point spacing for the areas of interest. The default of 0 will automatically determine the best value for the input dataset. | Double |
| XY Max Error_(m)(Optional) | The maximum x,y error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. The default is 0.1. | Double |
| Z Max Error (m)(Optional) | The maximum z-error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. The default is 0.1. | Double |
| Geographic Transformation (Optional) | The datum transformation method that will be used when the input layer's spatial reference uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS_1984_3 will work correctly even if the datum conversion is from WGS 1984 to NAD 1927. | String |
| Input Coordinate System (Optional) | The coordinate system of the input LAZ files. This parameter is only used for LAZ files that do not contain spatial reference information in their header or have a PRJ file. | Coordinate System |
| Scene Layer Version (Optional) | The Indexed 3D Scene Layer (I3S) version of the resulting point cloud scene layer package. Specifying a version provides support for backward compatibility and allows scene layer packages to be shared with earlier versions of ArcGIS.1.x—Supported in all ArcGIS clients. This is the default.2.x—Supported in ArcGIS Pro 2.1.2 or later and can be published to ArcGIS Online and ArcGIS Enterprise 10.6.1 or later. | String |
| in_layer | The 3D points, multipatch feature layers, or LAS data (LAS, ZLAS, or LAZ) that will be used to create a scene layer package. LAS data can also be specified by selecting the parent folder that contains the desired files. | Layer File; Feature Layer; File; Folder; LAS Dataset Layer |
| out_slpk | The output scene layer package (.slpk file). | File |
| attributes[attributes,...](Optional) | The source data attributes to be included in the scene layer package. These values will be accessible when the content is consumed in other viewers. Select attributes are required for the desired rendering and filtering option (for example, intensity, returns, class codes, and RGB). To reduce storage, unneeded attributes should be excluded.INTENSITY— The return strength of the laser pulse for each lidar point.RGB—RGB imagery information collected for each lidar point.FLAGS—Classification and scan direction flags.CLASS_CODE—Classification code values.RETURNS—Discrete return number from the lidar pulse.USER_DATA—A customizable attribute that can be any number in the range from 0 through 255.POINT_SRC_ID—For aerial lidar, this value typically identifies the flight path that collected a given lidar point.GPS_TIME— The GPS time stamp at which the laser point was emitted from the aircraft. The time is in GPS seconds of the week.SCAN_ANGLE—The angular direction of the laser scanner for a given lidar point. This value can range from -90 through 90.NEAR_INFRARED—Near infrared records collected for each lidar point. | String |
| spatial_reference(Optional) | The spatial reference of the output scene layer package. It can be any projected coordinate system or GCS_WGS_1984. If a z-datum is defined, the linear unit must match that of the horizontal coordinate system. If the horizontal coordinate system is expressed in geographic coordinates, the z-datum must use meters. GCS_WGS_1984 is the default coordinate system. The spatial reference can be specified by any of the following: Specifying the path to a .prj fileReferencing a geodataset with the desired spatial reference Using an arcpy.SpatialReference object | Spatial Reference |
| point_size_m(Optional) | The point size of the lidar data. For airborne lidar data, the default of 0 or a value close to the average point spacing is usually best. For terrestrial lidar data, the point size should match the desired point spacing for the areas of interest. The default of 0 will automatically determine the best value for the input dataset. | Double |
| xy_max_error_m(Optional) | The maximum x,y error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. The default is 0.1. | Double |
| z_max_error_m(Optional) | The maximum z-error tolerated. A higher tolerance will result in better data compression and more efficient data transfer. The default is 0.1. | Double |
| transform_method[transform_method,...](Optional) | The datum transformation method that will be used when the input layer's spatial reference uses a datum that differs from the output coordinate system. All transformations are bidirectional, regardless of the direction implied by their names. For example, NAD_1927_to_WGS_1984_3 will work correctly even if the datum conversion is from WGS 1984 to NAD 1927. | String |
| in_coordinate_system(Optional) | The coordinate system of the input LAZ files. This parameter is only used for LAZ files that do not contain spatial reference information in their header or have a PRJ file. | Coordinate System |
| scene_layer_version(Optional) | The Indexed 3D Scene Layer (I3S) version of the resulting point cloud scene layer package. Specifying a version provides support for backward compatibility and allows scene layer packages to be shared with earlier versions of ArcGIS.1.X—Supported in all ArcGIS clients. This is the default.2.X—Supported in ArcGIS Pro 2.1.2 or later and can be published to ArcGIS Online and ArcGIS Enterprise 10.6.1 or later. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateSceneLayerPackage(in_layer, out_slpk, {attributes}, {spatial_reference}, {point_size_m}, {xy_max_error_m}, {z_max_error_m}, {transform_method}, {in_coordinate_system}, {scene_layer_version})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'c:/gis_data'

arcpy.management.CreateSceneLayerPackage(
    "Milan.lyrx", "Milan.slpk", 
    ["INTENSITY", "RGB", "CLASS_CODE", "FLAGS", "RETURNS"], 
    arcpy.SpatialReference(4326), 0, 0.1, 0.1, 
    ['ITRF_2000_To_WGS_1984 + WGS_1984_To_WGS_1984_EGM2008_2.5x2.5_Height'], 
    arcpy.SpatialReference(104124), '2.X')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'c:/gis_data'

arcpy.management.CreateSceneLayerPackage(
    "Milan.lyrx", "Milan.slpk", 
    ["INTENSITY", "RGB", "CLASS_CODE", "FLAGS", "RETURNS"], 
    arcpy.SpatialReference(4326), 0, 0.1, 0.1, 
    ['ITRF_2000_To_WGS_1984 + WGS_1984_To_WGS_1984_EGM2008_2.5x2.5_Height'], 
    arcpy.SpatialReference(104124), '2.X')
```

---

## Create Spatial Reference (Data Management)

## Summary

Creates a spatial reference for use in ModelBuilder.

## Usage

- You can create a spatial reference with a set coordinate system, spatial domains, and precision. The spatial domains and precision of the output spatial reference can be further modified using the XY Domain, Z Domain, M Domain, Template XYDomains, and Grow XYDomain By Percentage parameters.
- XY, Z, and M extents are not the same as spatial reference domains. The XY, Z, and M domains in a spatial reference define the valid range of coordinate values that can be stored in a feature class. The feature class extents reflect the actual range of coordinate values that exist in the feature class. These extents cannot be larger than the domains.
- The Template XYDomains parameter does not have to be in the same coordinate system as that specified in Spatial Reference or Spatial Reference Template. If they are different, the extents will be projected to match.
- If the Spatial Reference and Spatial Reference Template parameters are both set, the Spatial Reference parameter will take priority.
- All the parameters of the tool are optional. If no parameters are specified, the spatial reference will be defined as Unknown, and the XY Domain will assume standard defaults.
- In ModelBuilder, the output of this tool can be used as input to tools with a spatial reference parameter (for example, Create Feature Class, Create Feature Dataset, and Make XY Event Layer).
- In Python, the SpatialReference class can also be used to create a spatial reference.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Spatial Reference(Optional) | The name of the spatial reference to be created. | Spatial Reference |
| Spatial Reference Template(Optional) | The feature class or layer to be used as a template to set the value for the spatial reference. | Feature Layer; Raster Dataset |
| XY Domain(Optional) | The allowable coordinate range for x,y coordinates. | Envelope |
| Z Domain (min max)(Optional) | The allowable coordinate range for z-values. | String |
| M Domain (min max)(Optional) | The allowable coordinate range for m-values. | String |
| Template XYDomains(Optional) | The feature classes or layers that can be used to define the XY Domain. | Feature Layer |
| Grow XYDomain By Percentage(Optional) | The percentage by which the XY Domain will be expanded. | Double |
| spatial_reference(Optional) | The name of the spatial reference to be created. | Spatial Reference |
| spatial_reference_template(Optional) | The feature class or layer to be used as a template to set the value for the spatial reference. | Feature Layer; Raster Dataset |
| xy_domain(Optional) | The allowable coordinate range for x,y coordinates. | Envelope |
| z_domain(Optional) | The allowable coordinate range for z-values. | String |
| m_domain(Optional) | The allowable coordinate range for m-values. | String |
| template[template,...](Optional) | The feature classes or layers that can be used to define the XY Domain. | Feature Layer |
| expand_ratio(Optional) | The percentage by which the XY Domain will be expanded. | Double |

## Code Samples

### Example 1

```python
arcpy.management.CreateSpatialReference({spatial_reference}, {spatial_reference_template}, {xy_domain}, {z_domain}, {m_domain}, {template}, {expand_ratio})
```

### Example 2

```python
# This script reprojects a shapefile in Redlands folder
# from NAD 1983 UTM Zone 11N
# to NAD 1983 StatePlane California V FIPS 0405 (US Feet)

# import system modules
import arcpy
 
try:
    # set the workspace environment
    arcpy.env.workspace = r"C:\data\Redlands"

    # create a spatial reference object to be used as output coordinate system
    out_sr = arcpy.CreateSpatialReference_management("NAD 1983 StatePlane California V FIPS 0405 (US Feet)")

    # use the output of CreateSpatialReference as input to Project tool
    # to reproject the shapefile
    arcpy.Project_management("citylimit_Project1.shp", "city_CA_FIPS0405", out_sr)

except arcpy.ExecuteError:
    # print geoprocessing message
    print(arcpy.GetMessages(2))
          
except Exception as ex:
    # print the exception message
    print(ex.args[0])
```

### Example 3

```python
# This script reprojects a shapefile in Redlands folder
# from NAD 1983 UTM Zone 11N
# to NAD 1983 StatePlane California V FIPS 0405 (US Feet)

# import system modules
import arcpy
 
try:
    # set the workspace environment
    arcpy.env.workspace = r"C:\data\Redlands"

    # create a spatial reference object to be used as output coordinate system
    out_sr = arcpy.CreateSpatialReference_management("NAD 1983 StatePlane California V FIPS 0405 (US Feet)")

    # use the output of CreateSpatialReference as input to Project tool
    # to reproject the shapefile
    arcpy.Project_management("citylimit_Project1.shp", "city_CA_FIPS0405", out_sr)

except arcpy.ExecuteError:
    # print geoprocessing message
    print(arcpy.GetMessages(2))
          
except Exception as ex:
    # print the exception message
    print(ex.args[0])
```

---

## Create Spatial Sampling Locations (Data Management)

## Summary

Creates sample locations within a continuous study area using simple random, stratified, systematic (gridded), or cluster sampling designs.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Study Area | The input study area where sample locations will be created. The study area must be polygons or an integer (categorical) raster. For rasters, cells with null values will not be included in the study area. | Feature Layer; Raster Layer |
| Output Features | The output features representing the sample locations. For simple random and stratified sampling, the output features will be points. For cluster sampling, the output will be polygons. For systematic sampling, the output can be points or polygons. | Feature Class |
| Sampling Method (Optional) | Specifies the sampling method that will be used to create the sample locations.Simple random—Points will be randomly created in the study area, and all locations have the same likelihood of being sampled. All boundaries between individual polygons or raster regions will be ignored. This is the default.Stratified by individual polygon—Each polygon will be a different stratum, and points will be randomly and independently created in each polygon. The input study area must be polygons.Stratified by contiguous raster region—Each region of a categorical raster will be a stratum, and sample points will be randomly and independently created in each region. A raster region is a contiguous block of cells with the same value that are connected by shared cell edges. If two regions have the same value but are not connected by shared edges, they will be different strata. The input study area must be a raster.Stratified by strata ID field—Each polygon or raster region with the same strata ID field value will be a stratum, and sample points will be randomly and independently created in each stratum. The polygons or raster cells are not required to be contiguous to be in the same stratum.Systematic—Sample locations will be created using a gridded tessellation in the study area. The sample locations can be created as polygons or as points (centroids of the tessellated polygons).Cluster—Sample polygons will be created by randomly selecting polygons from a tessellation of the study area. | String |
| Strata ID Field (Optional) | For stratified sampling by strata ID field, the strata ID field defining the strata. | Field |
| Strata Sample Count Allocation Method (Optional) | For stratified sampling, specifies the method that will be used to determine the number of sample locations that will be created in each stratum. Equal count in each stratum—The same number of sample locations will be created in each stratum. Provide the value in the Number of Samples Per Strata parameter. This is the default.Count proportional to stratum area—The number of sample locations in each stratum will be proportional to the area of the stratum. Provide the total number of samples in the Number of Samples parameter.Count equal to population field—The number of sample locations in each stratum will be equal to the values of a population field. Provide the field in the Population Field parameter. This option is not available when stratifying by contiguous raster region.Count proportional to population field—The number of sample locations in each stratum will be proportional to the values of a population field. Provide the field in the Population Field parameter and the total number of samples in the Number of Samples parameter. This option is not available when stratifying by contiguous raster region. | String |
| Bin Shape (Optional) | For systematic and cluster sampling, specifies the shape of the polygons that will be generated in the gridded tessellation. Hexagon—Hexagon-shaped features will be generated. The top and bottom side of each hexagon will be parallel with the x-axis of the coordinate system (the top and bottom are flat).Transverse hexagon—Transverse hexagon-shaped features will be generated. The right and left side of each hexagon will be parallel with the y-axis of the dataset's coordinate system (the top and bottom are pointed).Square—Square-shaped features will be generated. The top and bottom side of each square will be parallel with the x-axis of the coordinate system, and the right and left sides will be parallel with the y-axis of the coordinate system.Diamond—Diamond-shaped features will be generated. The sides of each polygon will be rotated 45 degrees away from the x-axis and y-axis of the coordinate system.Triangle—Triangular-shaped features will be generated. Each triangle will be a regular three-sided equilateral polygon.H3 hexagon—Hexagon-shaped features will be generated based on the H3 Hexagonal hierarchical geospatial indexing system. | String |
| Bin Size [count or area](Optional) | For systematic and cluster sampling, the size of each polygon in the tessellation. The value can be provided as a count (the total number of tessellated polygons created in the study area) or as an area (the area of each tessellated polygon). For count input, the default is 100. For area input, a value must be provided.If a count is provided, the tool will attempt to create the specified number of sample locations. If the exact number cannot be created, a warning will be returned. | Areal Unit; Long |
| H3 Resolution(Optional) | For systematic or cluster sampling with H3 hexagon bins, specifies the H3 resolution of the hexagons.With each increasing resolution value, the area of the polygons will be one seventh the size. 0—Hexagons will be created at the H3 resolution of 0, with an average area of 4,357,449.416078381 square kilometers.1—Hexagons will be created at the H3 resolution of 1, with an average area of 609,788.441794133 square kilometers. 2—Hexagons will be created at the H3 resolution of 2, with an average area of 86,801.780398997 square kilometers.3—Hexagons will be created at the H3 resolution of 3, with an average area of 12,393.434655088 square kilometers.4—Hexagons will be created at the H3 resolution of 4, with an average area of 1,770.347654491 square kilometers.5—Hexagons will be created at the H3 resolution of 5, with an average area of 252.903858182 square kilometers.6—Hexagons will be created at the H3 resolution of 6, with an average area of 36.129062164 square kilometers.7—Hexagons will be created at the H3 resolution of 7, with an average area of 5.161293360 square kilometers. This is the default.8—Hexagons will be created at the H3 resolution of 8, with an average area of 0.737327598 square kilometers.9—Hexagons will be created at the H3 resolution of 9, with an average area of 0.105332513 square kilometers.10—Hexagons will be created at the H3 resolution of 10, with an average area of 0.015047502 square kilometers.11—Hexagons will be created at the H3 resolution of 11, with an average area of 0.002149643 square kilometers.12—Hexagons will be created at the H3 resolution of 12, with an average area of 0.000307092 square kilometers.13—Hexagons will be created at the H3 resolution of 13, with an average area of 0.000043870 square kilometers.14—Hexagons will be created at the H3 resolution of 14, with an average area of 0.000006267 square kilometers.15—Hexagons will be created at the H3 resolution of 15, with an average area of 0.000000895 square kilometers. | Long |
| Number of Samples (Optional) | The number of sample locations that will be created. This parameter always applies to simple random and cluster sampling. For stratified sampling, this parameter applies when the sample count will be proportional to the stratum area or proportional to a population field. For simple random and stratified sampling, the default is 100. For cluster sampling, the default is 10. | Long |
| Number of Samples Per Stratum (Optional) | For stratified sampling with an equal sample count in each stratum, the number of sample locations created within each stratum. The total number of samples will be this value multiplied by the number of strata. The default is 100. | Long |
| Population Field (Optional) | The population field for stratified sampling when the sample count is equal or proportional to a population field. | Field |
| Output Geometry Type (Optional) | For systematic sampling, specifies whether the sample locations will be tessellated polygons or centroids (points) of the tessellated polygons.Point—Centroids of the tessellated polygons will be created as sample locations. This is the default.Polygon—Tessellated polygons will be created as sample locations. | String |
| Minimum Distance Between Sample Points (Optional) | For simple random and stratified sampling, the smallest allowed distance between sample locations. For simple random sampling, all points will be at least this distance apart. For stratified sampling, points within the same stratum will be at least this distance apart, but points in neighboring strata may be closer than this distance.For large distances, fewer sample locations than were expected may be created to keep the locations sufficiently far apart. In this case, a warning message will be returned. | Linear Unit |
| Spatial Relationship (Optional) | Specifies which polygons from a background tessellation will be included as sampling locations. This parameter applies to cluster sampling and to systematic sampling when the output geometry type is polygon.Have their center in—The centroids of the polygons must be within the study area to be included. This is the default.Completely within—The polygons must be completely within the study area to be included.Intersect—The polygons must intersect the study area to be included. | String |
| in_study_area | The input study area where sample locations will be created. The study area must be polygons or an integer (categorical) raster. For rasters, cells with null values will not be included in the study area. | Feature Layer; Raster Layer |
| out_features | The output features representing the sample locations. For simple random and stratified sampling, the output features will be points. For cluster sampling, the output will be polygons. For systematic sampling, the output can be points or polygons. | Feature Class |
| sampling_method(Optional) | Specifies the sampling method that will be used to create the sample locations.RANDOM—Points will be randomly created in the study area, and all locations have the same likelihood of being sampled. All boundaries between individual polygons or raster regions will be ignored. This is the default.STRAT_POLY—Each polygon will be a different stratum, and points will be randomly and independently created in each polygon. The input study area must be polygons.STRAT_RAST—Each region of a categorical raster will be a stratum, and sample points will be randomly and independently created in each region. A raster region is a contiguous block of cells with the same value that are connected by shared cell edges. If two regions have the same value but are not connected by shared edges, they will be different strata. The input study area must be a raster.STRAT_ID—Each polygon or raster region with the same strata ID field value will be a stratum, and sample points will be randomly and independently created in each stratum. The polygons or raster cells are not required to be contiguous to be in the same stratum.SYSTEMATIC—Sample locations will be created using a gridded tessellation in the study area. The sample locations can be created as polygons or as points (centroids of the tessellated polygons).CLUSTER—Sample polygons will be created by randomly selecting polygons from a tessellation of the study area. | String |
| strata_id_field(Optional) | For stratified sampling by strata ID field, the strata ID field defining the strata. | Field |
| strata_count_method(Optional) | For stratified sampling, specifies the method that will be used to determine the number of sample locations that will be created in each stratum. EQUAL—The same number of sample locations will be created in each stratum. Provide the value in the num_samples_per_strata parameter. This is the default.PROP_AREA—The number of sample locations in each stratum will be proportional to the area of the stratum. Provide the total number of samples in the num_samples parameter.FIELD—The number of sample locations in each stratum will be equal to the values of a population field. Provide the field in the population_field parameter. This option is not available when stratifying by contiguous raster region.PROP_FIELD—The number of sample locations in each stratum will be proportional to the values of a population field. Provide the field in the population_field parameter and the total number of samples in the num_samples parameter. This option is not available when stratifying by contiguous raster region. | String |
| bin_shape(Optional) | For systematic and cluster sampling, specifies the shape of the polygons that will be generated in the gridded tessellation. HEXAGON—Hexagon-shaped features will be generated. The top and bottom side of each hexagon will be parallel with the x-axis of the coordinate system (the top and bottom are flat).TRANSVERSE_HEXAGON—Transverse hexagon-shaped features will be generated. The right and left side of each hexagon will be parallel with the y-axis of the dataset's coordinate system (the top and bottom are pointed).SQUARE—Square-shaped features will be generated. The top and bottom side of each square will be parallel with the x-axis of the coordinate system, and the right and left sides will be parallel with the y-axis of the coordinate system.DIAMOND—Diamond-shaped features will be generated. The sides of each polygon will be rotated 45 degrees away from the x-axis and y-axis of the coordinate system.TRIANGLE—Triangular-shaped features will be generated. Each triangle will be a regular three-sided equilateral polygon.H3_HEXAGON—Hexagon-shaped features will be generated based on the H3 Hexagonal hierarchical geospatial indexing system. | String |
| bin_size(Optional) | For systematic and cluster sampling, the size of each polygon in the tessellation. The value can be provided as a count (the total number of tessellated polygons created in the study area) or as an area (the area of each tessellated polygon). For count input, the default is 100. For area input, a value must be provided.If a count is provided, the tool will attempt to create the specified number of sample locations. If the exact number cannot be created, a warning will be returned. | Areal Unit; Long |
| h3_resolution(Optional) | For systematic or cluster sampling with H3 hexagon bins, specifies the H3 resolution of the hexagons.With each increasing resolution value, the area of the polygons will be one seventh the size. 0—Hexagons will be created at the H3 resolution of 0, with an average area of 4,357,449.416078381 square kilometers.1—Hexagons will be created at the H3 resolution of 1, with an average area of 609,788.441794133 square kilometers. 2—Hexagons will be created at the H3 resolution of 2, with an average area of 86,801.780398997 square kilometers.3—Hexagons will be created at the H3 resolution of 3, with an average area of 12,393.434655088 square kilometers.4—Hexagons will be created at the H3 resolution of 4, with an average area of 1,770.347654491 square kilometers.5—Hexagons will be created at the H3 resolution of 5, with an average area of 252.903858182 square kilometers.6—Hexagons will be created at the H3 resolution of 6, with an average area of 36.129062164 square kilometers.7—Hexagons will be created at the H3 resolution of 7, with an average area of 5.161293360 square kilometers. This is the default.8—Hexagons will be created at the H3 resolution of 8, with an average area of 0.737327598 square kilometers.9—Hexagons will be created at the H3 resolution of 9, with an average area of 0.105332513 square kilometers.10—Hexagons will be created at the H3 resolution of 10, with an average area of 0.015047502 square kilometers.11—Hexagons will be created at the H3 resolution of 11, with an average area of 0.002149643 square kilometers.12—Hexagons will be created at the H3 resolution of 12, with an average area of 0.000307092 square kilometers.13—Hexagons will be created at the H3 resolution of 13, with an average area of 0.000043870 square kilometers.14—Hexagons will be created at the H3 resolution of 14, with an average area of 0.000006267 square kilometers.15—Hexagons will be created at the H3 resolution of 15, with an average area of 0.000000895 square kilometers. | Long |
| num_samples(Optional) | The number of sample locations that will be created. This parameter always applies to simple random and cluster sampling. For stratified sampling, this parameter applies when the sample count will be proportional to the stratum area or proportional to a population field. For simple random and stratified sampling, the default is 100. For cluster sampling, the default is 10. | Long |
| num_samples_per_strata(Optional) | For stratified sampling with an equal sample count in each stratum, the number of sample locations created within each stratum. The total number of samples will be this value multiplied by the number of strata. The default is 100. | Long |
| population_field(Optional) | The population field for stratified sampling when the sample count is equal or proportional to a population field. | Field |
| geometry_type(Optional) | For systematic sampling, specifies whether the sample locations will be tessellated polygons or centroids (points) of the tessellated polygons.POINT—Centroids of the tessellated polygons will be created as sample locations. This is the default.POLYGON—Tessellated polygons will be created as sample locations. | String |
| min_distance(Optional) | For simple random and stratified sampling, the smallest allowed distance between sample locations. For simple random sampling, all points will be at least this distance apart. For stratified sampling, points within the same stratum will be at least this distance apart, but points in neighboring strata may be closer than this distance.For large distances, fewer sample locations than were expected may be created to keep the locations sufficiently far apart. In this case, a warning message will be returned. | Linear Unit |
| spatial_relationship(Optional) | Specifies which polygons from a background tessellation will be included as sampling locations. This parameter applies to cluster sampling and to systematic sampling when the output geometry type is polygon.HAVE_THEIR_CENTER_IN—The centroids of the polygons must be within the study area to be included. This is the default.COMPLETELY_WITHIN—The polygons must be completely within the study area to be included.INTERSECT—The polygons must intersect the study area to be included. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateSpatialSamplingLocations(in_study_area, out_features, {sampling_method}, {strata_id_field}, {strata_count_method}, {bin_shape}, {bin_size}, {h3_resolution}, {num_samples}, {num_samples_per_strata}, {population_field}, {geometry_type}, {min_distance}, {spatial_relationship})
```

### Example 2

```python
# Create 50 sampling locations in the dissolved California counties.
import arcpy
arcpy.management.CreateSpatialSamplingLocations(
    in_study_area="CA_counties",
    out_features="outputSamplingLocations"
    sampling_method="RANDOM",
    strata_id_field=None,
    strata_count_method="EQUAL",
    bin_shape="HEXAGON",
    bin_size=None,
    h3_resolution=7,
    num_samples=50,
    num_samples_per_strata=100,
    population_field=None,
    geometry_type="POINT",
    min_distance="15 NauticalMilesInt",
    spatial_relationship = "HAVE_THEIR_CENTER_IN"
)
```

### Example 3

```python
# Create 50 sampling locations in the dissolved California counties.
import arcpy
arcpy.management.CreateSpatialSamplingLocations(
    in_study_area="CA_counties",
    out_features="outputSamplingLocations"
    sampling_method="RANDOM",
    strata_id_field=None,
    strata_count_method="EQUAL",
    bin_shape="HEXAGON",
    bin_size=None,
    h3_resolution=7,
    num_samples=50,
    num_samples_per_strata=100,
    population_field=None,
    geometry_type="POINT",
    min_distance="15 NauticalMilesInt",
    spatial_relationship = "HAVE_THEIR_CENTER_IN"
)
```

### Example 4

```python
# Simple random sampling

# Create 50 sample points in a polygon study area.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SRS"

# Define the sampling method and number of samples. 
samplingMethod = "RANDOM"
numSamples=50

# Define the minimum distance between any two points.
minDistance= "15 NauticalMilesInt"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", "", "", "", "", numSamples, "", "", "", 
                     minDistance)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 5

```python
# Simple random sampling

# Create 50 sample points in a polygon study area.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SRS"

# Define the sampling method and number of samples. 
samplingMethod = "RANDOM"
numSamples=50

# Define the minimum distance between any two points.
minDistance= "15 NauticalMilesInt"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", "", "", "", "", numSamples, "", "", "", 
                     minDistance)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 6

```python
# Stratify by individual polygons

# Create 100 sample points in each polygon.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBIP"

# Define the sampling method.
samplingMethod = "STRAT_POLY"

# Create 100 samples in each polygon.
strataCountMethod = "EQUAL"
numSamplesPerStrata=100

# Define the minimum distance between any two points in the same polygon.
minDistance= "15 Meters"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, "", strataCountMethod, "", "", "", "", 
                     numSamplesPerStrata, "", "", minDistance)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 7

```python
# Stratify by individual polygons

# Create 100 sample points in each polygon.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBIP"

# Define the sampling method.
samplingMethod = "STRAT_POLY"

# Create 100 samples in each polygon.
strataCountMethod = "EQUAL"
numSamplesPerStrata=100

# Define the minimum distance between any two points in the same polygon.
minDistance= "15 Meters"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, "", strataCountMethod, "", "", "", "", 
                     numSamplesPerStrata, "", "", minDistance)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 8

```python
# Stratify by contiguous raster region

# Create 100 points in a raster study area with number of samples in
# each region proportional to the area of the region.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/raster_study_area.tif"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBCRR"

# Define the sampling method.
samplingMethod = "STRAT_RAST"

# Create 100 points and allocate proportionally to the area of the regions.
strataCountMethod = "PROP_AREA"
numSamples=100

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", strataCountMethod, "", "", "", numSamples)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 9

```python
# Stratify by contiguous raster region

# Create 100 points in a raster study area with number of samples in
# each region proportional to the area of the region.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/raster_study_area.tif"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBCRR"

# Define the sampling method.
samplingMethod = "STRAT_RAST"

# Create 100 points and allocate proportionally to the area of the regions.
strataCountMethod = "PROP_AREA"
numSamples=100

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", strataCountMethod, "", "", "", numSamples)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 10

```python
# Stratify by strata ID field

# Create sample points in each land use category of a raster.
# Use a population field to define the number of samples in each category.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/land_use_raster.tif"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBSIDF"

# Define the sampling method.
samplingMethod = "STRAT_ID"

# All raster cells with the same value are in the same stratum.
strataIDField = "LandUse"

# Define the number of samples using a population field.
strataCountMethod = "FIELD"
populationField="Population"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, strataIDField, strataCountMethod, "", "", "", 
                     "", "", populationField)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 11

```python
# Stratify by strata ID field

# Create sample points in each land use category of a raster.
# Use a population field to define the number of samples in each category.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/land_use_raster.tif"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SBSIDF"

# Define the sampling method.
samplingMethod = "STRAT_ID"

# All raster cells with the same value are in the same stratum.
strataIDField = "LandUse"

# Define the number of samples using a population field.
strataCountMethod = "FIELD"
populationField="Population"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, strataIDField, strataCountMethod, "", "", "", 
                     "", "", populationField)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 12

```python
# Systematic sampling

# Create sample points in a hexagonal tessellation in a polygon study area.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SYS"

# Define the sampling method.
samplingMethod = "SYSTEMATIC"

# Create points in a hexagonal tessellation.
binShape = "HEXAGON"
binSize = "10000 SquareFeet"
outputGeometryType = "POINT"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", "", binShape, binSize, "", "", "", "", 
                     outputGeometryType)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 13

```python
# Systematic sampling

# Create sample points in a hexagonal tessellation in a polygon study area.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_SYS"

# Define the sampling method.
samplingMethod = "SYSTEMATIC"

# Create points in a hexagonal tessellation.
binShape = "HEXAGON"
binSize = "10000 SquareFeet"
outputGeometryType = "POINT"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures, 
                     samplingMethod, "", "", binShape, binSize, "", "", "", "", 
                     outputGeometryType)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 14

```python
# Cluster sampling

# Create 100 cluster polygons that are diamond shaped.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_CLUST"

# Define the sampling method.
samplingMethod = "CLUSTER"

# Create a diamond tessellation and randomly choose 100 polygons.
binShape = "DIAMOND"
binSize = "1000000 SquareFeet"
numSamples=100
spatialRelationship = "INTERSECT"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, "", "", binShape, binSize, "", numSamples, "",
                     "", "", "", spatialRelationship)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 15

```python
# Cluster sampling

# Create 100 cluster polygons that are diamond shaped.

# Import system modules.
import arcpy

# Allow overwriting output.
arcpy.env.overwriteOutput = True

# Define the study area and output features.
inputStudyArea = "C:/samplingdata/inputs.gdb/study_area_polygons"
outputFeatures = "C:/samplingdata/outputs.gdb/out_samples_CLUST"

# Define the sampling method.
samplingMethod = "CLUSTER"

# Create a diamond tessellation and randomly choose 100 polygons.
binShape = "DIAMOND"
binSize = "1000000 SquareFeet"
numSamples=100
spatialRelationship = "INTERSECT"

# Run tool.
try:
    arcpy.management.CreateSpatialSamplingLocations(inputStudyArea, outputFeatures,
                     samplingMethod, "", "", binShape, binSize, "", numSamples, "",
                     "", "", "", spatialRelationship)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

---

## Create Spatial Type (Data Management)

## Summary

Adds the ST_Geometry SQL type, subtypes, and functions to an Oracle or a PostgreSQL database. This allows you to use the ST_Geometry SQL type to store geometries in a database that does not contain a geodatabase. You can also use this tool to upgrade the existing ST_Geometry type, subtypes, and functions in an Oracle or a PostgreSQL database.

## Usage

- This tool is supported in Oracle databases.
- This tool is supported in PostgreSQL databases and geodatabases in PostgreSQL that were created to use PostGIS spatial types only.
- This tool is not supported in database service offerings.
- There are preparatory steps you must complete before you run the Create Spatial Type tool. For instructions on running this tool, including the preparatory steps, see the topic appropriate to your database management system:Add the ST_Geometry type to an Oracle databaseAdd the ST_Geometry type to a PostgreSQL databaseFor instructions on upgrading the ST_Geometry type, see either Upgrade the ST_Geometry type in an Oracle database or Upgrade the ST_Geometry type in a PostgreSQL database.
- Add the ST_Geometry type to an Oracle database
- Add the ST_Geometry type to a PostgreSQL database

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The input_database is the database connection file (.sde) that connects to the Oracle or PostgreSQL database. You must connect as a database administrator user; in Oracle, you must connect as the sys user. | Workspace |
| SDE User Password | The password for the sde database user. If the sde user does not exist in the database, it will be created and will use the password you provide. The password policy of the underlying database will be enforced. If the sde user does exist in the database or database cluster, this password must match the existing password. | Encrypted String |
| Tablespace Name (Optional) | The name of a tablespace that will be set as the default tablespace for the sde user in Oracle. If the tablespace name does not exist, it will be created in the Oracle default storage location. If a tablespace with the specified name does exist, it will be set as the sde user's default. | String |
| ST_Geometry Shape Library Path (Optional) | The location on the Oracle server where the st_shape library resides. | File |
| input_database | The input_database is the database connection file (.sde) that connects to the Oracle or PostgreSQL database. You must connect as a database administrator user; in Oracle, you must connect as the sys user. | Workspace |
| sde_user_password | The password for the sde database user. If the sde user does not exist in the database, it will be created and will use the password you provide. The password policy of the underlying database will be enforced. If the sde user does exist in the database or database cluster, this password must match the existing password. | Encrypted String |
| tablespace_name(Optional) | The name of a tablespace that will be set as the default tablespace for the sde user in Oracle. If the tablespace name does not exist, it will be created in the Oracle default storage location. If a tablespace with the specified name does exist, it will be set as the sde user's default. | String |
| st_shape_library_path(Optional) | The location on the Oracle server where the st_shape library resides. | File |

## Code Samples

### Example 1

```python
arcpy.management.CreateSpatialType(input_database, sde_user_password, {tablespace_name}, {st_shape_library_path})
```

### Example 2

```python
import arcpy
arcpy.CreateSpatialType_management(
    "D:/connections/connection_to_db_ora.sde", "ed$pwd", "sdetbsp", 
    r"c:\st_geometry\st_shapelib.dll")
```

### Example 3

```python
import arcpy
arcpy.CreateSpatialType_management(
    "D:/connections/connection_to_db_ora.sde", "ed$pwd", "sdetbsp", 
    r"c:\st_geometry\st_shapelib.dll")
```

### Example 4

```python
import arcpy
arcpy.CreateSpatialType_management(
    "/ragsrh/users/connections/connection_to_sp_pg.sde", "$Upass", "sdetbsp", 
    "/st_geometry/libst_shapelib.so")
```

### Example 5

```python
import arcpy
arcpy.CreateSpatialType_management(
    "/ragsrh/users/connections/connection_to_sp_pg.sde", "$Upass", "sdetbsp", 
    "/st_geometry/libst_shapelib.so")
```

---

## Create Spatially Balanced Points (Data Management)

## Summary

Creates a set of sample points based on inclusion probabilities, resulting in a spatially balanced sample design. This tool is typically used for designing a monitoring network by suggesting locations to take samples, and a preference for particular locations can be defined using an inclusion probability raster.

## Usage

- The input probability raster must contain only values between 0 and 1. The higher the value, the more likely the cell will be included in the sample design.
- All values in the study area should have inclusion probabilities >= 0, while all areas outside the study area should have Null values.
- The cell size of the inclusion probability raster determines the finest resolution at which samples will be generated. In other words, the points the tool creates will always be located at the centers of the raster cells. Using a smaller cell size for the inclusion probability raster will result in more possible locations for the points to be created.
- When point, line, or polygon features are converted to raster (to obtain the input probability raster), the following should be considered: The cell size (resolution) should be fine enough to distinguish all the important features in the population. To accomplish this, the cell size can be set to less than half the minimum distance between features. This distance can be calculated with the Generate Near Table tool. For line and polygon features, the cell size should be set so features (such as meandering streams) are adequately represented in the resulting raster. For example, you may not be able to represent a complex river with a large raster cell size; curves in the river may be smoothed over if the cell size is too large. The precision with which sample locations can be located in the field should also be considered. For example, if locations are to be found using a GPS with a positional accuracy of 10 meters, the cell size should be 10 meters. Be mindful of the size of the inclusion probability raster, because as the number of cells increases, the processing time also increases.
- The cell size (resolution) should be fine enough to distinguish all the important features in the population. To accomplish this, the cell size can be set to less than half the minimum distance between features. This distance can be calculated with the Generate Near Table tool.
- For line and polygon features, the cell size should be set so features (such as meandering streams) are adequately represented in the resulting raster. For example, you may not be able to represent a complex river with a large raster cell size; curves in the river may be smoothed over if the cell size is too large.
- The precision with which sample locations can be located in the field should also be considered. For example, if locations are to be found using a GPS with a positional accuracy of 10 meters, the cell size should be 10 meters.
- Be mindful of the size of the inclusion probability raster, because as the number of cells increases, the processing time also increases.
- To avoid outputs that appear spatially unbalanced, it is recommended that the number of sample locations be less than 1 percent of the number of cells in the inclusion probability raster.
- In the Random number generator environment, only the Mersenne Twister option is supported. If other options are chosen, Mersenne twister will be used instead.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Inclusion Probability Raster | Defines the inclusion probabilities for each location in the area of interest. The location values must range from 0 (low inclusion probability) to 1 (high inclusion probability). | Raster Layer; Mosaic Layer |
| Number of Output Points | The number of sample locations that will be created. | Long |
| Output Point Feature Class | The output feature class containing the selected sample locations and their inclusion probabilities. | Feature Class |
| in_probability_raster | Defines the inclusion probabilities for each location in the area of interest. The location values must range from 0 (low inclusion probability) to 1 (high inclusion probability). | Raster Layer; Mosaic Layer |
| number_output_points | The number of sample locations that will be created. | Long |
| out_feature_class | The output feature class containing the selected sample locations and their inclusion probabilities. | Feature Class |

## Code Samples

### Example 1

```python
arcpy.management.CreateSpatiallyBalancedPoints(in_probability_raster, number_output_points, out_feature_class)
```

### Example 2

```python
import arcpy
arcpy.management.CreateSpatiallyBalancedPoints("ca_prob", "10", "outpoints")
```

### Example 3

```python
import arcpy
arcpy.management.CreateSpatiallyBalancedPoints("ca_prob", "10", "outpoints")
```

### Example 4

```python
# Description: This tool generates a set of sample points based on inclusion
#   probabilities. The resulting sample design is spatially balanced, meaning
#   that the spatial independence between samples is maximized, making the 
#   design more efficient than sampling the study area at random.

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/dmpyexamples/data.gdb/data"

# Set local variables
inProb = "ca_prob"
numberPoints = 10
outPoints = "C:/dmpyexamples/output.gdb/csbp"

# Run CreateSpatiallyBalancedPoints
arcpy.management.CreateSpatiallyBalancedPoints(inProb, numberPoints, outPoints)
```

### Example 5

```python
# Description: This tool generates a set of sample points based on inclusion
#   probabilities. The resulting sample design is spatially balanced, meaning
#   that the spatial independence between samples is maximized, making the 
#   design more efficient than sampling the study area at random.

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/dmpyexamples/data.gdb/data"

# Set local variables
inProb = "ca_prob"
numberPoints = 10
outPoints = "C:/dmpyexamples/output.gdb/csbp"

# Run CreateSpatiallyBalancedPoints
arcpy.management.CreateSpatiallyBalancedPoints(inProb, numberPoints, outPoints)
```

---

## Create SQLite Database (Data Management)

## Summary

Creates a GeoPackage or an SQLite database that contains the ST_Geometry or SpatiaLite spatial type.

## Usage

- The Output Database Name parameter value will be automatically assigned an extension based on the value of the Spatial Type parameter. If the spatial type parameter value is ST_Geometry or SpatiaLite, the output name will have an .sqlite extension. If the spatial type parameter value is GeoPackage, the output name will have a .gpkg extension.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Database Name | The location of the SQLite database or GeoPackage that will be created and the name of the file. The .sqlite extension will be automatically assigned if the Spatial Type parameter value is ST_Geometry or SpatiaLite. If the Spatial Type parameter value is GeoPackage or any of the GeoPackage versions, the .gpkg extension will be automatically assigned. | File |
| Spatial Type (Optional) | Specifies the spatial type that will be installed with the new SQLite database or the GeoPackage version that will be created.ST_Geometry—The Esri spatial storage type will be installed. This is the default.SpatiaLite—SpatiaLite spatial storage type will be installed.GeoPackage (creates the latest supported GeoPackage version that is supported by ArcGIS)—The latest version of OGC GeoPackage that is supported by ArcGIS will be created.GeoPackage 1.0—An OGC GeoPackage 1.0 dataset will be created.GeoPackage 1.1—An OGC GeoPackage 1.1 dataset will be created. GeoPackage 1.2.1—An OGC GeoPackage 1.2.1 dataset will be created.GeoPackage 1.3—An OGC GeoPackage 1.3 dataset will be created.GeoPackage 1.4—An OGC GeoPackage 1.4 dataset will be created. | String |
| out_database_name | The location of the SQLite database or GeoPackage that will be created and the name of the file. The .sqlite extension will be automatically assigned if the spatial_type parameter value is ST_GEOMETRY or SPATIALITE. If the spatial_type parameter value is GEOPACKAGE, the .gpkg extension will be automatically assigned. | File |
| spatial_type(Optional) | Specifies the spatial type that will be installed with the new SQLite database or the GeoPackage version that will be created.ST_GEOMETRY—The Esri spatial storage type will be installed. This is the default.SPATIALITE—SpatiaLite spatial storage type will be installed.GEOPACKAGE—The latest version of OGC GeoPackage that is supported by ArcGIS will be created.GEOPACKAGE_1.0—An OGC GeoPackage 1.0 dataset will be created.GEOPACKAGE_1.1—An OGC GeoPackage 1.1 dataset will be created. GEOPACKAGE_1.2—An OGC GeoPackage 1.2.1 dataset will be created.GEOPACKAGE_1.3—An OGC GeoPackage 1.3 dataset will be created.GEOPACKAGE_1.4—An OGC GeoPackage 1.4 dataset will be created. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateSQLiteDatabase(out_database_name, {spatial_type})
```

### Example 2

```python
import arcpy
arcpy.management.CreateSQLiteDatabase('c:/data/example.gpkg', 'GEOPACKAGE_1.2')
```

### Example 3

```python
import arcpy
arcpy.management.CreateSQLiteDatabase('c:/data/example.gpkg', 'GEOPACKAGE_1.2')
```

### Example 4

```python
import arcpy

# Run CreateSQLiteDatabase
arcpy.management.CreateSQLiteDatabase('C:/data/example.sqlite', 'ST_GEOMETRY')
```

### Example 5

```python
import arcpy

# Run CreateSQLiteDatabase
arcpy.management.CreateSQLiteDatabase('C:/data/example.sqlite', 'ST_GEOMETRY')
```

---

## Create Table (Data Management)

## Summary

Creates a geodatabase table or a dBASE table.

## Usage

- To create a dBASE table in a folder, append the .dbf extension to the Table Name parameter value.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Table Location | The workspace where the output table will be created. | Workspace |
| Table Name | The name of the table that will be created. | String |
| Template Datasets(Optional) | One or more datasets from which the attribute schema will be used to define the output table. Fields in the template datasets will be added to the output table. | Table View |
| Configuration Keyword(Optional) | The configuration keyword that determines the storage parameters of the table in an enterprise geodatabase. | String |
| Table Alias Name (Optional) | The alternate name of the output table that will be created. | String |
| OID Type | Specifies whether the output Object ID field will be 32 bit or 64 bit.Same as template—The output Object ID field type (32 bit or 64 bit) will be the same as the Object ID field of the first template dataset. This is the default.64-bit—The output Object ID field will be 64 bit. 32-bit—The output Object ID field will be 32 bit. | String |
| out_path | The workspace where the output table will be created. | Workspace |
| out_name | The name of the table that will be created. | String |
| template[template,...](Optional) | One or more datasets from which the attribute schema will be used to define the output table. Fields in the template datasets will be added to the output table. | Table View |
| config_keyword(Optional) | The configuration keyword that determines the storage parameters of the table in an enterprise geodatabase. | String |
| out_alias(Optional) | The alternate name of the output table that will be created. | String |
| oid_type | Specifies whether the output Object ID field will be 32 bit or 64 bit.SAME_AS_TEMPLATE—The output Object ID field type (32 bit or 64 bit) will be the same as the Object ID field of the first template dataset. This is the default.64_BIT—The output Object ID field will be 64 bit. 32_BIT—The output Object ID field will be 32 bit. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateTable(out_path, out_name, {template}, {config_keyword}, {out_alias}, oid_type)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateTable("C:/output", "habitatTemperatures.dbf", 
                             "vegtable.dbf")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CreateTable("C:/output", "habitatTemperatures.dbf", 
                             "vegtable.dbf")
```

### Example 4

```python
# Name: CreateTable_Example2.py
# Description: Create a table to store temperature data in gnatcatcher habitat areas

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
out_path = "C:/output"
out_name = "habitatTemperatures.dbf"
template = "vegtable.dbf"
config_keyword = ""

# Run CreateTable
arcpy.management.CreateTable(out_path, out_name, template, config_keyword)
```

### Example 5

```python
# Name: CreateTable_Example2.py
# Description: Create a table to store temperature data in gnatcatcher habitat areas

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
out_path = "C:/output"
out_name = "habitatTemperatures.dbf"
template = "vegtable.dbf"
config_keyword = ""

# Run CreateTable
arcpy.management.CreateTable(out_path, out_name, template, config_keyword)
```

---

## Create Topology (Data Management)

## Summary

Creates a topology. The topology will not contain any feature classes or rules.

## Usage

- If the Cluster Tolerance parameter is blank or set to 0, the xy tolerance of the feature dataset which contains the topology will be used.
- There is a range allowed for the cluster tolerance value, this range is derived from the precision of the spatial reference of the feature dataset in which the topology is contained. If the value entered is larger than the maximum cluster tolerance, the maximum value will be used instead. If the value entered is smaller than the minimum, the minimum value will be used.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Dataset | The feature dataset in which the topology will be created. | Feature Dataset |
| Output Topology | The name of the topology to be created. This name must be unique across the entire geodatabase. | String |
| Cluster Tolerance(Optional) | The cluster tolerance to be set on the topology. The larger the value, the more likely vertices will be to cluster together. | Double |
| in_dataset | The feature dataset in which the topology will be created. | Feature Dataset |
| out_name | The name of the topology to be created. This name must be unique across the entire geodatabase. | String |
| in_cluster_tolerance(Optional) | The cluster tolerance to be set on the topology. The larger the value, the more likely vertices will be to cluster together. | Double |

## Code Samples

### Example 1

```python
arcpy.management.CreateTopology(in_dataset, out_name, {in_cluster_tolerance})
```

### Example 2

```python
# Name: CreateTopology_Example.py
# Description: Creates a new topology (these must reside within a feature dataset)

# Import system modules
import arcpy
 
arcpy.env.workspace = "h:/workspace"
arcpy.CreateTopology_management("d:/landuse.gdb/landuse", "landuse_Topology")
```

### Example 3

```python
# Name: CreateTopology_Example.py
# Description: Creates a new topology (these must reside within a feature dataset)

# Import system modules
import arcpy
 
arcpy.env.workspace = "h:/workspace"
arcpy.CreateTopology_management("d:/landuse.gdb/landuse", "landuse_Topology")
```

---

## Create Trajectory Dataset (Data Management)

## Summary

Creates an empty trajectory dataset in a geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Location | The geodatabase where the trajectory dataset will be stored. | Workspace |
| Trajectory Dataset Name | The name of the trajectory dataset that will be created. | String |
| Coordinate System | The spatial reference of the trajectory dataset that will be created. | Coordinate System |
| in_workspace | The geodatabase where the trajectory dataset will be stored. | Workspace |
| in_dataset_name | The name of the trajectory dataset that will be created. | String |
| coordinate_system | The spatial reference of the trajectory dataset that will be created. | Coordinate System |

## Code Samples

### Example 1

```python
arcpy.management.CreateTrajectoryDataset(in_workspace, in_dataset_name, coordinate_system)
```

### Example 2

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_workspace = r"C:\temp\trajectory_data.gdb"
in_dataset_name = "trajectory_dataset"
coordinate_system = "GEOGCS["GCS_WGS_1984",DATUM["D_WGS_1984",SPHEROID["WGS_1984",6378137.0,298.257223563]],
					PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]"

# Execute
Trajectory_output = arcpy.CreateTrajectoryDataset_management(in_workspace, in_dataset_name, coordinate_system)
```

### Example 3

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_workspace = r"C:\temp\trajectory_data.gdb"
in_dataset_name = "trajectory_dataset"
coordinate_system = "GEOGCS["GCS_WGS_1984",DATUM["D_WGS_1984",SPHEROID["WGS_1984",6378137.0,298.257223563]],
					PRIMEM["Greenwich",0.0],UNIT["Degree",0.0174532925199433]]"

# Execute
Trajectory_output = arcpy.CreateTrajectoryDataset_management(in_workspace, in_dataset_name, coordinate_system)
```

---

## Create Unregistered Feature Class (Data Management)

## Summary

Creates an empty feature class in an enterprise database or enterprise geodatabase. The feature class is not registered with the geodatabase.

## Usage

- This tool creates only simple feature classes such as point, multipoint, polygon, and polyline.
- The empty feature class created by this tool will either have a field named OBJECTID of type integer or contain the same field names and types of any selected input template feature class.
- Unregistered feature classes can be registered with the geodatabase using the Register With Geodatabase tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Feature Class Location | The enterprise database or enterprise geodatabase in which the feature class will be created. | Workspace; Feature Dataset |
| Feature Class Name | The name of the feature class that will be created. | String |
| Geometry Type (Optional) | Specifies the geometry type of the feature class. This parameter is only relevant for those geometry types that store dimensionality metadata, such as ST_Geometry in PostgreSQL, PostGIS Geometry, and Oracle SDO_Geometry.Point—The geometry type will be point.Multipoint—The geometry type will be multipoint.Polyline—The geometry type will be polyline.Polygon—The geometry type will be polygon. This is the default. | String |
| Template Feature Classes (Optional) | An existing feature class or list of feature classes with fields and attribute schema that will be used to define the fields in the output feature class. | Feature Layer |
| Has M(Optional) | Specifies whether the feature class will have linear measurement values (m-values).No—The output feature class will not have m-values. This is the default.Yes—The output feature class will have m-values.Same as the template feature class—The output feature class will have m-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has m-values. | String |
| Has Z(Optional) | Specifies whether the feature class will have elevation values (z-values).No—The output feature class will not have z-values. This is the default.Yes—The output feature class will have z-values.Same as the template feature class—The output feature class will have z-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has z-values. | String |
| Spatial Reference (Optional) | The spatial reference of the output feature dataset. On the Spatial Reference Properties dialog box, you can select, import, or create a new coordinate system. To set aspects of the spatial reference, such as the x,y-, z-, or m-domain, resolution, or tolerance, use the Environments dialog box. | Spatial Reference |
| Configuration Keyword (Optional) | Specifies the default storage parameters (configurations) for geodatabases in a relational database management system (RDBMS). This setting is applicable only when using enterprise geodatabase tables.Configuration keywords are set by the database administrator.Learn more about configuration keywords | String |
| out_path | The enterprise database or enterprise geodatabase in which the feature class will be created. | Workspace; Feature Dataset |
| out_name | The name of the feature class that will be created. | String |
| geometry_type(Optional) | Specifies the geometry type of the feature class. This parameter is only relevant for those geometry types that store dimensionality metadata, such as ST_Geometry in PostgreSQL, PostGIS Geometry, and Oracle SDO_Geometry.POINT—The geometry type will be point.MULTIPOINT—The geometry type will be multipoint.POLYLINE—The geometry type will be polyline.POLYGON—The geometry type will be polygon. This is the default. | String |
| template[template,...](Optional) | An existing feature class or list of feature classes with fields and attribute schema that will be used to define the fields in the output feature class. | Feature Layer |
| has_m(Optional) | Specifies whether the feature class will have linear measurement values (m-values).DISABLED—The output feature class will not have m-values. This is the default.ENABLED—The output feature class will have m-values.SAME_AS_TEMPLATE—The output feature class will have m-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has m-values. | String |
| has_z(Optional) | Specifies whether the feature class will have elevation values (z-values).DISABLED—The output feature class will not have z-values. This is the default.ENABLED—The output feature class will have z-values.SAME_AS_TEMPLATE—The output feature class will have z-values if the dataset specified in the Template Feature Class parameter (template parameter in Python) has z-values. | String |
| spatial_reference(Optional) | The spatial reference of the output feature dataset. You can specify the spatial reference in the following ways: Enter the path to a .prj file, such as C:/workspace/watershed.prj. Reference a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland.Define a spatial reference object before using this tool, such as sr = arcpy.SpatialReference("Sinusoidal (Africa)"), which you then use as the spatial reference parameter. | Spatial Reference |
| config_keyword(Optional) | Specifies the default storage parameters (configurations) for geodatabases in a relational database management system (RDBMS). This setting is applicable only when using enterprise geodatabase tables.Configuration keywords are set by the database administrator.Learn more about configuration keywords | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateUnRegisteredFeatureclass(out_path, out_name, {geometry_type}, {template}, {has_m}, {has_z}, {spatial_reference}, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.management.CreateUnRegisteredFeatureclass(
    r'Database Connections\Connection to Organization.sde', "New_FC", "POINT", 
    "", "DISABLED", "DISABLED")
```

### Example 3

```python
import arcpy
arcpy.management.CreateUnRegisteredFeatureclass(
    r'Database Connections\Connection to Organization.sde', "New_FC", "POINT", 
    "", "DISABLED", "DISABLED")
```

---

## Create Unregistered Table (Data Management)

## Summary

Creates an empty table in an enterprise database or enterprise geodatabase. The table is not registered with the geodatabase.

## Usage

- The empty table created by this tool will either have a field named OBJECTID of type integer or contain the same field names and types of any selected input template table.
- Unregistered tables can be registered with the geodatabase using the Register With Geodatabase tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Table Location | The enterprise database or enterprise geodatabase in which the table will be created. | Workspace |
| Table Name | The name of the table that will be created. | String |
| Template Datasets(Optional) | An existing dataset or list of datasets with fields and attribute schema that will be used to define the fields in the output table. | Table View |
| Configuration Keyword(Optional) | Specifies the default storage parameters (configurations) for geodatabases in a relational database management system (RDBMS). This setting is applicable only when using enterprise geodatabase tables.Configuration keywords are set by the database administrator.Learn more about configuration keywords | String |
| out_path | The enterprise database or enterprise geodatabase in which the table will be created. | Workspace |
| out_name | The name of the table that will be created. | String |
| template[template,...](Optional) | An existing dataset or list of datasets with fields and attribute schema that will be used to define the fields in the output table. | Table View |
| config_keyword(Optional) | Specifies the default storage parameters (configurations) for geodatabases in a relational database management system (RDBMS). This setting is applicable only when using enterprise geodatabase tables.Configuration keywords are set by the database administrator.Learn more about configuration keywords | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateUnRegisteredTable(out_path, out_name, {template}, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.management.CreateUnRegisteredTable(
    r'Database Connections\Connection to Organization.sde', 'New_Table')
```

### Example 3

```python
import arcpy
arcpy.management.CreateUnRegisteredTable(
    r'Database Connections\Connection to Organization.sde', 'New_Table')
```

---

## Create Vector Tile Index (Data Management)

## Summary

Creates a multiscale mesh of polygons that can be used as index polygons when creating vector tile packages.

## Usage

- The resulting mesh of polygons is multiscale, representing different levels of detail as defined in the input map. The highest level of detail polygons are sized to enclose no more than the specified vertex count from features from the input map as determined by their density, distribution, and the inherent generalization that occurs when creating vector tiles. The maximum level of detail of the resulting polygons will not exceed 16.
- The PTS field value in each polygon indicates the number of vertices each polygon contains from the source data. Examine high values in this field across the index polygons to find areas that are high in vertex count and may result in poor performing vector tiles.
- The LOD field value in each polygon indicates the level of detail (LOD). Sort the field values to find the maximum LOD for the map. The maximum LOD represented by the index polygons may not match the maximum LOD specified in the tiling scheme. Since vector tile layers support a lightweight and efficient tile solution, the maximum LOD should be sufficient. Vector tile layers use oversampling for viewing detail beyond the maximum LOD.
- The LEAF field value in each polygon indicates whether the tile will overzoom as you render the vector tile layer. The LEAF field values are as follows: LEAF = 0 indicates that the tile will not overzoom.LEAF = 1 indicates that the tile will overzoom when zooming beyond its LOD value. For example, if the maximum LOD is 10 for the map, the vector tile layer allows zooming of detail beyond LOD 10. As you zoom in to larger scales beyond LOD 10, the tiles will continue to render features appropriately for those scales. For maps that are considered continuous data, outputting to raster tiles may better maintain proper detail.
- LEAF = 0 indicates that the tile will not overzoom.
- LEAF = 1 indicates that the tile will overzoom when zooming beyond its LOD value.
- The output feature class is suited for use with the Create Vector Tile Package tool as the input index polygons. The Create Vector Tile Package tool will use these polygons to create tiles that are optimized for feature density across the multiple levels of detail being created. The smallest and largest LOD values will be stored as minLOD and maxLOD values for the vector tiles.This tool will use the index polygons as follows:When using an indexed tile structure—The tool will use the output polygons to create tiles that are optimized for feature density across the multiple levels of detail being created. The smallest and largest LOD values will be stored as minLOD and maxLOD values for the vector tiles.When using a flat tile structure—The tool will use the output polygons to generate an intermediate indexed cache from which the flat cache will be generated, speeding up the process of generating vector tiles for the map or basemap.
- When using an indexed tile structure—The tool will use the output polygons to create tiles that are optimized for feature density across the multiple levels of detail being created. The smallest and largest LOD values will be stored as minLOD and maxLOD values for the vector tiles.
- When using a flat tile structure—The tool will use the output polygons to generate an intermediate indexed cache from which the flat cache will be generated, speeding up the process of generating vector tiles for the map or basemap.
- To learn more about creating vector tiles, see Author a map for vector tile creation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | The input map with the feature distribution and vertex density that determine the size and arrangement of output polygons. The input map is typically one that you will subsequently use to create vector tiles using the Create Vector Tile Package tool. | Map |
| Output Tile Feature Class | The output polygon feature class of indexed tiles at each level of detail. Each tile encloses a manageable number of input vertices not exceeding the number specified by the Maximum Vertex Count parameter. | Feature Class |
| Package for ArcGIS Online \| Bing Maps \| Google Maps | Specifies whether the tiling scheme will be generated from an existing map service or for ArcGIS Online, Bing Maps, and Google Maps. Checked—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you check this parameter, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.Unchecked—The tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the Tiling scheme parameter.Online—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this option, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.Existing—The tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the tiling_scheme parameter. | Boolean |
| Tiling scheme (Optional) | The vector tile service or tiling scheme file that will be used if the Package for ArcGIS Online \| Bing Maps \| Google Maps parameter is not checked. The tiling scheme tile size must be 512 by 512 and must have consecutive scales in a ratio of two. | Map Server; File |
| Maximum Vertex Count (Optional) | The ideal number of vertices from all visible layers to be enclosed by each polygon in the output feature class. The default value is the recommended count of 10,000 vertices. | Long |
| in_map | The input map with the feature distribution and vertex density that determine the size and arrangement of output polygons. The input map is typically one that you will subsequently use to create vector tiles using the Create Vector Tile Package tool. | Map |
| out_featureclass | The output polygon feature class of indexed tiles at each level of detail. Each tile encloses a manageable number of input vertices not exceeding the number specified by the vertex_count parameter. | Feature Class |
| service_type | Specifies whether the tiling scheme will be generated from an existing map service or for ArcGIS Online, Bing Maps, and Google Maps. ONLINE—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. The ArcGIS Online/Bing Maps/Google Maps tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this option, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.EXISTING—The tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the tiling_scheme parameter. | Boolean |
| tiling_scheme(Optional) | The vector tile service or tiling scheme file that will be used if the service_type parameter is set to EXISTING. The tiling scheme tile size must be 512 by 512 and must have consecutive scales in a ratio of two. | Map Server; File |
| vertex_count(Optional) | The ideal number of vertices from all visible layers to be enclosed by each polygon in the output feature class. The default value is the recommended count of 10,000 vertices. | Long |

## Code Samples

### Example 1

```python
arcpy.management.CreateVectorTileIndex(in_map, out_featureclass, service_type, {tiling_scheme}, {vertex_count})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/cartography.gdb/transportation"
arcpy.CreateVectorTileIndex_management("CURRENT", "tiles", "ONLINE", "", 10000)
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/cartography.gdb/transportation"
arcpy.CreateVectorTileIndex_management("CURRENT", "tiles", "ONLINE", "", 10000)
```

### Example 4

```python
# Name: CreateVectorTileIndex.py
# Description: Find all the maps in the project and
#   create vector tile index polygon feature class for each map

# import system modules
import os
import arcpy

#set environment settings
arcpy.env.overwriteOutput = True
outputPath = "C://Tilepackages//"

# Loop through the project, find all the maps, and
#   creates vector tile index polygon for each map,
#   using the same name as the map

p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")for m in p.listMaps():
      print("Creating Vector Tile Index for: " + m.name)
      arcpy.CreateVectorTileIndex_management(m, outputPath + m.name + '.shp', "ONLINE", "", 10000)
```

### Example 5

```python
# Name: CreateVectorTileIndex.py
# Description: Find all the maps in the project and
#   create vector tile index polygon feature class for each map

# import system modules
import os
import arcpy

#set environment settings
arcpy.env.overwriteOutput = True
outputPath = "C://Tilepackages//"

# Loop through the project, find all the maps, and
#   creates vector tile index polygon for each map,
#   using the same name as the map

p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")for m in p.listMaps():
      print("Creating Vector Tile Index for: " + m.name)
      arcpy.CreateVectorTileIndex_management(m, outputPath + m.name + '.shp', "ONLINE", "", 10000)
```

---

## Create Vector Tile Package (Data Management)

## Summary

Generates vector tiles from a map or basemap and packages the tiles in a single .vtpk file.

## Usage

- The input map must have a description and tags for the tool to run. To add description and tags, right-click the map name in the Contents pane and click Properties. On the Metadata tab, provide a description and tags for the map.
- Some symbology cannot be resolved in the tile creation process. Avoid symbolizing layers with hatched or gradient fills, markers along lines or polygon outlines, or most symbol effects. The only symbol effects that will be honored in the output tiles are the Move effect (often used to mimic drop-shadow effects on building features), the Offset effect, and the Dash effect. Complex dash patterns from the Dash effect will be resolved to simple (on-off) dashes in the output tiles.
- If the resulting tile package is less than 500 GB, you can upload it directly in a browser to ArcGIS Online or ArcGIS Enterprise or use the Share Package tool to upload it to ArcGIS Online or ArcGIS Enterprise. See Publish a vector tile layer from a cache dataset for more information.
- Specify an Index Polygons parameter value to speed up the process of generating vector tiles for a map or basemap, as the tool will not need to create optimized index polygons during processing.
- To learn more about creating and symbolizing vector tiles, see Author a map for vector tile creation and Symbology in vector tiles.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | The map from which tiles will be generated and packaged. The input map must have metadata description and tags. | Map |
| Output File | The output vector tile package. The file extension of the package will be .vtpk. | File |
| Package for ArcGIS Online \| Bing Maps \| Google Maps | Specifies whether the tiling scheme will be generated from an existing map service or if map tiles will be generated for ArcGIS Online, Bing Maps, and Google Maps. Checked—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. This tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you check this parameter, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.Unchecked—A tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the Tiling scheme parameter.Online—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. This tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this option, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.Existing—A tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the tiling_scheme parameter. | Boolean |
| Tiling scheme (Optional) | A vector tile service or tiling scheme file that will be used if the Package for ArcGIS Online \| Bing Maps \| Google Maps parameter is not checked. The tiling scheme tile size must be 512 by 512 and must have consecutive scales in a ratio of two. | Map Server; File |
| Tiling Format (Optional) | Specifies whether the tile generation structure will be optimized with an indexed structure or as a flat array of all tiles at all levels of detail. The optimized indexed structure is the default and results in a smaller cache. Indexed—Tiles that are based on an index of feature density that optimizes the tile generation and file sizes will be produced. This is the default.Flat—Regular tiles for each level of detail will be produced without regard to feature density. This cache is larger than that produced with an indexed structure. | String |
| Minimum Cached Scale (Optional) | The minimum (smallest) scale at which tiles will be generated. This does not need to be the smallest scale in the tiling scheme. The minimum cached scale determines which scales will be used to generate cache. | Double |
| Maximum Cached Scale (Optional) | The maximum (largest) scale at which tiles will be generated. This does not need to be the largest scale in the tiling scheme. The maximum cached scale determines which scales will be used to generate cache. | Double |
| Index Polygons (Optional) | An index of tiles based on feature density.Use the Create Vector Tile Index tool to create index polygons. If no index polygons are specified for this parameter, optimized index polygons will be generated during processing to aid in tile creation, but they will not be saved or output. The index polygons must use the same coordinate system as the Tiling Scheme parameter value. | Feature Layer |
| Summary (Optional) | The summary information that will be added to properties of the output vector tile package. | String |
| Tags (Optional) | The tag information that will be added to the properties of the output vector tile package. Separate multiple tags with commas or semicolons. | String |
| in_map | The map from which tiles will be generated and packaged. The input map must have metadata description and tags. | Map |
| output_file | The output vector tile package. The file extension of the package will be .vtpk. | File |
| service_type | Specifies whether the tiling scheme will be generated from an existing map service or if map tiles will be generated for ArcGIS Online, Bing Maps, and Google Maps. ONLINE—The ArcGIS Online/Bing Maps/Google Maps tiling scheme will be used. This tiling scheme allows you to overlay cache tiles with tiles from these online mapping services. ArcGIS Pro includes this tiling scheme as a built-in option when loading a tiling scheme. When you choose this option, the data frame of the source map must use the WGS84 Web Mercator (Auxiliary Sphere) projected coordinate system. This is the default.EXISTING—A tiling scheme from an existing vector tile service will be used. Only tiling schemes with scales that double in progression through levels and have 512-by-512 tile size are supported. You must specify a vector tile service or tiling scheme file for the tiling_scheme parameter. | Boolean |
| tiling_scheme(Optional) | A vector tile service or tiling scheme file that will be used if the service_type parameter is set to EXISTING. The tiling scheme tile size must be 512 by 512 and must have consecutive scales in a ratio of two. | Map Server; File |
| tile_structure(Optional) | Specifies whether the tile generation structure will be optimized with an indexed structure or as a flat array of all tiles at all levels of detail. The optimized indexed structure is the default and results in a smaller cache. INDEXED—Tiles that are based on an index of feature density that optimizes the tile generation and file sizes will be produced. This is the default.FLAT—Regular tiles for each level of detail will be produced without regard to feature density. This cache is larger than that produced with an indexed structure. | String |
| min_cached_scale(Optional) | The minimum (smallest) scale at which tiles will be generated. This does not need to be the smallest scale in the tiling scheme. The minimum cached scale determines which scales will be used to generate cache. | Double |
| max_cached_scale(Optional) | The maximum (largest) scale at which tiles will be generated. This does not need to be the largest scale in the tiling scheme. The maximum cached scale determines which scales will be used to generate cache. | Double |
| index_polygons(Optional) | An index of tiles based on feature density.Use the Create Vector Tile Index tool to create index polygons. If no index polygons are specified for this parameter, optimized index polygons will be generated during processing to aid in tile creation, but they will not be saved or output. The index polygons must use the same coordinate system as the tiling_scheme parameter value. | Feature Layer |
| summary(Optional) | The summary information that will be added to properties of the output vector tile package. | String |
| tags(Optional) | The tag information that will be added to the properties of the output vector tile package. Separate multiple tags with commas or semicolons. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateVectorTilePackage(in_map, output_file, service_type, {tiling_scheme}, {tile_structure}, {min_cached_scale}, {max_cached_scale}, {index_polygons}, {summary}, {tags})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/TilePackageExample"
aprx = arcpy.mp.ArcGISProject("CURRENT")
map = aprx.listMaps()[0]
arcpy.management.CreateVectorTilePackage(map, 'Example.vtpk', "ONLINE", "", "INDEXED", 295828763.795777, 564.248588)
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/TilePackageExample"
aprx = arcpy.mp.ArcGISProject("CURRENT")
map = aprx.listMaps()[0]
arcpy.management.CreateVectorTilePackage(map, 'Example.vtpk', "ONLINE", "", "INDEXED", 295828763.795777, 564.248588)
```

### Example 4

```python
# Name: CreateVectorTilePackage.py
# Description: Find all the maps in the project and
#   create a vector tile package for each map
# import system modules
import os
import arcpy

#set environment settings
arcpy.env.overwriteOutput = True
outputPath = "C://Tilepackages//"

# Loop through the project, find all the maps, and
#   create a vector tile package for each map,
#   using the same name as the map
p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
for m in p.listMaps():
    print("Packaging " + m.name)
    arcpy.CreateVectorTilePackage_management(m, outputPath + m.name + '.vtpk', "ONLINE", "", "INDEXED", 295828763.795777, 564.248588)
```

### Example 5

```python
# Name: CreateVectorTilePackage.py
# Description: Find all the maps in the project and
#   create a vector tile package for each map
# import system modules
import os
import arcpy

#set environment settings
arcpy.env.overwriteOutput = True
outputPath = "C://Tilepackages//"

# Loop through the project, find all the maps, and
#   create a vector tile package for each map,
#   using the same name as the map
p = arcpy.mp.ArcGISProject("c:\\temp\\myproject.aprx")
for m in p.listMaps():
    print("Packaging " + m.name)
    arcpy.CreateVectorTilePackage_management(m, outputPath + m.name + '.vtpk', "ONLINE", "", "INDEXED", 295828763.795777, 564.248588)
```

---

## Create Version (Data Management)

## Summary

Creates a new version in a specified geodatabase or feature service.

## Usage

- The output version name is prefixed by the username, for example, USER1.Maintenance.
- A version's permission can only be changed by its owner (the user who created it).
- This tool supports creating versions for branch versioned datasets when the Input Workspace parameter value is a feature service with the version management capability enabled.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The enterprise geodatabase that contains the parent version and will contain the new version.For branch versioning, use a feature service URL (for example, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer). | Workspace |
| Parent Version | The geodatabase, or version of a geodatabase, on which the new version will be based. | String |
| Version Name | The name of the version that will be created. | String |
| Access Permission(Optional) | Specifies the permission access level for the version to protect it from being edited or viewed by users other than the owner.Private (owner only)—Only the owner or the geodatabase administrator can view and modify the version or versioned data. This is the default.Public (any user)—Any user can view the version. Any user who has been granted read/write (update, insert, and delete) permissions on datasets can modify datasets in the version.Protected (only the owner can edit)—Any user can view the version, but only the owner or the geodatabase administrator can edit the version or datasets in the version. | String |
| Version Description(Optional) | The description of the version that will be created. The description cannot exceed 64 characters. | String |
| in_workspace | The enterprise geodatabase that contains the parent version and will contain the new version.For branch versioning, use a feature service URL (for example, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer). | Workspace |
| parent_version | The geodatabase, or version of a geodatabase, on which the new version will be based. | String |
| version_name | The name of the version that will be created. | String |
| access_permission(Optional) | Specifies the permission access level for the version to protect it from being edited or viewed by users other than the owner.PRIVATE—Only the owner or the geodatabase administrator can view and modify the version or versioned data. This is the default.PUBLIC—Any user can view the version. Any user who has been granted read/write (update, insert, and delete) permissions on datasets can modify datasets in the version.PROTECTED—Any user can view the version, but only the owner or the geodatabase administrator can edit the version or datasets in the version. | String |
| version_description(Optional) | The description of the version that will be created. The description cannot exceed 64 characters. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateVersion(in_workspace, parent_version, version_name, {access_permission}, {version_description})
```

### Example 2

```python
# Description: Creates a new version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/Connections/whistler@gdb.sde"
parentVersion = "dbo.DEFAULT"
versionName = "myVersion"
versionAccess = "PUBLIC"
versionDescription = "Version's description"

# Run CreateVersion
arcpy.management.CreateVersion(inWorkspace, parentVersion, versionName, versionAccess, versionDescription)
```

### Example 3

```python
# Description: Creates a new version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/Connections/whistler@gdb.sde"
parentVersion = "dbo.DEFAULT"
versionName = "myVersion"
versionAccess = "PUBLIC"
versionDescription = "Version's description"

# Run CreateVersion
arcpy.management.CreateVersion(inWorkspace, parentVersion, versionName, versionAccess, versionDescription)
```

---

## Create Versioned View (Data Management)

## Summary

Creates a versioned view on a table or feature class.

## Usage

- The table or feature class must be registered as versioned.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table or Feature Class | Input table or feature class for which a versioned view will be created. | Table View |
| Versioned View Name (Optional) | Name for the versioned view that is created. If nothing is specified the output versioned view name is the name of the table or feature class with _evw appended to the end. | String |
| in_dataset | Input table or feature class for which a versioned view will be created. | Table View |
| in_name(Optional) | Name for the versioned view that is created. If nothing is specified the output versioned view name is the name of the table or feature class with _evw appended to the end. | String |

## Code Samples

### Example 1

```python
arcpy.management.CreateVersionedView(in_dataset, {in_name})
```

### Example 2

```python
arcpy.CreateVersionedView_management("Database Connections\\admin.sde\\bender.GDB.cities", "cities_MV_view")
```

### Example 3

```python
arcpy.CreateVersionedView_management("Database Connections\\admin.sde\\bender.GDB.cities", "cities_MV_view")
```

---

## Create Voxel Scene Layer Content (Data Management)

## Summary

Creates a scene layer package (.slpk file) from a voxel layer input.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Voxel Layer | The input voxel layer or layer file. | Voxel Layer; Layer File |
| Output Scene Layer Package | The output scene layer package (.slpk file). | File |
| in_dataset | The input voxel layer or layer file. | Voxel Layer; Layer File |
| out_slpk | The output scene layer package (.slpk file). | File |

## Code Samples

### Example 1

```python
arcpy.management.CreateVoxelSceneLayerContent(in_dataset, out_slpk)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.CreateVoxelSceneLayerContent_management("pm10.lyrx", "voxel.slpk")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/temp"
arcpy.CreateVoxelSceneLayerContent_management("pm10.lyrx", "voxel.slpk")
```

---

## Define Mosaic Dataset NoData (Data Management)

## Summary

Specifies one or more values to be represented as NoData.

## Usage

- NoData can be used to define pixel values that surround an image; however, the mosaic dataset can be made more efficient if the footprints are recomputed to remove these boundary areas. To recompute the footprints you can edit them manually or use the Build Footprints tool.
- You can specify multiple NoData values with the Bands For NoData Value parameter. Use a space delimiter between each value you want to define as NoData.
- This tool inserts the Mask function within the function chain for each raster item within a mosaic dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset where you want to update the NoData values. | Mosaic Layer |
| Number of Bands | The number of bands in the mosaic dataset. | Long |
| Bands for NoData Value(Optional) | Specify the NoData value for each band. Each band can have a unique NoData value defined, or you can use the same value for all bands. Choose the band from the drop-down list and then enter a value or multiple values. If you choose multiple NoData values, separate each value with a space. If the function chain for each raster within the mosaic dataset contains the Composite Bands function, or if your raster data was added with a raster type that adds the Composite Bands function to each raster's function chain, then any value you specify will apply to all bands. | Value Table |
| Bands For Valid Data Range(Optional) | Specify a range of values to display for each band. Values outside of this range will be classified as NoData. When working with composite bands, the range will apply to all bands. | Value Table |
| Query Definition (Optional) | An SQL statement to select specific raster in the mosaic dataset. Only the selected rasters will have their NoData values changed. | SQL Expression |
| Composite NoData value from each band (Optional) | Choose whether all bands must be NoData in order for the pixel to be classified as NoData.Unchecked—If any band has pixels of NoData, then the pixel is classified as NoData. This is the default.Checked—All bands must have pixels of NoData for the pixel to be classified as NoData. | Boolean |
| in_mosaic_dataset | The mosaic dataset where you want to update the NoData values. | Mosaic Layer |
| num_bands | The number of bands in the mosaic dataset. | Long |
| bands_for_nodata_value[band {NoData value},...](Optional) | Define values for each or all bands. Each band can have a unique NoData value defined, or the same value can be specified for all bands. If you want to define multiple NoData values for each band selection, use a space delimiter between each NoData value within the bands_for_nodata_value parameter.The Mask function inserted by this tool is inserted before the Composite Bands function in the function chain. Therefore, if the function chain for each raster within the mosaic dataset contains the Composite Bands function, or if your raster data was added with a raster type that adds the Composite Bands function to each raster’s function chain, then any value you specify will apply to all bands. | Value Table |
| bands_for_valid_data_range[band {minimum value} {maximum value},...](Optional) | Specify a range of values to display for each band. Values outside of this range will be classified as NoData. When working with composite bands, the range will apply to all bands. | Value Table |
| where_clause(Optional) | An SQL statement to select specific raster in the mosaic dataset. Only the selected rasters will have their NoData values changed. | SQL Expression |
| Composite_nodata_value(Optional) | Choose whether all bands must be NoData in order for the pixel to be classified as NoData.NO_COMPOSITE_NODATA—If any of the bands have pixels of NoData, then the pixel is classified as NoData. This is the default.COMPOSITE_NODATA—All of the bands must have pixels of NoData in order for the pixel to be classified as NoData. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DefineMosaicDatasetNoData(in_mosaic_dataset, num_bands, {bands_for_nodata_value}, {bands_for_valid_data_range}, {where_clause}, {Composite_nodata_value})
```

### Example 2

```python
import arcpy
arcpy.DefineMosaicDatasetNodata_management(
     "c:/workspace/Nodata.gdb/md", "3",  "ALL_BANDS '0 9'", "#", 
     "OBJECTID=2",  "COMPOSITE_NODATA")
```

### Example 3

```python
import arcpy
arcpy.DefineMosaicDatasetNodata_management(
     "c:/workspace/Nodata.gdb/md", "3",  "ALL_BANDS '0 9'", "#", 
     "OBJECTID=2",  "COMPOSITE_NODATA")
```

### Example 4

```python
#Specify multiple Nodata values for all bands in one Catalog item

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "Nodata.gdb/md"
noofbands = "3"
nodataval = "ALL_BANDS '0 9'"
nodatarange = "#"
query = "OBJECTID=2"
mode = "#"

arcpy.DefineMosaicDatasetNoData_management(mdname, noofbands, nodataval, 
                                           nodatarange, query, mode)
```

### Example 5

```python
#Specify multiple Nodata values for all bands in one Catalog item

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "Nodata.gdb/md"
noofbands = "3"
nodataval = "ALL_BANDS '0 9'"
nodatarange = "#"
query = "OBJECTID=2"
mode = "#"

arcpy.DefineMosaicDatasetNoData_management(mdname, noofbands, nodataval, 
                                           nodatarange, query, mode)
```

---

## Define Overviews (Data Management)

## Summary

Lets you set how mosaic dataset overviews are generated. The settings made with this tool are used by the Build Overviews tool.

## Usage

- This tool is used when there are specific parameters you need to set to generate your overviews, such as Defining the location to write the filesDefining an extent that varies from the boundaryDefining the properties of the overview images, such as the resampling or compression methodsDefining the overview sampling factorOtherwise, you can check the option to Update Overviews using the Add Rasters to Mosaic Dataset tool and the defaults will be used.
- Defining the location to write the files
- Defining an extent that varies from the boundary
- Defining the properties of the overview images, such as the resampling or compression methods
- Defining the overview sampling factor
- Use the Build Overviews tool to generate the overviews after they've been defined with this tool.
- You can use a polygon feature class to define the footprint of the overview. If you do not wish to use all the polygons in the feature class you can make a selection on the layer in the table of contents or use a tool such as Select Layer By Attribute or Select Layer By Location to select the desired polygons.
- The default tile size is 128 by 128. The tile size can be changed in the Environment Settings.
- This tool can take a long time to run if the boundary contains a large number of vertices.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that you want to build overviews on. | Mosaic Layer |
| Output Location (Optional) | The folder or geodatabase to store the overviews. | Workspace |
| Extent from Dataset (Optional) | A raster dataset or feature class to define the extent of the overviews. | Raster Layer; Feature Layer |
| Extent (Optional) | Manually set the extent using the following minimum and maximum x and y coordinates.The mosaic dataset boundary will determine the extent of the overviews if you do not define an extent. | Envelope |
| Pixel Size (Optional) | If you prefer not to use all the raster's pyramids, specify a base pixel size at which your overviews will be generated.The units for this parameter are the same as the spatial reference of the mosaic dataset. | Double |
| Number Of Levels (Optional) | Specify the number of levels of overviews that you want to generate overviews. A value of -1 will determine an optimal value for you. | Long |
| Number Of Rows (Optional) | Set the number of rows (in pixels) for each tile.Larger values will result in fewer, larger individual overviews, and increase the likelihood that you will need to regenerate lower level overviews. A smaller value will result in more, smaller files. | Long |
| Number Of Columns (Optional) | Set the number of columns (in pixels) for each tile.Larger values will result in fewer, larger individual overviews, and increase the likelihood that you will need to regenerate lower level overviews. A smaller value will result in more, smaller files. | Long |
| Overview Sampling Factor (Optional) | Set a ratio to determine the size of the next overview. For example, if the cell size of the first level is 10, and the overview factor is 3, then the next overview pixel size will be 30. | Long |
| Force Overview Tiles (Optional) | Generate overviews at all levels, or only above existing pyramid levels.Unchecked—Create overviews above the raster pyramid levels. This is the default.Checked—Create overviews at all levels. | Boolean |
| Resampling Method (Optional) | Choose an algorithm for aggregating pixel values in the overviews.Nearest—The fastest resampling method because it minimizes changes to pixel values. Suitable for discrete data, such as land cover. If the Raster MetadataData Type is thematic, then nearest neighbor will be the default.Bilinear—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.This is the default, unless the Raster Metadata Data Type is thematic.Cubic— Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |
| Compression Method (Optional) | Define the type of data compression to store the overview images.JPEG—A lossy compression. This is the default, unless the Raster Metadata Data Type is thematic. This compression method is only valid if the mosaic dataset items adhere to JPEG specifications.JPEG Luna and Chroma—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components.None—No data compression.LZW—A lossless compression. If the Raster Metadata Data Type is thematic, then nearest neighbor will be the default. | String |
| Compression Quality (Optional) | Choose a value from 1 - 100. Higher values generate better quality outputs, but they create larger files. | Long |
| in_mosaic_dataset | The mosaic dataset that you want to build overviews on. | Mosaic Layer |
| overview_image_folder(Optional) | The folder or geodatabase to store the overviews. | Workspace |
| in_template_dataset(Optional) | A raster dataset or feature class to define the extent of the overviews. | Raster Layer; Feature Layer |
| extent(Optional) | Set the extent using minimum and maximum x and y coordinates. This is specified as space delimited in the following order: X-minimum X-maximum Y-minimum Y-maximum. The mosaic dataset boundary will determine the extent of the overviews if you do not define an extent. | Envelope |
| pixel_size(Optional) | If you prefer not to use all the raster's pyramids, specify a base pixel size at which your overviews will be generated.The units for this parameter are the same as the spatial reference of the mosaic dataset. | Double |
| number_of_levels(Optional) | Specify the number of levels of overviews that you want to generate overviews. A value of -1 will determine an optimal value for you. | Long |
| tile_rows(Optional) | Set the number of rows (in pixels) for each tile.Larger values will result in fewer, larger individual overviews, and increase the likelihood that you will need to regenerate lower level overviews. A smaller value will result in more, smaller files. | Long |
| tile_cols(Optional) | Set the number of columns (in pixels) for each tile.Larger values will result in fewer, larger individual overviews, and increase the likelihood that you will need to regenerate lower level overviews. A smaller value will result in more, smaller files. | Long |
| overview_factor(Optional) | Set a ratio to determine the size of the next overview. For example, if the cell size of the first level is 10, and the overview factor is 3, then the next overview pixel size will be 30. | Long |
| force_overview_tiles(Optional) | Generate overviews at all levels, or only above existing pyramid levels.NO_FORCE_OVERVIEW_TILES—Create overviews above the raster pyramid levels. This is the default.FORCE_OVERVIEW_TILES— Create overviews at all levels. | Boolean |
| resampling_method(Optional) | Choose an algorithm for aggregating pixel values in the overviews.NEAREST—The fastest resampling method because it minimizes changes to pixel values. Suitable for discrete data, such as land cover. If the Raster MetadataData Type is thematic, then nearest neighbor will be the default.BILINEAR—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.This is the default, unless the Raster Metadata Data Type is thematic.CUBIC— Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |
| compression_method(Optional) | Define the type of data compression to store the overview images.JPEG—A lossy compression. This is the default, unless the Raster Metadata Data Type is thematic. This compression method is only valid if the mosaic dataset items adhere to JPEG specifications.JPEG_YCbCr—A lossy compression using the luma (Y) and chroma (Cb and Cr) color space components.None—No data compression.LZW—A lossless compression. If the Raster Metadata Data Type is thematic, then nearest neighbor will be the default. | String |
| compression_quality(Optional) | Choose a value from 1 - 100. Higher values generate better quality outputs, but they create larger files. | Long |

## Code Samples

### Example 1

```python
arcpy.management.DefineOverviews(in_mosaic_dataset, {overview_image_folder}, {in_template_dataset}, {extent}, {pixel_size}, {number_of_levels}, {tile_rows}, {tile_cols}, {overview_factor}, {force_overview_tiles}, {resampling_method}, {compression_method}, {compression_quality})
```

### Example 2

```python
import arcpy
arcpy.DefineOverviews_management("c:/workspace/fgdb.gdb/md01", 
                                 "c:/temp", "#", "#", "30", "6", "4000", 
                                 "4000", "2", "CUBIC", "JPEG", "50")
```

### Example 3

```python
import arcpy
arcpy.DefineOverviews_management("c:/workspace/fgdb.gdb/md01", 
                                 "c:/temp", "#", "#", "30", "6", "4000", 
                                 "4000", "2", "CUBIC", "JPEG", "50")
```

### Example 4

```python
#Define Overviews to the default location
#Define Overviews for all levels - ignore the primary Raster pyramid
#Define Overviews compression and resampling method

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
arcpy.DefineOverviews_management("DefineOVR.gdb/md", "#", "#", "#", "#", 
                                 "#", "#", "#", "#", "FORCE_OVERVIEW_TILES",
                                     "BILINEAR", "JPEG", "50")
```

### Example 5

```python
#Define Overviews to the default location
#Define Overviews for all levels - ignore the primary Raster pyramid
#Define Overviews compression and resampling method

import arcpy
arcpy.env.workspace = "C:/Workspace"

    
arcpy.DefineOverviews_management("DefineOVR.gdb/md", "#", "#", "#", "#", 
                                 "#", "#", "#", "#", "FORCE_OVERVIEW_TILES",
                                     "BILINEAR", "JPEG", "50")
```

---

## Define Projection (Data Management)

## Summary

Overwrites the coordinate system information (map projection and datum) stored with a dataset. This tool is intended for datasets that have an unknown or incorrect coordinate system defined.

## Usage

- All geographic datasets have a coordinate system that is used throughout ArcGIS to display, measure, and transform geographic data. If the coordinate system for a dataset is unknown or incorrect, you can use this tool to specify the correct coordinate system.
- This tool only updates the existing coordinate system information; it does not modify any geometry. To transform the geometry to another coordinate system, use the Project tool.
- When a dataset with a known coordinate system is input to this tool, the tool will issue a warning message but will run successfully.
- All feature classes in a geodatabase feature dataset are in the same coordinate system. The coordinate system for a feature dataset should be determined when it is created.
- The tool does not support the following: A feature class in an enterprise geodatabase.A feature class in a feature dataset.A feature dataset that contains a feature class.
- A feature class in an enterprise geodatabase.
- A feature class in a feature dataset.
- A feature dataset that contains a feature class.
- The tool will update the associated .wld file of a CAD or BIM file to retain the data's geographic adjusted position.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset or Feature Class | The dataset or feature class whose projection will be defined. | Feature Layer; Geodataset |
| Coordinate System | The coordinate system that will be applied to the input. | Coordinate System |
| in_dataset | The dataset or feature class whose projection will be defined. | Feature Layer; Geodataset |
| coor_system | The coordinate system that will be applied to the input. Valid values are a SpatialReference object, a file with a .prj extension, or a string representation of a coordinate system. | Coordinate System |

## Code Samples

### Example 1

```python
arcpy.management.DefineProjection(in_dataset, coor_system)
```

### Example 2

```python
import arcpy
infc = r"C:\data\citylim_unk.shp"
sr = arcpy.SpatialReference("NAD 1983 UTM Zone 11N")
arcpy.DefineProjection_management(infc, sr)
```

### Example 3

```python
import arcpy
infc = r"C:\data\citylim_unk.shp"
sr = arcpy.SpatialReference("NAD 1983 UTM Zone 11N")
arcpy.DefineProjection_management(infc, sr)
```

### Example 4

```python
# Name: DefineProjection.py 
# Description: Records the coordinate system information for the specified input dataset or feature class

# import system modules
import arcpy

# set workspace environment
arcpy.env.workspace = "C:/data"

try:
    # set local variables
    in_dataset = "citylim_unk.shp" #"forest.shp"
    
    # get the coordinate system by describing a feature class
    dsc = arcpy.Describe("citylim_utm11.shp")
    coord_sys = dsc.spatialReference
    
    # run the tool
    arcpy.DefineProjection_management(in_dataset, coord_sys)
    
    # print messages when the tool runs successfully
    print(arcpy.GetMessages(0))
    
except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 5

```python
# Name: DefineProjection.py 
# Description: Records the coordinate system information for the specified input dataset or feature class

# import system modules
import arcpy

# set workspace environment
arcpy.env.workspace = "C:/data"

try:
    # set local variables
    in_dataset = "citylim_unk.shp" #"forest.shp"
    
    # get the coordinate system by describing a feature class
    dsc = arcpy.Describe("citylim_utm11.shp")
    coord_sys = dsc.spatialReference
    
    # run the tool
    arcpy.DefineProjection_management(in_dataset, coord_sys)
    
    # print messages when the tool runs successfully
    print(arcpy.GetMessages(0))
    
except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

---

## Delete Attribute Rule (Data Management)

## Summary

Deletes one or more attribute rules from a dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class containing the attribute rules that will be deleted. | Table View |
| Rule Names | The names of the rules that will be deleted from the dataset. | String |
| Type (Optional) | Specifies the type of attribute rules that will be deleted.Calculation—Calculation rules, filtered from the Rule Names parameter value, will be deleted .Constraint—Constraint rules, filtered from the Rule Names parameter value, will be deleted.Validation—Validation rules, filtered from the Rule Names parameter value, will be deleted.Calculation—Calculation rules will be deleted.Constraint—Constraint rules will be deleted.Validation—Validation rules will be deleted. | String |
| in_table | The table or feature class containing the attribute rules that will be deleted. | Table View |
| names[names,...] | The names of the rules that will be deleted from the dataset. | String |
| type(Optional) | Specifies the type of attribute rules that will be deleted.CALCULATION—Calculation rules will be deleted.CONSTRAINT—Constraint rules will be deleted.VALIDATION—Validation rules will be deleted. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteAttributeRule(in_table, names, {type})
```

### Example 2

```python
import arcpy
arcpy.management.DeleteAttributeRule(
    "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", "Rule A;Rule B", 
    "CALCULATION")
```

### Example 3

```python
import arcpy
arcpy.management.DeleteAttributeRule(
    "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", "Rule A;Rule B", 
    "CALCULATION")
```

---

## Delete Coded Value From Domain (Data Management)

## Summary

Removes a value from a coded value domain.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The workspace containing the domain to be updated. | Workspace |
| Domain Name | The name of the coded value domain to be updated. | String |
| Code Value | The value(s) to be deleted from the specified domain. | String |
| in_workspace | The workspace containing the domain to be updated. | Workspace |
| domain_name | The name of the coded value domain to be updated. | String |
| code[code,...] | The value(s) to be deleted from the specified domain. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteCodedValueFromDomain(in_workspace, domain_name, code)
```

### Example 2

```python
import arcpy
arcpy.env.workspace =  "C:/data"
arcpy.DeleteCodedValueFromDomain_management("montgomery.gdb", "DistDiam", ["20","24"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace =  "C:/data"
arcpy.DeleteCodedValueFromDomain_management("montgomery.gdb", "DistDiam", ["20","24"])
```

### Example 4

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"
# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)

# Process: Create a table from the domain to edit it with ArcMap editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)
 
# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

### Example 5

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"
# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)

# Process: Create a table from the domain to edit it with ArcMap editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)
 
# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

---

## Delete Colormap (Data Management)

## Summary

Removes the color map associated with a raster dataset.

## Usage

- This tool will not work when the color map is internally stored in the attribute table of an IMG or a TIFF dataset. If the attribute table contains the fields Red, Green, and Blue, this tool cannot be used.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset that containing the color map you want to remove. | Raster Layer |
| in_raster | The raster dataset that containing the color map you want to remove. | Raster Layer |

## Code Samples

### Example 1

```python
arcpy.management.DeleteColormap(in_raster)
```

### Example 2

```python
import arcpy
arcpy.DeleteColormap_management("c:/data/delcolormap.tif")
```

### Example 3

```python
import arcpy
arcpy.DeleteColormap_management("c:/data/delcolormap.tif")
```

### Example 4

```python
##====================================
##Delete Colormap
##Usage: CalculateStatistics_management in_raster
    
import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Delete the colormap of single band image if exist
arcpy.DeleteColormap_management("nocolormap.img")
```

### Example 5

```python
##====================================
##Delete Colormap
##Usage: CalculateStatistics_management in_raster
    
import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Delete the colormap of single band image if exist
arcpy.DeleteColormap_management("nocolormap.img")
```

---

## Delete Database Sequence (Data Management)

## Summary

Deletes a database sequence from a geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The full path to the location of the file or mobile geodatabase from which you want to delete a sequence or the database connection file (.sde) to connect to the enterprise geodatabase from which you want to delete a sequence. The user specified in the database connection must have the following permissions in the database:Db2—DBADM authorityOracle—Must be the sequence owner or have the DROP ANY SEQUENCE system privilegePostgreSQL—Must be the sequence ownerSAP HANA—Must be a standard userSQL Server—ALTER OR CONTROL permission on the database schema where the sequence is stored | Workspace |
| Sequence Name | The name of the database sequence you want to delete. Once deleted, the sequence cannot be used to generate sequence IDs when called from existing custom applications or expressions. | String |
| in_workspace | The full path to the location of the file or mobile geodatabase from which you want to delete a sequence or the database connection file (.sde) to connect to the enterprise geodatabase from which you want to delete a sequence. The user specified in the database connection must have the following permissions in the database:Db2—DBADM authorityOracle—Must be the sequence owner or have the DROP ANY SEQUENCE system privilegePostgreSQL—Must be the sequence ownerSAP HANA—Must be a standard userSQL Server—ALTER OR CONTROL permission on the database schema where the sequence is stored | Workspace |
| seq_name | The name of the database sequence you want to delete. Once deleted, the sequence cannot be used to generate sequence IDs when called from existing custom applications or expressions. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteDatabaseSequence(in_workspace, seq_name)
```

### Example 2

```python
import arcpy
arcpy.management.DeleteDatabaseSequence(r"C:/myconnections/mygdb.sde", 
                                        "custom_sequence")
```

### Example 3

```python
import arcpy
arcpy.management.DeleteDatabaseSequence(r"C:/myconnections/mygdb.sde", 
                                        "custom_sequence")
```

### Example 4

```python
import arcpy
arcpy.management.DeleteDatabaseSequence(r"C:/geodatabases/myfilegdb.gdb", 
                                        "my_ids")
```

### Example 5

```python
import arcpy
arcpy.management.DeleteDatabaseSequence(r"C:/geodatabases/myfilegdb.gdb", 
                                        "my_ids")
```

---

## Delete Domain (Data Management)

## Summary

Deletes a domain from a workspace.

## Usage

- A domain cannot be deleted if it is associated with a feature class or table. Use the Remove Domain From Field tool to remove the association between a feature class or table and a domain.
- You can also manage domains in Domains view which can be opened by clicking the Domains button found in the Design group on the Data ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase that contains the domain to be deleted. | Workspace |
| Domain Name | The name of the domain to be deleted. | String |
| in_workspace | The geodatabase that contains the domain to be deleted. | Workspace |
| domain_name | The name of the domain to be deleted. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteDomain(in_workspace, domain_name)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DeleteDomain_management("montgomery.gdb", "DistDiam")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DeleteDomain_management("montgomery.gdb", "DistDiam")
```

### Example 4

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"

# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)

# Process: Create a table from the domain to edit it with standard editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)
 
# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

### Example 5

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"

# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)

# Process: Create a table from the domain to edit it with standard editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)
 
# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

---

## Delete Features (Data Management)

## Summary

Deletes all or the selected subset of features from the input.

## Usage

- This tool accepts layers with selections as input and will delete only those features that are selected. To delete specific features from a feature class, convert the feature class to a layer using the Make Feature Layer tool or by adding it to the display. A selection can then be applied using the Select Layer By Attribute or Select Layer By Location tool, by querying a map layer, or by selecting features interactively using selection tools from the Selection group on the Map tab.
- This tool deletes both the geometry and attributes of the Input Features value.
- The Extent environment is honored by this tool. Only the features that are within or intersect the output extent environment will be deleted. If the input layer has a selection, only the selected features that are within or intersect the output extent will be deleted.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature class, shapefile, or layer containing features to be deleted. | Feature Layer |
| in_features | The feature class, shapefile, or layer containing features to be deleted. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.DeleteFeatures(in_features)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/output.gdb/majorrds2")
arcpy.management.DeleteFeatures("C:/output/output.gdb/majorrds2")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/output.gdb/majorrds2")
arcpy.management.DeleteFeatures("C:/output/output.gdb/majorrds2")
```

### Example 4

```python
# Description: Delete features from a feature class based on an expression
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "parcels"
outFeatures = "C:/output/output.gdb/new_parcels"
tempLayer = "parcelsLayer"
expression = arcpy.AddFieldDelimiters(tempLayer, "PARCEL_ID") + " = 'Cemetery'"
 

# Run CopyFeatures to make a new copy of the feature class
arcpy.management.CopyFeatures(inFeatures, outFeatures)
 
# Run MakeFeatureLayer
arcpy.management.MakeFeatureLayer(outFeatures, tempLayer)
 
# Run SelectLayerByAttribute to determine which features to delete
arcpy.management.SelectLayerByAttribute(tempLayer, "NEW_SELECTION", 
                                        expression)
 
# Run GetCount and if some features have been selected, 
#  run DeleteFeatures to remove the selected features.
if int(arcpy.management.GetCount(tempLayer)[0]) > 0:
    arcpy.management.DeleteFeatures(tempLayer)
```

### Example 5

```python
# Description: Delete features from a feature class based on an expression
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/airport.gdb"
 
# Set local variables
inFeatures = "parcels"
outFeatures = "C:/output/output.gdb/new_parcels"
tempLayer = "parcelsLayer"
expression = arcpy.AddFieldDelimiters(tempLayer, "PARCEL_ID") + " = 'Cemetery'"
 

# Run CopyFeatures to make a new copy of the feature class
arcpy.management.CopyFeatures(inFeatures, outFeatures)
 
# Run MakeFeatureLayer
arcpy.management.MakeFeatureLayer(outFeatures, tempLayer)
 
# Run SelectLayerByAttribute to determine which features to delete
arcpy.management.SelectLayerByAttribute(tempLayer, "NEW_SELECTION", 
                                        expression)
 
# Run GetCount and if some features have been selected, 
#  run DeleteFeatures to remove the selected features.
if int(arcpy.management.GetCount(tempLayer)[0]) > 0:
    arcpy.management.DeleteFeatures(tempLayer)
```

---

## Delete Field Group (Data Management)

## Summary

Deletes a field group from a table or feature class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase feature class or table that will have the field group deleted. | Table View |
| Field Group Name | The name of the field group that will be deleted. | String |
| target_table | The input geodatabase feature class or table that will have the field group deleted. | Table View |
| name | The name of the field group that will be deleted. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteFieldGroup(target_table, name)
```

### Example 2

```python
import arcpy
arcpy.DeleteFieldGroup_management("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC", "MyFieldGroup")
```

### Example 3

```python
import arcpy
arcpy.DeleteFieldGroup_management("C:\\MyProject\\myConn.sde\\mygdb.USER1.myFC", "MyFieldGroup")
```

---

## Delete Field (Data Management)

## Summary

Deletes one or more fields from a table, feature class, feature layer, or raster dataset.

## Usage

- You can specify either the fields to delete or the fields to keep. To delete fields, use the Fields parameter to specify the fields to delete, and set the Method parameter to the Delete Fields option. To keep fields, use the Fields parameter to specify the fields to keep, and set the Method parameter to the Keep Fields option.
- To delete fields, use the Fields parameter to specify the fields to delete, and set the Method parameter to the Delete Fields option.
- To keep fields, use the Fields parameter to specify the fields to keep, and set the Method parameter to the Keep Fields option.
- Fields cannot be deleted from nonnative, read-only data formats in ArcGIS, such as VPF and CAD datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the fields to be deleted. The existing input table will be modified. | Mosaic Layer; Raster Layer; Table View |
| Fields | The fields to be deleted or kept from the input table, as specified by the Method parameter. Only nonrequired fields can be deleted. | Field |
| Method(Optional) | Specifies whether the fields specified by the Fields parameter will be deleted or kept.Delete Fields—The fields specified by the Fields parameter will be deleted. This is the default.Keep Fields—The fields specified by the Fields parameter will be kept; all other fields will be deleted.Delete Fields—The fields specified by the drop_field parameter will be deleted. This is the default.Keep Fields—The fields specified by the drop_field parameter will be kept; all other fields will be deleted. | String |
| in_table | The table containing the fields to be deleted. The existing input table will be modified. | Mosaic Layer; Raster Layer; Table View |
| drop_field[drop_field,...] | The fields to be dropped or kept from the input table, as specified by the method parameter. Only nonrequired fields can be deleted. | Field |
| method(Optional) | Specifies whether the fields specified by the drop_field parameter will be deleted or kept.DELETE_FIELDS—The fields specified by the drop_field parameter will be deleted. This is the default.KEEP_FIELDS—The fields specified by the drop_field parameter will be kept; all other fields will be deleted. | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteField(in_table, drop_field, {method})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/majorrds_copy.shp")
arcpy.management.DeleteField("C:/output/majorrds_copy.shp", 
                             ["STREET_NAM", "LABEL", "CLASS"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/majorrds_copy.shp")
arcpy.management.DeleteField("C:/output/majorrds_copy.shp", 
                             ["STREET_NAM", "LABEL", "CLASS"])
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/majorrds_copy.shp")

arcpy.management.DeleteField("C:/output/majorrds_copy.shp", 
                             ["STREET_ALIAS", "DISTRICT_ID"], "KEEP_FIELDS")
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("majorrds.shp", "C:/output/majorrds_copy.shp")

arcpy.management.DeleteField("C:/output/majorrds_copy.shp", 
                             ["STREET_ALIAS", "DISTRICT_ID"], "KEEP_FIELDS")
```

### Example 6

```python
# Name: DeleteField_Example3.py
# Description: Keep several fields from a feature class and delete all the rest of the fields
  
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFeatures = "accident.dbf"
outFeatureClass = "C:/output/new_accident.dbf"
fields = ["STREET_NAM", "LABEL", "CLASS"]
method = "KEEP_FIELDS"
 
# Run CopyFeatures to make a new copy of the feature class
#  Use CopyRows if you have a table
arcpy.management.CopyFeatures(inFeatures, outFeatureClass)
 
# Run DeleteField
arcpy.management.DeleteField(outFeatureClass, fields, method)
```

### Example 7

```python
# Name: DeleteField_Example3.py
# Description: Keep several fields from a feature class and delete all the rest of the fields
  
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFeatures = "accident.dbf"
outFeatureClass = "C:/output/new_accident.dbf"
fields = ["STREET_NAM", "LABEL", "CLASS"]
method = "KEEP_FIELDS"
 
# Run CopyFeatures to make a new copy of the feature class
#  Use CopyRows if you have a table
arcpy.management.CopyFeatures(inFeatures, outFeatureClass)
 
# Run DeleteField
arcpy.management.DeleteField(outFeatureClass, fields, method)
```

### Example 8

```python
# Description: Delete unnecessary fields from a feature class or table.
 
# Import system modules
import arcpy
 
# Get user-supplied input and output arguments
inTable = arcpy.GetParameterAsText(0)
updatedTable = arcpy.GetParameterAsText(1)

# Describe the input (need to test the dataset and data types)
desc = arcpy.Describe(inTable)

# Make a copy of the input (so you can maintain the original as is)
if desc.datasetType == "FeatureClass":
    arcpy.management.CopyFeatures(inTable, updatedTable)
else:
    arcpy.management.CopyRows(inTable, updatedTable)

# Use ListFields to get a list of field objects
fieldObjList = arcpy.ListFields(updatedTable)

# Create an empty list that will be populated with field names        
fieldNameList = []

# For each field in the object list, add the field name to the
# name list. Exclude required fields to prevent errors
for field in fieldObjList:
    if not field.required:
        fieldNameList.append(field.name)

# dBASE tables require a field other than an OID and Shape. If this is
# the case, retain an extra field (the first one in the original list)
if desc.dataType in ["ShapeFile", "DbaseTable"]:
    fieldNameList = fieldNameList[1:]

# Run DeleteField to delete all fields in the field list. 
arcpy.management.DeleteField(updatedTable, fieldNameList)
```

### Example 9

```python
# Description: Delete unnecessary fields from a feature class or table.
 
# Import system modules
import arcpy
 
# Get user-supplied input and output arguments
inTable = arcpy.GetParameterAsText(0)
updatedTable = arcpy.GetParameterAsText(1)

# Describe the input (need to test the dataset and data types)
desc = arcpy.Describe(inTable)

# Make a copy of the input (so you can maintain the original as is)
if desc.datasetType == "FeatureClass":
    arcpy.management.CopyFeatures(inTable, updatedTable)
else:
    arcpy.management.CopyRows(inTable, updatedTable)

# Use ListFields to get a list of field objects
fieldObjList = arcpy.ListFields(updatedTable)

# Create an empty list that will be populated with field names        
fieldNameList = []

# For each field in the object list, add the field name to the
# name list. Exclude required fields to prevent errors
for field in fieldObjList:
    if not field.required:
        fieldNameList.append(field.name)

# dBASE tables require a field other than an OID and Shape. If this is
# the case, retain an extra field (the first one in the original list)
if desc.dataType in ["ShapeFile", "DbaseTable"]:
    fieldNameList = fieldNameList[1:]

# Run DeleteField to delete all fields in the field list. 
arcpy.management.DeleteField(updatedTable, fieldNameList)
```

---

## Delete Identical (Data Management)

## Summary

Deletes records from a feature class or table that have identical values in a set of fields. If the geometry field is selected, feature geometries are compared.

## Usage

- For every set of identical records, the tool deletes all but the first of the identical records. The order of the identical records will be in the same order as returned by the Find Identical tool.
- The values from multiple fields in the input dataset are compared. If more than one field is specified, records are matched by the values in the first field, then by the values of the second field, and so on.
- With feature class or feature layer input, use the geometry field in the Field(s) parameter to compare feature geometries to find identical features by location. The XY Tolerance and Z Tolerance parameters are only valid when the geometry field is selected as one of the input fields.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The table or feature class that will have its identical records deleted. | Table View |
| Field(s) | The field or fields whose values will be compared to find identical records. | Field |
| XY Tolerance(Optional) | The x,y tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature. | Linear Unit |
| Z Tolerance(Optional) | The z-tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature. | Double |
| in_dataset | The table or feature class that will have its identical records deleted. | Table View |
| fields[fields,...] | The field or fields whose values will be compared to find identical records. | Field |
| xy_tolerance(Optional) | The x,y tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature. | Linear Unit |
| z_tolerance(Optional) | The z-tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature. | Double |

## Code Samples

### Example 1

```python
arcpy.management.DeleteIdentical(in_dataset, fields, {xy_tolerance}, {z_tolerance})
```

### Example 2

```python
import arcpy
arcpy.management.DeleteIdentical("C:/data/fireincidents.shp", ["ZONE", "INTENSITY"])
```

### Example 3

```python
import arcpy
arcpy.management.DeleteIdentical("C:/data/fireincidents.shp", ["ZONE", "INTENSITY"])
```

### Example 4

```python
# Name: DeleteIdentical_Example2.py
# Description: Delete identical features in a dataset based on Shape (geometry) and a TEXT field.

# Import system modules
import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/sbfire.gdb"

# Set input feature class
in_dataset = "fireincidents"

# Set the field on which the identical records are found
fields = ["Shape", "INTENSITY"]

# Set the XY tolerance within which identical records will be deleted
xy_tol = "0.02 Miles"

# Set the Z tolerance to default
z_tol = ""

# Run Delete Identical 
arcpy.management.DeleteIdentical(in_dataset, fields, xy_tol, z_tol)
```

### Example 5

```python
# Name: DeleteIdentical_Example2.py
# Description: Delete identical features in a dataset based on Shape (geometry) and a TEXT field.

# Import system modules
import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/sbfire.gdb"

# Set input feature class
in_dataset = "fireincidents"

# Set the field on which the identical records are found
fields = ["Shape", "INTENSITY"]

# Set the XY tolerance within which identical records will be deleted
xy_tol = "0.02 Miles"

# Set the Z tolerance to default
z_tol = ""

# Run Delete Identical 
arcpy.management.DeleteIdentical(in_dataset, fields, xy_tol, z_tol)
```

---

## Delete Mosaic Dataset (Data Management)

## Summary

Deletes a mosaic dataset, its overviews, and its item cache from disk.

## Usage

- This tool is used to delete a mosaic dataset in its entirety, including any of the tables within the database and, optionally, any overviews or caches created with it.
- You may not want to delete the overviews or cache when these are used in other mosaic datasets. The cache is usually created when LAS data, LAS datasets, or terrains are used in the mosaic dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that you want to delete. | Mosaic Layer |
| Delete Overview Images (Optional) | Deletes all overviews associated with the mosaic dataset.Checked—Delete the overviews associated with the mosaic dataset. This is the default.Unchecked—Do not delete the overviews. | Boolean |
| Delete Item Cache (Optional) | Delete the item cache associated with the mosaic dataset.Checked—Delete the item cache associated with the mosaic dataset. This is the default.Unchecked—Do not delete the item cache. | Boolean |
| in_mosaic_dataset | The mosaic dataset that you want to delete. | Mosaic Layer |
| delete_overview_images(Optional) | Delete all overviews associated with the mosaic dataset.DELETE_OVERVIEW_IMAGES—Delete the overviews associated with the mosaic dataset. This is the default.NO_DELETE_OVERVIEW_IMAGES—Do not delete the overviews. | Boolean |
| delete_item_cache(Optional) | Delete the item cache associated with the mosaic dataset.DELETE_ITEM_CACHE—Delete the item cache associated with the mosaic dataset. This is the default.NO_DELETE_ITEM_CACHE—Do not delete the item cache. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DeleteMosaicDataset(in_mosaic_dataset, {delete_overview_images}, {delete_item_cache})
```

### Example 2

```python
import arcpy
arcpy.DeleteMosaicDataset_management(
     "C:/Workspace/fileGDB.gdb/md2delete",
     "DELETE_OVERVIEW_IMAGES", "NO_DELETE_ITEM_CACHE")
```

### Example 3

```python
import arcpy
arcpy.DeleteMosaicDataset_management(
     "C:/Workspace/fileGDB.gdb/md2delete",
     "DELETE_OVERVIEW_IMAGES", "NO_DELETE_ITEM_CACHE")
```

### Example 4

```python
#Delete mosaic dataset including the overview images

import arcpy
arcpy.env.workspace = "C:/Workspace"

mosaicds = "fileGDB.gdb/md2delete"
delOvr = "DELETE_OVERVIEW_IMAGES"
delCache = "NO_DELETE_ITEM_CACHE"

    
arcpy.DeleteMosaicDataset_management(mosaicds, delOvr, delCache)
```

### Example 5

```python
#Delete mosaic dataset including the overview images

import arcpy
arcpy.env.workspace = "C:/Workspace"

mosaicds = "fileGDB.gdb/md2delete"
delOvr = "DELETE_OVERVIEW_IMAGES"
delCache = "NO_DELETE_ITEM_CACHE"

    
arcpy.DeleteMosaicDataset_management(mosaicds, delOvr, delCache)
```

---

## Delete Multiple (Data Management)

## Summary

Permanently deletes multiple data items of the same or different data types. All types of geographic data supported by ArcGIS, as well as toolboxes and workspaces (folders and geodatabases), can be deleted. If a specified item is a workspace, all contained items will also be deleted.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data Element | The input data that will be deleted. The data type is necessary in the event of a name conflict between data types. For example, a geodatabase can contain a feature class with an identical name to a relationship class. In this case, specify the relevant keyword. FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | Value Table |
| in_data[[Input Data Element, {Data Type},...] | The input data that will be deleted. The data type is necessary in the event of a name conflict between data types. For example, a geodatabase can contain a feature class with an identical name to a relationship class. In this case, specify the relevant keyword. FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.DeleteMultiple(in_data)
```

### Example 2

```python
import arcpy
arcpy.management.DeleteMultiple(
    in_data=[[r'C:\dataToDelete\target.gdb\fabric1', 'ParcelFabric'],
             [r'C:\dataToDelete\ctg83.gdb\ctg83FDS', 'Topology']]
)
```

### Example 3

```python
import arcpy
arcpy.management.DeleteMultiple(
    in_data=[[r'C:\dataToDelete\target.gdb\fabric1', 'ParcelFabric'],
             [r'C:\dataToDelete\ctg83.gdb\ctg83FDS', 'Topology']]
)
```

### Example 4

```python
import arcpy

arcpy.env.overwriteOutput = True
arcpy.management.Copy(r"D:\deleteMulti\dataToDelete", r"C:\dataToDelete")
arcpy.env.workspace = r"C:\dataToDelete"
arcpy.management.DeleteMultiple(
    in_data=[['C:\dataToDelete\convertlabels.gdb\points', 'FeatureClass'],
             ['C:\dataToDelete\deleteMultiple.gdb\issue7725', 'FeatureDataset'],
             ['C:\dataToDelete\RelationshipData.gdb\Destination2_really_long_name', 'RelationshipClass'],
             ['C:\dataToDelete\SMALL.gdb\Small_1', 'ParcelFabric'],
             ['C:\dataToDelete\addRaster_colorBal.gdb\colorCorrected', 'MosaicDataset']]
)
```

### Example 5

```python
import arcpy

arcpy.env.overwriteOutput = True
arcpy.management.Copy(r"D:\deleteMulti\dataToDelete", r"C:\dataToDelete")
arcpy.env.workspace = r"C:\dataToDelete"
arcpy.management.DeleteMultiple(
    in_data=[['C:\dataToDelete\convertlabels.gdb\points', 'FeatureClass'],
             ['C:\dataToDelete\deleteMultiple.gdb\issue7725', 'FeatureDataset'],
             ['C:\dataToDelete\RelationshipData.gdb\Destination2_really_long_name', 'RelationshipClass'],
             ['C:\dataToDelete\SMALL.gdb\Small_1', 'ParcelFabric'],
             ['C:\dataToDelete\addRaster_colorBal.gdb\colorCorrected', 'MosaicDataset']]
)
```

---

## Delete Raster Attribute Table (Data Management)

## Summary

Removes the raster attribute table associated with a raster dataset.

## Usage

- The input raster dataset can only have a single band.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset containing the attribute table you want to remove. | Raster Layer |
| in_raster | The raster dataset containing the attribute table you want to remove. | Raster Layer |

## Code Samples

### Example 1

```python
arcpy.management.DeleteRasterAttributeTable(in_raster)
```

### Example 2

```python
##====================================
##Delete Raster Attribute Table
##Usage: DeleteRasterAttributeTable_management in_raster
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Delete the attribute table of single band image if exist
arcpy.DeleteRasterAttributeTable_management("image.tif")
```

### Example 3

```python
##====================================
##Delete Raster Attribute Table
##Usage: DeleteRasterAttributeTable_management in_raster
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Delete the attribute table of single band image if exist
arcpy.DeleteRasterAttributeTable_management("image.tif")
```

### Example 4

```python
##====================================
##Delete Raster Attribute Table
##Usage: DeleteRasterAttributeTable_management in_raster
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Delete the attribute table of single band image if exist
arcpy.DeleteRasterAttributeTable_management("image.tif")
```

### Example 5

```python
##====================================
##Delete Raster Attribute Table
##Usage: DeleteRasterAttributeTable_management in_raster
    
import arcpy
arcpy.env.workspace = "C:/Workspace"

##Delete the attribute table of single band image if exist
arcpy.DeleteRasterAttributeTable_management("image.tif")
```

---

## Delete Rows (Data Management)

## Summary

Deletes all or the selected subset of rows from the input.

## Usage

- The Input Rows parameter value can be a dBASE table, an enterprise or file geodatabase table or feature class, shapefile, layer, or table view.
- If this tool is used on feature data, the entire row, including the geometry, will be deleted.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rows | The feature class, layer, table, or table view whose rows will be deleted. | Table View |
| in_rows | The feature class, layer, table, or table view whose rows will be deleted. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.DeleteRows(in_rows)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyRows("accident.dbf", "C:/output/accident2.dbf")
arcpy.management.DeleteRows("C:/output/accident2.dbf")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.CopyRows("accident.dbf", "C:/output/accident2.dbf")
arcpy.management.DeleteRows("C:/output/accident2.dbf")
```

### Example 4

```python
# Description: Delete rows from a table based on an expression
 
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
inTable = "accident.dbf"
outTable = "C:/output/new_accident.dbf"
tempTableView = "accidentTableView"
expression = arcpy.AddFieldDelimiters(tempTableView, "Measure") + " = 0"
 
# Run CopyRows to make a new copy of the table
arcpy.management.CopyRows(inTable, outTable)

# Run MakeTableView
arcpy.management.MakeTableView(outTable, tempTableView)

# Run SelectLayerByAttribute to determine which rows to delete
arcpy.management.SelectLayerByAttribute(tempTableView, "NEW_SELECTION", 
                                        expression)

# Run GetCount and if some features have been selected, run
#  DeleteRows to remove the selected rows.
if int(arcpy.management.GetCount(tempTableView)[0]) > 0:
    arcpy.management.DeleteRows(tempTableView)
```

### Example 5

```python
# Description: Delete rows from a table based on an expression
 
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
inTable = "accident.dbf"
outTable = "C:/output/new_accident.dbf"
tempTableView = "accidentTableView"
expression = arcpy.AddFieldDelimiters(tempTableView, "Measure") + " = 0"
 
# Run CopyRows to make a new copy of the table
arcpy.management.CopyRows(inTable, outTable)

# Run MakeTableView
arcpy.management.MakeTableView(outTable, tempTableView)

# Run SelectLayerByAttribute to determine which rows to delete
arcpy.management.SelectLayerByAttribute(tempTableView, "NEW_SELECTION", 
                                        expression)

# Run GetCount and if some features have been selected, run
#  DeleteRows to remove the selected rows.
if int(arcpy.management.GetCount(tempTableView)[0]) > 0:
    arcpy.management.DeleteRows(tempTableView)
```

---

## Delete Schema Geodatabase (Data Management)

## Summary

Deletes a geodatabase from a user's schema in Oracle.

## Usage

- The Delete Schema Geodatabase tool only deletes user-schema geodatabases in Oracle; it cannot delete the sde geodatabase.
- Only the owner of the schema geodatabase can run the Delete Schema Geodatabase tool.
- You must remove all data from the user-schema geodatabase before you can delete the geodatabase.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection file (.sde) of the user-schema geodatabase to be deleted. You must connect as the schema owner. | Workspace |
| input_database | The database connection file (.sde) of the user-schema geodatabase to be deleted. You must connect as the schema owner. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.DeleteSchemaGeodatabase(input_database)
```

### Example 2

```python
import arcpy
schema_gdb = "usr/connections/land@ora11204.sde"
arcpy.management.DeleteSchemaGeodatabase(schema_gdb)
```

### Example 3

```python
import arcpy
schema_gdb = "usr/connections/land@ora11204.sde"
arcpy.management.DeleteSchemaGeodatabase(schema_gdb)
```

### Example 4

```python
# Set the necessary product code
import arceditor

# Import arcpy module
import arcpy

# Local variables:
schema_gdb = "usr/connections/land@ora11204.sde"

# Process: Delete Schema Geodatabase
arcpy.management.DeleteSchemaGeodatabase(schema_gdb)
```

### Example 5

```python
# Set the necessary product code
import arceditor

# Import arcpy module
import arcpy

# Local variables:
schema_gdb = "usr/connections/land@ora11204.sde"

# Process: Delete Schema Geodatabase
arcpy.management.DeleteSchemaGeodatabase(schema_gdb)
```

---

## Delete Version (Data Management)

## Summary

Deletes the specified version from the input enterprise geodatabase.

## Usage

- Traditional versioning—The version owner or the geodatabase administrator can delete a traditional version. For more information, see Delete a traditional version.
- Branch versioning—The version owner or the version administrator for the web feature layer can delete a branch version. For more information, see Delete a branch version.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The database connection file to the enterprise geodatabase containing the version to be deleted.For branch versioning, use a feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or a feature layer portal item.You can also delete a branch version using a database connection file (connected to a branch versioned workspace) when connected as the geodatabase admin user. | Workspace |
| Version Name | The name of the version to be deleted.For branch versioning, if the input workspace is a database connection file, the name of the branch version to delete should be fully qualified (for example, servicename.portaluser.versionname). If the input workspace is a feature service URL, the name of the branch version to delete should not include the service name (for example, portaluser.versionname). | String |
| in_workspace | The database connection file to the enterprise geodatabase containing the version to be deleted.For branch versioning, use a feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or a feature layer portal item.You can also delete a branch version using a database connection file (connected to a branch versioned workspace) when connected as the geodatabase admin user. | Workspace |
| version_name | The name of the version to be deleted.For branch versioning, if the input workspace is a database connection file, the name of the branch version to delete should be fully qualified (for example, servicename.portaluser.versionname). If the input workspace is a feature service URL, the name of the branch version to delete should not include the service name (for example, portaluser.versionname). | String |

## Code Samples

### Example 1

```python
arcpy.management.DeleteVersion(in_workspace, version_name)
```

### Example 2

```python
# Description: Deletes a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/Connections/whistler@gdb.sde"
versionName = "myVersion2"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

### Example 3

```python
# Description: Deletes a version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/Connections/whistler@gdb.sde"
versionName = "myVersion2"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

### Example 4

```python
# Description: Deletes a branch version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/project_path/SQL_SERV.sde"
versionName = "BuildingService.portaluser.Viewer"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

### Example 5

```python
# Description: Deletes a branch version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "c:/project_path/SQL_SERV.sde"
versionName = "BuildingService.portaluser.Viewer"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

### Example 6

```python
# Description: Deletes a branch version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "https://myserver.mydomain.com/server/rest/services/MyService/FeatureServer"
versionName = "PORTALUSER1.newversion2"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

### Example 7

```python
# Description: Deletes a branch version

# Import system modules
import arcpy

# Set local variables
inWorkspace = "https://myserver.mydomain.com/server/rest/services/MyService/FeatureServer"
versionName = "PORTALUSER1.newversion2"

# Run DeleteVersion
arcpy.management.DeleteVersion(inWorkspace, versionName)
```

---

## Delete (Data Management)

## Summary

Permanently deletes data. All types of geographic data supported by ArcGIS, as well as toolboxes and workspaces (folders and geodatabases), can be deleted. If the specified item is a workspace, all contained items will also be deleted.

## Usage

- Data currently in use in another ArcGIS application cannot be deleted.
- Deleting a shapefile also deletes ancillary files such as the metadata, projection, and index files.
- Deleting a folder moves the folder to the system Recycle Bin, where it can be restored or permanently deleted.
- Feature classes and tables participating in a network analysis dataset or a topology cannot be deleted.
- Deleting a mosaic dataset will only delete the mosaic dataset. To delete a mosaic dataset in its entirety—including any tables in the database and, optionally, any overviews or caches created with it—use the Delete Mosaic Dataset tool.
- Deleting a database connection file does not delete the enterprise database. A database connection file is simply a shortcut to the database.
- Deleting a relationship class deletes the row corresponding to that relationship from the relationship table.
- Deleting a layer removes only the layer; its source data is not deleted.
- This tool does not delete hosted feature services.
- This tool can be used to delete the memory workspace. When the memory workspace is deleted, all datasets in the workspace are deleted, but the memory workspace remains, allowing you to continue writing to it.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data Element | The input data that will be deleted. | Data Element; Graph; Layer; Table View; Utility Network |
| Data type(Optional) | The type of data on disk to be deleted.This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |
| in_data[in_data,...] | The input data that will be deleted. | Data Element; Graph; Layer; Table View; Utility Network |
| data_type(Optional) | The type of data on disk to be deleted.This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.Delete(in_data, {data_type})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Copy("majorrds.shp", "majorrdsCopy.shp")
arcpy.management.Delete("majorrdsCopy.shp")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Copy("majorrds.shp", "majorrdsCopy.shp")
arcpy.management.Delete("majorrdsCopy.shp")
```

### Example 4

```python
# Name: Delete_Example2.py
# Description: Delete majorrdsCopy.shp 

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_data = "majorrds.shp"
out_data = "majorrdscopy.shp"

# Run Copy
arcpy.management.Copy(in_data, out_data)

# Run Delete
arcpy.management.Delete(out_data)
```

### Example 5

```python
# Name: Delete_Example2.py
# Description: Delete majorrdsCopy.shp 

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_data = "majorrds.shp"
out_data = "majorrdscopy.shp"

# Run Copy
arcpy.management.Copy(in_data, out_data)

# Run Delete
arcpy.management.Delete(out_data)
```

### Example 6

```python
import arcpy
arcpy.env.workspace = r"C:\dataToDelete"
arcpy.management.Delete(['NIRrG_ps8.afr', 'redlands.tpkx', 'colormap.img'])
```

### Example 7

```python
import arcpy
arcpy.env.workspace = r"C:\dataToDelete"
arcpy.management.Delete(['NIRrG_ps8.afr', 'redlands.tpkx', 'colormap.img'])
```

---

## Detect Feature Changes (Data Management)

## Summary

Finds where the update line features spatially match the base line features and detects spatial changes, attribute changes, or both, as well as no change. It then creates an output feature class containing matched update features with information about their changes, unmatched update features, and unmatched base features.

## Usage

- A typical use case for this tool is that you maintain a set of line features, roads, for example, and receive updates periodically from partners as a new set of road features. You want to know which of the updated features are changes to existing base features or are new features to be added, and which base features are old and should be deleted. This tool finds the matching features between the update and base line datasets and detects spatial changes, attribute changes, both spatial and attribute changes, or no changes, and creates an output feature class containing the feature change information.
- The output feature class contains all participating update features (matched and unmatched) and any unmatched base features. Information about the detected changes are written to the following fields: UPDATE_FID—The feature ID of the update feature. The value is -1 for an unmatched base feature. BASE_FID—The feature ID of the base feature. The value is -1 for an unmatched update feature. CHANGE_TYPE—The type of change detected, with values as follows:S—A matched update feature with a spatial change.A—Aa matched update feature with an attribute change.SA—A matched update feature with both spatial and attribute changes.S_LD—A spatial change as well as opposite line directions.SA_LD—Spatial and attribute changes as well as opposite line directions.NC—A matched update feature with no change.N—An unmatched update feature that is new to the base data.D—An unmatched base feature that may need to be deleted from the base data.
- UPDATE_FID—The feature ID of the update feature. The value is -1 for an unmatched base feature.
- BASE_FID—The feature ID of the base feature. The value is -1 for an unmatched update feature.
- CHANGE_TYPE—The type of change detected, with values as follows:S—A matched update feature with a spatial change.A—Aa matched update feature with an attribute change.SA—A matched update feature with both spatial and attribute changes.S_LD—A spatial change as well as opposite line directions.SA_LD—Spatial and attribute changes as well as opposite line directions.NC—A matched update feature with no change.N—An unmatched update feature that is new to the base data.D—An unmatched base feature that may need to be deleted from the base data.
- S—A matched update feature with a spatial change.
- A—Aa matched update feature with an attribute change.
- SA—A matched update feature with both spatial and attribute changes.
- S_LD—A spatial change as well as opposite line directions.
- SA_LD—Spatial and attribute changes as well as opposite line directions.
- NC—A matched update feature with no change.
- N—An unmatched update feature that is new to the base data.
- D—An unmatched base feature that may need to be deleted from the base data.
- The feature matching process is done first based on the Search Distance and the optional Match Fields parameter values. An output match table can be produced to store the match information.The Search Distance parameter is used in finding match candidates. Use a distance large enough to catch most of the shifts between corresponding features, but not too large to cause unnecessary processing of too many candidates and potentially getting wrong matches. Once match candidates are found, they are evaluated by a set of geometric measures to determine if they are similar enough to be considered spatially matched. If you specify one or more pairs of fields for the Match Fields parameter, spatially matched candidates are compared to these field values to help determine the correct match. For example, if the update and base features both have a STREET_NAME field containing street names, and an update feature spatially matches two base features but only one base candidate has the same STREET_NAME value as the update feature, it is the better match. The comparison of text strings is not case sensitive, meaning that First St is considered the same as first st.
- The Output Match Table parameter is optional. The match table provides complete feature matching information, including the source and target FIDs, match groups, match relationships, and the level of confidence of the matching derived from spatial and attribute matching conditions. This information can help you understand the match situations and aid in postinspection, postediting, and further analysis. See About feature matching and the match table for details.In the output match table, the values in the SRC_FID and TGT_FID fields represent update feature IDs and base feature IDs, respectively.
- The process of identifying changes occurs after feature matching. The spatial and attribute conditions, as well as line directions of all matched update features, are compared with corresponding base features to determine their CHANGE_TYPE values. A spatial change (change type S) is detected when one or both of the following occur: The matched update features differ from their corresponding base features in topology. For example, one update feature matches two base features. In each match group, any portions of the update or base features fall outside the Change Tolerance parameter value.The Change Tolerance parameter value serves as the width of a buffer zone around the update features or the base features. For each match group, an update buffer is created around all update features and a base buffer around all base features. Then all the update features are compared to the base buffer and all base features are compared to the update buffer. If any portions of the update features fall outside the base buffer or if any portions of the base features fall outside the update buffer, or if both occur, it is considered a spatial change. When a value greater than the XY Tolerance of the input data is specified, the output will include the following new fields: LEN_PCT—This field stores length percentage values based on match groups. LEN_ABS—This field stores absolute length values in feature units based on match groups. The values of these fields are determined based on the match group that the output feature belongs to, depending on the following conditions:If only update features fall outside the base buffer:Lu = total length of the outside portions of the update features.Pu (the length percentage) = Lu / the total length of all the update features * 100. LEN_PCT = PuLEN_ABS = LuIf only base features fall outside the update buffer:Lb = total length of the outside portions of the base features.Pb (the length percentage) = Lb / the total length of all the base features * 100. LEN_PCT = PbLEN_ABS = Lb If both of the above occur:The larger of Lu and Lb and its associated percentage value are written. In case of Lb being the written, a negative sign is added for the same reason mentioned above. The percentage value ranges from 0 to 100 in which 0 means both update and base features in a match group are completely within the change tolerance zones,; and 100 means all update or base features are outside the change tolerance zones.The LEN_PCT and LEN_ABS values can help you determine whether a change is significant. The higher the values, the more dramatic the change. Only features with spatial changes are assigned these values; others are assigned -1. An attribute change (change type A) is detected based on the Compare Fields parameter value. If you specify one or more pairs of fields for the Compare Fields parameter, matched features are compared to these fields to determine if there is an attribute change. The comparison of text strings is not case sensitive, meaning that First St is considered the same as first st. If both spatial and attribute changes are detected for a matched update feature, it is assigned the change type SA. If no spatial and attribute changes are detected for a matched update feature, it is considered no change and is assigned the change type NC. Line direction change (S_LD or SA_LD change types) is considered a spatial change and can be detected as an option. The matched update features are assigned change type S_LD if the direction of any of their lines are opposite from one or more matched base features, with or without the spatial changes described above. In other words, they are the S or NC types if line directions are not compared. The matched update features are assigned change type SA_LD if the direction of any of their lines are opposite from one or more matched base features, in addition to the attribute change or the spatial and attribute changes described above. In other words, they are the A or SA types if line directions are not compared.
- The matched update features differ from their corresponding base features in topology. For example, one update feature matches two base features.
- In each match group, any portions of the update or base features fall outside the Change Tolerance parameter value.The Change Tolerance parameter value serves as the width of a buffer zone around the update features or the base features. For each match group, an update buffer is created around all update features and a base buffer around all base features. Then all the update features are compared to the base buffer and all base features are compared to the update buffer. If any portions of the update features fall outside the base buffer or if any portions of the base features fall outside the update buffer, or if both occur, it is considered a spatial change. When a value greater than the XY Tolerance of the input data is specified, the output will include the following new fields: LEN_PCT—This field stores length percentage values based on match groups. LEN_ABS—This field stores absolute length values in feature units based on match groups. The values of these fields are determined based on the match group that the output feature belongs to, depending on the following conditions:If only update features fall outside the base buffer:Lu = total length of the outside portions of the update features.Pu (the length percentage) = Lu / the total length of all the update features * 100. LEN_PCT = PuLEN_ABS = LuIf only base features fall outside the update buffer:Lb = total length of the outside portions of the base features.Pb (the length percentage) = Lb / the total length of all the base features * 100. LEN_PCT = PbLEN_ABS = Lb If both of the above occur:The larger of Lu and Lb and its associated percentage value are written. In case of Lb being the written, a negative sign is added for the same reason mentioned above. The percentage value ranges from 0 to 100 in which 0 means both update and base features in a match group are completely within the change tolerance zones,; and 100 means all update or base features are outside the change tolerance zones.The LEN_PCT and LEN_ABS values can help you determine whether a change is significant. The higher the values, the more dramatic the change. Only features with spatial changes are assigned these values; others are assigned -1.
- LEN_PCT—This field stores length percentage values based on match groups.
- LEN_ABS—This field stores absolute length values in feature units based on match groups.
- If only update features fall outside the base buffer:Lu = total length of the outside portions of the update features.Pu (the length percentage) = Lu / the total length of all the update features * 100. LEN_PCT = PuLEN_ABS = Lu
- Lu = total length of the outside portions of the update features.
- Pu (the length percentage) = Lu / the total length of all the update features * 100.
- LEN_PCT = Pu
- LEN_ABS = Lu
- If only base features fall outside the update buffer:Lb = total length of the outside portions of the base features.Pb (the length percentage) = Lb / the total length of all the base features * 100. LEN_PCT = PbLEN_ABS = Lb
- Lb = total length of the outside portions of the base features.
- Pb (the length percentage) = Lb / the total length of all the base features * 100.
- LEN_PCT = Pb
- LEN_ABS = Lb
- If both of the above occur:The larger of Lu and Lb and its associated percentage value are written. In case of Lb being the written, a negative sign is added for the same reason mentioned above.
- The larger of Lu and Lb and its associated percentage value are written. In case of Lb being the written, a negative sign is added for the same reason mentioned above.
- The matched update features are assigned change type S_LD if the direction of any of their lines are opposite from one or more matched base features, with or without the spatial changes described above. In other words, they are the S or NC types if line directions are not compared.
- The matched update features are assigned change type SA_LD if the direction of any of their lines are opposite from one or more matched base features, in addition to the attribute change or the spatial and attribute changes described above. In other words, they are the A or SA types if line directions are not compared.
- The union of input extents is used as the processing extent. The counts of participating source and target features are reported in the processing messages.
- Feature matching accuracy relies on data quality, complexity, and similarities of the two inputs.Minimize data errors and select relevant features as input through preprocessing. In general, it is helpful when the features in an input dataset are topologically correct, have valid geometry, and are singlepart and not duplicate; otherwise, unexpected results may occur.
- You can review the detected changes in the output feature class. You may find that the spatial differences between the two data sources are significant and decide that one of them should be adjusted to better match the other. You can also transfer attributes between update features and base features. You can use the rubbersheeting and attribute transfer tools in the Conflation toolset to make the changes.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Update Features | The line features that will be compared to the base features. | Feature Layer |
| Base Features | The line features that will be compared to the update features for change detection. | Feature Layer |
| Output Feature Class | The output line feature class containing the change information. The output contains all participating update features (matched and unmatched) and any unmatched base features. | Feature Class |
| Search Distance | The distance that will be used to search for match candidates. A distance must be specified and it must be greater than zero. You can choose a preferred unit. The default is the feature unit. | Linear Unit |
| Match Fields (Optional) | The match fields from the update and base features. If specified, each pair of fields are compared for match candidates to help determine the right match. | Value Table |
| Output Match Table (Optional) | The output table containing complete feature matching information. | Table |
| Change Tolerance (Optional) | The distance used to determine if there is a spatial change. All matched update features and base features are compared to this tolerance. If any portions of the update or the base features fall outside the zone around the matched feature, it is considered a spatial change. The value must be greater than the XY Tolerance of the input data so this process can be performed and the output will include the LEN_PCT and LEN_ABS fields. The default is 0, meaning this process is not performed. Any value between 0 and the data's XY Tolerance (inclusively) will make the process irrelevant and will be replaced by 0. You can choose a unit; the default is the feature unit. | Linear Unit |
| Compare Fields (Optional) | The fields that will determine if there is an attribute change between the matched update and base features. | Value Table |
| Compare line direction(Optional) | Specifies whether line directions will be compared for matched features.Unchecked—Line directions will not be compared for matched features. This is the default.Checked—Line directions will be compared for matched features. | Boolean |
| update_features | The line features that will be compared to the base features. | Feature Layer |
| base_features | The line features that will be compared to the update features for change detection. | Feature Layer |
| out_feature_class | The output line feature class containing the change information. The output contains all participating update features (matched and unmatched) and any unmatched base features. | Feature Class |
| search_distance | The distance that will be used to search for match candidates. A distance must be specified and it must be greater than zero. You can choose a preferred unit. The default is the feature unit. | Linear Unit |
| match_fields[[source_field, target_field],...](Optional) | The match fields from the update and base features. If specified, each pair of fields are compared for match candidates to help determine the right match. | Value Table |
| out_match_table(Optional) | The output table containing complete feature matching information. | Table |
| change_tolerance(Optional) | The distance used to determine if there is a spatial change. All matched update features and base features are compared to this tolerance. If any portions of the update or the base features fall outside the zone around the matched feature, it is considered a spatial change. The value must be greater than the XY Tolerance of the input data so this process can be performed and the output will include the LEN_PCT and LEN_ABS fields. The default is 0, meaning this process is not performed. Any value between 0 and the data's XY Tolerance (inclusively) will make the process irrelevant and will be replaced by 0. You can choose a unit; the default is the feature unit. | Linear Unit |
| compare_fields[[source_field, target_field],...](Optional) | The fields that will determine if there is an attribute change between the matched update and base features. | Value Table |
| compare_line_direction(Optional) | Specifies whether line directions will be compared for matched features.NO_COMPARE_DIRECTION—Line directions will not be compared for matched features. This is the default.COMPARE_DIRECTION—Line directions will be compared for matched features. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DetectFeatureChanges(update_features, base_features, out_feature_class, search_distance, {match_fields}, {out_match_table}, {change_tolerance}, {compare_fields}, {compare_line_direction})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DetectFeatureChanges_edit("update_Roads.shp",
                                "base_Roads.shp", "output_changes.shp"
                                "25 Feet", #, #, "7.6 Meters",
                                ["rdClass", "roadClass"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DetectFeatureChanges_edit("update_Roads.shp",
                                "base_Roads.shp", "output_changes.shp"
                                "25 Feet", #, #, "7.6 Meters",
                                ["rdClass", "roadClass"])
```

### Example 4

```python
# Name:        DetectFeatureChanges_example_script2.py
# Description: Perform change detection between newly received road data and
#              existing road data and find the number of new roads and the
#              total length of them.
# Author:      Esri
# -----------------------------------------------------------------------

# Import system modules
import arcpy
from arcpy import env

# Set environment settings
env.overwriteOutput = True
env.workspace = r"D:\conflationTools\ScriptExamples\data.gdb"

# Set local variables
updateFeatures = "updateRoads"
baseFeatures = "baseRoads"
dfcOutput = "dfc_out"

search_distance = "300 Feet"
match_fields = "RD_NAME FULLNAME"

statsTable = "new_roads_stats"

# Perform spatial change detection
arcpy.DetectFeatureChanges_management(updateFeatures, baseFeatures, dfcOutput, search_distance, match_fields)

# ====================================================================================
# Note 1:  The result of DetectFeatureChanges may contain errors; see tool reference.
#          Inspection and editing may be necessary to ensure correct CHANGE_TYPE N, which
#          represents un-matched update feautres, before further calculations.
#
#          One of the quick ways of checking whether the CHANGE_TYPE N features have
#          matching base features is to find their mid-points and use them to search for
#          features in base data, as processed below.
# ====================================================================================

# ======== Check update roads with CHANGE_TYPE N for potential match
# Make Feature Layer with selection of CHANGE_TYPE = 'N' (un-matched update features)
arcpy.MakeFeatureLayer_management(dfcOutput, "sel_N_layer", "CHANGE_TYPE = 'N'")

# Get mid-points of the selected features; the mid-points carry all the attributes.
arcpy.FeatureVerticesToPoints_management("sel_N_layer", "in_memory\midPts", "MID")

# Find nearest base features from the mid-points
arcpy.Near_analysis("in_memory\midPts", baseFeatures, "300 Feet")

# ====================================================================================
# Note 2:  At this point you can manually inspect the midPts by the NEAR_DIST values; 
#          the lower the values, the higher chance (not always) a match was missed in the 
#          dfc process. Delete features from midPts that have found matching base features 
#          before further process.
# ====================================================================================

# Transfer CHANGE_TYPE values from features of midPts to update features
arcpy.JoinField_management(updateFeatures, "OBJECTID", "in_memory\midPts", "UPDATE_FID", "CHANGE_TYPE")

# Get the count of new roads and the total length; the remaining roads have
# Null values for CHANGE_TYPE.
arcpy.Frequency_analysis(updateFeatures, statsTable, "CHANGE_TYPE", "Shape_Length")
```

### Example 5

```python
# Name:        DetectFeatureChanges_example_script2.py
# Description: Perform change detection between newly received road data and
#              existing road data and find the number of new roads and the
#              total length of them.
# Author:      Esri
# -----------------------------------------------------------------------

# Import system modules
import arcpy
from arcpy import env

# Set environment settings
env.overwriteOutput = True
env.workspace = r"D:\conflationTools\ScriptExamples\data.gdb"

# Set local variables
updateFeatures = "updateRoads"
baseFeatures = "baseRoads"
dfcOutput = "dfc_out"

search_distance = "300 Feet"
match_fields = "RD_NAME FULLNAME"

statsTable = "new_roads_stats"

# Perform spatial change detection
arcpy.DetectFeatureChanges_management(updateFeatures, baseFeatures, dfcOutput, search_distance, match_fields)

# ====================================================================================
# Note 1:  The result of DetectFeatureChanges may contain errors; see tool reference.
#          Inspection and editing may be necessary to ensure correct CHANGE_TYPE N, which
#          represents un-matched update feautres, before further calculations.
#
#          One of the quick ways of checking whether the CHANGE_TYPE N features have
#          matching base features is to find their mid-points and use them to search for
#          features in base data, as processed below.
# ====================================================================================

# ======== Check update roads with CHANGE_TYPE N for potential match
# Make Feature Layer with selection of CHANGE_TYPE = 'N' (un-matched update features)
arcpy.MakeFeatureLayer_management(dfcOutput, "sel_N_layer", "CHANGE_TYPE = 'N'")

# Get mid-points of the selected features; the mid-points carry all the attributes.
arcpy.FeatureVerticesToPoints_management("sel_N_layer", "in_memory\midPts", "MID")

# Find nearest base features from the mid-points
arcpy.Near_analysis("in_memory\midPts", baseFeatures, "300 Feet")

# ====================================================================================
# Note 2:  At this point you can manually inspect the midPts by the NEAR_DIST values; 
#          the lower the values, the higher chance (not always) a match was missed in the 
#          dfc process. Delete features from midPts that have found matching base features 
#          before further process.
# ====================================================================================

# Transfer CHANGE_TYPE values from features of midPts to update features
arcpy.JoinField_management(updateFeatures, "OBJECTID", "in_memory\midPts", "UPDATE_FID", "CHANGE_TYPE")

# Get the count of new roads and the total length; the remaining roads have
# Null values for CHANGE_TYPE.
arcpy.Frequency_analysis(updateFeatures, statsTable, "CHANGE_TYPE", "Shape_Length")
```

---

## Diagnose Version Metadata (Data Management)

## Summary

Identifies inconsistencies in the system tables used to manage traditional versions and states in a geodatabase.

## Usage

- This tool can be run on enterprise geodatabases.
- This tool does not support geodatabases in SAP HANA because they don't support traditional versioning.
- Only the geodatabase administrator can run the Diagnose Version Metadata tool.
- This tool is usually run at the direction of Esri Technical Support.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection (.sde file) to the enterprise geodatabase in which traditional versioning system table inconsistencies may exist.The connection must be made as the geodatabase administrator. | Workspace |
| Diagnostic Version Metadata Log | The name and location of the output log file.The log file is an ASCII file containing a list of the system tables in the specified version that contain inconsistent records, as well as the database connection file used. | File |
| input_database | The database connection (.sde file) to the enterprise geodatabase in which traditional versioning system table inconsistencies may exist.The connection must be made as the geodatabase administrator. | Workspace |
| out_log | The name and location of the output log file.The log file is an ASCII file containing a list of the system tables in the specified version that contain inconsistent records, as well as the database connection file used. | File |

## Code Samples

### Example 1

```python
arcpy.management.DiagnoseVersionMetadata(input_database, out_log)
```

### Example 2

```python
import arcpy

input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
arcpy.DiagnoseVersionMetadata_management(input_database, out_log)
```

### Example 3

```python
import arcpy

input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
arcpy.DiagnoseVersionMetadata_management(input_database, out_log)
```

### Example 4

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"

# Process: Diagnose Version Metadata
arcpy.DiagnoseVersionMetadata_management(input_database, out_log)
```

### Example 5

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"

# Process: Diagnose Version Metadata
arcpy.DiagnoseVersionMetadata_management(input_database, out_log)
```

---

## Diagnose Version Tables (Data Management)

## Summary

Identifies inconsistencies in the delta (A and D) tables of datasets that are registered for traditional versioning.

## Usage

- This tool can be run on enterprise geodatabases.
- This tool does not support geodatabases in SAP HANA because they don't support traditional versioning.
- Only the geodatabase administrator can run the Diagnose Version Tables tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection (.sde file) to the enterprise geodatabase in which delta table inconsistencies may exist. The connection must be made as the geodatabase administrator. | Workspace |
| Diagnose Version Tables Log | The path and name of the output log file. The log file is an ASCII file containing a list of the tables in the specified version that contain inconsistent records, as well as information about the connection file, geodatabase version, and tables for which the tool was run. | File |
| Target Version (Optional) | The geodatabase version with the delta tables that will be checked for inconsistencies. The drop-down list is populated with the existing versions in the geodatabase specified for the Input Database Connection parameter. If no version is selected, all versions will be processed. | String |
| Input Tables(Optional) | A single table or a text file containing a list of versioned tables with the associated delta tables to be checked for inconsistencies. Use fully-qualified table names in the text file, and place one table name per line. If no file is specified, all tables in the geodatabase are processed. | String |
| input_database | The database connection (.sde file) to the enterprise geodatabase in which delta table inconsistencies may exist. The connection must be made as the geodatabase administrator. | Workspace |
| out_log | The path and name of the output log file. The log file is an ASCII file containing a list of the tables in the specified version that contain inconsistent records, as well as information about the connection file, geodatabase version, and tables for which the tool was run. | File |
| target_version(Optional) | The geodatabase version with the delta tables that will be checked for inconsistencies. If no version is specified, all versions are processed. | String |
| input_tables[input_tables,...](Optional) | A single table or a text file containing a list of versioned tables with the associated delta tables to be checked for inconsistencies. Use fully-qualified table names in the text file, and place one table name per line. If no file is specified, all tables in the geodatabase are processed. | String |

## Code Samples

### Example 1

```python
arcpy.management.DiagnoseVersionTables(input_database, out_log, {target_version}, {input_tables})
```

### Example 2

```python
import arcpy

input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

arcpy.DiagnoseVersionTables_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 3

```python
import arcpy

input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

arcpy.DiagnoseVersionTables_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 4

```python
# Description: diagnose version metadata

# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

# Process: Diagnose Version Tables
arcpy.DiagnoseVersionTables_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 5

```python
# Description: diagnose version metadata

# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_diagnose.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

# Process: Diagnose Version Tables
arcpy.DiagnoseVersionTables_management(input_database, out_log, target_version, 
                                       input_tables)
```

---

## Dice (Data Management)

## Summary

Subdivides a feature into smaller features based on a specified vertex limit. This tool is intended as a way to subdivide extremely large features that cause issues with drawing, analysis, editing, and/or performance but are difficult to split up with standard editing and geoprocessing tools. This tool should not be used in any cases other than those where tools are failing to complete successfully due to the size of features.

## Usage

- The input can be a multipoint, line, or polygon feature layer or feature class.
- No default vertex limit is provided. The number of vertices in a single feature that may cause issues due to being excessively large is dependent on your hardware configuration.
- Features that do not exceed the vertex limit will be written to the output feature class as is.
- Attribute values from the input feature classes will be carried across to the output feature class unless the input is a layer or layers created by the Make Feature Layer or Make Table View tool and the field's Use Ratio Policy is checked. If a feature in an overlay operation is split, the attributes of resulting features are a ratio of the original feature's value. The ratio is based on the ratio in which the original geometry is divided. If the geometry is divided equally, each new feature's attribute gets one-half of the value of the original object's attribute. Use Ratio Policy only applies to numeric field types. Geoprocessing tools do not honor geodatabase feature class or table field split policies.
- The splitting of polygons may create new vertices.
- Polygon components (think of this as the outer boundary of a part, and all the holes and other parts that it contains) will be grouped together in the output.
- Lines are only diced at a vertex.
- For line and polygon feature classes the number of vertices specified in the Vertex Limit parameter is not always the vertex count you get in the output after a feature has been diced.
- This tool does not use the Output Coordinate System environment. This means there will be no projecting of features prior to processing. You always end up with the same coordinate system as the input. Any projecting should be done after the Dice tool has been run. This is done because projecting the problem feature may cause a system failure if it exceeds system resources.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input feature class or feature layer. The geometry type must be multipoint, line, or polygon. | Feature Layer |
| Output Feature Class | The output feature class of diced features. | Feature Class |
| Vertex Limit | Features with geometries that exceed this vertex limit will be subdivided before being written to the output feature class. | Long |
| in_features | The input feature class or feature layer. The geometry type must be multipoint, line, or polygon. | Feature Layer |
| out_feature_class | The output feature class of diced features. | Feature Class |
| vertex_limit | Features with geometries that exceed this vertex limit will be subdivided before being written to the output feature class. | Long |

## Code Samples

### Example 1

```python
arcpy.management.Dice(in_features, out_feature_class, vertex_limit)
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/data/gdb/thailand.gdb"
arcpy.Dice_management('thailandBoundary', 'thai_Dice_1mill', 1000000)
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/data/gdb/thailand.gdb"
arcpy.Dice_management('thailandBoundary', 'thai_Dice_1mill', 1000000)
```

### Example 4

```python
# Dice.py
# Description: Simple example showing use of Dice tool

 
# Import system modules
import arcpy
from arcpy import env
env.workspace = "C:/data/gdb/canada.gdb"

# Set variables
fcName = "coastline"
outFcName = "coastline_Dice_750k"
vertLimit = 750000

#Process: Use the Dice function
arcpy.Dice_management (fcName, outFcName, vertLimit)
```

### Example 5

```python
# Dice.py
# Description: Simple example showing use of Dice tool

 
# Import system modules
import arcpy
from arcpy import env
env.workspace = "C:/data/gdb/canada.gdb"

# Set variables
fcName = "coastline"
outFcName = "coastline_Dice_750k"
vertLimit = 750000

#Process: Use the Dice function
arcpy.Dice_management (fcName, outFcName, vertLimit)
```

---

## Disable Archiving (Data Management)

## Summary

Disables archiving on a geodatabase feature class, table, or feature dataset.

## Usage

- Archiving is only supported on enterprise geodatabases. File and personal geodatabases do not support archiving.
- The input dataset must be from a database connection established as the data owner.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The geodatabase feature class, table, or feature dataset for which archiving will be disabled. | Table; Feature Class; Feature Dataset |
| Preserve History Table (Optional) | Specifies whether records that are not from the current moment will be preserved. If the table or feature class is versioned, the history table will become available. For nonversioned data, a table or feature class will be created with an appended _h that contains the history information.Checked—Records that are not from the current moment will be preserved. This is the default.Unchecked—Records that are not from the current moment will not be preserved; they will be deleted. | Boolean |
| in_dataset | The geodatabase feature class, table, or feature dataset for which archiving will be disabled. | Table; Feature Class; Feature Dataset |
| preserve_history(Optional) | Specifies whether records that are not from the current moment will be preserved.If the table or feature class is versioned, the history table or feature will become enabled.For nonversioned data, a table or feature class will be created that contains the history information. The name of the new dataset will be the same as the input with an appended _h.PRESERVE—Records that are not from the current moment will be preserved. This is the default.DELETE—Records that are not from the current moment will not be preserved; they will be deleted. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DisableArchiving(in_dataset, {preserve_history})
```

### Example 2

```python
import arcpy
arcpy.DisableArchiving_management("Database Connections//sam.hills", "PRESERVE")
```

### Example 3

```python
import arcpy
arcpy.DisableArchiving_management("Database Connections//sam.hills", "PRESERVE")
```

### Example 4

```python
# Name: DisableArchiving_Example.py
# Description: Disable archiving on a dataset

# Import system modules
import arcpy

# Set local variables
in_dataset = "C:/Data/connections/intense.sde/intense.carbine.bike_routes"

# Run program
desc = arcpy.Describe(in_dataset)
if desc.isArchived == True:
  arcpy.DisableArchiving_management(in_dataset)
  print('Successfully disabled archiving on: {0}'.format(in_dataset))
else:
  print('Archiving has already been disabled.')
```

### Example 5

```python
# Name: DisableArchiving_Example.py
# Description: Disable archiving on a dataset

# Import system modules
import arcpy

# Set local variables
in_dataset = "C:/Data/connections/intense.sde/intense.carbine.bike_routes"

# Run program
desc = arcpy.Describe(in_dataset)
if desc.isArchived == True:
  arcpy.DisableArchiving_management(in_dataset)
  print('Successfully disabled archiving on: {0}'.format(in_dataset))
else:
  print('Archiving has already been disabled.')
```

---

## Disable Attachments (Data Management)

## Summary

Disables attachments on a geodatabase feature class or table. The tool deletes the attachment relationship class and attachment table.

## Usage

- If the input dataset is from an enterprise geodatabase, it must be from a database connection established as the data owner.
- This tool permanently deletes all attachments internally stored in the geodatabase and associated with the Input Dataset. If attachments are enabled after being disabled, no attachments that were previously associated with the feature class or table will be present.
- If the geodatabase feature class or table does not have attachments enabled, a warning message will appear and no processing will occur.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The geodatabase table or feature class for which attachments will be disabled. The input must be in a version 10 or later geodatabase. | Table View |
| in_dataset | The geodatabase table or feature class for which attachments will be disabled. The input must be in a version 10 or later geodatabase. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.DisableAttachments(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.DisableAttachments_management(r"C:\Data\City.gdb\Parcels")
```

### Example 3

```python
import arcpy
arcpy.DisableAttachments_management(r"C:\Data\City.gdb\Parcels")
```

### Example 4

```python
# Name: DisableAttachments_Example.py
# Description: GDB Attachments are no longer required, so disable
#              attachments on the input dataset

# Import system modules
import arcpy

# Set the geoprocessing workspace to the feature dataset LandRecord
# in the geodatabase City.gdb
arcpy.env.workspace = r"C:\Data\City.gdb\LandRecord"

# Set local variables
input = "Parcels"

# Use DisableAttachments to delete all attachment files from the gdb
# and disable attachment handling
arcpy.DisableAttachments_management(input)
```

### Example 5

```python
# Name: DisableAttachments_Example.py
# Description: GDB Attachments are no longer required, so disable
#              attachments on the input dataset

# Import system modules
import arcpy

# Set the geoprocessing workspace to the feature dataset LandRecord
# in the geodatabase City.gdb
arcpy.env.workspace = r"C:\Data\City.gdb\LandRecord"

# Set local variables
input = "Parcels"

# Use DisableAttachments to delete all attachment files from the gdb
# and disable attachment handling
arcpy.DisableAttachments_management(input)
```

---

## Disable Attribute Rules (Data Management)

## Summary

Disables one or more attribute rules for a dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class that contains the attribute rule to be disabled. | Table View |
| Rule Names | The names of the rules to disable for the dataset. | String |
| Type (Optional) | Specifies the type of attribute rules to disable.Calculation—Filters the Rule Names parameter to display only calculation type rules.Constraint—Filters the Rule Names parameter to display only constraint type rules.Validation—Filters the Rule Names parameter to display only validation type rules.Calculation—Disable a calculation rule.Constraint—Disable a constraint rule.Validation—Disable a validation rule. | String |
| in_table | The table or feature class that contains the attribute rule to be disabled. | Table View |
| names[names,...] | The names of the rules to disable for the dataset. | String |
| type(Optional) | Specifies the type of attribute rules to disable. The tool will verify that the type of rule specified in this parameter matches the rule type specified. If they do not match, the rule will not be disabled.CALCULATION—Disable a calculation rule.CONSTRAINT—Disable a constraint rule.VALIDATION—Disable a validation rule. | String |

## Code Samples

### Example 1

```python
arcpy.management.DisableAttributeRules(in_table, names, {type})
```

### Example 2

```python
import arcpy
arcpy.DisableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                       "Rule A;Rule B", "CALCULATION")
```

### Example 3

```python
import arcpy
arcpy.DisableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                       "Rule A;Rule B", "CALCULATION")
```

### Example 4

```python
import arcpy
arcpy.DisableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                       "Calculation Rule A;Constraint Rule A")
```

### Example 5

```python
import arcpy
arcpy.DisableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                       "Calculation Rule A;Constraint Rule A")
```

### Example 6

```python
import arcpy
fc = "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData"
desc = arcpy.Describe(fc).attributeRules
for rule in desc:
    if rule.isEnabled == True and rule.type == "esriARTConstraint":
        print("Disabling rule: {}".format(rule.name))
        arcpy.DisableAttributeRules_management(fc, rule.name)
```

### Example 7

```python
import arcpy
fc = "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData"
desc = arcpy.Describe(fc).attributeRules
for rule in desc:
    if rule.isEnabled == True and rule.type == "esriARTConstraint":
        print("Disabling rule: {}".format(rule.name))
        arcpy.DisableAttributeRules_management(fc, rule.name)
```

---

## Disable COGO (Data Management)

## Summary

Disables COGO on a line feature class and removes COGO fields and COGO-enabled labeling and symbology. COGO fields can be deleted.

## Usage

- Disables the following COGO fields on the selected line feature class: ArcLength, Direction, Distance, Radius, and Radius2. These fields can be deleted.
- Removes COGO-related labeling and symbology from the line feature class.
- If a line feature class is not COGO enabled, the Traverse tool will not store any entered dimensions.Learn more about the Traverse tool
- Use the Enable COGO tool to enable COGO on a line feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Line Features | The line feature class that will have COGO disabled. | Feature Layer |
| in_line_features | The line feature class that will have COGO disabled. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.DisableCOGO(in_line_features)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"E:\ArcGISXI\Mont\Montgomery.gdb"
arcpy.DisableCOGO_management("Landbase\Road_cl")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"E:\ArcGISXI\Mont\Montgomery.gdb"
arcpy.DisableCOGO_management("Landbase\Road_cl")
```

### Example 4

```python
import arcpy

#Variable to contain the path of the feature class that will have COGO disabled
lineFeatureClass = r"d:\test.gdb\myLineFC"

#Check to see if the feature class is already COGO enabled by using .isCOGOEnabled on a Describe
if arcpy.Describe(lineFeatureClass).isCOGOEnabled == True:
    #If it returns True, run DisableCOGO_management and pass the feature class
    arcpy.DisableCOGO_management(lineFeatureClass)
else:
    print("{} is not COGO Enabled".format(lineFeatureClass))
```

### Example 5

```python
import arcpy

#Variable to contain the path of the feature class that will have COGO disabled
lineFeatureClass = r"d:\test.gdb\myLineFC"

#Check to see if the feature class is already COGO enabled by using .isCOGOEnabled on a Describe
if arcpy.Describe(lineFeatureClass).isCOGOEnabled == True:
    #If it returns True, run DisableCOGO_management and pass the feature class
    arcpy.DisableCOGO_management(lineFeatureClass)
else:
    print("{} is not COGO Enabled".format(lineFeatureClass))
```

---

## Disable Editor Tracking (Data Management)

## Summary

Disables editor tracking on a feature class, table, feature dataset, or mosaic dataset.

## Usage

- If the input dataset is from an enterprise geodatabase, it must be from a database connection established as the data owner.
- When editor tracking is disabled on a field, editor tracking information will no longer be recorded in these fields when edits are made. You can reenable editor tracking with the Enable Editor Tracking tool.
- This tool operates on feature classes, tables, feature datasets, and mosaic datasets in a 10.0 or later release geodatabase.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The feature class, table, feature dataset, or mosaic dataset in which editor tracking will be disabled. | Dataset; Topology; Network Dataset |
| Disable Creator Tracking (Optional) | Specifies whether editor tracking for the creator field will be disabled.Checked—Editor tracking for the creator field will be disabled. This is the default.Unchecked—Editor tracking for the creator field will not be disabled. | Boolean |
| Disable Creation Date Tracking (Optional) | Specifies whether editor tracking for the creation date field will be disabled.Checked—Editor tracking for the creation date field will be disabled. This is the default.Unchecked—Editor tracking for the creation date field will not be disabled. | Boolean |
| Disable Last Editor Tracking (Optional) | Specifies whether editor tracking for the last editor field will be disabled.Checked—Editor tracking for the last editor field will be disabled. This is the default.Unchecked—Editor tracking for the last editor field will not be disabled. | Boolean |
| Disable Last Edit Date Tracking (Optional) | Specifies whether editor tracking for the last edit date field will be disabled.Checked—Editor tracking for the last edit date field will be disabled. This is the default.Unchecked—Editor tracking for the last edit date field will not be disabled. | Boolean |
| in_dataset | The feature class, table, feature dataset, or mosaic dataset in which editor tracking will be disabled. | Dataset; Topology; Network Dataset |
| creator(Optional) | Specifies whether editor tracking for the creator field will be disabled.DISABLE_CREATOR—Editor tracking for the creator field will be disabled. This is the default.NO_DISABLE_CREATOR—Editor tracking for the creator field will not be disabled. | Boolean |
| creation_date(Optional) | Specifies whether editor tracking for the creation date field will be disabled.DISABLE_CREATION_DATE—Editor tracking for the creation date field will be disabled. This is the default.NO_DISABLE_CREATION_DATE—Editor tracking for the creation date field will not be disabled. | Boolean |
| last_editor(Optional) | Specifies whether editor tracking for the last editor field will be disabled.DISABLE_LAST_EDITOR—Editor tracking for the last editor field will be disabled. This is the default.NO_DISABLE_LAST_EDITOR—Editor tracking for the last editor field will not be disabled. | Boolean |
| last_edit_date(Optional) | Specifies whether editor tracking for the last edit date field will be disabled.DISABLE_LAST_EDIT_DATE—Editor tracking for the last edit date field will be disabled. This is the default.NO_DISABLE_LAST_EDIT_DATE—Editor tracking for the last edit date field will not be disabled. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DisableEditorTracking(in_dataset, {creator}, {creation_date}, {last_editor}, {last_edit_date})
```

### Example 2

```python
# Name: DisableEditorTracking.py
# Description: Disables editor tracking on a feature class.

# Import arcpy module
import arcpy

# Local variables:
Buildings = "d:\\RC.gdb\\RC\\Buildings"

# Process: Disable Editor Tracking
arcpy.DisableEditorTracking_management(Buildings,
                                       "DISABLE_CREATOR",
                                       "DISABLE_CREATION_DATE",
                                       "DISABLE_LAST_EDITOR",
                                       "DISABLE_LAST_EDIT_DATE")
```

### Example 3

```python
# Name: DisableEditorTracking.py
# Description: Disables editor tracking on a feature class.

# Import arcpy module
import arcpy

# Local variables:
Buildings = "d:\\RC.gdb\\RC\\Buildings"

# Process: Disable Editor Tracking
arcpy.DisableEditorTracking_management(Buildings,
                                       "DISABLE_CREATOR",
                                       "DISABLE_CREATION_DATE",
                                       "DISABLE_LAST_EDITOR",
                                       "DISABLE_LAST_EDIT_DATE")
```

---

## Disable Feature Binning (Data Management)

## Summary

Disables database computed feature binning on a feature class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature class for which database computed feature binning will be disabled. | Feature Layer |
| in_features | The feature class for which database computed feature binning will be disabled. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.DisableFeatureBinning(in_features)
```

### Example 2

```python
import arcpy
arcpy.management.DisableFeatureBinning("C:\\MyProject\\sdeConnection.sde\\mygdb.user1.global_earthquakes")
```

### Example 3

```python
import arcpy
arcpy.management.DisableFeatureBinning("C:\\MyProject\\sdeConnection.sde\\mygdb.user1.global_earthquakes")
```

---

## Disable Replica Tracking (Data Management)

## Summary

Disables replica tracking on data.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The enterprise geodatabase table, feature class, feature dataset, attributed relationship class, or many-to-many relationship class on which to disable replica tracking. | Table; Feature Class; Feature Dataset |
| in_dataset | The enterprise geodatabase table, feature class, feature dataset, attributed relationship class, or many-to-many relationship class on which to disable replica tracking. | Table; Feature Class; Feature Dataset |

## Code Samples

### Example 1

```python
arcpy.management.DisableReplicaTracking(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.DisableReplicaTracking_management("C:\\MyProject\\mySdeConn.sde\\database.user1.stores")
```

### Example 3

```python
import arcpy
arcpy.DisableReplicaTracking_management("C:\\MyProject\\mySdeConn.sde\\database.user1.stores")
```

---

## Dissolve (Data Management)

## Summary

Aggregates features based on specified attributes.

## Usage

- The attributes of the features that are aggregated by this tool can be summarized or described using a variety of statistics. The statistic used to summarize attributes is added to the output feature class as a single field with the naming standard of statistic type + underscore + input field name. For example, if the SUM statistics type is used on a field named POP, the output will include a field named SUM_POP.
- Dissolve can create very large features in the output feature class, especially when there is a small number of unique values in the Dissolve Fields parameter or when dissolving all features into a single feature. Very large features may cause processing or display problems or poor performance when drawn on a map or when edited. Problems may also occur if the dissolve output created a feature at the maximum size on one machine and this output was moved to a machine with less available memory. To avoid these potential problems, use the Create multipart features parameter to create single-part features to split potentially larger multipart features into many smaller features. For extremely large features created by the Dissolve tool, the Dice tool can be used to split the large features to solve processing, display, or performance problems.
- Null values are excluded from all statistical calculations. For example, the average of 10, 5, and a null is 7.5 ((10 + 5) / 2). The count returns the number of values included in the statistical calculation, which in this case is 2.
- For better performance and scalability, this tool uses a tiling process to handle very large datasets. For details, see Tiled processing of large datasets.
- The availability of physical memory may limit the amount (and complexity) of input features that can be processed and dissolved into a single output feature. This limitation may cause an error to occur, as the dissolve process may require more memory than is available. To prevent this, Dissolve may divide and process the input features using an adaptive tiling algorithm. To determine the features that have been tiled, run the Frequency tool on the result of this tool, specifying the same fields used in the dissolve process for the Frequency Fields parameter. Any record with a frequency value of 2 has been tiled. Tile boundaries are preserved in the output features to prevent the creation of features that are too large to be used by ArcGIS.Caution:Running Dissolve on the output of a previous dissolve process will rarely reduce the number of features in the output when the original processing divided and processed the inputs using adaptive tiling. The maximum size of any output feature is determined by the amount of available memory at run time; output containing tiles is an indicator that dissolving further with the available resources will cause an out-of-memory situation or result in a feature that is unusable. Additionally, running the Dissolve tool a second time on output that was created this way may result in slow performance for little to no gain and may cause an unexpected failure.
- The Unsplit lines parameter only applies to line input. When the default is specified, lines are dissolved into a single feature; otherwise, only two lines that have a common endpoint (known as a pseudonode) are merged into one continuous line.
- If the Input Features parameter value's geometry type is either point or multipoint and the Create multipart features parameter is checked, the output will be a multipoint feature class. Otherwise, if the Create multipart features parameter is unchecked, the output will be a point feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The features to be aggregated. | Feature Layer |
| Output Feature Class | The feature class to be created that will contain the aggregated features. | Feature Class |
| Dissolve Fields(Optional) | The field or fields on which features will be aggregated. If no fields are specified, the tool will dissolve all features together. | Field |
| Statistics Fields(Optional) | Specifies the field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all calculations.By default, the tool will not calculate any statistics.Numeric attribute fields can be summarized using any statistic. Text attribute fields can be summarized using minimum, maximum, count, first, last, unique, concatenate, and mode statistics.Sum—The values for the specified field will be added together.Mean—The average for the specified field will be calculated.Minimum—The smallest value of the specified field will be identified.Maximum—The largest value of the specified field will be identified.Range—The range of values (maximum minus minimum) for the specified field will be calculated.Standard deviation—The standard deviation of values for the specified field will be calculated.Count—The number of values in the specified field will be identified.First—The specified field value of the first record in the input will be used.Last—The specified field value of the last record in the input will be used.Median—The median of the specified field will be calculated.Variance—The variance of the specified field will be calculated. Unique—The number of unique values of the specified field will be counted.Concatenate—The values for the specified field will be concatenated. The values can be separated using the Concatenation Separator parameter.Mode—The mode (the most common value) for the specified field will be identified. If more than one value is equally common, the lowest value will be returned. | Value Table |
| Create multipart features(Optional) | Specifies whether multipart features will be allowed in the output feature class.Checked—Multipart features will be allowed in the output feature class. This is the default.Unchecked—Multipart features will not be allowed in the output feature class. Individual features will be created for each part. | Boolean |
| Unsplit lines(Optional) | Specifies how line features will be dissolved.Unchecked—Lines will be dissolved into a single feature. This is the default. Checked—Lines will only be dissolved when two lines have an end vertex in common. | Boolean |
| Concatenation Separator (Optional) | A character or characters that will be used to concatenate values when the Concatenation option is used for the Statistics Fields parameter. By default, the tool will concatenate values without a separator. | String |
| in_features | The features to be aggregated. | Feature Layer |
| out_feature_class | The feature class to be created that will contain the aggregated features. | Feature Class |
| dissolve_field[dissolve_field,...](Optional) | The field or fields on which features will be aggregated. If no fields are specified, the tool will dissolve all features together. | Field |
| statistics_fields[[field, {statistic_type}],...](Optional) | Specifies the field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all calculations.By default, the tool will not calculate any statistics.Numeric attribute fields can be summarized using any statistic. Text attribute fields can be summarized using minimum, maximum, count, first, last, unique, concatenate, and mode statistics.SUM—The values for the specified field will be added together.MEAN—The average for the specified field will be calculated.MIN—The smallest value of the specified field will be identified.MAX—The largest value of the specified field will be identified.RANGE—The range of values (maximum minus minimum) for the specified field will be calculated.STD—The standard deviation of values for the specified field will be calculated.COUNT—The number of values in the specified field will be identified.FIRST—The specified field value of the first record in the input will be used.LAST—The specified field value of the last record in the input will be used.MEDIAN—The median of the specified field will be calculated.VARIANCE—The variance of the specified field will be calculated. UNIQUE—The number of unique values of the specified field will be counted.CONCATENATE—The values for the specified field will be concatenated. The values can be separated using the concatenation_separator parameter.MODE—The mode (the most common value) for the specified field will be identified. If more than one value is equally common, the lowest value will be returned. | Value Table |
| multi_part(Optional) | Specifies whether multipart features will be allowed in the output feature class.MULTI_PART—Multipart features will be allowed in the output feature class. This is the default. SINGLE_PART—Multipart features will not be allowed in the output feature class. Individual features will be created for each part. | Boolean |
| unsplit_lines(Optional) | Specifies how line features will be dissolved.DISSOLVE_LINES—Lines will be dissolved into a single feature. This is the default. UNSPLIT_LINES—Lines will only be dissolved when two lines have an end vertex in common. | Boolean |
| concatenation_separator(Optional) | A character or characters that will be used to concatenate values when the CONCATENATION option is used for the statistics_fields parameter. By default, the tool will concatenate values without a separator. | String |

## Code Samples

### Example 1

```python
arcpy.management.Dissolve(in_features, out_feature_class, {dissolve_field}, {statistics_fields}, {multi_part}, {unsplit_lines}, {concatenation_separator})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Taxlots"
arcpy.management.Dissolve("taxlots", "C:/output/output.gdb/taxlots_dissolved",
                          ["LANDUSE", "TAXCODE"], "", "SINGLE_PART", 
                          "DISSOLVE_LINES")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Taxlots"
arcpy.management.Dissolve("taxlots", "C:/output/output.gdb/taxlots_dissolved",
                          ["LANDUSE", "TAXCODE"], "", "SINGLE_PART", 
                          "DISSOLVE_LINES")
```

### Example 4

```python
# Name: Dissolve_Example2.py
# Description: Dissolve features based on common attributes

# Import system modules
import arcpy

arcpy.env.workspace = "C:/data/Portland.gdb/Taxlots"

# Set local variables
inFeatures = "taxlots"
tempLayer = "taxlotsLyr"
expression = arcpy.AddFieldDelimiters(inFeatures, "LANDUSE") + " <> ''"
outFeatureClass = "C:/output/output.gdb/taxlots_dissolved"
dissolveFields = ["LANDUSE", "TAXCODE"]

# Run MakeFeatureLayer and SelectLayerByAttribute.  This is only to exclude 
# features that are not desired in the output.
arcpy.management.MakeFeatureLayer(inFeatures, tempLayer)
arcpy.management.SelectLayerByAttribute(tempLayer, "NEW_SELECTION", expression)

# Run Dissolve using LANDUSE and TAXCODE as Dissolve Fields
arcpy.management.Dissolve(tempLayer, outFeatureClass, dissolveFields, "", 
                          "SINGLE_PART", "DISSOLVE_LINES")
```

### Example 5

```python
# Name: Dissolve_Example2.py
# Description: Dissolve features based on common attributes

# Import system modules
import arcpy

arcpy.env.workspace = "C:/data/Portland.gdb/Taxlots"

# Set local variables
inFeatures = "taxlots"
tempLayer = "taxlotsLyr"
expression = arcpy.AddFieldDelimiters(inFeatures, "LANDUSE") + " <> ''"
outFeatureClass = "C:/output/output.gdb/taxlots_dissolved"
dissolveFields = ["LANDUSE", "TAXCODE"]

# Run MakeFeatureLayer and SelectLayerByAttribute.  This is only to exclude 
# features that are not desired in the output.
arcpy.management.MakeFeatureLayer(inFeatures, tempLayer)
arcpy.management.SelectLayerByAttribute(tempLayer, "NEW_SELECTION", expression)

# Run Dissolve using LANDUSE and TAXCODE as Dissolve Fields
arcpy.management.Dissolve(tempLayer, outFeatureClass, dissolveFields, "", 
                          "SINGLE_PART", "DISSOLVE_LINES")
```

---

## Domain To Table (Data Management)

## Summary

Creates a table from an attribute domain.

## Usage

- Creating a table from an attribute domain allows for additional editing of the table. For example, a table could be created from a coded value domain, additional code values could be added to the coded value list, and the Table To Domain tool could be used to update the original domain.
- You can also manage domains in Domains view which can be opened by clicking the Domains button found in the Design group on the Data ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The workspace containing the attribute domain to be converted to a table. | Workspace |
| Domain Name | The name of the existing attribute domain. | String |
| Output Table | The table to be created. | Table |
| Code Field | The name of the field in the created table that will store code values. | String |
| Field Description | The name of the field in the created table that will store code value descriptions. | String |
| Configuration Keyword(Optional) | For geodatabase tables, the custom storage keywords for creating the table. | String |
| in_workspace | The workspace containing the attribute domain to be converted to a table. | Workspace |
| domain_name | The name of the existing attribute domain. | String |
| out_table | The table to be created. | Table |
| code_field | The name of the field in the created table that will store code values. | String |
| description_field | The name of the field in the created table that will store code value descriptions. | String |
| configuration_keyword(Optional) | For geodatabase tables, the custom storage keywords for creating the table. | String |

## Code Samples

### Example 1

```python
arcpy.management.DomainToTable(in_workspace, domain_name, out_table, code_field, description_field, {configuration_keyword})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DomainToTable_management("montgomery.gdb", "DistDiam", "diameters", "code", "descript")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.DomainToTable_management("montgomery.gdb", "DistDiam", "diameters", "code", "descript")
```

### Example 4

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
try:
    # Set the workspace (to avoid having to type in the full path to the data every time)
    arcpy.env.workspace = "C:/data"
 
    # set local parameters
    inFeatures = "Montgomery.gdb/Water/DistribMains"
    inField = "MATERIAL"
    dWorkspace = "Montgomery.gdb"
    domName = "Material"
    codedValue =  "ACP: Asbestos concrete"
    codeField = "TYPE"
    fieldDesc= "DESRIPT"
    # Process: Remove the constraint from the material field
    arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
    # Edit the domain values
    # Process: Remove a coded value from the domain
    arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)
 
    # Process: Create a table from the domain to edit it with ArcMap editing tools
    arcpy.DomainToTable_management(dWorkspace, domname, dWorkspace + os.sep + domname , codeField, fieldDesc)
 
    # Process: Delete the domain
    arcpy.DeleteDomain_management(dWorkspace, domName)
 
    # Edit the domain table outside of geoprocessing
    # and then bring the domain back in with the TableToDomain process
 
except Exception as err:
    print(err.args[0])
```

### Example 5

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
try:
    # Set the workspace (to avoid having to type in the full path to the data every time)
    arcpy.env.workspace = "C:/data"
 
    # set local parameters
    inFeatures = "Montgomery.gdb/Water/DistribMains"
    inField = "MATERIAL"
    dWorkspace = "Montgomery.gdb"
    domName = "Material"
    codedValue =  "ACP: Asbestos concrete"
    codeField = "TYPE"
    fieldDesc= "DESRIPT"
    # Process: Remove the constraint from the material field
    arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
    # Edit the domain values
    # Process: Remove a coded value from the domain
    arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)
 
    # Process: Create a table from the domain to edit it with ArcMap editing tools
    arcpy.DomainToTable_management(dWorkspace, domname, dWorkspace + os.sep + domname , codeField, fieldDesc)
 
    # Process: Delete the domain
    arcpy.DeleteDomain_management(dWorkspace, domName)
 
    # Edit the domain table outside of geoprocessing
    # and then bring the domain back in with the TableToDomain process
 
except Exception as err:
    print(err.args[0])
```

---

## Downgrade Attachments (Data Management)

## Summary

Downgrades the attachments functionality of a feature class or table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The feature class or table that will have its attachments functionality downgraded. | Table View |
| in_dataset | The feature class or table that will have its attachments functionality downgraded. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.DowngradeAttachments(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.management.DowngradeAttachments("C:\\MyProject\\MyGDB.gdb\\MyFC")
```

### Example 3

```python
import arcpy
arcpy.management.DowngradeAttachments("C:\\MyProject\\MyGDB.gdb\\MyFC")
```

---

## Download Rasters (Data Management)

## Summary

Downloads the source files from an image service or mosaic dataset.

## Usage

- The raster datasets you download are the source files, unless you convert them to another format. Format conversion can be forced, or will only occur when required. Downloaded files are converted if clipping occurs or the source file cannot be downloaded as a raster.
- You can download selected rasters or LAS files from an image service or a mosaic dataset to a specified folder in the original file format.
- If a clipping extent is specified, the rasters that intersect the clip extent will be clipped and then converted to a specified format.
- You can choose to download the data in the same folder structure as the source.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input | The image service or mosaic dataset to download. | Image Service; Mosaic Layer; Raster Layer; String |
| Output Folder | The destination for the image service or mosaic dataset. | Folder |
| Query Definition (Optional) | An SQL expression to limit the download to raster datasets that satisfy the expression. | SQL Expression |
| Selection Feature (Optional) | Limits the download to an extent of a feature class or bounding box. All raster datasets that intersect the extent will be downloaded.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Clipping Using Selection Feature(Optional) | Specify if you want to clip the downloaded images based on the geometry of a feature. Any raster that intersects the clipping geometry will be clipped and then downloaded. This is useful when your area of interest is not a rectangle. When downloaded images are clipped, you need to specify an output format for the clipped images.Unchecked—The files will be clipped based on the minimum bounding rectangle that has been specified. This is the default.Checked—The files will be clipped based on the geometry of the Selection Feature. | Boolean |
| Convert Rasters (Optional) | Choose whether to always convert your rasters to the specified format, or to only convert when it is necessary.Unchecked—Do not convert the raster datasets to a new format.Checked—Convert the downloaded raster datasets into another format. If you used Selection Feature to limit the extent, then you need to specify a format in the Output Format parameter. | Boolean |
| Output Format (Optional) | Choose a output format for the downloaded raster datasets.Tiff—Tagged Image File Format. This is the default.Bil—Esri band interleaved by line.Bsq—Esri band sequential.Bip—Esri band interleaved by pixel.Bmp—Bitmap.ENVI Dat—ENVI DAT file.Imagine image—ERDAS IMAGINE.Jpeg—Joint Photographics Experts Group. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100.Gif—Graphic interchange format.Jp2—JPEG 2000. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100.Png—Portable Network Graphics. | String |
| Compression Method (Optional) | Choose the compression method to use with the specified Output Format.None—No compression will occur. This is the default.Jpeg—Lossy compression that uses the public JPEG compression algorithm. If you choose JPEG, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. This compression can be used for JPEG files and TIFF files.Lzw—Lossless compression that preserves all raster cell values.Packbits—PackBits compression for TIFF files.Rle—Run-length encoding for IMG files.Ccitt Group 3—Lossless compression for 1-bit data.Ccitt Group 4—Lossless compression for 1-bit data.Ccitt 1D—Lossless compression for 1-bit data. | String |
| Compression Quality (Optional) | Set a value from 1 - 100. Higher values will have better image quality, but less compression. | Long |
| Maintain Folder Structure (Optional) | Determines the folder structure of the downloaded rasters. Checked—replicates the hierarchical folder structure used to store the source raster datasets.Unchecked—raster datasets will be downloaded into the output folder as a flat folder structure | Boolean |
| in_image_service | The image service or mosaic dataset to download. | Image Service; Mosaic Layer; Raster Layer; String |
| out_folder | The destination for the image service or mosaic dataset. | Folder |
| where_clause(Optional) | An SQL expression to limit the download to raster datasets that satisfy the expression. | SQL Expression |
| selection_feature(Optional) | Limits the download to an extent of a feature class or bounding box. All raster datasets that intersect the extent will be downloaded.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| clipping(Optional) | Specify if you want to clip the downloaded images based on the geometry of a feature. Any raster that intersects the clipping geometry will be clipped and then downloaded. This is useful when your area of interest is not a rectangle. When downloaded images are clipped, you need to specify an output format for the clipped images.NO_CLIPPING—The files will be clipped based on the minimum bounding rectangle that has been specified. This is the default.CLIPPING—The files will be clipped based on the geometry of the selection_feature. | Boolean |
| convert_rasters(Optional) | Choose whether to always convert your rasters to the specified format, or to only convert when it is necessary. CONVERT_AS_REQUIRED—Do not convert the raster datasets to a new format.ALWAYS_CONVERT—Convert the downloaded raster datasets into another format. If you used selection_feature to limit the extent, then you need to specify a format in the format parameter. | Boolean |
| format(Optional) | Choose a output format for the downloaded raster datasets.TIFF—Tagged Image File Format. This is the default.BIL—Esri band interleaved by line.BSQ—Esri band sequential.BIP—Esri band interleaved by pixel.BMP—Bitmap.ENVI—ENVI DAT file.IMAGINE Image—ERDAS IMAGINE.JPEG—Joint Photographics Experts Group. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100.GIF—Graphic interchange format.JP2—JPEG 2000. If chosen, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100.PNG—Portable Network Graphics. | String |
| compression_method(Optional) | Choose the compression method to use with the specified Output Format.NONE—No compression will occur. This is the default.JPEG—Lossy compression that uses the public JPEG compression algorithm. If you choose JPEG, you can also specify the compression quality. The valid compression quality value ranges are from 0 to 100. This compression can be used for JPEG files and TIFF files.LZW—Lossless compression that preserves all raster cell values.PACKBITS—PackBits compression for TIFF files.RLE—Run-length encoding for IMG files.CCITT_GROUP3—Lossless compression for 1-bit data.CCITT_GROUP4—Lossless compression for 1-bit data.CCITT_1D—Lossless compression for 1-bit data. | String |
| compression_quality(Optional) | Set a value from 1 - 100. Higher values will have better image quality, but less compression. | Long |
| MAINTAIN_FOLDER(Optional) | Determines the folder structure of the downloaded rasters. MAINTAIN_FOLDER—Replicate the hierarchical folder structure used to store the source raster datasets.NO_MAINTAIN_FOLDER—Raster datasets will be downloaded into the out_folder as a flat folder structure. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.DownloadRasters(in_image_service, out_folder, {where_clause}, {selection_feature}, {clipping}, {convert_rasters}, {format}, {compression_method}, {compression_quality}, {MAINTAIN_FOLDER})
```

### Example 2

```python
import arcpy
DownloadRasters_management(
     "http://srv/arcgis/services/Ext/MDpan/ImageServer?", "c:/dload/", 
     "AcquisitionDate = date '1999-08-18'", "", 
     "c:/workspace/clippingfeat.shp", "TIFF", "JPEG", "75", 
     "MAINTAIN_FOLDER", "CONVERT_AS_REQUIRED")
```

### Example 3

```python
import arcpy
DownloadRasters_management(
     "http://srv/arcgis/services/Ext/MDpan/ImageServer?", "c:/dload/", 
     "AcquisitionDate = date '1999-08-18'", "", 
     "c:/workspace/clippingfeat.shp", "TIFF", "JPEG", "75", 
     "MAINTAIN_FOLDER", "CONVERT_AS_REQUIRED")
```

### Example 4

```python
##Download Rasters from image services URL
##Maintain the original sensor data folder structure

import arcpy
arcpy.env.workspace = r"\\myworkstation\Workspace\downloadras"
    
arcpy.DownloadRasters_management(
     "http://serv1/arcgis/services/Ext/MD_LS_pan/ImageServer?",
     "downloadFolder", "AcquisitionDate = date '1999-08-18 00:00:00'",
     "", "", "", "", "", "MAINTAIN_FOLDER")
```

### Example 5

```python
##Download Rasters from image services URL
##Maintain the original sensor data folder structure

import arcpy
arcpy.env.workspace = r"\\myworkstation\Workspace\downloadras"
    
arcpy.DownloadRasters_management(
     "http://serv1/arcgis/services/Ext/MD_LS_pan/ImageServer?",
     "downloadFolder", "AcquisitionDate = date '1999-08-18 00:00:00'",
     "", "", "", "", "", "MAINTAIN_FOLDER")
```

---

## Edit Raster Function (Data Management)

## Summary

Adds, replaces, or removes a function chain in a mosaic dataset or a raster layer that contains a raster function.

## Usage

- To apply the rft.xml to items in a mosaic dataset, you must select the items in the attribute table or define a query using the Make Mosaic Layer tool.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The mosaic dataset or a raster layer. If you use a raster layer, it must have a function applied. | Mosaic Layer; Raster Layer |
| Mosaic Dataset Items (Optional) | Applies the function chain to every item in the mosaic dataset individually or to the mosaic dataset as a whole.Unchecked—Edits affect the functions associated with the mosaic dataset. This is the default. Checked—Edits affect the functions associated with all of the items within the mosaic dataset. | Boolean |
| Edit Options (Optional) | Insert, replace, or remove a function chain.Insert—Insert the function chain above the Function Name of the existing chain. Specify the function chain below in the Function Name parameter. This is the default.Replace—Replace the existing function chain with the function chain specified in this tool. Specify the function chain below in the Function Name parameter.Remove— Remove the function chain starting from the function specified in the Function Name parameter. | String |
| Raster Function Template (Optional) | Choose the function chain (rft.xml file) that you want to insert or replace. | File |
| Function Name (Optional) | Choose where to insert, replace, or remove the function chain within the existing function chain. If you Insert the function, it will be inserted above the function specified in the Function Name parameter. | String |
| in_mosaic_dataset | The mosaic dataset or a raster layer. If you use a raster layer, it must have a function applied. | Mosaic Layer; Raster Layer |
| edit_mosaic_dataset_item(Optional) | Determines if edits affect functions or the entire mosaic dataset.EDIT_MOSAIC_DATASET—Edits affect the functions associated with the mosaic dataset. This is the default. EDIT_MOSAIC_DATASET_ITEM—Edits affect the functions associated with all of the items within the mosaic dataset. | Boolean |
| edit_options(Optional) | Insert, replace, or remove a function chain.INSERT—Insert the function chain above the Function Name of the existing chain. Specify the function chain in the location_function_name parameter. This is the default.REPLACE—Replace the existing function chain with the function chain specified in this tool. Specify the function chain below in the location_function_name parameter.REMOVE— Remove the function chain starting from the function specified in the location_function_name parameter. | String |
| function_chain_definition(Optional) | Choose the function chain (rft.xml file) that you want to insert or replace. | File |
| location_function_name(Optional) | Choose where to insert, replace, or remove the function chain within the existing function chain. | String |

## Code Samples

### Example 1

```python
arcpy.management.EditRasterFunction(in_mosaic_dataset, {edit_mosaic_dataset_item}, {edit_options}, {function_chain_definition}, {location_function_name})
```

### Example 2

```python
import arcpy
arcpy.EditRasterFunction_management(
     "C:/Workspace/editfunction.gdb/md", "EDIT_MOSAIC_DATASET", 
     "INSERT", "C:/workspace/hillshade.rft.xml", "Stretch Function")
```

### Example 3

```python
import arcpy
arcpy.EditRasterFunction_management(
     "C:/Workspace/editfunction.gdb/md", "EDIT_MOSAIC_DATASET", 
     "INSERT", "C:/workspace/hillshade.rft.xml", "Stretch Function")
```

### Example 4

```python
#Add raster function on top of mosaic dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "editfunction.gdb/md"
editmode = "EDIT_MOSAIC_DATASET"
editmethod = "INSERT"
funcfile = "C:/workspace/hillshade.rft.xml"
funcname = "#"

arcpy.EditRasterFunction_management(mdname, editmode, editmethod, 
                                    funcfile, funcname)
```

### Example 5

```python
#Add raster function on top of mosaic dataset

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "editfunction.gdb/md"
editmode = "EDIT_MOSAIC_DATASET"
editmethod = "INSERT"
funcfile = "C:/workspace/hillshade.rft.xml"
funcname = "#"

arcpy.EditRasterFunction_management(mdname, editmode, editmethod, 
                                    funcfile, funcname)
```

---

## Eliminate Polygon Part (Data Management)

## Summary

Creates a new output feature class containing the features from the input polygons with some parts or holes of a specified size deleted.

## Usage

- Since polygon holes are considered parts of the polygon, they can be deleted or filled using this tool. If the hole area is smaller than the specified size, the hole will be eliminated and the space will be filled in the output. Any parts that are inside the deleted hole will also be eliminated in the output.
- Part size can be specified as an area, a percent, or a combination of both. Use the Condition parameter to determine how the part size will be specified. The Condition parameter AREA_AND_PERCENT and AREA_OR_PERCENT options are used to eliminate parts using both the area and percent criteria.
- Polygon part percent is calculated as a percentage of the feature's total outer area, including the area of any holes. For example, if a polygon with a hole has an area of 75 square meters, with the hole covering 25 square meters, the polygons total outer area is 100 square meters. To eliminate this hole, an area greater than 25 square meters or a percentage greater than 25% would need to be specified. If the input is a multipart polygon, a feature's outer area is the sum of the area covered by all polygon parts.
- For multipart polygons, the area of each part will be compared with the specified area. If an individual polygon part is smaller than the specified size, the part will be eliminated in the output.
- If all of a polygon feature's parts are smaller than the specified size, the largest part will be kept in the output while all other parts will be eliminated.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input feature class or layer whose features will be copied to the output feature class, with some parts or holes eliminated. | Feature Layer |
| Output Feature Class | The output polygon feature class containing the remaining parts. | Feature Class |
| Condition (Optional) | Specify how the parts to be eliminated will be determined. Area—Parts with an area less than that specified will be eliminated.Percent—Parts with a percent of the total outer area less than that specified will be eliminated.Area and percent—Parts with an area and percent less than that specified will be eliminated. Only if a polygon part meets both the area and percent criteria will it be deleted. Area or percent—Parts with an area or percent less than that specified will be eliminated. If a polygon part meets either the area or percent criteria, it will be deleted. | String |
| Area(Optional) | Eliminate parts smaller than this area. | Areal Unit |
| Percentage(Optional) | Eliminate parts smaller than this percentage of a feature's total outer area. | Double |
| Eliminate contained parts only(Optional) | Determines what parts can be eliminated. Checked - Only parts totally contained by other parts can be eliminated. This is the default. Unchecked - Any parts can be eliminated. | Boolean |
| in_features | The input feature class or layer whose features will be copied to the output feature class, with some parts or holes eliminated. | Feature Layer |
| out_feature_class | The output polygon feature class containing the remaining parts. | Feature Class |
| condition(Optional) | Specify how the parts to be eliminated will be determined. AREA—Parts with an area less than that specified will be eliminated.PERCENT—Parts with a percent of the total outer area less than that specified will be eliminated.AREA_AND_PERCENT—Parts with an area and percent less than that specified will be eliminated. Only if a polygon part meets both the area and percent criteria will it be deleted. AREA_OR_PERCENT—Parts with an area or percent less than that specified will be eliminated. If a polygon part meets either the area or percent criteria, it will be deleted. | String |
| part_area(Optional) | Eliminate parts smaller than this area. | Areal Unit |
| part_area_percent(Optional) | Eliminate parts smaller than this percentage of a feature's total outer area. | Double |
| part_option(Optional) | Determines what parts can be eliminated.CONTAINED_ONLY—Only parts totally contained by other parts can be eliminated. This is the default. ANY—Any parts can be eliminated. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.EliminatePolygonPart(in_features, out_feature_class, {condition}, {part_area}, {part_area_percent}, {part_option})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.EliminatePolygonPart_management("buildings.shp", "output.gdb/remaining_buildings", "AREA", 10)
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.EliminatePolygonPart_management("buildings.shp", "output.gdb/remaining_buildings", "AREA", 10)
```

### Example 4

```python
# Name: EliminatePolygonPart_Example2.py
# Description: Eliminate small islands before simplifying and smoothing lake boundaries
 
# Import system modules
import arcpy
from arcpy import env
 
# Set environment settings
env.workspace = "C:/data/Portland.gdb/Hydrography"
 
# Set local variables
inLakeFeatures = "lakes"
eliminatedFeatures = "lakes_eliminated"
simplifiedFeatures = "lakes_simplified"
smoothedFeatures = "lakes_smoothed"

# Eliminate small islands in lake polygons.
arcpy.EliminatePolygonPart_management(inLakeFeatures, eliminatedFeatures, "AREA", 100, "", "CONTAINED_ONLY")
 
# Simplify lake polygons.
arcpy.SimplifyPolygon_cartography(eliminatedFeatures, simplifiedFeatures, "POINT_REMOVE", 50, 200, "RESOLVE_ERRORS", "KEEP_COLLAPSED_POINTS")
 
# Smooth lake polygons.
arcpy.SmoothPolygon_cartography(simplifiedFeatures, smoothedFeatures, "BEZIER_INTERPOLATION")
```

### Example 5

```python
# Name: EliminatePolygonPart_Example2.py
# Description: Eliminate small islands before simplifying and smoothing lake boundaries
 
# Import system modules
import arcpy
from arcpy import env
 
# Set environment settings
env.workspace = "C:/data/Portland.gdb/Hydrography"
 
# Set local variables
inLakeFeatures = "lakes"
eliminatedFeatures = "lakes_eliminated"
simplifiedFeatures = "lakes_simplified"
smoothedFeatures = "lakes_smoothed"

# Eliminate small islands in lake polygons.
arcpy.EliminatePolygonPart_management(inLakeFeatures, eliminatedFeatures, "AREA", 100, "", "CONTAINED_ONLY")
 
# Simplify lake polygons.
arcpy.SimplifyPolygon_cartography(eliminatedFeatures, simplifiedFeatures, "POINT_REMOVE", 50, 200, "RESOLVE_ERRORS", "KEEP_COLLAPSED_POINTS")
 
# Smooth lake polygons.
arcpy.SmoothPolygon_cartography(simplifiedFeatures, smoothedFeatures, "BEZIER_INTERPOLATION")
```

---

## Eliminate (Data Management)

## Summary

Eliminates polygons by merging them with neighboring polygons that have the largest area or the longest shared border. Eliminate is often used to remove small sliver polygons that are the result of overlay operations, such as those performed by Intersect and Union tools.

## Usage

- Features that will be eliminated are determined by a selection applied to a polygon layer. The selection must be determined in a previous step using the Select Layer by Attribute tool or the Select Layer by Location tool, or by querying a layer in a map.
- The Eliminate tool may not eliminate all selected features, depending on your dataset. Selected features cannot be merged with a neighboring selected feature. For example, a selected feature that is surrounded by other selected features, or only borders on other selected features cannot be merged. To eliminate the selected features that were not merged, select the features again and run the tool again.A selected feature that has no neighboring features with a common boundary cannot be merged.
- The Input Layer parameter must include a selection; otherwise, the tool will fail.
- The Exclusion Expression and Exclusion Layer parameters are not mutually exclusive and can be used together to give full control over what is eliminated.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The layer with the polygons that will be merged with neighboring polygons. | Feature Layer |
| Output Feature Class | The feature class to be created. | Feature Class |
| Eliminating polygon by border(Optional) | Specifies whether the selected polygon will be merged with a polygon with the longest shared border or the largest area.Checked—The selected polygon will be merged with the neighboring polygon with the longest shared border. This is the default. Unchecked—The selected polygon will be merged with the neighboring polygon with the largest area. | Boolean |
| Exclusion Expression(Optional) | An SQL expression that will be used to identify features that will not be altered. | SQL Expression |
| Exclusion Layer(Optional) | An input polyline or polygon feature class or layer that defines polygon boundaries, or portions thereof, that will not be eliminated. | Feature Layer |
| in_features | The layer with the polygons that will be merged with neighboring polygons. | Feature Layer |
| out_feature_class | The feature class to be created. | Feature Class |
| selection(Optional) | Specifies whether the selected polygon will be merged with a polygon with the longest shared border or the largest area.LENGTH—The selected polygon will be merged with the neighboring polygon with the longest shared border. This is the default. AREA—The selected polygon will be merged with the neighboring polygon with the largest area. | Boolean |
| ex_where_clause(Optional) | An SQL expression that will be used to identify features that will not be altered. For more information on SQL syntax, see the SQL reference for elements used in query expressions help topic. | SQL Expression |
| ex_features(Optional) | An input polyline or polygon feature class or layer that defines polygon boundaries, or portions thereof, that will not be eliminated. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.Eliminate(in_features, out_feature_class, {selection}, {ex_where_clause}, {ex_features})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Census"
arcpy.MakeFeatureLayer_management("blockgrp", "blocklayer")
arcpy.SelectLayerByAttribute_management("blocklayer", "NEW_SELECTION", 
                                        '"Area_Sq_Miles" < 0.15')
arcpy.Eliminate_management("blocklayer", "C:/output/output.gdb/eliminate_output", 
                           "LENGTH", '"OBJECTID" = 9')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Census"
arcpy.MakeFeatureLayer_management("blockgrp", "blocklayer")
arcpy.SelectLayerByAttribute_management("blocklayer", "NEW_SELECTION", 
                                        '"Area_Sq_Miles" < 0.15')
arcpy.Eliminate_management("blocklayer", "C:/output/output.gdb/eliminate_output", 
                           "LENGTH", '"OBJECTID" = 9')
```

### Example 4

```python
# Name: Eliminate_Example2.py
# Description: Eliminate features based on a selection.
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/Portland.gdb/Census"
 
# Set local variables
inFeatures = "blockgrp"
tempLayer = "blocklayer"
expression = '"Area_Sq_Miles" < 0.15'
outFeatureClass = "C:/output/output.gdb/eliminate_output"
exclusionExpression = '"OBJECTID" = 9'
 
# Execute MakeFeatureLayer
arcpy.MakeFeatureLayer_management(inFeatures, tempLayer)
 
# Execute SelectLayerByAttribute to define features to be eliminated
arcpy.SelectLayerByAttribute_management(tempLayer, "NEW_SELECTION", expression)
 
# Execute Eliminate
arcpy.Eliminate_management(tempLayer, outFeatureClass, "LENGTH", 
                           exclusionExpression)
```

### Example 5

```python
# Name: Eliminate_Example2.py
# Description: Eliminate features based on a selection.
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/Portland.gdb/Census"
 
# Set local variables
inFeatures = "blockgrp"
tempLayer = "blocklayer"
expression = '"Area_Sq_Miles" < 0.15'
outFeatureClass = "C:/output/output.gdb/eliminate_output"
exclusionExpression = '"OBJECTID" = 9'
 
# Execute MakeFeatureLayer
arcpy.MakeFeatureLayer_management(inFeatures, tempLayer)
 
# Execute SelectLayerByAttribute to define features to be eliminated
arcpy.SelectLayerByAttribute_management(tempLayer, "NEW_SELECTION", expression)
 
# Execute Eliminate
arcpy.Eliminate_management(tempLayer, outFeatureClass, "LENGTH", 
                           exclusionExpression)
```

---

## Enable Archiving (Data Management)

## Summary

Enables archiving on a table, feature class, or feature dataset.

## Usage

- Enabling archiving provides the functionality to record and access changes made to a dataset over time.
- Archiving is only supported on enterprise and mobile geodatabases. File geodatabases do not support archiving.
- The input dataset must be from a database connection established as the data owner.
- Archiving can be enabled on traditional versioned datasets in an enterprise geodatabase or on nonversioned datasets in an enterprise geodatabase or mobile geodatabase. Branch versioned datasets have archiving enabled automatically during the register a dataset as branch versioned process.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The name of the dataset on which archiving will be enabled. | Table; Feature Class; Feature Dataset |
| in_dataset | The name of the dataset on which archiving will be enabled. | Table; Feature Class; Feature Dataset |

## Code Samples

### Example 1

```python
arcpy.management.EnableArchiving(in_dataset)
```

### Example 2

```python
arcpy.EnableArchiving_management("Database Connections//toolbox.county.parcels")
```

### Example 3

```python
arcpy.EnableArchiving_management("Database Connections//toolbox.county.parcels")
```

### Example 4

```python
# Name: EnableArchiving_Example.py
# Description: Enable archiving on a dataset

# Import system modules
import arcpy

# Set local variables
in_dataset = 'C:/Data/connections/Redlands.sde/TEST.TOOLBOX.rdlsstreets'

# Describe the properties of the dataset to see if archiving is enabled.
desc = arcpy.Describe(in_dataset)
isArch = desc.IsArchived

# Enable Archiving if it is not already enabled.
if isArch == False:
    # Execute EnableArchiving
    arcpy.EnableArchiving_management(in_dataset)
    print("{0} has been enabled for archiving.".format(in_dataset))
elif isArch == True:
    # If IsArch = True, then archiving is already enabled
    print("{0} already has archiving enabled.".format(in_dataset))
```

### Example 5

```python
# Name: EnableArchiving_Example.py
# Description: Enable archiving on a dataset

# Import system modules
import arcpy

# Set local variables
in_dataset = 'C:/Data/connections/Redlands.sde/TEST.TOOLBOX.rdlsstreets'

# Describe the properties of the dataset to see if archiving is enabled.
desc = arcpy.Describe(in_dataset)
isArch = desc.IsArchived

# Enable Archiving if it is not already enabled.
if isArch == False:
    # Execute EnableArchiving
    arcpy.EnableArchiving_management(in_dataset)
    print("{0} has been enabled for archiving.".format(in_dataset))
elif isArch == True:
    # If IsArch = True, then archiving is already enabled
    print("{0} already has archiving enabled.".format(in_dataset))
```

---

## Enable Attachments (Data Management)

## Summary

Enables attachments on a geodatabase feature class or table. The tool creates the necessary attachment relationship class and attachment table that will store attachment files internally.

## Usage

- If the input dataset is from an enterprise geodatabase, it must be from a database connection established as the data owner.
- Attachments must first be enabled using this tool before they can be added using the Add Attachments tool.
- If the geodatabase feature class or table already has attachments enabled, a warning message will appear and no processing will occur.
- Datasets that have replica tracking enabled and are nonversioned with archiving enabled will have archiving and replica tracking enabled on the attachments table.
- Datasets that are branch versioned will not be registered as branch versioning or have replica tracking enabled on the attachments table automatically. This can be handled manually by using the Register As Versioned and Enable Replica Tracking tools.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The geodatabase table or feature class for which attachments will be enabled. The input must be in a version 10 or later geodatabase. | Table View |
| in_dataset | The geodatabase table or feature class for which attachments will be enabled. The input must be in a version 10 or later geodatabase. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.EnableAttachments(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.EnableAttachments_management(r"C:\Data\City.gdb\Parcels")
```

### Example 3

```python
import arcpy
arcpy.EnableAttachments_management(r"C:\Data\City.gdb\Parcels")
```

### Example 4

```python
"""
Example: You have a folder of digital photographs of vacant homes; the photos
         are named according to the ParcelID of the house in the picture. You'll 
         add these photos to a parcel feature class as attachments.
"""

import csv
import arcpy
import os

input = r"C:\Data\City.gdb\Parcels"
inputField = "ParcelID"
matchTable = r"C:\Data\matchtable.csv"
matchField = "ParcelID"
pathField = "Picture" 
picFolder = r"C:\Pictures"

# Create a new Match Table .csv file
writer = csv.writer(open(matchTable, "wb"), delimiter=",")

# Write a header row (the table will have two columns: ParcelID and Picture)
writer.writerow([matchField, pathField])

# Iterate through each picture in the directory and write a row to the table
for file in os.listdir(picFolder):
    if str(file).find(".jpg") > -1:
        writer.writerow([str(file).replace(".jpg", ""), file])

del writer

# The input feature class must first be GDB attachments enabled
arcpy.EnableAttachments_management(input)

# Use the match table with the Add Attachments tool
arcpy.AddAttachments_management(input, inputField, matchTable, matchField, 
                                pathField, picFolder)
```

### Example 5

```python
"""
Example: You have a folder of digital photographs of vacant homes; the photos
         are named according to the ParcelID of the house in the picture. You'll 
         add these photos to a parcel feature class as attachments.
"""

import csv
import arcpy
import os

input = r"C:\Data\City.gdb\Parcels"
inputField = "ParcelID"
matchTable = r"C:\Data\matchtable.csv"
matchField = "ParcelID"
pathField = "Picture" 
picFolder = r"C:\Pictures"

# Create a new Match Table .csv file
writer = csv.writer(open(matchTable, "wb"), delimiter=",")

# Write a header row (the table will have two columns: ParcelID and Picture)
writer.writerow([matchField, pathField])

# Iterate through each picture in the directory and write a row to the table
for file in os.listdir(picFolder):
    if str(file).find(".jpg") > -1:
        writer.writerow([str(file).replace(".jpg", ""), file])

del writer

# The input feature class must first be GDB attachments enabled
arcpy.EnableAttachments_management(input)

# Use the match table with the Add Attachments tool
arcpy.AddAttachments_management(input, inputField, matchTable, matchField, 
                                pathField, picFolder)
```

---

## Enable Attribute Rules (Data Management)

## Summary

Enables one or more attribute rules in a dataset

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class that contains the attribute rule to be enabled. | Table View |
| Rule Names | The names of the rules to enable for the dataset. | String |
| Type (Optional) | Specifies the type of attribute rules to enable.Calculation—Filters the Rule Names parameter to display only calculation type rules.Constraint—Filters the Rule Names parameter to display only constraint type rules.Validation—Filters the Rule Names parameter to display only validation type rules.Calculation—Enable a calculation rule.Constraint—Enable a constraint rule.Validation—Enable a validation rule. | String |
| in_table | The table or feature class that contains the attribute rule to be enabled. | Table View |
| names[names,...] | The names of the rules to enable for the dataset. | String |
| type(Optional) | Specifies the type of attribute rules to enable. The tool will verify that the type of rule specified in this parameter matches the rule type specified. If they do not match, the rule will not be enabled.CALCULATION—Enable a calculation rule.CONSTRAINT—Enable a constraint rule.VALIDATION—Enable a validation rule. | String |

## Code Samples

### Example 1

```python
arcpy.management.EnableAttributeRules(in_table, names, {type})
```

### Example 2

```python
import arcpy
arcpy.EnableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                      "Rule A;Rule B", "CALCULATION")
```

### Example 3

```python
import arcpy
arcpy.EnableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                      "Rule A;Rule B", "CALCULATION")
```

### Example 4

```python
import arcpy
arcpy.EnableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                      "Calculation Rule A;Constraint Rule A")
```

### Example 5

```python
import arcpy
arcpy.EnableAttributeRules_management("C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData", 
                                      "Calculation Rule A;Constraint Rule A")
```

### Example 6

```python
import arcpy
fc = "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData"
desc = arcpy.Describe(fc).attributeRules
for rule in desc:
    if rule.isEnabled == False and rule.type == "esriARTConstraint":
        print("Enabling rule: {}".format(rule.name))
        arcpy.EnableAttributeRules_management(fc, rule.name)
```

### Example 7

```python
import arcpy
fc = "C:\\MyProject\\MyDatabase.sde\\pro.USER1.campusData"
desc = arcpy.Describe(fc).attributeRules
for rule in desc:
    if rule.isEnabled == False and rule.type == "esriARTConstraint":
        print("Enabling rule: {}".format(rule.name))
        arcpy.EnableAttributeRules_management(fc, rule.name)
```

---

## Enable COGO (Data Management)

## Summary

Enables COGO on a line feature class and adds COGO fields and COGO-enabled labeling to a line feature class. COGO fields store dimensions that are used to create line features in relation to each other.

## Usage

- The tool adds the following COGO fields to the selected line feature class: Arc Length, Direction, Distance, Radius, and Radius2. All fields are of type double.
- The tool adds COGO-related labeling and symbology to the selected line feature class. Lines are drawn with added COGO symbology, and a label expression labels each line with its COGO dimensions if they exist.
- Run the Disable COGO tool to disable COGO on the line feature class. The COGO fields can be deleted.
- If one or more of the COGO fields already exist and are of the correct type, only the remaining, missing COGO fields are added.
- If a line feature class is COGO enabled, editing tools such as the Traverse tool populate the COGO fields with the dimensions provided.
- The Direction field stores the direction (bearing) of the line from its start point to its endpoint. The direction value is stored in the database as north azimuth (decimal degrees). You can display the direction in other units by setting display units for your project.
- The Distance field stores the distance (length) of the line. The distance is stored in the database in the linear unit of the projection. You can display the distance in other units by setting display units for your project.
- The ArcLength field stores the arc distance between the start point and endpoint of a curved line. The arc length distance is stored in the database in the linear unit of the projection. You can display the arc length distance in other units by setting display units for your project.
- The Radius field stores the distance between the curve center point and the curve line. The radius distance is stored in the database in the linear unit of the projection. You can display the radius distance in other units by setting display units for your project.
- The Radius2 field stores the second radius for a spiral curve. This radius can be set to infinity.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Line Features | The line feature class that will be COGO enabled. | Feature Layer |
| in_line_features | The line feature class that will be COGO enabled. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.EnableCOGO(in_line_features)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "E:\ArcGISXI\Mont\Montgomery.gdb"
arcpy.EnableCOGO_management("\Landbase\Road_cl")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "E:\ArcGISXI\Mont\Montgomery.gdb"
arcpy.EnableCOGO_management("\Landbase\Road_cl")
```

### Example 4

```python
import arcpy

# Variable to contain the path of the feature class that is to be COGO enabled
lineFeatureClass = r"d:\test.gdb\myLineFC"

# Check to see if the feature class is already enabled by using .isCOGOEnabled on a Describe
if arcpy.Describe(lineFeatureClass).isCOGOEnabled == False:
    # If it returns False, run EnableCOGO_management and pass the feature class
    arcpy.EnableCOGO_management(lineFeatureClass)
else:
    print("{} is already COGO Enabled".format(lineFeatureClass))
```

### Example 5

```python
import arcpy

# Variable to contain the path of the feature class that is to be COGO enabled
lineFeatureClass = r"d:\test.gdb\myLineFC"

# Check to see if the feature class is already enabled by using .isCOGOEnabled on a Describe
if arcpy.Describe(lineFeatureClass).isCOGOEnabled == False:
    # If it returns False, run EnableCOGO_management and pass the feature class
    arcpy.EnableCOGO_management(lineFeatureClass)
else:
    print("{} is already COGO Enabled".format(lineFeatureClass))
```

---

## Enable Editing Templates (Data Management)

## Summary

Enables a geodatabase to store editing templates.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The workspace containing the editing templates that will be enabled. | Workspace |
| in_workspace | The workspace containing the editing templates that will be enabled. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.EnableEditingTemplates(in_workspace)
```

### Example 2

```python
import arcpy
arcpy.management.EnableEditingTemplates(r'C:\path\to\gdb.gdb')
```

### Example 3

```python
import arcpy
arcpy.management.EnableEditingTemplates(r'C:\path\to\gdb.gdb')
```

---

## Enable Editor Tracking (Data Management)

## Summary

Enables editor tracking for a feature class, table, feature dataset, or relationship class in a geodatabase.

## Usage

- If the input dataset is from an enterprise geodatabase, it must be from a database connection established as the data owner.
- This tool can add fields if they do not exist. If fields do exist, they must be of the correct data type. The values specified for the Creator Field and Last Editor Field parameters must be string fields, and the values specified for the Creation Date Field and Last Edit Date Field parameters must be date fields.
- Editor tracking applies to operations on existing datasets only. It does not apply to operations that create datasets. For example, if you copy a dataset to create a new one, tracking values will not update in the new dataset.
- You can record the dates of edits in either the time zone in which the database is located or in coordinated universal time (UTC).If you plan to copy or replicate data across time zones or edit through a feature service, use UTC. Since editors can apply edits from potentially anywhere in the world, UTC works well because it ensures that times are recorded in a universally accepted and consistent way.Configuring editor tracking to use the time zone in which the database is located is only recommended if you are certain that all edits will be performed in that time zone.
- You can run this tool on a dataset that has editor tracking enabled but only to enable tracking of additional information. For example, if a dataset is only tracking the creator and creation date, you can run this tool to add tracking of the editor and last edited date. This tool cannot disable tracking on a field, switch tracking from one field to another, or switch between UTC and the database time zone. To perform any of these operations, disable editor tracking on the input dataset before you run this tool.
- Not all relationship class types can have editor tracking enabled. Editor tracking can only be enabled on table-based relationship classes (that is, many-to-many or attributed relationship classes).
- If editor tracking is enabled on a feature class that has attachments, editor tracking will also be enabled on the attachment table. If you add new fields, those fields will also be added to the attachment table. If you reuse existing fields, and those fields do not exist in the attachment table, they will be added.
- At ArcGIS Pro 2.4, feature datasets are supported as input for this tool. If you add new fields to each item in the feature dataset, the fields will be named created_user, created_date, last_edited_user, and last_edited_date by default. If you use existing fields, the fields must already exist for all items in the feature dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The feature class, table, feature dataset, or relationship class in which editor tracking will be enabled. | Dataset; Topology; Network Dataset |
| Creator Field (Optional) | The name of the field that will store the names of users who create features or records. If this field already exists, it must be a string field. | String |
| Creation Date Field (Optional) | The name of the field that will store the date that features or records are created. If this field already exists, it must be a low precision date field. | String |
| Last Editor Field (Optional) | The name of the field that will store the names of users who last edited features or records. If this field already exists, it must be a string field. | String |
| Last Edit Date Field (Optional) | The name of the field that will store the date that features or records were last edited. If this field already exists, it must be a low precision date field. | String |
| Add fields (Optional) | Specifies whether fields will be added if they don't exist.Unchecked—Fields will not be added. Fields specified must already exist in the Input Dataset parameter value. This is the default. Checked—Fields will be added if they do not exist. You must specify the names of the fields to add in the Creator Field, Creation Date Field, Last Editor Field, and Last Edit Date Field parameters. | Boolean |
| Record Dates in (Optional) | Specifies the time format in which the created date and last edited date will be recorded.UTC (Coordinated Universal Time)—Dates will be recorded in UTC. This is the default.Time zone of database—Dates will be recorded in the time zone in which the database is located. | String |
| in_dataset | The feature class, table, feature dataset, or relationship class in which editor tracking will be enabled. | Dataset; Topology; Network Dataset |
| creator_field(Optional) | The name of the field that will store the names of users who create features or records. If this field already exists, it must be a string field. | String |
| creation_date_field(Optional) | The name of the field that will store the date that features or records are created. If this field already exists, it must be a low precision date field. | String |
| last_editor_field(Optional) | The name of the field that will store the names of users who last edited features or records. If this field already exists, it must be a string field. | String |
| last_edit_date_field(Optional) | The name of the field that will store the date that features or records were last edited. If this field already exists, it must be a low precision date field. | String |
| add_fields(Optional) | Specifies whether fields will be added if they don't exist.NO_ADD_FIELDS—Fields will not be added. Fields specified must already exist in the in_dataset parameter value. This is the default. ADD_FIELDS—Fields will be added if they do not exist. You must specify the names of the fields to add in the creator_field, creation_date_field, last_editor_field, and last_edit_date_field parameters. | Boolean |
| record_dates_in(Optional) | Specifies the time format in which the created date and last edited date will be recorded.UTC—Dates will be recorded in UTC. This is the default.DATABASE_TIME—Dates will be recorded in the time zone in which the database is located. | String |

## Code Samples

### Example 1

```python
arcpy.management.EnableEditorTracking(in_dataset, {creator_field}, {creation_date_field}, {last_editor_field}, {last_edit_date_field}, {add_fields}, {record_dates_in})
```

### Example 2

```python
import arcpy
arcpy.management.EnableEditorTracking("d:/RC.gdb/Buildings", "Creator", "Created", "Editor", "Edited", "ADD_FIELDS", "UTC")
```

### Example 3

```python
import arcpy
arcpy.management.EnableEditorTracking("d:/RC.gdb/Buildings", "Creator", "Created", "Editor", "Edited", "ADD_FIELDS", "UTC")
```

### Example 4

```python
# Name: EnableEditorTracking_Ex02.py
# Description: Enables editor tracking for all datasets in a geodatabase

# Import system modules
import arcpy
import os

# Set the workspace
workspace = arcpy.GetParameterAsText(0)

# Set the workspace environment
arcpy.env.workspace = "d:/test/data.gdb"

# Get all the stand alone tables and feature classes
dataList = arcpy.ListTables() + arcpy.ListFeatureClasses()

# For feature datasets, get all of the feature classes
# from the list and add them to the master list
for dataset in arcpy.ListDatasets("", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses()

# Run the EnableEditorTracking tool for each dataset
for dataset in dataList:
    print(f'Enabling tracking on {dataset}')
    arcpy.management.EnableEditorTracking(dataset, "ET_CREATOR",
        "ET_CREATED", "ET_EDITOR", "ET_EDITED", "ADD_FIELDS", "UTC")
print('Enabling complete')
```

### Example 5

```python
# Name: EnableEditorTracking_Ex02.py
# Description: Enables editor tracking for all datasets in a geodatabase

# Import system modules
import arcpy
import os

# Set the workspace
workspace = arcpy.GetParameterAsText(0)

# Set the workspace environment
arcpy.env.workspace = "d:/test/data.gdb"

# Get all the stand alone tables and feature classes
dataList = arcpy.ListTables() + arcpy.ListFeatureClasses()

# For feature datasets, get all of the feature classes
# from the list and add them to the master list
for dataset in arcpy.ListDatasets("", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses()

# Run the EnableEditorTracking tool for each dataset
for dataset in dataList:
    print(f'Enabling tracking on {dataset}')
    arcpy.management.EnableEditorTracking(dataset, "ET_CREATOR",
        "ET_CREATED", "ET_EDITOR", "ET_EDITED", "ADD_FIELDS", "UTC")
print('Enabling complete')
```

---

## Enable Enterprise Geodatabase (Data Management)

## Summary

Creates geodatabase system tables, stored procedures, functions, and types in an existing database, which enable geodatabase functionality in the database.

## Usage

- ArcGIS Pro (Desktop Standard or Desktop Advanced) or ArcGIS Server must be installed on the computer from which you connect to the database.
- You can use this tool to create a geodatabase in the following database cloud service offerings provided your ArcGIS software is running in the same cloud platform and region as the database service: Microsoft Azure SQL Database, Amazon Relational Database Service (RDS) for SQL Server, Google Cloud SQL for SQL Server, Amazon Aurora (PostgreSQL-compatible edition), Amazon RDS for PostgreSQL, Google Cloud SQL for PostgreSQL, Microsoft Azure Database for PostgreSQL, Amazon RDS for Oracle, and Autonomous Transaction Processing and Autonomous Data Warehouse workload types in Oracle Autonomous Database.
- You can use this tool to create a geodatabase in SAP HANA Cloud. The ArcGIS client from which you run this tool should be as geographically close as possible to the SAP HANA Cloud region where your database service exists.
- You must enable a spatial type in the database before you run this tool on PostgreSQL. If you place the st_geometry library in the PostgreSQL lib directory on the PostgreSQL server before running this tool, the geodatabase will use the ST_Geometry type to store spatial data. If you enable PostGIS in the database, the geodatabase will store spatial data in PostGIS geometry columns. For PostgreSQL-based cloud database-as-a-service instances, ensure that PostGIS is available in the instance before you enable a geodatabase; ST_Geometry is not supported in database services.
- Before you run this tool on Amazon RDS for Oracle, you must enable Oracle Spatial in the database.
- The database connection file you provide must connect as a user who qualifies as a geodatabase administrator. Connect as the sde user to create a geodatabase in an IBM Db2, Oracle, PostgreSQL, or SAP HANA database or database service. For Microsoft SQL Server, you can connect as a user named sde to create an sde-schema geodatabase or as a user who is dbo in the SQL Server instance to create a geodatabase in the dbo schema in the database. For database services based on SQL Server, you must connect as a user named sde and create an sde-schema geodatabase.Note:Creating or upgrading user-schema geodatabases in Oracle is no longer supported.
- If you connect to a SQL Server database, the SQL Server instance must use a case-insensitive collation. The database must also have READ_COMMITTED_SNAPSHOT and ALLOW_SNAPSHOT_ISOLATION set to ON.
- The user who creates the geodatabase must have specific privileges. See the page appropriate to your database or comparable database service for information on the required privileges to create a geodatabase:Privileges for geodatabases in Db2Privileges for geodatabases in OraclePrivileges for geodatabases in PostgreSQLPrivileges for geodatabases in SQL ServerPrivileges for geodatabases in SAP HANA
- Privileges for geodatabases in Db2
- Privileges for geodatabases in Oracle
- Privileges for geodatabases in PostgreSQL
- Privileges for geodatabases in SQL Server
- Privileges for geodatabases in SAP HANA
- You must configure the st_geometry library before you can enable geodatabase functionality in Oracle. See Create a geodatabase in Oracle for details. This does not apply to database services based on Oracle.
- To generate a license file for an enterprise geodatabases licensed with ArcGIS Enterprise on Kubernetes, use the exportGeodatabaseLicense REST operation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The path and database connection file (.sde) name for the database in which geodatabase functionality will be enabled. The database connection must connect as a user that qualifies as a geodatabase administrator. | Workspace |
| Authorization File | The keycodes file that was created when ArcGIS Server was authorized. If you have not done so, authorize ArcGIS Server to create this file.This file is in the <drive>\Program Files\ESRI\License<release#>\sysgen folder on Windows or the /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. Tip:The /.wine directory is a hidden directory.You may need to copy the keycodes file from the ArcGIS Server machine to a location that is accessible to the tool. | File |
| input_database | The path and database connection file (.sde) name for the database in which geodatabase functionality will be enabled. The database connection must connect as a user that qualifies as a geodatabase administrator. | Workspace |
| authorization_file | The keycodes file that was created when ArcGIS Server was authorized. If you have not done so, authorize ArcGIS Server to create this file.This file is in the <drive>\Program Files\ESRI\License<release#>\sysgen folder on Windows or the /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen directory on Linux. Tip:The /.wine directory is a hidden directory.You may need to copy the keycodes file from the ArcGIS Server machine to a location that is accessible to the tool. | File |

## Code Samples

### Example 1

```python
arcpy.management.EnableEnterpriseGeodatabase(input_database, authorization_file)
```

### Example 2

```python
import arcpy
arcpy.management.EnableEnterpriseGeodatabase("C:\myconnections\database1.sde", 
                                             "C:\authfiles\keycodes")
```

### Example 3

```python
import arcpy
arcpy.management.EnableEnterpriseGeodatabase("C:\myconnections\database1.sde", 
                                             "C:\authfiles\keycodes")
```

---

## Enable Feature Binning (Data Management)

## Summary

Enables database computation for feature binning on a feature class.

## Usage

- Amazon Redshift
- IBM Db2
- Google BigQuery
- Microsoft SQL Server
- Oracle
- PostgreSQL
- SAP HANA
- Snowflake

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature class for which database computed feature binning will be enabled. Supported input types are point and multipoint feature classes stored in a mobile geodatabase, enterprise geodatabase or database, or point feature classes stored in a cloud data warehouse. The data cannot be versioned or archive enabled. | Feature Layer |
| Bin Type (Optional) | Specifies the type of binning that will be enabled. If you are using SAP HANA data, only the Square, Flat hexagon, and Pointy hexagon options are supported. If you are using Snowflake or Redshift data, only the Geohash option is supported.Flat hexagon—The flat hexagon binning scheme, also known as flat geohex or flat hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a flat edge of the hexagon on top. This is the default for Microsoft SQL Server, Oracle, PostgreSQL, and BigQuery data.Pointy hexagon—The pointy hexagon binning scheme, also known as pointy geohex or pointy hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a point of the hexagon on top.Square—The square binning scheme, also known as geosquare or squarebinning, will be enabled. The tiles are a tessellation of squares This is the default for Db2 and SAP HANA data.Geohash—The geohash binning scheme, in which the tiles are a tessellation of rectangles, will be enabled. Because geohash bins always use the WGS84 geographic coordinate system (GCS WGS84, EPSG WKID 4326), you cannot specify a bin coordinate system for geohash bins. This is the default and only option for Snowflake and Redshift data. | String |
| Bin Coordinate Systems (Optional) | The coordinate systems that will be used to visualize the aggregated output feature layer. You can choose up to two coordinate systems to visualize the output layer. By default, the coordinate system of the input feature class is used. Custom coordinate systems are not supported.This parameter does not apply to BigQuery, Redshift, or Snowflake. For those platforms, the coordinate system of the input feature class is used. | Coordinate System |
| Summary Statistics (Optional) | Specifies the statistics that will be summarized and stored in the bin cache. Statistics are used to symbolize bins and provide aggregate information for all the points in a bin. One summary statistic, the total feature count (shape_count), is always available. You can define up to five additional summary statistics.Field—The field on which the summary statistics will be calculated. Supported field types are short integer, long integer, big integer, float, and double.Statistic Type—The type of statistic that will be calculated for the specified field. Statistics are calculated for all features in the bin. Available statistics types are as follows: Mean (AVG)—Calculates the average for the specified fieldMinimum (MIN)—Finds the smallest value for all records of the specified fieldMaximum (MAX)—Finds the largest value for all records of the specified fieldStandard deviation (STDDEV)—Calculates the standard deviation value for the fieldSum (SUM)—Adds the total value for the specified field | Value Table |
| Generate Binning Cache (Optional) | Specifies whether a static cache of the aggregated results will be generated or visualizations will be aggregated on the fly. The cache is not necessarily created for all levels of detail.Checked—A static cache of the aggregated results will be generated. It is recommended that you use this option for better performance. However, changes to the underlying data will not be updated in the cache unless the Manage Feature Bin Cache tool is run. A static cache is generated by default for data in IBM Db2, Microsoft SQL Server, Oracle, and PostgreSQL.To generate a static cache for feature classes in PostgreSQL that use PostGIS spatial types, GDAL libraries must be installed in the database.A static cache is always generated for data in BigQuery, Redshift, and Snowflake. Unchecked—A static cache of the aggregated results will not be generated, and visualizations will be aggregated on the fly. This is the only option for SAP HANA data. | Boolean |
| in_features | The feature class for which database computed feature binning will be enabled. Supported input types are point and multipoint feature classes stored in a mobile geodatabase, enterprise geodatabase or database, or point feature classes stored in a cloud data warehouse. The data cannot be versioned or archive enabled. | Feature Layer |
| bin_type(Optional) | Specifies the type of binning that will be enabled. If you are using SAP HANA data, only the SQUARE, FLAT_HEXAGON, and POINTY_HEXAGON options are supported. If you are using Snowflake or Redshift data, only the GEOHASH option is supported.FLAT_HEXAGON—The flat hexagon binning scheme, also known as flat geohex or flat hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a flat edge of the hexagon on top. This is the default for Microsoft SQL Server, Oracle, PostgreSQL, and BigQuery data.POINTY_HEXAGON—The pointy hexagon binning scheme, also known as pointy geohex or pointy hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a point of the hexagon on top.SQUARE—The square binning scheme, also known as geosquare or squarebinning, will be enabled. The tiles are a tessellation of squares This is the default for Db2 and SAP HANA data.GEOHASH—The geohash binning scheme, in which the tiles are a tessellation of rectangles, will be enabled. Because geohash bins always use the WGS84 geographic coordinate system (GCS WGS84, EPSG WKID 4326), you cannot specify a bin coordinate system for geohash bins. This is the default and only option for Snowflake and Redshift data. | String |
| bin_coord_sys[bin_coord_sys,...](Optional) | The coordinate systems that will be used to visualize the aggregated output feature layer. You can specify up to two coordinate systems to visualize the output layer. By default, the coordinate system of the input feature class is used. Custom coordinate systems are not supported.This parameter does not apply to BigQuery, Redshift, or Snowflake. For those platforms, the coordinate system of the input feature class is used. | Coordinate System |
| summary_stats[[Field, Statistic Type],...](Optional) | Specifies the statistics that will be summarized and stored in the bin cache. Statistics are used to symbolize bins and provide aggregate information for all the points in a bin. One summary statistic, the total feature count (shape_count), is always available. You can define up to five additional summary statistics.Field—The field on which the summary statistics will be calculated. Supported field types are short integer, long integer, big integer, float, and double.Statistic Type—The type of statistic that will be calculated for the specified field. Statistics are calculated for all features in the bin. Available statistics types are as follows: Mean (AVG)—Calculates the average for the specified fieldMinimum (MIN)—Finds the smallest value for all records of the specified fieldMaximum (MAX)—Finds the largest value for all records of the specified fieldStandard deviation (STDDEV)—Calculates the standard deviation value for the fieldSum (SUM)—Adds the total value for the specified field | Value Table |
| generate_static_cache(Optional) | Specifies whether a static cache of the aggregated results will be generated or visualizations will be aggregated on the fly. The cache is not necessarily created for all levels of detail.STATIC_CACHE—A static cache of the aggregated results will be generated. It is recommended that you use this option for better performance. However, changes to the underlying data will not be updated in the cache unless the Manage Feature Bin Cache tool is run. A static cache is generated by default for data in IBM Db2, Microsoft SQL Server, Oracle, and PostgreSQL.To generate a static cache for feature classes in PostgreSQL that use PostGIS spatial types, GDAL libraries must be installed in the database.A static cache is always generated for data in BigQuery, Redshift, and Snowflake.DYNAMIC—A static cache of the aggregated results will not be generated, and visualizations will be aggregated on the fly. This is the only option for SAP HANA data. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.EnableFeatureBinning(in_features, {bin_type}, {bin_coord_sys}, {summary_stats}, {generate_static_cache})
```

### Example 2

```python
import arcpy

bin_coord_sys = arcpy.SpatialReference('GCS_WGS_1984')
arcpy.management.EnableFeatureBinning(
    "lod_gdb.elec.Earthquakes",
    "SQUARE",
    bin_coord_sys,
    "depth_km MAX",
    "STATIC_CACHE")
```

### Example 3

```python
import arcpy

bin_coord_sys = arcpy.SpatialReference('GCS_WGS_1984')
arcpy.management.EnableFeatureBinning(
    "lod_gdb.elec.Earthquakes",
    "SQUARE",
    bin_coord_sys,
    "depth_km MAX",
    "STATIC_CACHE")
```

---

## Enable Replica Tracking (Data Management)

## Summary

Enables replica tracking on data, allowing you to work with offline maps and in distributed collaborations. Replica tracking applies to services that are configured with the sync capability with the option of creating a version for each downloaded map.

## Usage

- Branch versioning
- Nonversioned archiving

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The enterprise geodatabase table, feature class, feature dataset, attributed relationship class, or many-to-many relationship class on which replica tracking will be enabled. | Table; Feature Class; Feature Dataset |
| in_dataset | The enterprise geodatabase table, feature class, feature dataset, attributed relationship class, or many-to-many relationship class on which replica tracking will be enabled. | Table; Feature Class; Feature Dataset |

## Code Samples

### Example 1

```python
arcpy.management.EnableReplicaTracking(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.management.EnableReplicaTracking("C:\\MyProject\\mySdeConn.sde\\database.user1.stores")
```

### Example 3

```python
import arcpy
arcpy.management.EnableReplicaTracking("C:\\MyProject\\mySdeConn.sde\\database.user1.stores")
```

---

## Encode Field (Data Management)

## Summary

Converts categorical values (string, integer, or date) into multiple numerical fields, each representing a category. The encoded numerical fields can be used in most data science and statistical workflows including regression models.

## Usage

- One-hot—Converts each categorical value into a new column and assigns 0 or 1. where 1 represents the presence of that categorical value.
- One-cold—Converts each categorical value into a new column and assigns 0 or 1. where 0 represents the presence of that categorical value.
- Temporal—Converts each date value in the field to encode into integer values (0, 1, 2, and so on) based on the time step interval. All the dates falling under the same time step interval are encoded together with the same integer. Three fields will be created if you use the Temporal method: a time step field that contains the encoded time steps, a start time field that contains the start time of the time interval, and an end time field that contains the end time of the time interval.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class containing the field to be encoded. Fields will be added to the existing input table and will not create a new output table. | Table View; Raster Layer; Mosaic Layer |
| Field to Encode | The field containing the categorical or temporal values to be encoded. | Field |
| Encoding Method (Optional) | Specifies the method to use to encode the values contained in the Field to Encode parameter.One-hot—Each categorical value will be converted to a new field and the values 0 and 1 will be assigned, where 1 represents the presence of that categorical value. This is the default.One-cold— Each categorical value will be converted to a new field and the values 0 and 1 will be assigned, where 0 represents the presence of that categorical value.Temporal—Each temporal value in the Field to Encode parameter will be converted to an integer based on the time step interval, time step alignment, and reference time specified. | String |
| Time Step Interval (Optional) | The number of seconds, minutes, hours, days, weeks, or years that will represent a single time step. The temporal value will be aggregated into a certain time step it is within. If no value is provided, the default time step interval is based on two algorithms that are used to determine the optimal number and width of the time step intervals. The smaller of the two results is used as the time step interval. | Time Unit |
| Time Step Alignment (Optional) | Specifies how aggregation will occur based on the Time Step Interval parameter value.End time— Time steps will align to the last time event and aggregate back in time. This is the default.Start time— Time steps will align to the first time event and aggregate forward in time.Reference time— Time steps will align to the date and time specified in the Reference Time parameter. Aggregation is performed forward and backward in time from the reference time until reaching the first and last temporal values. | String |
| Reference Time (Optional) | The date and time to which the time-step intervals will align. For example, to bin your data weekly from Monday to Sunday, set a reference time of Sunday at midnight to ensure that the time steps break between Sunday and Monday at midnight.The value can be a date and time or solely a date; it cannot be solely a time. The expected format is determined by the computer's regional time settings. | Date |
| in_table | The input table or feature class containing the field to be encoded. Fields will be added to the existing input table and will not create a new output table. | Table View; Raster Layer; Mosaic Layer |
| field | The field containing the categorical or temporal values to be encoded. | Field |
| method(Optional) | Specifies the method to use to encode the values contained in the Field to Encode parameter.ONEHOT—Each categorical value will be converted to a new field and the values 0 and 1 will be assigned, where 1 represents the presence of that categorical value. This is the default.ONECOLD— Each categorical value will be converted to a new field and the values 0 and 1 will be assigned, where 0 represents the presence of that categorical value.TEMPORAL—Each temporal value in the Field to Encode parameter will be converted to an integer based on the time step interval, time step alignment, and reference time specified. | String |
| time_step_interval(Optional) | The number of seconds, minutes, hours, days, weeks, or years that will represent a single time step. The temporal value will be aggregated into a certain time step it is within. If no value is provided, the default time step interval is based on two algorithms that are used to determine the optimal number and width of the time step intervals. The smaller of the two results is used as the time step interval. | Time Unit |
| time_step_alignment(Optional) | Specifies how aggregation will occur based on the Time Step Interval parameter value.END_TIME— Time steps will align to the last time event and aggregate back in time. This is the default.START_TIME— Time steps will align to the first time event and aggregate forward in time.REFERENCE_TIME— Time steps will align to the date and time specified in the Reference Time parameter. Aggregation is performed forward and backward in time from the reference time until reaching the first and last temporal values. | String |
| reference_time(Optional) | The date and time to which the time-step intervals will align. For example, to bin your data weekly from Monday to Sunday, set a reference time of Sunday at midnight to ensure that the time steps break between Sunday and Monday at midnight.The value can be a date and time or solely a date; it cannot be solely a time. The expected format is determined by the computer's regional time settings. | Date |

## Code Samples

### Example 1

```python
arcpy.management.EncodeField(in_table, field, {method}, {time_step_interval}, {time_step_alignment}, {reference_time})
```

### Example 2

```python
arcpy.management.EncodeField("San_Francisco_Crimes", 
                    "Category", "ONEHOT", '', None, "END_TIME")
```

### Example 3

```python
arcpy.management.EncodeField("San_Francisco_Crimes", 
                    "Category", "ONEHOT", '', None, "END_TIME")
```

### Example 4

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Encoded\\MyData.gdb"
    inputFeatures = 'San_Francisco_Crimes'

    # Set input features, dependent variable, and explanatory variable.
    in_table = 'San_Francisco_Crimes'
    field = 'Dates'

    # Set encoding Method
    encoding_method = "TEMPORAL"

    # Set time Step Interval
    time_step_interval = '1 Days'

    # Set Time Step Alignment
    time_step_alignment = "START_TIME"

    # Run Encode Field Tool.
    arcpy.management.EncodeField(in_table, field, encoding_method, 
                    None, time_step_interval, time_step_alignment)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 5

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Encoded\\MyData.gdb"
    inputFeatures = 'San_Francisco_Crimes'

    # Set input features, dependent variable, and explanatory variable.
    in_table = 'San_Francisco_Crimes'
    field = 'Dates'

    # Set encoding Method
    encoding_method = "TEMPORAL"

    # Set time Step Interval
    time_step_interval = '1 Days'

    # Set Time Step Alignment
    time_step_alignment = "START_TIME"

    # Run Encode Field Tool.
    arcpy.management.EncodeField(in_table, field, encoding_method, 
                    None, time_step_interval, time_step_alignment)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

---

## Evaluate Rules (Data Management)

## Summary

Evaluates geodatabase rules and functionality.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | A file geodatabase, mobile geodatabase, or feature service URL. An example of a feature service URL is https://myserver/server/rest/services/myservicename/FeatureServer. | Workspace |
| Evaluation Types | Specifies the types of evaluation that will be used.Calculation rules—Batch calculation attribute rules will be evaluated.Validation rules—Validation attribute rules will be evaluated. | String |
| Extent (Optional) | The extent to be evaluated. If there is a selection in the map, only selected features within the specified extent will be evaluated.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Async (Optional) | Specifies whether the evaluation will run synchronously or asynchronously. This parameter is only supported when the input workspace is a feature service.Checked—The evaluation will run asynchronously. This option dedicates server resources to run the evaluation with a longer time-out. Running asynchronously is recommended when evaluating large datasets that contain many features requiring calculation or validation. This is the default. Unchecked—The evaluation will run synchronously. This option has a shorter time-out and is best used when evaluating an extent with a small number of features requiring calculation or validation. | Boolean |
| in_workspace | A file geodatabase, mobile geodatabase, or feature service URL. An example of a feature service URL is https://myserver/server/rest/services/myservicename/FeatureServer. | Workspace |
| evaluation_types[evaluation_types,...] | Specifies the types of evaluation that will be used.CALCULATION_RULES—Batch calculation attribute rules will be evaluated.VALIDATION_RULES—Validation attribute rules will be evaluated. | String |
| extent(Optional) | The extent to be evaluated. If there is a selection in the map, only selected features within the specified extent will be evaluated.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| run_async(Optional) | Specifies whether the evaluation will run synchronously or asynchronously. This parameter is only supported when the input workspace is a feature service.ASYNC—The evaluation will run asynchronously. This option dedicates server resources to run the evaluation with a longer time-out. Running asynchronously is recommended when evaluating large datasets that contain many features requiring calculation or validation. This is the default. SYNC—The evaluation will run synchronously. This option has a shorter time-out and is best used when evaluating an extent with a small number of features requiring calculation or validation.Legacy:In earlier releases, this parameter was named async. At ArcGIS Pro 2.4, the parameter name was changed to run_async to avoid conflicts with the reserved Python keyword async. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.EvaluateRules(in_workspace, evaluation_types, {extent}, {run_async})
```

### Example 2

```python
# Import arcpy
import arcpy

# Assign local variables
in_workspace = "https://myserver/server/rest/services/myservicename/FeatureServer"
extent = "-113.187897827702 38.0958054854392 -113.142166008849 38.1404599940719"

# Run the evaluation
arcpy.management.EvaluateRules(in_workspace, "VALIDATION_RULES", extent, "ASYNC")
```

### Example 3

```python
# Import arcpy
import arcpy

# Assign local variables
in_workspace = "https://myserver/server/rest/services/myservicename/FeatureServer"
extent = "-113.187897827702 38.0958054854392 -113.142166008849 38.1404599940719"

# Run the evaluation
arcpy.management.EvaluateRules(in_workspace, "VALIDATION_RULES", extent, "ASYNC")
```

---

## Export 3D Objects (Data Management)

## Summary

Exports 3D object features to one or more 3D model file formats.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The 3D object feature layer that will be exported. | Feature Layer |
| Target Folder | The existing directory that will contain the output 3D models. | Folder |
| 3D Formats To Export | Specifies the 3D formats that will be exported. Collada (.dae)—The COLLADA format will be exported.Autodesk Drawing (.dwg)—The DWG format will be exported.Autodesk (.fbx)—The Autodesk FilmBox format will be exported.Khronos Group glTF binary (.glb)—The binary Graphics Library Transmission format will be exported.Khronos Group glTF json (.gltf)—The JSON Graphics Library Transmission format will be exported.Industry Foundation Classes (.ifc)—The Industry Foundation Classes format will be exported.Wavefront (.obj)—The Wavefront format will be exported.Universal Scene Description (.usdc)— The Universal Scene Description format will be exported. Compressed Universal Scene Description (.usdz)— The compressed version of the Universal Scene Description format will be exported. | String |
| Output Folder Name Field (Optional) | The text field in the input feature's attribute table that contains the name to be used for each output folder. If no name field is provided, the output folder will be named after the object ID of the input features. | Field |
| Overwrite output folder | Specifies whether existing 3D models in the output directory will be overwritten.Checked—Existing 3D models in the output directory will be overwritten.Unchecked—Existing 3D models in the output directory will not be overwritten. This is the default. | Boolean |
| in_features | The 3D object feature layer that will be exported. | Feature Layer |
| target_folder | The existing directory that will contain the output 3D models. | Folder |
| formats[formats,...] | Specifies the 3D formats that will be exported. FMT3D_DAE—The COLLADA format will be exported.FMT3D_DWG—The DWG format will be exported.FMT3D_FBX—The Autodesk FilmBox format will be exported.FMT3D_GLB—The binary Graphics Library Transmission format will be exported.FMT3D_GLTF—The JSON Graphics Library Transmission format will be exported.FMT3D_IFC—The Industry Foundation Classes format will be exported.FMT3D_OBJ—The Wavefront format will be exported.FMT3D_USDC— The Universal Scene Description format will be exported. FMT3D_USDZ— The compressed version of the Universal Scene Description format will be exported. | String |
| name_field(Optional) | The text field in the input feature's attribute table that contains the name to be used for each output folder. If no name field is provided, the output folder will be named after the object ID of the input features. | Field |
| overwrite | Specifies whether existing 3D models in the output directory will be overwritten.OVERWRITE—Existing 3D models in the output directory will be overwritten.NO_OVERWRITE—Existing 3D models in the output directory will not be overwritten. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.Export3DObjects(in_features, target_folder, formats, {name_field}, overwrite)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/data'
arcpy.management.Export3DObjects("city_models.gdb/Downtown_Buildings", "exported_models", 
                                ["FMT3D_DAE", "FMT3D_OBJ"], "Model_Name", "OVERWRITE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/data'
arcpy.management.Export3DObjects("city_models.gdb/Downtown_Buildings", "exported_models", 
                                ["FMT3D_DAE", "FMT3D_OBJ"], "Model_Name", "OVERWRITE")
```

### Example 4

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

# Create a feature layer from the 3D Object feature class
feature_class = "city_models.gdb/Downtown_Buildings"
feature_layer = os.path.basename(feature_class)
arcpy.MakeFeatureLayer_management(feature_class, feature_layer)

# Select a subset of features to export
# The default OBJECTID field is used below to process a subset of features
object_ids = '1,5,10'
sql_query = f"OBJECTID IN ({object_ids})"
arcpy.management.SelectLayerByAttribute(feature_layer, "NEW_SELECTION", sql_query)

# Export the selected features to model files on disk
arcpy.management.Export3DObjects(feature_layer, "exported_models", ["FMT3D_GLB"])
arcpy.management.Delete(feature_layer)
```

### Example 5

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

# Create a feature layer from the 3D Object feature class
feature_class = "city_models.gdb/Downtown_Buildings"
feature_layer = os.path.basename(feature_class)
arcpy.MakeFeatureLayer_management(feature_class, feature_layer)

# Select a subset of features to export
# The default OBJECTID field is used below to process a subset of features
object_ids = '1,5,10'
sql_query = f"OBJECTID IN ({object_ids})"
arcpy.management.SelectLayerByAttribute(feature_layer, "NEW_SELECTION", sql_query)

# Export the selected features to model files on disk
arcpy.management.Export3DObjects(feature_layer, "exported_models", ["FMT3D_GLB"])
arcpy.management.Delete(feature_layer)
```

---

## Export Acknowledgement Message (Data Management)

## Summary

Creates an output acknowledgement file to acknowledge the reception of previously received data change messages.

## Usage

- Use this tool when synchronizing a replica while disconnected. First run the Export Data Change Message tool, which creates a delta file with changes to synchronize. Then copy and import the delta file to the relative replica using the Import Message tool. If the delta file gets lost and you want to resend, use the Re-Export Unacknowledged Messages tool to regenerate the delta file. After the changes are imported, you can export an acknowledgment file from the relative replica using the Export Acknowledgement Message tool. Copy and import the acknowledgment file using the Import Message tool. If the acknowledgment is not received, the next time changes are sent, they will include the new changes and the previously sent changes.
- The geodatabase can be a local geodatabase or a geodata service.
- The output acknowledgement file must be XML.
- This tool is not applicable for check-out replicas.
- To synchronize replicas in a connected mode, see the Synchronize Changes tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Export from Replica Geodatabase | Specifies the replica geodatabase from which to export the acknowledgement message. The geodatabase may be local or remote. | Workspace ; GeoDataServer |
| Output Acknowledgement File | Specifies the delta file to export to. | File |
| Replica | The replica from which the acknowledgement message will be exported. | String |
| in_geodatabase | Specifies the replica geodatabase from which to export the acknowledgement message. The geodatabase may be local or remote. | Workspace ; GeoDataServer |
| out_acknowledgement_file | Specifies the delta file to export to. | File |
| in_replica | The replica from which the acknowledgement message will be exported. | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportAcknowledgementMessage(in_geodatabase, out_acknowledgement_file, in_replica)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportAcknowledgementMessage("MySDEdata.sde", "AcknowledgementMessage.xml" , "MyReplica1")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportAcknowledgementMessage("MySDEdata.sde", "AcknowledgementMessage.xml" , "MyReplica1")
```

### Example 4

```python
# Name: ExportAcknowledgement_Example2.py
# Description: Exports an acknowledgement message from a replica geodatabase (SDE).

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
output_file = "AcknowledgementMessage.xml"
replica_name = "MyReplica1"
arcpy.management.ExportAcknowledgementMessage(in_geodatabase, output_file , replica_name)
```

### Example 5

```python
# Name: ExportAcknowledgement_Example2.py
# Description: Exports an acknowledgement message from a replica geodatabase (SDE).

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
output_file = "AcknowledgementMessage.xml"
replica_name = "MyReplica1"
arcpy.management.ExportAcknowledgementMessage(in_geodatabase, output_file , replica_name)
```

---

## Export Attachments (Data Management)

## Summary

Exports file attachments from the records of a geodatabase feature class or table to a specified folder. Attachments can also be exported to subdirectories based on an attribute value from a specified attribute column. Exported attachments can be renamed using one or more field attribute values.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The geodatabase table or feature class from which attachments will be exported.The input must be stored in a version 10.0 or later geodatabase, and the table must have attachments enabled. | Table View |
| Output Location | The folder where the attachment files will be exported. | Folder |
| Subdirectory Field(Optional) | A field from the Input Dataset parameter value that will be used to create subdirectory names. | Field |
| Name Format(Optional) | Specifies the format that will be used for naming exported attachments.Use Original Filenames—The output file names will use the original file names stored in the geodatabase.Use Field Values Only—The output file names will use the field values of the Name Fields parameter values. Multiple values will be concatenated with an underscore.Add Field Values as Prefix—The output file names will use the original file names with a prefix from the field values of the Name Fields parameter values.Add Field Values as Suffix—The output file names will use the original file names with a suffix from the field values of the Name Fields parameter values. Multiple values will be concatenated with an underscore. | String |
| Name Fields (Optional) | The field names from the Input Dataset parameter value that will be used to rename the exported attachments. If multiple fields are specified, the output files will use the field values concatenated with an underscore in the order they are specified.For example, if two field names are specified, and Name Format parameter is set to Use Field Values Only, the field values for the first record are Main and Street, and the attachment is a .jpg file, the exported file will be named Main_Street.jpg.This parameter is enabled when the Name Format parameter is set to Use Field Values Only, Add Field Values as Prefix, or Add Field Values as Suffix. | Field |
| in_dataset | The geodatabase table or feature class from which attachments will be exported.The input must be stored in a version 10.0 or later geodatabase, and the table must have attachments enabled. | Table View |
| out_location | The folder where the attachment files will be exported. | Folder |
| subdirectory_field(Optional) | A field from the in_dataset parameter value that will be used to create subdirectory names. | Field |
| name_format(Optional) | Specifies the format that will be used for naming exported attachments.ORIGINAL—The output file names will use the original file names stored in the geodatabase.REPLACE—The output file names will use the field values of the name_fields parameter values.PREFIX—The output file names will use the original file names with a prefix from the field values of the name_fields parameter values.SUFFIX—The output file names will use the original file names with a suffix from the field values of the name_fields parameter values. | String |
| name_fields[name_fields,...](Optional) | The fields from the in_dataset parameter value that will be used to rename the exported attachments. If multiple fields are specified, the output files will use the field values concatenated with an underscore in the order they are specified.For example, if two field names are specified, and name_format parameter is set to REPLACE, the field values for the first record are Main and Street, and the attachment is a .jpg file, the exported file will be named Main_Street.jpg.This parameter is enabled when the name_format parameter is set to REPLACE, PREFIX, or SUFFIX. | Field |

## Code Samples

### Example 1

```python
arcpy.management.ExportAttachments(in_dataset, out_location, {subdirectory_field}, {name_format}, {name_fields})
```

### Example 2

```python
import arcpy
arcpy.management.ExportAttachments(r"C:\Data\National.gdb\Airports",
                                   r"C:\OutputFolder", 'CODE', 'REPLACE', ['NAME', 'CODE'])
```

### Example 3

```python
import arcpy
arcpy.management.ExportAttachments(r"C:\Data\National.gdb\Airports",
                                   r"C:\OutputFolder", 'CODE', 'REPLACE', ['NAME', 'CODE'])
```

### Example 4

```python
import arcpy

# Import system variables
import arcpy

# Set the workspace
arcpy.env.workspace = r"C:\National.gdb"

# Set local variables
in_dataset = "Airports"
out_location = r"C:\Output_Images"
subdir_field = 'Code'
output_names = 'REPLACE'
name_fields = ['NAME', 'CODE']

# Select the Salt Lake City Airport and download all attachments to a subdirectory named SLC.
layerSelection = arcpy.management.SelectLayerByAttribute(in_dataset, 'NEW_SELECTION',
                                                         "Name = 'Salt Lake City'")

# Export the attachments with the layer selection set and renamed using field values.
arcpy.management.ExportAttachments(layerSelection, out_location, subdir_field,
                                   output_names, name_fields)
```

### Example 5

```python
import arcpy

# Import system variables
import arcpy

# Set the workspace
arcpy.env.workspace = r"C:\National.gdb"

# Set local variables
in_dataset = "Airports"
out_location = r"C:\Output_Images"
subdir_field = 'Code'
output_names = 'REPLACE'
name_fields = ['NAME', 'CODE']

# Select the Salt Lake City Airport and download all attachments to a subdirectory named SLC.
layerSelection = arcpy.management.SelectLayerByAttribute(in_dataset, 'NEW_SELECTION',
                                                         "Name = 'Salt Lake City'")

# Export the attachments with the layer selection set and renamed using field values.
arcpy.management.ExportAttachments(layerSelection, out_location, subdir_field,
                                   output_names, name_fields)
```

---

## Export Attribute Rules (Data Management)

## Summary

Exports attribute rules from a dataset to a comma-separated values file (.csv).

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class from which the attribute rules will be exported. | Table View |
| Output File | The folder location and name of the .csv file that will be created. | File |
| in_table | The table or feature class from which the attribute rules will be exported. | Table View |
| out_csv_file | The folder location and name of the .csv file that will be created. | File |

## Code Samples

### Example 1

```python
arcpy.management.ExportAttributeRules(in_table, out_csv_file)
```

### Example 2

```python
import arcpy
arcpy.management.ExportAttributeRules("C:\\MyProject\\MyDatabase.sde\\pro.USER1.GasPipes", 
                                      "C:\\MyProject\\ExpAttrRulesFrBuilding.csv")
```

### Example 3

```python
import arcpy
arcpy.management.ExportAttributeRules("C:\\MyProject\\MyDatabase.sde\\pro.USER1.GasPipes", 
                                      "C:\\MyProject\\ExpAttrRulesFrBuilding.csv")
```

---

## Export Contingent Values (Data Management)

## Summary

Exports field groups and contingent values to a .csv file.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase table or feature class from which the field groups and contingent values will be exported. | Table View |
| Field Groups Output File (.csv) | The location and name of the output .csv file that will be created with specific column names containing information about the field groups of the target table. | File |
| Contingent Values Output File (.csv) | The location and name of the output .csv file that will be created with specific column names containing information about the contingent values of the target table. | File |
| target_table | The input geodatabase table or feature class from which the field groups and contingent values will be exported. | Table View |
| field_groups_file | The location and name of the output .csv file that will be created with specific column names containing information about the field groups of the target table. | File |
| contingent_values_file | The location and name of the output .csv file that will be created with specific column names containing information about the contingent values of the target table. | File |

## Code Samples

### Example 1

```python
arcpy.management.ExportContingentValues(target_table, field_groups_file, contingent_values_file)
```

### Example 2

```python
import arcpy
arcpy.ExportContingentValues_management("C:\\MyProject\\myConn.sde\\pro.USER1.Animals",
                                        "C:\\MyProject\\MyFieldGroups.csv",
                                        "C:\\MyProject\\MyContingentValues.csv")
```

### Example 3

```python
import arcpy
arcpy.ExportContingentValues_management("C:\\MyProject\\myConn.sde\\pro.USER1.Animals",
                                        "C:\\MyProject\\MyFieldGroups.csv",
                                        "C:\\MyProject\\MyContingentValues.csv")
```

---

## Export Data Change Message (Data Management)

## Summary

Creates an output delta file containing updates from an input replica.

## Usage

- The geodatabase can be a local geodatabase or a geodata service.
- The output delta file can be a delta file geodatabase (.gdb) or a delta XML file (.xml). When specifying the output delta file, you must include the appropriate suffix (.gdb or .xml).
- Use this tool when synchronizing a replica while disconnected. First run the Export Data Change Message tool, which creates a delta file with changes to synchronize. Then copy and import the delta file to the relative replica using the Import Message tool. If the delta file gets lost and you want to resend, use the Re-Export Unacknowledged Messages tool to regenerate the delta file. After the changes are imported, you can export an acknowledgment file from the relative replica using the Export Acknowledgement Message tool. Copy and import the acknowledgment file using the Import Message tool. If the acknowledgment is not received, the next time changes are sent, they will include the new changes and the previously sent changes.
- To synchronize replicas in a connected mode, see the Synchronize Changes tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Export from Replica Geodatabase | The replica geodatabase from which the data change message will be exported. The geodatabase can be local or remote. | Workspace;GeoDataServer |
| Output Data Changes File | The output delta file. | File |
| Replica | The replica containing the updates to be exported. | String |
| Switch to Receiver once the message has been exported | Specifies whether the replica role will be changed from a sender to a receiver. The receiver may not send replica updates until updates from the relative replica sender arrive.Unchecked—The replica role will not be changed. This is the default.Checked—The replica role will be changed from a sender to a receiver. | Boolean |
| Include unacknowledged data changes | Specifies whether data changes that were previously exported for which no acknowledgment message was received will be included.Unchecked—Data changes that were previously sent will not be included.Checked—All data changes that were previously exported for which no acknowledgment message was received will be included. This is the default. | Boolean |
| Include new data changes since last export | Specifies whether all data changes made since the last exported data change message will be included.Unchecked—Data changes made since the last exported data change message will not be included.Checked—All data changes made since the last exported data change message will be included. This is the default. Specifies whether all data changes made since the last exported data change message will be included.NO_NEW_CHANGES—Data changes made since the last exported data change message will not be included.NEW_CHANGES—All data changes made since the last exported data change message will be included. This is the default. | Boolean |
| in_geodatabase | The replica geodatabase from which the data change message will be exported. The geodatabase can be local or remote. | Workspace;GeoDataServer |
| out_data_changes_file | The output delta file. | File |
| in_replica | The replica containing the updates to be exported. | String |
| switch_to_receiver | Specifies whether the replica will be changed from a sender to a receiver. The receiver may not send replica updates until updates from the relative replica sender arrive.DO_NOT_SWITCH—The replica role will not be changed. This is the default.SWITCH—The replica role will be changed from a sender to receiver. | Boolean |
| include_unacknowledged_changes | Specifies whether data changes that were previously exported for which no acknowledgment message was received will be included.NO_UNACKNOWLEDGED—Data changes that were previously sent will not be included.UNACKNOWLEDGED—All data changes that were previously exported for which no acknowledgment message was received will be included. This is the default. | Boolean |
| include_new_changes | Specifies whether all data changes made since the last exported data change message will be included.NO_NEW_CHANGES—Data changes made since the last exported data change message will not be included.NEW_CHANGES—All data changes made since the last exported data change message will be included. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ExportDataChangeMessage(in_geodatabase, out_data_changes_file, in_replica, switch_to_receiver, include_unacknowledged_changes, include_new_changes)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportDataChangeMessage("MySDEdata.sde", "Changes.gdb", 
                                         "MyReplica1", "SWITCH", "TRUE", "TRUE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportDataChangeMessage("MySDEdata.sde", "Changes.gdb", 
                                         "MyReplica1", "SWITCH", "TRUE", "TRUE")
```

### Example 4

```python
# Name: ExportDataChangesMessage_Example2.py
# Description: Export a data change message to a delta file geodatabase (.gdb).

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
out_dataChanges = "Changes.gdb"
replica_name = "MyReplica1"
switch_directions = "SWITCH"
acknowledge = "TRUE"
new_changes = "TRUE"

# Run ExportDataChangeMessage
arcpy.management.ExportDataChangeMessage(in_geodatabase, out_dataChanges, 
                                         replica_name, switch_directions, 
                                         acknowledge, new_changes)
```

### Example 5

```python
# Name: ExportDataChangesMessage_Example2.py
# Description: Export a data change message to a delta file geodatabase (.gdb).

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
in_geodatabase = "MySDEdata.sde"
out_dataChanges = "Changes.gdb"
replica_name = "MyReplica1"
switch_directions = "SWITCH"
acknowledge = "TRUE"
new_changes = "TRUE"

# Run ExportDataChangeMessage
arcpy.management.ExportDataChangeMessage(in_geodatabase, out_dataChanges, 
                                         replica_name, switch_directions, 
                                         acknowledge, new_changes)
```

---

## Export Frame And Camera Parameters (Data Management)

## Summary

Exports frame and camera parameters from a mosaic dataset that contains frame imagery.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| Output File | The output file containing the frame and camera parameters. Supported file formats include .csv and .txt. | File |
| Output Format (Optional) | Specifies the output file format for the frame and camera parameters.Esri Frame and Camera Table—The frame and camera parameters will be exported as an Esri Frames and Camera table (.csv file). This is the default.Pix4D Calibrated Camera Parameters—The frame and camera parameters will be exported using the Pix4D calibrated camera parameters format (.txt file). | String |
| input_mosaic_dataset | The input mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| output_file | The output file containing the frame and camera parameters. Supported file formats include .csv and .txt. | File |
| output_format(Optional) | Specifies the output file format for the frame and camera parameters.ESRI_FRAME_AND_CAMERA_TABLE—The frame and camera parameters will be exported as an Esri Frames and Camera table (.csv file). This is the default.PIX4D_CALIBRATED_CAMERA_PARAMETERS—The frame and camera parameters will be exported using the Pix4D calibrated camera parameters format (.txt file). | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportFrameAndCameraParameters(input_mosaic_dataset, output_file, {output_format})
```

### Example 2

```python
#ExportFrameAndCameraParameters

import arcpy

arcpy.management.ExportFrameAndCameraParameters("Image Collection", 	
	r"C:\Data\FrameandCameraTable.csv", "ESRI_FRAME_AND_CAMERA_TABLE")
```

### Example 3

```python
#ExportFrameAndCameraParameters

import arcpy

arcpy.management.ExportFrameAndCameraParameters("Image Collection", 	
	r"C:\Data\FrameandCameraTable.csv", "ESRI_FRAME_AND_CAMERA_TABLE")
```

---

## Export Geodatabase Configuration Keywords (Data Management)

## Summary

Exports the configuration keywords, parameters, and values from the specified enterprise geodatabase to an editable file. Change parameter values or add custom configuration keywords to the file and use the Import Geodatabase Configuration Keywords tool to import the changes to the geodatabase.

## Usage

- This tool only works with enterprise geodatabases.
- Only the geodatabase administrator can run the Export Geodatabase Configuration Keywords tool.
- This tool is not supported for geodatabases in SAP HANA.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The connection file for the enterprise geodatabase from which you want to export configuration keywords, parameters, and values. You must connect as the geodatabase administrator. | Workspace |
| Output File | The full path to and name of the ASCII text file to be created. The file will contain all the configuration keywords, parameters, and values from the enterprise geodatabase's DBTUNE (or SDE_DBTUNE) system table. | File |
| input_database | The connection file for the enterprise geodatabase from which you want to export configuration keywords, parameters, and values. You must connect as the geodatabase administrator. | Workspace |
| out_file | The full path to and name of the ASCII text file to be created. The file will contain all the configuration keywords, parameters, and values from the enterprise geodatabase's DBTUNE (or SDE_DBTUNE) system table. | File |

## Code Samples

### Example 1

```python
arcpy.management.ExportGeodatabaseConfigurationKeywords(input_database, out_file)
```

### Example 2

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
output_file = "C:\\temp\\keyword.txt"
arcpy.ExportGeodatabaseConfigurationKeywords_management(ent_gdb,output_file)
```

### Example 3

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
output_file = "C:\\temp\\keyword.txt"
arcpy.ExportGeodatabaseConfigurationKeywords_management(ent_gdb,output_file)
```

### Example 4

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
output_file = "C:\\temp\\keyword.txt"

# Process: Export configuration keywords to a text file
arcpy.ExportGeodatabaseConfigurationKeywords_management(ent_gdb,output_file)
```

### Example 5

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
output_file = "C:\\temp\\keyword.txt"

# Process: Export configuration keywords to a text file
arcpy.ExportGeodatabaseConfigurationKeywords_management(ent_gdb,output_file)
```

---

## Export Mosaic Dataset Geometry (Data Management)

## Summary

Creates a feature class showing the footprints, boundary, seamlines or spatial resolutions of a mosaic dataset.

## Usage

- When you set Geometry Type to Cell size level, you are exporting the union of items that have the same resolution level. This is useful to quickly see the coverage of your imagery for each cell size level.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that you want to export the geometry from. | Mosaic Layer |
| Output Feature Class | Name the feature class you are creating. | Feature Class |
| Query Definition (Optional) | An SQL expression to export specific rasters in the mosaic dataset. | SQL Expression |
| Geometry Type (Optional) | The type of geometry to export. Footprint— Create a feature class showing the footprints of each image.Boundary— Create a feature class showing the boundary of the mosaic dataset.Seamline— Create a feature class showing the seamlines.Cell size level— Create a feature class based on cell size level of features in your mosaic dataset. | String |
| in_mosaic_dataset | The mosaic dataset that you want to export the geometry from. | Mosaic Layer |
| out_feature_class | Name the feature class you are creating. | Feature Class |
| where_clause(Optional) | An SQL expression to export specific rasters in the mosaic dataset. | SQL Expression |
| geometry_type(Optional) | The type of geometry to export. FOOTPRINT— Create a feature class showing the footprints of each image.BOUNDARY— Create a feature class showing the boundary of the mosaic dataset.SEAMLINE— Create a feature class showing the seamlines.LEVEL— Create a feature class based on cell size level of features in your mosaic dataset. | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportMosaicDatasetGeometry(in_mosaic_dataset, out_feature_class, {where_clause}, {geometry_type})
```

### Example 2

```python
import arcpy
arcpy.ExportMosaicDatasetGeometry_management(
     "c:/workspace/exportmd.gdb/md",
     "c:/workspace/exportmd.gdb/footprint_export",
     "OBJECTID = 1", "FOOTPRINT")
```

### Example 3

```python
import arcpy
arcpy.ExportMosaicDatasetGeometry_management(
     "c:/workspace/exportmd.gdb/md",
     "c:/workspace/exportmd.gdb/footprint_export",
     "OBJECTID = 1", "FOOTPRINT")
```

### Example 4

```python
#Export Mosaic Dataset Geometry


import arcpy

arcpy.env.workspace = "c:/workspace"

#Export footprint from a single record in a mosaic dataset
mdname = "exportmd_footprints.gdb/md"
out_FC = "C:/workspace/LANDSAT_footprints"
where_clause = "OBJECTID = 1"
geometry_type = "FOOTPRINT"

arcpy.ExportMosaicDatasetGeometry_management(
     mdname, out_FC, where_clause, geometry_type)
```

### Example 5

```python
#Export Mosaic Dataset Geometry


import arcpy

arcpy.env.workspace = "c:/workspace"

#Export footprint from a single record in a mosaic dataset
mdname = "exportmd_footprints.gdb/md"
out_FC = "C:/workspace/LANDSAT_footprints"
where_clause = "OBJECTID = 1"
geometry_type = "FOOTPRINT"

arcpy.ExportMosaicDatasetGeometry_management(
     mdname, out_FC, where_clause, geometry_type)
```

---

## Export Mosaic Dataset Items (Data Management)

## Summary

Saves a copy of processed images in a mosaic dataset to a specified folder and raster file format.

## Usage

- By default, all items will be exported to the specified folder. Use the Query Definition parameter or interactively select specific records in the mosaic dataset to export a subset of the images.
- The images will be exported with all the processing from the function chains applied. Only the function chains at the item level are applied; function chains at the mosaic dataset level are ignored. This tool does not export the raw source images.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that contains the images that will be exported. | Mosaic Layer |
| Output Folder | The folder where the images will be saved. | Folder |
| Output Base Name(Optional) | A prefix that will be added to the name of each item after it is copied. The prefix will be followed by the Object ID value from the mosaic dataset footprints table.If no base name is set, the text in the Name field of the mosaic dataset item will be used. | String |
| Query Definition(Optional) | An SQL expression that will be used to save selected images in the mosaic dataset. | SQL Expression |
| Output Format(Optional) | Specifies the format that will be used for the output raster datasets.TIFF—TIFF format will be used. This is the default.Cloud Optimized GeoTIFF—Cloud Optimized GeoTIFF format will be used.BMP—BMP format will be used.ENVI DAT—ENVI DAT format will be used.Esri BIL—Esri BIL format will be used.Esri BIP—Esri BIP format will be used.Esri BSQ—Esri BSQ format will be used.GIF—GIF format will be used.Esri Grid—Esri Grid format will be used.ERDAS IMAGINE—ERDAS IMAGINE format will be used.JPEG 2000—JPEG 2000 format will be used.JPEG—JPEG format will be used.PNG—PNG format will be used.Cloud raster format—Cloud raster format will be used.Meta raster format—Meta raster format will be used. | String |
| NoData Value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset.It is recommended that you specify a NoData value if the output images will be clipped. | String |
| Clip Type(Optional) | Specifies the output extent that will be used for the raster datasets. If you specify an extent or feature class that covers an area larger than the raster data, the output will have the larger extent.No clipping—The output will not be clipped. This is the default.Clip to extent—An extent will be used to clip the output.Clip to feature class—A feature class extent will be used to clip the output. | String |
| Clipping Template(Optional) | The feature class or bounding box that will be used to limit the extent.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Cell Size(Optional) | The horizontal (x) and vertical (y) dimensions of the output cells.If the cell size is not provided, the spatial resolution of the input will be used. | Point |
| Export images to image space (Optional) | Specifies whether raster items will be exported in map space or image space.Unchecked—Raster items will be exported in map space. This is the default.Checked—Raster items will be exported in image space. | Boolean |
| Remove image distortions (Optional) | Specifies whether lens distortion will be removed from the exported raster in image space.Unchecked—Lens distortion will not be removed from the exported raster in image space. This is the default.Checked—Lens distortion will be removed from the exported raster in image space. | Boolean |
| Band Method (Optional) | Specifies the method that will be used to select bands.This parameter is enabled when the Export images to image space parameter is checked.All bands—All bands will be exported. This is the default.Band IDs—Bands will be exported based on the Band ID Selection parameter value.Band names—Bands will be exported based on the Band Name Selection parameter value. | String |
| Band Name Selection(Optional) | The name of the band that will be exported from the input mosaic dataset. This parameter is enabled when the Band Method parameter is set to Band names. | Value Table |
| Band ID Selection(Optional) | The ID number of the band that will be exported from the input mosaic dataset. This parameter is enabled when the Band Method parameter is set to Band IDs. | Value Table |
| in_mosaic_dataset | The mosaic dataset that contains the images that will be exported. | Mosaic Layer |
| out_folder | The folder where the images will be saved. | Folder |
| out_base_name(Optional) | A prefix that will be added to the name of each item after it is copied. The prefix will be followed by the Object ID value from the mosaic dataset footprints table.If no base name is set, the text in the Name field of the mosaic dataset item will be used. | String |
| where_clause(Optional) | An SQL expression that will be used to save selected images in the mosaic dataset. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |
| format(Optional) | Specifies the format that will be used for the output raster datasets.TIFF—TIFF format will be used. This is the default.Cloud Optimized GeoTIFF—Cloud Optimized GeoTIFF format will be used.BMP—BMP format will be used.ENVI—ENVI DAT format will be used.Esri BIL—Esri BIL format will be used.Esri BIP—Esri BIP format will be used.Esri BSQ—Esri BSQ format will be used.GIF—GIF format will be used.GRID—Esri Grid format will be used.IMAGINE IMAGE—ERDAS IMAGINE format will be used.JP2—JPEG 2000 format will be used.JPEG—JPEG format will be used.PNG—PNG format will be used.CRF—Cloud raster format will be used.MRF—Meta raster format will be used. | String |
| nodata_value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset.It is recommended that you specify a NoData value if the output images will be clipped. | String |
| clip_type(Optional) | Specifies the output extent that will be used for the raster datasets. If you specify an extent or feature class that covers an area larger than the raster data, the output will have the larger extent.NONE—The output will not be clipped. This is the default.EXTENT—An extent will be used to clip the output.FEATURE_CLASS—A feature class extent will be used to clip the output. | String |
| template_dataset(Optional) | The feature class or bounding box that will be used to limit the extent.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| cell_size(Optional) | The horizontal (x) and vertical (y) dimensions of the output cells.If the cell size is not provided, the spatial resolution of the input will be used. | Point |
| image_space(Optional) | Specifies whether raster items will be exported in map space or image space.MAPSPACE—Raster items will be exported in map space. This is the default.IMAGESPACE—Raster items will be exported in image space. | Boolean |
| remove_distortion(Optional) | Specifies whether lens distortion will be removed from the exported raster in image space.REMOVED—Lens distortion will be removed from the exported raster in image space. NOTREMOVE—Lens distortion will not be removed from the exported raster in image space. This is the default. | Boolean |
| band_method(Optional) | Specifies the method that will be used to select bands.This parameter is enabled when the image_space parameter is set to IMAGESPACE.ALL_BANDS—All bands will be exported. This is the default.BAND_IDS—Bands will be exported based on the band_id_selection parameter value.BAND_NAMES—Bands will be exported based on the band_name_selection parameter value. | String |
| band_name_selection[band_name_selection,...](Optional) | The name of the band that will be exported from the input mosaic dataset. This parameter is enabled when the band_method parameter is set to BAND_NAMES. | Value Table |
| band_id_selection[band_id_selection,...](Optional) | The ID number of the band that will be exported from the input mosaic dataset. This parameter is enabled when the band_method parameter is set to BAND_IDS. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.ExportMosaicDatasetItems(in_mosaic_dataset, out_folder, {out_base_name}, {where_clause}, {format}, {nodata_value}, {clip_type}, {template_dataset}, {cell_size}, {image_space}, {remove_distortion}, {band_method}, {band_name_selection}, {band_id_selection})
```

### Example 2

```python
import arcpy
arcpy.ExportMosaicDatasetItems_management(
     "c:/workspace/exportmditems.gdb/export_all_items", 
     "c:/workspace/export_all_items_out", 
     "allitems", "", "TIFF", "", "NONE", "", "")
```

### Example 3

```python
import arcpy
arcpy.ExportMosaicDatasetItems_management(
     "c:/workspace/exportmditems.gdb/export_all_items", 
     "c:/workspace/export_all_items_out", 
     "allitems", "", "TIFF", "", "NONE", "", "")
```

### Example 4

```python
#Export Mosaic Dataset items
                                                                       
import arcpy
arcpy.env.workspace = "c:/workspace"
    
#export mosaic dataset items using feature class as clipping extent
imdname = "exportmditem.gdb/exportmd" 
outfolder = "c:/workspace/outfolder"
basename = "Landsat8"
query = "OBJECTID = 1"
out_format = "TIFF"
nodata_value = "#"
cliptype = "FEATURE_CLASS"
clip_featureclass = "c:/workspace/featureclassdb.gdb/clip_FC"
cell_size = "#"

arcpy.ExportMosaicDatasetItems_management(imdname, outfolder, basename, 
     query, out_format, nodata_value, cliptype, clip_featureclass, cell_size)
```

### Example 5

```python
#Export Mosaic Dataset items
                                                                       
import arcpy
arcpy.env.workspace = "c:/workspace"
    
#export mosaic dataset items using feature class as clipping extent
imdname = "exportmditem.gdb/exportmd" 
outfolder = "c:/workspace/outfolder"
basename = "Landsat8"
query = "OBJECTID = 1"
out_format = "TIFF"
nodata_value = "#"
cliptype = "FEATURE_CLASS"
clip_featureclass = "c:/workspace/featureclassdb.gdb/clip_FC"
cell_size = "#"

arcpy.ExportMosaicDatasetItems_management(imdname, outfolder, basename, 
     query, out_format, nodata_value, cliptype, clip_featureclass, cell_size)
```

### Example 6

```python
import arcpy
 
arcpy.management.ExportMosaicDatasetItems(in_mosaic_dataset=r"c:\workspace\exportmditems.gdb\ImageCollection", out_folder=r"C:\workspace\ExportMDItem", out_base_name="Band_ID", where_clause="", format="TIFF", nodata_value="", clip_type="NONE", template_dataset="DEFAULT", cell_size=None, image_space="IMAGESPACE", remove_distortion="REMOVED", band_method="BAND_IDS", band_name_selection= None, band_id_selection="1;2;3;4")
```

### Example 7

```python
import arcpy
 
arcpy.management.ExportMosaicDatasetItems(in_mosaic_dataset=r"c:\workspace\exportmditems.gdb\ImageCollection", out_folder=r"C:\workspace\ExportMDItem", out_base_name="Band_ID", where_clause="", format="TIFF", nodata_value="", clip_type="NONE", template_dataset="DEFAULT", cell_size=None, image_space="IMAGESPACE", remove_distortion="REMOVED", band_method="BAND_IDS", band_name_selection= None, band_id_selection="1;2;3;4")
```

---

## Export Mosaic Dataset Paths (Data Management)

## Summary

Creates a table of the file path for each item in a mosaic dataset. You can specify whether the table contains all the file paths or just the ones that are broken.

## Usage

- The output of this tool is a table, either in a geodatabase or a .dbf file.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset containing the file paths to export. | Mosaic Layer |
| Output Table | The table to create. The table can be a geodatabase table or a .dbf file.The SourceOID field in the output table is derived from the OID of the row in the original mosaic dataset table. | Table |
| Query Definition (Optional) | An SQL expression to select specific rasters for export. | SQL Expression |
| Export Mode (Optional) | Populate the table with either all of the paths, or only the broken paths.All paths—Export all paths to the table. This is the default.Broken paths only—Export only broken paths to the table. | String |
| Types of paths to export (Optional) | Choose to export file paths from only the source raster, only the cache, or both. The default is to export all path types.Raster—Export file paths from rasters.Item cache—Export file paths from item cache. | String |
| in_mosaic_dataset | The mosaic dataset containing the file paths to export. | Mosaic Layer |
| out_table | The table to create. The table can be a geodatabase table or a .dbf file.The SourceOID field in the output table is derived from the OID of the row in the original mosaic dataset table. | Table |
| where_clause(Optional) | An SQL expression to select specific rasters for export. | SQL Expression |
| export_mode(Optional) | Populate the table with either all of the paths, or only the broken paths.ALL—Export all paths to the table. This is the default.BROKEN—Export only broken paths to the table. | String |
| types_of_paths[type_of_path,...](Optional) | Choose to export file paths from only the source raster, only the cache, or both. The default is to export all path types.RASTER—Export file paths from rasters.ITEM_CACHE—Export file paths from item cache. | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportMosaicDatasetPaths(in_mosaic_dataset, out_table, {where_clause}, {export_mode}, {types_of_paths})
```

### Example 2

```python
import arcpy
arcpy.ExportMosaicDatasetPaths_management(
     "C:/Workspace/exportmd.gdb/md", "C:/workspace/brokenpaths.dbf",
     "#", "BROKEN", "RASTER")
```

### Example 3

```python
import arcpy
arcpy.ExportMosaicDatasetPaths_management(
     "C:/Workspace/exportmd.gdb/md", "C:/workspace/brokenpaths.dbf",
     "#", "BROKEN", "RASTER")
```

### Example 4

```python
#Export broken raster path in FGDB Mosaic Dataset to dbf table

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "exportmd.gdb/md"
outtable = "C:/workspace/brokenpaths.dbf"
query = "#"
mode = "BROKEN"
pathtype = "RASTER"

arcpy.ExportMosaicDatasetPaths_management(mdname, outtable, query, 
                                          mode, pathtype)
```

### Example 5

```python
#Export broken raster path in FGDB Mosaic Dataset to dbf table

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "exportmd.gdb/md"
outtable = "C:/workspace/brokenpaths.dbf"
query = "#"
mode = "BROKEN"
pathtype = "RASTER"

arcpy.ExportMosaicDatasetPaths_management(mdname, outtable, query, 
                                          mode, pathtype)
```

---

## Export Raster World File (Data Management)

## Summary

Creates a world file based on the pixel size and the location of the upper left pixel.

## Usage

- If the transformation cannot be expressed as a world file, this tool will write an approximate affine transformation into the world file, with an x on the end of the extension name. For example, a TIFF image with this approximate affine transformation has the extension .tfwx. This is to signify that this is not a standard world file; it is only an approximation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Dataset | The raster dataset from which you want to create the world file. | Raster Dataset |
| in_raster_dataset | The raster dataset from which you want to create the world file. | Raster Dataset |

## Code Samples

### Example 1

```python
arcpy.management.ExportRasterWorldFile(in_raster_dataset)
```

### Example 2

```python
import arcpy
arcpy.ExportRasterWorldFile_management("c:/data/image.tif")
```

### Example 3

```python
import arcpy
arcpy.ExportRasterWorldFile_management("c:/data/image.tif")
```

### Example 4

```python
##====================================
##Export Raster World File
##Usage: ExportRasterWorldFile_management in_raster

import arcpy
arcpy.env.workspace = "C:/Workspace"

##Export tfw file from the intput Raster Dataset
arcpy.ExportRasterWorldFile_management("image.tif")
```

### Example 5

```python
##====================================
##Export Raster World File
##Usage: ExportRasterWorldFile_management in_raster

import arcpy
arcpy.env.workspace = "C:/Workspace"

##Export tfw file from the intput Raster Dataset
arcpy.ExportRasterWorldFile_management("image.tif")
```

---

## Export Replica Schema (Data Management)

## Summary

Creates a replica schema file with the schema of an input one- or two-way replica.

## Usage

- The output schema file must be in XML format. You must specify .xml as the file suffix.
- Modifying the schema of a replica to match the schema of a relative replica is a separate process from data synchronization. Use the following tools for this purpose:Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.Import the changes using the Import Replica Schema tool.To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.
- Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.
- Import the changes using the Import Replica Schema tool.
- To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.
- This tool is used when synchronizing replica schema. Additionally, see the Compare Replica Schema and Import Replica Schema tools.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Export from Replica Geodatabase | The replica geodatabase from which the replica schema will be exported. The geodatabase can be a local or remote geodatabase. | Workspace; GeoDataServer |
| Output Replica Schema File | The file to which the replica schema will be exported. | File |
| Replica | The replica from which the schema will be exported. | String |
| in_geodatabase | The replica geodatabase from which the replica schema will be exported. The geodatabase can be a local or remote geodatabase. | Workspace; GeoDataServer |
| output_replica_schema_file | The file to which the replica schema will be exported. | File |
| in_replica | The replica from which the schema will be exported. | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportReplicaSchema(in_geodatabase, output_replica_schema_file, in_replica)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportReplicaSchema("Countries.gdb", "replicaSchema.xml", "MyReplica1")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ExportReplicaSchema("Countries.gdb", "replicaSchema.xml", "MyReplica1")
```

### Example 4

```python
# Name: ExportReplicaSchema_Example2.py
# Description: Export replica schema from a file geodatabase with a replica

# Import system modules
import arcpy

# Set workspace
arcpy.env.worksapce = "C:/Data"

# Set local variables
replica_workspace = "Countries.gdb"
output_file = "replicaSchema.xml"
replica = "MyReplica1"

# Run ExportReplicaSchema
arcpy.management.ExportReplicaSchema(replica_workspace, output_file, replica)
```

### Example 5

```python
# Name: ExportReplicaSchema_Example2.py
# Description: Export replica schema from a file geodatabase with a replica

# Import system modules
import arcpy

# Set workspace
arcpy.env.worksapce = "C:/Data"

# Set local variables
replica_workspace = "Countries.gdb"
output_file = "replicaSchema.xml"
replica = "MyReplica1"

# Run ExportReplicaSchema
arcpy.management.ExportReplicaSchema(replica_workspace, output_file, replica)
```

---

## Export Report To PDF (Data Management)

## Summary

Exports an ArcGIS Pro report to a PDF file.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Report | The input report or .rptx file. | Report; File |
| PDF File | The output PDF file. | File |
| Expression(Optional) | An SQL expression used to select a subset of records. This expression is applied in addition to any existing expressions. | SQL Expression |
| Resolution (DPI) (Optional) | The resolution of the exported PDF in dots per inch (dpi). | Long |
| Image Quality (Optional) | Specifies the output image quality of the PDF. The image quality option controls the quality of rasterized data going into the export. Best—The highest available image quality. This is the default.Better—High image quality.Normal—A compromise between image quality and speed.Faster—Lower image quality to generate the report faster. Fastest—The lowest image quality to create the report the fastest. | String |
| Embed Fonts (Optional) | Specifies whether fonts are embedded in the output report. Font embedding allows text and markers built from font glyphs to be displayed correctly when the PDF is viewed on a computer that does not have the necessary fonts installed. Checked—Fonts will be embedded in the output report. This is the default.Unchecked—Fonts will not be embedded in the output report. | Boolean |
| Compress Vector Graphics (Optional) | Specifies whether to compress the vector content streams in the PDF. Checked—Vector graphics will be compressed. This option should be set unless clear text is desired for troubleshooting. This is the default.Unchecked—Vector graphics will not be compressed. | Boolean |
| Image Compression (Optional) | Specifies the compression scheme used to compress image or raster data in the output PDF file. No image compression—Do not compress image or raster data.Run-length encoded (RLE) compression—Uses Run-length encoded compression.Deflate compression—Uses Deflate, a lossless data compression.Lempel-Ziv-Welch (LZW) compression—Uses Lempel-Ziv-Welch, a lossless data compression.Joint Photographic Experts Group (JPEG) compression—Uses JPEG, a lossy data compression.Adaptive compression—Uses Adaptive, which automatically selects the best compression type for each image on the page. JPEG will be used for large images with many unique colors. Deflate will be used for all other images. This is the default. | String |
| Password Protect (Optional) | Specifies whether password protection is needed to view the output PDF report.Checked—The output PDF report document will require a password to open.Unchecked—The output PDF report document can be opened without providing a password. This is the default. | Boolean |
| PDF Password (Optional) | A password to restrict opening the PDF. | Encrypted String |
| Page Range Type (Optional) | Specifies the page range of the report to export. All pages—Export all pages. This is the default.Last page—Export the last page only.Odd numbered pages—Export the odd numbered pages.Even numbered pages—Export the even numbered pages.Custom page range—Export a custom page range. | String |
| Custom Page Range (Optional) | The pages to be exported when the Page Range Type parameter is set to Custom. You can set individual pages, ranges, or a combination of both separated by commas, such as 1, 3-5, 10. | String |
| Initial Page Number (Optional) | The initial page number of the report to create a page numbering offset to add additional pages to the beginning of the report. | Long |
| Final Page Number (Optional) | The page number to display on the last page of the exported PDF. | Long |
| in_report | The input report or .rptx file. | Report; File |
| out_pdf_file | The output PDF file. | File |
| expression(Optional) | An SQL expression used to select a subset of records. This expression is applied in addition to any existing expressions. For more information on SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |
| resolution(Optional) | The resolution of the exported PDF in dots per inch (dpi). | Long |
| image_quality(Optional) | Specifies the output image quality of the PDF. The image quality option controls the quality of rasterized data going into the export. BEST—The highest available image quality. This is the default.BETTER—High image quality.NORMAL—A compromise between image quality and speed.FASTER—Lower image quality to generate the report faster. FASTEST—The lowest image quality to create the report the fastest. | String |
| embed_font(Optional) | Specifies whether fonts are embedded in the output report. Font embedding allows text and markers built from font glyphs to be displayed correctly when the PDF is viewed on a computer that does not have the necessary fonts installed. EMBED_FONTS—Fonts will be embedded in the output report. This is the default.NO_EMBED_FONTS—Fonts will not be embedded in the output report. | Boolean |
| compress_vector_graphics(Optional) | Specifies whether to compress the vector content streams in the PDF.COMPRESS_GRAPHICS—Vector graphics will be compressed. This option should be set unless clear text is desired for troubleshooting. This is the default.NO_COMPRESS_GRAPHICS—Vector graphics will not be compressed. | Boolean |
| image_compression(Optional) | Specifies the compression scheme used to compress image or raster data in the output PDF file. NONE—Do not compress image or raster data.RLE—Uses Run-length encoded compression.DEFLATE—Uses Deflate, a lossless data compression.LZW—Uses Lempel-Ziv-Welch, a lossless data compression.JPEG—Uses JPEG, a lossy data compression.ADAPTIVE—Uses Adaptive, which automatically selects the best compression type for each image on the page. JPEG will be used for large images with many unique colors. Deflate will be used for all other images. This is the default. | String |
| password_protect(Optional) | Specifies whether password protection is needed to view the output PDF report.PASSWORD_PROTECT—The output PDF report document will require a password to open.NO_PASSWORD_PROTECT—The output PDF report document can be opened without providing a password. This is the default. | Boolean |
| pdf_password(Optional) | A password to restrict opening the PDF. | Encrypted String |
| page_range_type(Optional) | Specifies the page range of the report to export. ALL—Export all pages. This is the default.LAST—Export the last page only.ODD—Export the odd numbered pages.EVEN—Export the even numbered pages.CUSTOM—Export a custom page range. | String |
| custom_page_range(Optional) | The pages to be exported when the page_range_type parameter is set to CUSTOM. You can set individual pages, ranges, or a combination of both separated by commas, such as 1, 3-5, 10. | String |
| initial_page_number(Optional) | The initial page number of the report to create a page numbering offset to add additional pages to the beginning of the report. | Long |
| final_page_number(Optional) | The page number to display on the last page of the exported PDF. | Long |

## Code Samples

### Example 1

```python
arcpy.management.ExportReportToPDF(in_report, out_pdf_file, {expression}, {resolution}, {image_quality}, {embed_font}, {compress_vector_graphics}, {image_compression}, {password_protect}, {pdf_password}, {page_range_type}, {custom_page_range}, {initial_page_number}, {final_page_number})
```

### Example 2

```python
import arcpy
arcpy.ExportReportToPDF_management("C:/data/sample.rptx", "C:/data/samplePDF.pdf", 
                                   "STATEFIPS = 42", 96, 'BETTER', 'EMBED_FONTS', 
                                   'COMPRESS_GRAPHICS', 'ADAPTIVE', 
                                   'PASSWORD_PROTECT', "password", 'CUSTOM', 
                                   "1-10")
```

### Example 3

```python
import arcpy
arcpy.ExportReportToPDF_management("C:/data/sample.rptx", "C:/data/samplePDF.pdf", 
                                   "STATEFIPS = 42", 96, 'BETTER', 'EMBED_FONTS', 
                                   'COMPRESS_GRAPHICS', 'ADAPTIVE', 
                                   'PASSWORD_PROTECT', "password", 'CUSTOM', 
                                   "1-10")
```

### Example 4

```python
# ExportReportToPDF.py
# Simple example of the Export Report to PDF tool

# Import system variables
import arcpy

# Set the workspace
arcpy.env.workspace = "C:/data/cities.gdb"

# Set local variables
in_layer = 'cities'
in_report = 'US Cities Report'
out_PDF = 'C/data/cities.pdf'

# Modify the report data source by selecting only the records where 
# POP1990 < 50000
arcpy.SelectLayerByAttribute_management(in_layer, 'NEW_SELECTION', 
                                        'POP1990 < 50000')

# Export the report with the layer selection set
arcpy.ExportReportToPDF_management(in_report, out_PDF)
```

### Example 5

```python
# ExportReportToPDF.py
# Simple example of the Export Report to PDF tool

# Import system variables
import arcpy

# Set the workspace
arcpy.env.workspace = "C:/data/cities.gdb"

# Set local variables
in_layer = 'cities'
in_report = 'US Cities Report'
out_PDF = 'C/data/cities.pdf'

# Modify the report data source by selecting only the records where 
# POP1990 < 50000
arcpy.SelectLayerByAttribute_management(in_layer, 'NEW_SELECTION', 
                                        'POP1990 < 50000')

# Export the report with the layer selection set
arcpy.ExportReportToPDF_management(in_report, out_PDF)
```

### Example 6

```python
# ExportReporttoPDFAPI.py
# Simple example of exporting a report using Python

# Import system variables
import arcpy

# Identify all parcel reports in a project
aprx = arcpy.mp.ArcGISProject("C:/data/parcels/Parcels.aprx")
report = aprx.listReports("Parcels Report")[0]

# Export the report with a definition query
arcpy.ExportReportToPDF_management(report.name, "C:/data/parcels/ParcelsPDF.pdf", 
                                   ' "LotSize" > 325 ')
```

### Example 7

```python
# ExportReporttoPDFAPI.py
# Simple example of exporting a report using Python

# Import system variables
import arcpy

# Identify all parcel reports in a project
aprx = arcpy.mp.ArcGISProject("C:/data/parcels/Parcels.aprx")
report = aprx.listReports("Parcels Report")[0]

# Export the report with a definition query
arcpy.ExportReportToPDF_management(report.name, "C:/data/parcels/ParcelsPDF.pdf", 
                                   ' "LotSize" > 325 ')
```

---

## Export Tile Cache (Data Management)

## Summary

Exports tiles from an existing tile cache to a new tile cache or a tile package. The tiles can be either independently imported into other caches or accessed from ArcGIS Pro or mobile devices.

## Usage

- When exporting a tile cache as a tile package tpk file, the cache storage format is always Compact. When exporting a tile cache as a tile package tpkx file, the cache storage format is always Compact v2.
- This tool supports the Parallel Processing environment setting.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Tile Cache | An existing tile cache to be exported. | Raster Layer; Raster Dataset |
| Output Tile Cache Location | The output folder into which the tile cache or tile package will be exported. | Folder |
| Output Tile Cache Name | The name of the exported tile cache or tile package. | String |
| Export Cache As(Optional) | Specifies whether the cache will be exported as a tile cache or a tile package. Tile packages are suitable for ArcGIS Runtime and ArcGIS Mobile deployments.Tile cache—The cache will be exported as a stand-alone cache raster dataset. This is the default.Tile package (tpk)—The cache will be exported as a single compressed file (.tpk) in which the cache dataset is added as a layer and consolidated so that it can be shared easily. This type can be used in ArcMap as well as in ArcGIS Runtime and ArcGIS Mobile applications.Tile package (tpkx)—The cache will be exported using Compact_v2 storage format (.tpkx), which provides better performance on network shares and cloud storage directories. This improved and simplified package structure type is supported by newer versions of the ArcGIS platform such as ArcGIS Online, ArcGIS Pro 2.3, ArcGIS Enterprise 10.7, and ArcGIS Runtime 100.5. | String |
| Storage Format(Optional) | Determines the storage format of tiles.Compact—Group tiles into large files called bundles. This storage format is more efficient in terms of storage and mobility. Compact v2— Tiles are grouped in bundle files only. This format provides better performance on network shares and cloudstore directories. If the Export cache type parameter is set to Tile package (tpkx) then the extension of the tile package is (.tpkx), which is supported by newer versions of the ArcGIS Platform such as ArcGIS Online, ArcGIS Enterprise 11.4 and ArcGIS Runtime 100.5.This is the default.Exploded—Each tile is stored as an individual file. Note that this format cannot be used with tile packages. | String |
| Scales [Pixel Size] (Estimated Disk Space)(Optional) | A list of scale levels at which tiles will be exported. | Double |
| Area of Interest (Optional) | An area of interest that spatially constrains where tiles will be exported from the cache.The area of interest can be a feature class or a feature that you draw on the map.This parameter is useful if you want to export irregularly shaped areas, as the tool clips the cache dataset at pixel resolution. | Feature Set |
| in_cache_source | An existing tile cache to be exported. | Raster Layer; Raster Dataset |
| in_target_cache_folder | The output folder into which the tile cache or tile package will be exported. | Folder |
| in_target_cache_name | The name of the exported tile cache or tile package. | String |
| export_cache_type(Optional) | Specifies whether the cache will be exported as a tile cache or a tile package. Tile packages are suitable for ArcGIS Runtime and ArcGIS Mobile deployments. TILE_CACHE—The cache will be exported as a stand-alone cache raster dataset. This is the default.TILE_PACKAGE—The cache will be exported as a single compressed file (.tpk) in which the cache dataset is added as a layer and consolidated so that it can be shared easily. This type can be used in ArcMap as well as in ArcGIS Runtime and ArcGIS Mobile applications.TILE_PACKAGE_TPKX—The cache will be exported using Compact_v2 storage format (.tpkx), which provides better performance on network shares and cloud storage directories. This improved and simplified package structure type is supported by newer versions of the ArcGIS platform such as ArcGIS Online, ArcGIS Pro 2.3, ArcGIS Enterprise 10.7, and ArcGIS Runtime 100.5. | String |
| storage_format_type(Optional) | Determines the storage format of tiles.COMPACT—Group tiles into large files called bundles. This storage format is more efficient in terms of storage and mobility. COMPACT_V2— Tiles are grouped in bundle files only. This format provides better performance on network shares and cloudstore directories. If the Export cache type parameter is set to Tile package (tpkx) then the extension of the tile package is (.tpkx), which is supported by newer versions of the ArcGIS Platform such as ArcGIS Online, ArcGIS Enterprise 11.4 and ArcGIS Runtime 100.5.This is the default.EXPLODED—Each tile is stored as an individual file. Note that this format cannot be used with tile packages. | String |
| scales[scale,...](Optional) | A list of scale levels at which tiles will be exported. | Double |
| area_of_interest(Optional) | An area of interest that spatially constrains where tiles will be exported from the cache.The area of interest can be a feature class or a feature that you draw on the map.This parameter is useful if you want to export irregularly shaped areas, as the tool clips the cache dataset at pixel resolution. | Feature Set |

## Code Samples

### Example 1

```python
arcpy.management.ExportTileCache(in_cache_source, in_target_cache_folder, in_target_cache_name, {export_cache_type}, {storage_format_type}, {scales}, {area_of_interest})
```

### Example 2

```python
import arcpy

arcpy.ExportTileCache_management(
     "C:/Data/CacheDatasets/Source", "C:/Data/CacheDatasets", 
     "Target", "TILE_PACKAGE", "COMPACT", "4000;2000;1000", "#")
```

### Example 3

```python
import arcpy

arcpy.ExportTileCache_management(
     "C:/Data/CacheDatasets/Source", "C:/Data/CacheDatasets", 
     "Target", "TILE_PACKAGE", "COMPACT", "4000;2000;1000", "#")
```

### Example 4

```python
#Export tile cache for some levels to an EXPLODED format in 
#another location

import arcpy

    
cacheSource = "C:/Data/CacheDatasets/Source"
cacheTarget = "C:/Data/CacheDatasets"
cacheName = "Target"
cacheType = "TILE_CACHE"
storageFormat = "EXPLODED"
scales = "4000;2000;1000"
areaofinterest = "#"

arcpy.ExportTileCache_management(cacheSource, cacheTarget, cacheName,
     cacheType, storageFormat, scales, areaofinterest)
```

### Example 5

```python
#Export tile cache for some levels to an EXPLODED format in 
#another location

import arcpy

    
cacheSource = "C:/Data/CacheDatasets/Source"
cacheTarget = "C:/Data/CacheDatasets"
cacheName = "Target"
cacheType = "TILE_CACHE"
storageFormat = "EXPLODED"
scales = "4000;2000;1000"
areaofinterest = "#"

arcpy.ExportTileCache_management(cacheSource, cacheTarget, cacheName,
     cacheType, storageFormat, scales, areaofinterest)
```

---

## Export Topology Errors (Data Management)

## Summary

Exports the errors and exceptions from a geodatabase topology to the target geodatabase. All information associated with the errors and exceptions, such as the features referenced by the error or exception, is exported. Once the errors and exceptions are exported, the feature classes can be accessed using any license level of ArcGIS. The feature classes can be used with the Select Layer By Location tool and can be shared with other users who do not have access to the topology.

## Usage

- The default output location is the location of the specified topology.
- The output of the tool consists of three feature classes, one for each supported geometry type of topology error: points, lines, and polygons. The names of each feature class are created by combining a user-defined base name appended with either PointsErrors, LineErrors, or PolygonErrors.
- The default Base Name parameter value for the three output feature classes is the name of the specified topology.
- Three output feature classes are always created, even if there are no topology errors of each geometry type. Use the Get Count tool to determine if any of the feature classes are empty.
- The field collection of the output feature classes is fixed and can only be modified after exporting. Along with the standard geodatabase feature class fields (ObjectID, Shape, and optional shape length and area fields), the following fields are included and contain information regarding each topology error:Field nameTypeDescriptionOriginObjectClassNameStringOrigin Class NameOriginObjectIDIntegerOrigin Feature's Object IDDestinationObjectClassNameStringDestination Class NameDestinationObjectIDIntegerDestination Feature's Object IDRuleTypeStringDescription of the rule that was violated, obtained from the esriTopologyRuleType enumerationRuleDescriptionStringUser-friendly description of the rule that was violated. This description is the same as one provided in the Topology Error Inspector. IsExceptionIntegerIndicates if this error is an exception. A value of 1 identifies the error as an exception.
- Starting at ArcGIS Pro 2.6, the input topology layer can be from a topology service if the service is published with ArcGIS Enterprise 10.8.1 or later. If the layer has been added to the Contents pane, you can drag the layer to the Input Topology parameter, or you can provide the URL of the topology layer feature service, for example, https://myserver.mydomain.com/server/rest/services/myTopoService/FeatureServer/0.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology from which the errors will be exported. | Topology Layer |
| Output Location | The output workspace in which the feature classes will be created. The default is the workspace where the topology is located. If the input is a topology service, the default will be the default geodatabase for the project. | Feature Dataset; Workspace |
| Base Name | The name to prefix to each output feature class. This allows you to specify unique output names when running multiple exports to the same workspace. The default is the topology name. | String |
| in_topology | The topology from which the errors will be exported. | Topology Layer |
| out_path | The output workspace in which the feature classes will be created. | Feature Dataset; Workspace |
| out_basename | The name to prefix to each output feature class. This allows you to specify unique output names when running multiple exports to the same workspace. The default is the topology name. | String |

## Code Samples

### Example 1

```python
arcpy.management.ExportTopologyErrors(in_topology, out_path, out_basename)
```

### Example 2

```python
# Set the necessary product code
import arceditor

# Import arcpy module
import arcpy

# Local variables:
co_topo_FD_Topology = "C:/Testing/topology.gdb/my_topo_FD/my_topo_FD_Topology"

# Process: Export Topology Errors
arcpy.management.ExportTopologyErrors(co_topo_FD_Topology, 
                                      "C:/Testing/topology.gdb/my_topo_FD", 
                                      "my_topo_FD_Topology")
```

### Example 3

```python
# Set the necessary product code
import arceditor

# Import arcpy module
import arcpy

# Local variables:
co_topo_FD_Topology = "C:/Testing/topology.gdb/my_topo_FD/my_topo_FD_Topology"

# Process: Export Topology Errors
arcpy.management.ExportTopologyErrors(co_topo_FD_Topology, 
                                      "C:/Testing/topology.gdb/my_topo_FD", 
                                      "my_topo_FD_Topology")
```

---

## Export XML Workspace Document (Data Management)

## Summary

Creates a readable XML document of the geodatabase contents.

## Usage

- The output can be created as an .xml file or as a compressed .zip file that contains the .xml file. To create an .xml file, name the output file using the .xml extension. To create a compressed .zip file, name the output file using the .zip or .z extension.
- If the input is a geodatabase or a feature dataset, all data elements contained in that workspace will be exported. If you only want to export a subset of data elements to an file, you must copy them to a new geodatabase to be exported.
- If you export a feature class in a network, topology, relationship class, or terrain, all the feature classes participating in the network, topology, relationship class, or terrain will also be exported.
- Attribute rules and any sequences referenced in the attribute rule script expression will be included in the export.
- The tool messages will include the list of data element names that were exported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data | The input datasets that will be exported and represented in an XML workspace document. The input data can be a geodatabase, feature dataset, feature class, table, raster, or raster catalog. If there are multiple inputs, the inputs must be from the same workspace. Multiple input workspaces are not supported. | Feature Class; Feature Dataset; Raster Dataset; Table; Workspace |
| Output File | The XML workspace document file that will be created. The output can be XML (with an .xml file extension) or compressed XML (with a .zip or .z file extension). | File |
| Export Options (Optional) | Specifies whether the output XML workspace document will contain all of the data from the input (table and feature class records, including geometry) or only the schema. Data—The schema and the data will be exported. This is the default.Schema only—Only the schema will be exported. | String |
| Storage Type (Optional) | Specifies how feature geometry will be stored when data is exported from a feature class. Binary—The geometry will be stored in a compressed base64 binary format. This binary format will produce a smaller XML workspace document. Use this option when the XML workspace document will be read by a custom program that uses ArcObjects. This is the default.Normalized—The geometry will be stored in an uncompressed format. Using this option will result in a larger file. Use this option when the XML workspace document will be read by a custom program that does not use ArcObjects. | String |
| Export Metadata (Optional) | Specifies whether the metadata will be exported. Checked—If the input contains metadata, it will be exported. This is the default. Unchecked—Metadata will not be exported. | Boolean |
| in_data[in_data,...] | The input datasets that will be exported and represented in an XML workspace document. The input data can be a geodatabase, feature dataset, feature class, table, raster, or raster catalog. If there are multiple inputs, the inputs must be from the same workspace. Multiple input workspaces are not supported. | Feature Class; Feature Dataset; Raster Dataset; Table; Workspace |
| out_file | The XML workspace document file that will be created. The output can be XML (with an .xml file extension) or compressed XML (with a .zip or .z file extension). | File |
| export_type(Optional) | Specifies whether the output XML workspace document will contain all of the data from the input (table and feature class records, including geometry) or only the schema. DATA—The schema and the data will be exported. This is the default.SCHEMA_ONLY—Only the schema will be exported. | String |
| storage_type(Optional) | Specifies how feature geometry will be stored when data is exported from a feature class. BINARY—The geometry will be stored in a compressed base64 binary format. This binary format will produce a smaller XML workspace document. Use this option when the XML workspace document will be read by a custom program that uses ArcObjects. This is the default.NORMALIZED—The geometry will be stored in an uncompressed format. Using this option will result in a larger file. Use this option when the XML workspace document will be read by a custom program that does not use ArcObjects. | String |
| export_metadata(Optional) | Specifies whether the metadata will be exported.METADATA—If the input contains metadata, it will be exported. This is the default. NO_METADATA—Metadata will not be exported. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ExportXMLWorkspaceDocument(in_data, out_file, {export_type}, {storage_type}, {export_metadata})
```

### Example 2

```python
import arcpy
arcpy.management.ExportXMLWorkspaceDocument('c:/data/StJohns.gdb', 
                                            'c:/data/StJohns.xml', 
                                            'SCHEMA_ONLY', 'BINARY', 'METADATA')
```

### Example 3

```python
import arcpy
arcpy.management.ExportXMLWorkspaceDocument('c:/data/StJohns.gdb', 
                                            'c:/data/StJohns.xml', 
                                            'SCHEMA_ONLY', 'BINARY', 'METADATA')
```

### Example 4

```python
# Name: ExportXMLWorkspaceDocument.py
# Description: Export the contents of my geodatabase to an XML workspace document. 

# Import system modules
import arcpy

# Set local variables
in_data = 'c:/data/StJohns.gdb'
out_file = 'c:/data/StJohns.xml'
export_option = 'SCHEMA_ONLY'
storage_type = 'BINARY'
export_metadata = 'METADATA'

# Run ExportXMLWorkspaceDocument
arcpy.management.ExportXMLWorkspaceDocument(in_data, out_file, export_option, 
                                            storage_type, export_metadata)
```

### Example 5

```python
# Name: ExportXMLWorkspaceDocument.py
# Description: Export the contents of my geodatabase to an XML workspace document. 

# Import system modules
import arcpy

# Set local variables
in_data = 'c:/data/StJohns.gdb'
out_file = 'c:/data/StJohns.xml'
export_option = 'SCHEMA_ONLY'
storage_type = 'BINARY'
export_metadata = 'METADATA'

# Run ExportXMLWorkspaceDocument
arcpy.management.ExportXMLWorkspaceDocument(in_data, out_file, export_option, 
                                            storage_type, export_metadata)
```

---

## Extract Data From Geodatabase (Data Management)

## Summary

Extracts a subset of data from one geodatabase to another geodatabase or an .xml file.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Datasets to Extract | The data that will be extracted. | Table View; Dataset |
| Extract (Optional) | Specifies whether the schema and rows of the data or only the schema will be extracted. Data—The schema and rows will be extracted. This is the default.Schema only—Only the schema will be extracted. | String |
| Output Type (Optional) | Specifies the output type the data will be extracted to.Geodatabase—The data will be extracted to an existing geodatabase. This is the default.XML file—The data will be extracted to an XML workspace document.New file geodatabase—The data will be extracted to a new file geodatabase. Specify the location and name of the new file geodatabase using the Geodatabase Location and Geodatabase Name parameters.New mobile geodatabase—The data will be extracted to a new mobile geodatabase. Specify the location and name of the new mobile geodatabase using the Geodatabase Location and Geodatabase Name parameters. | String |
| Geodatabase to Extract Data to(Optional) | The geodatabase that will contain the extracted data when the Output Type parameter is set to Geodatabase. | Workspace |
| XML File to Extract Data to (Optional) | The name and location of the .xml file that will be created when the Output Type parameter is set to XML file. | File |
| Geodatabase Location(Optional) | The location of the file or mobile geodatabase that will be created for the extracted data. This parameter is required when the Output Type parameter is set to New file geodatabase or New mobile geodatabase. | Folder |
| Geodatabase Name(Optional) | The name of the file or mobile geodatabase that will be created for the extracted data. This parameter is required when the Output Type parameter is set to New file geodatabase or New mobile geodatabase. | String |
| Expand Feature Classes and Tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.Use defaults—The expanded feature classes and tables related to the feature classes and tables in the input datasets will be added. The default for feature classes is to extract all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to extract the schema only. This is the default.Add with schema only—Only the schema for the expanded feature classes and tables will be added.All rows—All rows for expanded feature classes and tables will be added.Do not add—No expanded feature classes or tables will be added. | String |
| Reuse Schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be extracted will be reused. Reusing the schema reduces the amount of time required to extract the data.Do not reuse—The schema will not be reused. This is the default.Reuse—The schema will be reused. | String |
| Extract Related Data(Optional) | Specifies whether rows related to rows existing in the data will be extracted. For example, a feature (f1) is inside the geometry filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the extracted data if you choose to get related data.Do not get related—Related data will not be extracted.Get related—Related data will be extracted. This is the default. | String |
| Extract Using Geometry Features(Optional) | The features that will be used to define the area to extract. | Feature Layer |
| Geometry Filter Type (Optional) | Specifies the spatial relationship between the Extract Using Geometry Features and Input Datasets parameter values and how that relationship will be filtered. The spatial relationship is applied to data in an extent defined by the area of interest (AOI) specified in the Extract Using Geometry Features parameter. Intersects—Features in the Input Datasets parameter value that intersect features in the Extract Using Geometry Features parameter value will be extracted.Contains—Features in the Input Datasets parameter value that are contained by the selected feature in the Extract Using Geometry Features parameter value will be extracted. | String |
| All records for tables (Optional) | Specifies whether all records or only the schema will be extracted for tables that do not have filters applied (such as selections or definition queries).Tables with applied filters will be honored. Checked—All records will be extracted to the geodatabase. This option will override the Expand Feature Classes and Tables parameter value.Unchecked—Only the schema will be extracted to the geodatabase for tables. This is the default. | Boolean |
| in_data[in_data,...] | The data that will be extracted. | Table View; Dataset |
| extract_type(Optional) | Specifies whether the schema and rows of the data or only the schema will be extracted. DATA—The schema and rows will be extracted. This is the default.SCHEMA_ONLY—Only the schema will be extracted. | String |
| out_type(Optional) | Specifies the output type the data will be extracted to. GEODATABASE—The data will be extracted to an existing geodatabase. This is the default.XML_FILE—The data will be extracted to an XML workspace document.NEW_FILE_GEODATABASE—The data will be extracted to a new file geodatabase. Specify the location and name of the new file geodatabase using the out_folder_path and out_name parameters.NEW_MOBILE_GEODATABASE—The data will be extracted to a new mobile geodatabase. Specify the location and name of the new mobile geodatabase using the out_folder_path and out_name parameters. | String |
| out_geodatabase(Optional) | The geodatabase that will contain the extracted data when the out_type parameter is set to GEODATABASE. | Workspace |
| out_xml(Optional) | The name and location of the .xml file that will be created when the out_type parameter is set to XML_FILE. | File |
| out_folder_path(Optional) | The location of the file or mobile geodatabase that will be created for the extracted data. This parameter is required when the out_type parameter is set to NEW_FILE_GEODATABASE or NEW_MOBILE_GEODATABASE. | Folder |
| out_name(Optional) | The name of the file or mobile geodatabase that will be created for the extracted data. This parameter is required when the out_type parameter is set to NEW_FILE_GEODATABASE or NEW_MOBILE_GEODATABASE. | String |
| expand_feature_classes_and_tables(Optional) | Specifies whether expanded feature classes and tables—such as those in networks, topologies, or relationship classes—will be added.USE_DEFAULTS—The expanded feature classes and tables related to the feature classes and tables in the input datasets will be added. The default for feature classes is to extract all features intersecting the spatial filter. If no spatial filter has been provided, all features will be included. The default for tables is to extract the schema only. This is the default.ADD_WITH_SCHEMA_ONLY—Only the schema for the expanded feature classes and tables will be added.ALL_ROWS—All rows for expanded feature classes and tables will be added.DO_NOT_ADD—No expanded feature classes or tables will be added. | String |
| reuse_schema(Optional) | Specifies whether a geodatabase that contains the schema of the data to be extracted will be reused. Reusing the schema reduces the amount of time required to extract the data. DO_NOT_REUSE—The schema will not be reused. This is the default.REUSE—The schema will be reused. | String |
| get_related_data(Optional) | Specifies whether rows related to rows existing in the data will be extracted. For example, a feature (f1) is inside the geometry filter and a related feature (f2) from another class is outside the filter. Feature f2 will be included in the extracted data if you choose to get related data. DO_NOT_GET_RELATED—Related data will not be extracted.GET_RELATED—Related data will be extracted. This is the default. | String |
| extract_using_geometry_features(Optional) | The features that will be used to define the area to extract. | Feature Layer |
| geometry_filter_type(Optional) | Specifies the spatial relationship between the extract_using_geometry_features and in_data parameter values and how that relationship will be filtered. The spatial relationship is applied to data in an extent defined by the area of interest (AOI) specified in the extract_using_geometry_features parameter. INTERSECTS—Features in the in_data parameter value that intersect features in the extract_using_geometry_features parameter value will be extracted.CONTAINS—Features in the in_data parameter value that are contained by the selected feature in the extract_using_geometry_features parameter value will be extracted. | String |
| all_records_for_tables(Optional) | Specifies whether all records or only the schema will be extracted for tables that do not have filters applied (such as selections or definition queries). Tables with applied filters will be honored. ALL_RECORDS_FOR_TABLES—All records will be extracted to the geodatabase. This option will override the expand_feature_classes_and_tables parameter value.SCHEMA_ONLY_FOR_TABLES—Only the schema will be extracted to the geodatabase. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ExtractDataFromGeodatabase(in_data, {extract_type}, {out_type}, {out_geodatabase}, {out_xml}, {out_folder_path}, {out_name}, {expand_feature_classes_and_tables}, {reuse_schema}, {get_related_data}, {extract_using_geometry_features}, {geometry_filter_type}, {all_records_for_tables})
```

### Example 2

```python
import arcpy
arcpy.management.ExtractDataFromGeodatabase(
    r"C:\MyProject\MyGDB.gdb\Bridges", "DATA", "NEW_FILE_GEODATABASE", "", "",
   r"C:\MyProject", "MyNewGDB")
```

### Example 3

```python
import arcpy
arcpy.management.ExtractDataFromGeodatabase(
    r"C:\MyProject\MyGDB.gdb\Bridges", "DATA", "NEW_FILE_GEODATABASE", "", "",
   r"C:\MyProject", "MyNewGDB")
```

### Example 4

```python
import arcpy
arcpy.management.ExtractDataFromGeodatabase(
    ["Layer1", "Layer2"], "SCHEMA_ONLY", "XML_FILE", "",
    r"C:\MyProject\MyXMLWkspDoc.xml")
```

### Example 5

```python
import arcpy
arcpy.management.ExtractDataFromGeodatabase(
    ["Layer1", "Layer2"], "SCHEMA_ONLY", "XML_FILE", "",
    r"C:\MyProject\MyXMLWkspDoc.xml")
```

### Example 6

```python
# Name: ExtractDataFromGeodatabase_Example3.py
# Description: Extract a subset of data by providing an extent using the Extent
#              environment setting.

# Import system modules
import arcpy

# Set the current workspace (to avoid having to specify the full path to each 
# input dataset)
arcpy.env.workspace = "C:\MyProject\MyData.gdb"

# Set the extent environment using the Extent class
arcpy.env.extent = arcpy.Extent(-107.0, 38.0, -104.0, 40.0)

# Set local variables
in_data = ["FC1", "FC2", "FC3"]
extract_type = "DATA"
out_type = "GEODATABASE"
out_geodatabase = "C:\MyProject\MyGDB.gdb"
out_folder_path = ""
out_name = ""
expand_fcs = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related_data = "GET_RELATED"
extract_using_geom = ""
geom_filter = ""
all_records = "ALL_RECORDS_FOR_TABLES"

# Run ExtractDataFromGeodatabase
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)
```

### Example 7

```python
# Name: ExtractDataFromGeodatabase_Example3.py
# Description: Extract a subset of data by providing an extent using the Extent
#              environment setting.

# Import system modules
import arcpy

# Set the current workspace (to avoid having to specify the full path to each 
# input dataset)
arcpy.env.workspace = "C:\MyProject\MyData.gdb"

# Set the extent environment using the Extent class
arcpy.env.extent = arcpy.Extent(-107.0, 38.0, -104.0, 40.0)

# Set local variables
in_data = ["FC1", "FC2", "FC3"]
extract_type = "DATA"
out_type = "GEODATABASE"
out_geodatabase = "C:\MyProject\MyGDB.gdb"
out_folder_path = ""
out_name = ""
expand_fcs = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related_data = "GET_RELATED"
extract_using_geom = ""
geom_filter = ""
all_records = "ALL_RECORDS_FOR_TABLES"

# Run ExtractDataFromGeodatabase
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)
```

### Example 8

```python
# Name: ExtractDataFromGeodatabase_Example4.py
# Description: Extract data to use a new coordinate system by using the
#              Output Coordinate System environment setting. Then run the tool a second
#              time to load the data using the reuse_schema option

# Import system modules
import arcpy

# Set the Output Coordinate System environment
arcpy.env.outputCoordinateSystem = arcpy.SpatialReference("GCS_North_American_1983_HARN")
arcpy.env.geographicTransformations = "NAD_1983_HARN_To_WGS_1984_2"

# Set local variables
in_data = "C:/MyProject/MyGDB.gdb/FC1"
extract_type = "SCHEMA_ONLY"
out_type = "NEW_FILE_GEODATABASE"
out_geodatabase = ""
out_xml = ""
out_folder_path = "C:\MyProject"
out_name = "MyOutputGDB.gdb"
expand_fcs = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related_data = "GET_RELATED"
extract_using_geom = ""
geom_filter = ""
all_records = "SCHEMA_ONLY_FOR_TABLES"

# Run ExtractDataFromGeodatabase with the schema only option
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)

# Rest the Output Coordinate System environment
arcpy.ClearEnvironment("outputCoordinateSystem")
arcpy.ClearEnvironment("geographicTransformations")

# Set variables that we are changing to load the data into the newly created schema in 
# the different coordinate system.
extract_type = "DATA"
out_type = "GEODATABASE"
out_geodatabase = "C:\MyProject\MyOutputGDB.gdb"
out_folder_path = ""
out_name = ""
reuse_schema = "REUSE"
all_records = "ALL_RECORDS_FOR_TABLES"

# Run ExtractDataFromGeodatabase with the data and reuse schema option
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)
```

### Example 9

```python
# Name: ExtractDataFromGeodatabase_Example4.py
# Description: Extract data to use a new coordinate system by using the
#              Output Coordinate System environment setting. Then run the tool a second
#              time to load the data using the reuse_schema option

# Import system modules
import arcpy

# Set the Output Coordinate System environment
arcpy.env.outputCoordinateSystem = arcpy.SpatialReference("GCS_North_American_1983_HARN")
arcpy.env.geographicTransformations = "NAD_1983_HARN_To_WGS_1984_2"

# Set local variables
in_data = "C:/MyProject/MyGDB.gdb/FC1"
extract_type = "SCHEMA_ONLY"
out_type = "NEW_FILE_GEODATABASE"
out_geodatabase = ""
out_xml = ""
out_folder_path = "C:\MyProject"
out_name = "MyOutputGDB.gdb"
expand_fcs = "USE_DEFAULTS"
reuse_schema = "DO_NOT_REUSE"
get_related_data = "GET_RELATED"
extract_using_geom = ""
geom_filter = ""
all_records = "SCHEMA_ONLY_FOR_TABLES"

# Run ExtractDataFromGeodatabase with the schema only option
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)

# Rest the Output Coordinate System environment
arcpy.ClearEnvironment("outputCoordinateSystem")
arcpy.ClearEnvironment("geographicTransformations")

# Set variables that we are changing to load the data into the newly created schema in 
# the different coordinate system.
extract_type = "DATA"
out_type = "GEODATABASE"
out_geodatabase = "C:\MyProject\MyOutputGDB.gdb"
out_folder_path = ""
out_name = ""
reuse_schema = "REUSE"
all_records = "ALL_RECORDS_FOR_TABLES"

# Run ExtractDataFromGeodatabase with the data and reuse schema option
arcpy.management.ExtractDataFromGeodatabase(in_data,
                                            extract_type,
                                            out_type,
                                            out_geodatabase,
                                            out_xml,
                                            out_folder_path,
                                            out_name,
                                            expand_fcs,
                                            reuse_schema,
                                            get_related_data,
                                            extract_using_geom,
                                            geom_filter,
                                            all_records)
```

### Example 10

```python
# Name: ExtractDataFromGeodatabase_Example5.py
# Description: Extract schema of multiple feature classes and import the
#              schema into an enterprise geodatabase

# Import system modules
import arcpy

# Set the current workspace (to avoid having to specify the full path to each input dataset)
arcpy.env.workspace = "C:\MyProject\MyData.gdb"

# Export the schema as an XML file from a feature class
result = arcpy.management.ExtractDataFromGeodatabase(
    ["FC1", "FC2"], "SCHEMA_ONLY", "XML_FILE", "",
    "C:/MyProject/MyXMLWkspDoc.xml")

# Get the path to the output XML file
out_xml = result[0]

# Pass the output XML into the Import XML Workspace Document tool
arcpy.management.ImportXMLWorkspaceDocument("C:/MyProject/mySDEConnection.sde", 
                                            out_xml, 
                                            "SCHEMA_ONLY", "DEFAULTS")
```

### Example 11

```python
# Name: ExtractDataFromGeodatabase_Example5.py
# Description: Extract schema of multiple feature classes and import the
#              schema into an enterprise geodatabase

# Import system modules
import arcpy

# Set the current workspace (to avoid having to specify the full path to each input dataset)
arcpy.env.workspace = "C:\MyProject\MyData.gdb"

# Export the schema as an XML file from a feature class
result = arcpy.management.ExtractDataFromGeodatabase(
    ["FC1", "FC2"], "SCHEMA_ONLY", "XML_FILE", "",
    "C:/MyProject/MyXMLWkspDoc.xml")

# Get the path to the output XML file
out_xml = result[0]

# Pass the output XML into the Import XML Workspace Document tool
arcpy.management.ImportXMLWorkspaceDocument("C:/MyProject/mySDEConnection.sde", 
                                            out_xml, 
                                            "SCHEMA_ONLY", "DEFAULTS")
```

### Example 12

```python
# Name: ExtractDataFromGeodatabase_Example6.py
# Description: Extract data and create a domain on output workspace

# Import system modules
import arcpy

# Export data to a new mobile geodatabase
result = arcpy.management.ExtractDataFromGeodatabase(
    "C:/MyProject/mySDEConnection.sde/myGDB.USER1.FC1", "DATA", "NEW_MOBILE_GEODATABASE", "", "",
    "C:/MyProject", "newMobileGDB")

# Get the path to the output mobile geodatabase
out_mobile_gdb = result[1]

# Pass the output geodatbase to the CreateDomain tool
arcpy.management.CreateDomain(out_mobile_gdb, "MyDomain", "My domain description", "SHORT", "CODED")
```

### Example 13

```python
# Name: ExtractDataFromGeodatabase_Example6.py
# Description: Extract data and create a domain on output workspace

# Import system modules
import arcpy

# Export data to a new mobile geodatabase
result = arcpy.management.ExtractDataFromGeodatabase(
    "C:/MyProject/mySDEConnection.sde/myGDB.USER1.FC1", "DATA", "NEW_MOBILE_GEODATABASE", "", "",
    "C:/MyProject", "newMobileGDB")

# Get the path to the output mobile geodatabase
out_mobile_gdb = result[1]

# Pass the output geodatbase to the CreateDomain tool
arcpy.management.CreateDomain(out_mobile_gdb, "MyDomain", "My domain description", "SHORT", "CODED")
```

---

## Extract Package (Data Management)

## Summary

Extracts the contents of a package to a specified folder. The output folder will be updated with the extracted contents of the input package.

## Usage

- The following are supported package types: Geoprocessing Packages (.gpk and .gpkx)Layer Packages (.lpk and .lpkx)Locator Packages (.gcpk)Map Packages (.mpk and .mpkx)Mobile Map Packages (.mmpk)Project Packages and Project Templates (.ppkx and .aptx)Scene Layer Packages (.slpk)Tile Packages (.tpk and .tpkx)Vector Tile Packages (.vtpk)
- Geoprocessing Packages (.gpk and .gpkx)
- Layer Packages (.lpk and .lpkx)
- Locator Packages (.gcpk)
- Map Packages (.mpk and .mpkx)
- Mobile Map Packages (.mmpk)
- Project Packages and Project Templates (.ppkx and .aptx)
- Scene Layer Packages (.slpk)
- Tile Packages (.tpk and .tpkx)
- Vector Tile Packages (.vtpk)
- The output folder can be a new folder or an existing folder. When extracting to an existing folder, the contents of the package will be appended to existing files and folders. If the output folder contains the extracted contents of the package, the existing contents will be overwritten.
- Packages with attachments will have their attached files unpacked to the commondata\userdata\ subfolder in the output folder. Typically, the files in a package are supporting files, such as .pdf or .docx, or an image. Browse to the extracted directory in Microsoft File Explorer to open these files.
- When extracting vector tile packages (.vtpk), the contents of the package will be extracted to the output folder. The cache storage format can be converted from compact (.bundle files) to exploded (.pbf files) using the Storage Format Type parameter. You can use the extracted .pbf files in other client applications, such as Mapbox.Note:Extracting a flat cache may be slow and result in the extraction of billions of tiles depending on the data extent, levels of detail of the package, and hardware configuration.
- When extracting vector, tile, or scene layer packages (.vtpk, .tpk, .tpkx, or .slpk), the Cache Package parameter is inactive.
- When extracting vector, tile, or version 1.7 and later scene layer packages, you can extract the contents of the package to a folder in the file system or to an object store located in the cloud such as Amazon S3, Azure Blob storage, or Alibaba OSS. This content is ready to serve as a tile or scene layer and the location you choose must be registered as a user-managed data store in ArcGIS Enterprise.
- You can build a connection file (.acs) using the Create Cloud Storage Connection File tool.
- Older versions of scene layer packages can be upgraded to the latest version using the Upgrade Scene Layer tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Package | The input package that will be extracted. | File |
| Output Folder(Optional) | The output folder that will contain the contents of the package. If the specified folder does not exist, a folder will be created. | Folder |
| Cache Package (Optional) | Specifies whether a copy of the package will be cached to your profile. When extracting a package, the output is first extracted to your user profile and appended with a unique ID before a copy is made to the directory specified in the Output Folder parameter. Downloading and extracting subsequent versions of the same package will only update this location. You do not need to manually create a cached version of the package in your user profile when using this parameter. This parameter is not active if the input package is a vector tile package (.vtpk) or a tile package (.tpk and .tpkx).Checked—A copy of the package will be extracted and cached to your profile. This is the default. Unchecked—A copy of the package will only be extracted to the output parameter specified; it will not be cached. | Boolean |
| Storage Format Type (Optional) | Specifies the storage format that will be used for the extracted cache. This parameter is applicable only when the input package is a vector tile package (.vtpk).Compact— The tiles will be grouped in bundle files using the Compact V2 storage format. This format provides better performance on network shares and cloud store directories. This is the default.Exploded— Each tile will be stored as an individual file. | String |
| Create Ready To Serve Cache Dataset (Optional) | Specifies whether a ready-to-serve format for ArcGIS Enterprise will be created. This parameter is active only when the input package is a vector tile package (.vtpk) or a tile package (.tpkx).Checked—A folder structure with an extracted cache that can be used to create a tile layer in ArcGIS Enterprise will be created. The file extension of the folder signifies the content it stores: .tiles (cache dataset) for tile layer packages or .vtiles (vector cache dataset) for vector tile packages.Unchecked—A folder structure with extracted contents of the package will be created. This is the default. | Boolean |
| Target Cloud Connection (Optional) | The target .acs file to which the package contents will be extracted. This parameter is enabled only when the input package is a scene layer package (.slpk), a vector tile package (.vtpk), or a tile package (.tpkx). | Folder |
| in_package | The input package that will be extracted. | File |
| output_folder(Optional) | The output folder that will contain the contents of the package. If the specified folder does not exist, a folder will be created. | Folder |
| cache_package(Optional) | Specifies whether a copy of the package will be cached to your profile. When extracting a package, the output is first extracted to your user profile and appended with a unique ID before a copy is made to the directory specified in the output_folder parameter. Downloading and extracting subsequent versions of the same package will only update this location. You do not need to manually create a cached version of the package in your user profile when using this parameter. This parameter is not enabled if the input package is a vector tile package (.vtpk) or a tile package (.tpk and .tpkx).CACHE— A copy of the package will be extracted and cached to your profile. This is the default. NO_CACHE—A copy of the package will only be extracted to the output parameter specified; it will not be cached. | Boolean |
| storage_format_type(Optional) | Specifies the storage format that will be used for the extracted cache. This parameter is applicable only when the input package is a vector tile package (.vtpk).COMPACT— The tiles will be grouped in bundle files using the Compact V2 storage format. This format provides better performance on network shares and cloud store directories. This is the default.EXPLODED— Each tile will be stored as an individual file. | String |
| create_ready_to_serve_format(Optional) | Specifies whether a ready-to-serve format for ArcGIS Enterprise will be created. This parameter is enabled only when the input package is a vector tile package (.vtpk) or a tile package (.tpkx).READY_TO_SERVE_CACHE_DATASET—A folder structure with an extracted cache that can be used to create a tile layer in ArcGIS Enterprise will be created. The file extension of the folder signifies the content it stores: .tiles (cache dataset) for tile layer packages or .vtiles (vector cache dataset) for vector tile packages.EXTRACTED_PACKAGE—A folder structure with extracted contents of the package will be created. This is the default. | Boolean |
| target_cloud_connection(Optional) | The target .acs file to which the package contents will be extracted. This parameter is enabled only when the input package is a scene layer package (.slpk), a vector tile package (.vtpk), or a tile package (.tpkx). | Folder |

## Code Samples

### Example 1

```python
arcpy.management.ExtractPackage(in_package, {output_folder}, {cache_package}, {storage_format_type}, {create_ready_to_serve_format}, {target_cloud_connection})
```

### Example 2

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\MyVectorPackage.vtpk", 
                                r"C:\Data\packages\Extracted", "CACHE", 
                                "COMPACT", "READY_TO_SERVE_CACHE_DATASET")
```

### Example 3

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\MyVectorPackage.vtpk", 
                                r"C:\Data\packages\Extracted", "CACHE", 
                                "COMPACT", "READY_TO_SERVE_CACHE_DATASET")
```

### Example 4

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\ChicagoBuildings.slpk", 
                                None, "CACHE", 
                                "COMPACT", "EXTRACTED_PACKAGE", 
                                r"C:\CloudConnections\AWS.acs")
```

### Example 5

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\ChicagoBuildings.slpk", 
                                None, "CACHE", 
                                "COMPACT", "EXTRACTED_PACKAGE", 
                                r"C:\CloudConnections\AWS.acs")
```

### Example 6

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\London.vtpk", 
                                None, "CACHE", 
                                "COMPACT", "EXTRACTED_PACKAGE", 
                                r"C:\CloudConnections\AWS.acs")
```

### Example 7

```python
import arcpy
arcpy.management.ExtractPackage(r"C:\Data\packages\London.vtpk", 
                                None, "CACHE", 
                                "COMPACT", "EXTRACTED_PACKAGE", 
                                r"C:\CloudConnections\AWS.acs")
```

### Example 8

```python
# Name: ExtractPackage.py
# Description: Find geoprocessing packages in a specified folder and extract 
#              contents.

import arcpy
import os

arcpy.env.overwriteOutput = True

# set folder that contains packages to extract
arcpy.env.workspace = "C:/geoprocessing/gpks" 
wrksp = arcpy.env.workspace

for gpk in arcpy.ListFiles("*.gpk"):
    print("Extracting... " + gpk)
    arcpy.management.ExtractPackage(gpk, os.path.splitext(gpk)[0])

print("done")
```

### Example 9

```python
# Name: ExtractPackage.py
# Description: Find geoprocessing packages in a specified folder and extract 
#              contents.

import arcpy
import os

arcpy.env.overwriteOutput = True

# set folder that contains packages to extract
arcpy.env.workspace = "C:/geoprocessing/gpks" 
wrksp = arcpy.env.workspace

for gpk in arcpy.ListFiles("*.gpk"):
    print("Extracting... " + gpk)
    arcpy.management.ExtractPackage(gpk, os.path.splitext(gpk)[0])

print("done")
```

---

## Extract Subdataset (Data Management)

## Summary

Creates a new raster dataset from a selection of an HDF or NITF dataset.

## Usage

- Subdataset file formats can be either Hierarchical Data Format (HDF) or National Imagery Transmission Format (NITF) files
- The data structure allows the file format to consist of multiple datasets in one parent file. In addition, each of the subdatasets can consist of one band or multiple bands.
- If you do not choose any subdatasets, the default will be to only return the first subdataset.
- When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a Compression type and Compression Quality within the Environment Settings.
- The GIF format does not support multiband; therefore, it is not a valid output format unless your raster dataset is single band.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The HDF or NITF dataset that has the layers you want to extract. | Raster Layer |
| Output Raster | The name, location, and format for the dataset you are creating.When storing the raster dataset in a file format, you need to specify the file extension:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE file.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFFno extension—Esri GRID When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a Compression type and Compression Quality within the Environment Settings. | Raster Dataset |
| Subdataset ID(Optional) | The subdatasets that you want to extract. | Value Table |
| in_raster | The HDF or NITF dataset that has the layers you want to extract. | Raster Layer |
| out_raster | The name, location, and format for the dataset you are creating.When storing the raster dataset in a file format, you need to specify the file extension:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE file.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFFno extension—Esri GRID When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a Compression type and Compression Quality within the Environment Settings. | Raster Dataset |
| subdataset_index[subdataset_index,...](Optional) | The subdatasets that you want to extract. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.ExtractSubDataset(in_raster, out_raster, {subdataset_index})
```

### Example 2

```python
import arcpy
arcpy.ExtractSubDataset_management("c:/data/MyNITF.ntf","extracted.tif", "2")
```

### Example 3

```python
import arcpy
arcpy.ExtractSubDataset_management("c:/data/MyNITF.ntf","extracted.tif", "2")
```

### Example 4

```python
##====================================
##Extract Subdataset
##Usage: ExtractSubdataset_management in_raster out_raster {ID;ID...}

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Extract 3-band subdataset from HDF
arcpy.ExtractSubDataset_management("MHDF.hdf", "subds.tif", "5;6;7")

##Extract 1-band subdataset from NITF
arcpy.ExtractSubDataset_management("MNITF.ntf","subds_ntf.tif", "2")
```

### Example 5

```python
##====================================
##Extract Subdataset
##Usage: ExtractSubdataset_management in_raster out_raster {ID;ID...}

import arcpy
arcpy.env.workspace = r"C:/Workspace"

##Extract 3-band subdataset from HDF
arcpy.ExtractSubDataset_management("MHDF.hdf", "subds.tif", "5;6;7")

##Extract 1-band subdataset from NITF
arcpy.ExtractSubDataset_management("MNITF.ntf","subds_ntf.tif", "2")
```

---

## Feature Compare (Data Management)

## Summary

Compares two feature classes or layers and returns the comparison results.

## Usage

- This tool returns messages showing the comparison result. By default, it will stop executing after encountering the first miscompare. To report all differences, check on the Continue Comparison parameter.
- Feature Compare can report differences with geometry, tabular values, spatial reference, and field definitions.
- Multiple sort fields may be specified. The first field is sorted, then the second field, and so on, in ascending order. Sorting by a common field in both the Input Base Features and the Input Test Features ensures that you are comparing the same row from each input dataset.
- By default, the compare type is set to All (ALL in Python). This means all properties of the features being compared will be checked, including such things as spatial reference, field properties, attributes, and geometry. However, you may choose a different compare type to check only specific properties of the features being compared.
- The Ignore Options provide the flexibility to omit properties such as measure attributes, z attributes, point ID attributes, and extension properties. Two feature classes may be identical, yet one has measures and z coordinates and the other does not. You can choose to ignore these properties. The Ignore extension properties (IGNORE_EXTENSION_PROPERTIES in Python) option refers to additional information added to a feature class or table. For example, the features of two annotation feature classes can be identical but the feature classes may have different extension properties, such as different symbols in the symbol collection and different editing behavior.
- The default XY Tolerance is determined by the default XY Tolerance of the Input Base Features. To minimize error, the value you choose for the compare tolerance should be as small as possible. If zero is entered for the XY Tolerance, an exact match is performed.
- The default M Tolerance and the default Z Tolerance is determined by the default M Tolerance and Z Tolerance of the Input Base Features. The units are the same as those of the Input Base Features. If zero is entered for the M Tolerance and Z Tolerance, an exact match is performed.
- When comparing Geometry only (GEOMETRY_ONLY in Python), the spatial references must match. If the spatial references are different, a miscompare will be reported. If the coordinate system is different for either input, the features will miscompare. This tool does not do projection on the fly.
- The Omit Fields parameter is a list of fields that are not included in the field count comparison—their field definitions and tabular values are ignored.
- Attribute tolerances can only be specified for numeric field types.
- The Output Compare File will contain all similarities and differences between the Input Base Features and the Input Test Features. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different. The has_error field indicates that the record contains an error. True indicates there is a difference.
- One of the first comparisons performed is a feature count. If the feature count is reported as being different and the Continue Compare parameter is True, the subsequent comparison messages may not accurately reflect additional differences between the Input Base Features and Input Test Features. This is due to the Feature Compare tool's inability to figure out where features have been added or removed in the Input Test Features and simply moves to the next row in each attribute table. At the location in the attribute table where a feature has been added or deleted, the tool will simply move to the next row and begin comparing the base feature with the wrong test feature because the correct one in the Input Test Data was deleted or a feature was added before it.
- When using this tool in Python, you can get the status of this tool using result.getOutput(1). The value will be 'true' when no differences are found and 'false' when differences are detected. Learn more about using tools in Python

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Base Features | The Input Base Features are compared with the Input Test Features. Input Base Features refers to data that you have declared valid. This base data has the correct geometry definitions, field definitions, and spatial reference. | Feature Layer |
| Input Test Features | The Input Test Features are compared against the Input Base Features. Input Test Features refers to data that you have made changes to by editing or compiling new features. | Feature Layer |
| Sort Field | The field or fields used to sort records in the Input Base Features and the Input Test Features. The records are sorted in ascending order. Sorting by a common field in both the Input Base Features and the Input Test Features ensures that you are comparing the same row from each input dataset. | Value Table |
| Compare Type(Optional) | The comparison type. The default is All, which will compare all properties of the features being compared.All—All properties of the feature classes will be compared. This is the default.Geometry only—Only the geometries of the feature classes will be compared.Attributes only—Only the attributes and their values will be compared.Schema only—Only the schema of the feature classes will be compared.Spatial Reference only—Only the spatial references of the two feature classes will be compared. | String |
| Ignore Options(Optional) | These properties will not be compared.Ignore Ms—Do not compare measure properties.Ignore Zs—Do not compare elevation properties.Ignore PointIDs—Do not compare point ID properties.Ignore extension properties—Do not compare extension properties.Ignore subtypes—Do not compare subtypes.Ignore relationship classes—Do not compare relationship classes.Ignore representation classes—Do not compare representation classes.Ignore field alias—Do not compare field aliases. | String |
| XY Tolerance(Optional) | The distance that determines the range in which features are considered equal. To minimize error, the value you choose for the compare tolerance should be as small as possible. By default, the compare tolerance is the XY tolerance of the input base features. | Linear Unit |
| M Tolerance(Optional) | The measure tolerance is the minimum distance between measures before they are considered equal. | Double |
| Z Tolerance(Optional) | The Z Tolerance is the minimum distance between z coordinates before they are considered equal. | Double |
| Attribute Tolerance(Optional) | The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. | Value Table |
| Omit Fields(Optional) | The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. | String |
| Continue Comparison(Optional) | Indicates whether to compare all properties after encountering the first mismatch.Unchecked—Stops after encountering the first mismatch. This is the default.Checked—Compares other properties after encountering the first mismatch. | Boolean |
| Output Compare File(Optional) | This file will contain all similarities and differences between the Input Base Features and the Input Test Features. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. This file will contain all similarities and differences between the in_base_features and the in_test_features. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. | File |
| in_base_features | The Input Base Features are compared with the Input Test Features. Input Base Features refers to data that you have declared valid. This base data has the correct geometry definitions, field definitions, and spatial reference. | Feature Layer |
| in_test_features | The Input Test Features are compared against the Input Base Features. Input Test Features refers to data that you have made changes to by editing or compiling new features. | Feature Layer |
| sort_field[sort_field,...] | The field or fields used to sort records in the Input Base Features and the Input Test Features. The records are sorted in ascending order. Sorting by a common field in both the Input Base Features and the Input Test Features ensures that you are comparing the same row from each input dataset. | Value Table |
| compare_type(Optional) | The comparison type. The default is All, which will compare all properties of the features being compared.ALL—All properties of the feature classes will be compared. This is the default.GEOMETRY_ONLY—Only the geometries of the feature classes will be compared.ATTRIBUTES_ONLY—Only the attributes and their values will be compared.SCHEMA_ONLY—Only the schema of the feature classes will be compared.SPATIAL_REFERENCE_ONLY—Only the spatial references of the two feature classes will be compared. | String |
| ignore_options[ignore_option,...](Optional) | These properties will not be compared.IGNORE_M—Do not compare measure properties.IGNORE_Z—Do not compare elevation properties.IGNORE_POINTID—Do not compare point ID properties.IGNORE_EXTENSION_PROPERTIES—Do not compare extension properties.IGNORE_SUBTYPES—Do not compare subtypes.IGNORE_RELATIONSHIPCLASSES—Do not compare relationship classes.IGNORE_REPRESENTATIONCLASSES—Do not compare representation classes.IGNORE_FIELDALIAS—Do not compare field aliases. | String |
| xy_tolerance(Optional) | The distance that determines the range in which features are considered equal. To minimize error, the value you choose for the compare tolerance should be as small as possible. By default, the compare tolerance is the XY tolerance of the input base features. | Linear Unit |
| m_tolerance(Optional) | The measure tolerance is the minimum distance between measures before they are considered equal. | Double |
| z_tolerance(Optional) | The Z Tolerance is the minimum distance between z coordinates before they are considered equal. | Double |
| attribute_tolerances[[Field, {Tolerance}],...](Optional) | The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. | Value Table |
| omit_field[omit_field,...](Optional) | The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. | String |
| continue_compare(Optional) | Indicates whether to compare all properties after encountering the first mismatch.NO_CONTINUE_COMPARE—Stops after encountering the first mismatch. This is the default. CONTINUE_COMPARE—Compares other properties after encountering the first mismatch. | Boolean |
| out_compare_file(Optional) | This file will contain all similarities and differences between the in_base_features and the in_test_features. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. | File |

## Code Samples

### Example 1

```python
arcpy.management.FeatureCompare(in_base_features, in_test_features, sort_field, {compare_type}, {ignore_options}, {xy_tolerance}, {m_tolerance}, {z_tolerance}, {attribute_tolerances}, {omit_field}, {continue_compare}, {out_compare_file})
```

### Example 2

```python
import arcpy
arcpy.FeatureCompare_management(
    r'C:/Workspace/baseroads.shp', r'C:/Workspace/newroads.shp', 'ROAD_ID', 
    'ALL', 'IGNORE_M;IGNORE_Z', '0.001 METERS', 0, 0, 'Shape_Length 0.001', '#', 
    'CONTINUE_COMPARE', r'C:/Workspace/roadcompare.txt')
```

### Example 3

```python
import arcpy
arcpy.FeatureCompare_management(
    r'C:/Workspace/baseroads.shp', r'C:/Workspace/newroads.shp', 'ROAD_ID', 
    'ALL', 'IGNORE_M;IGNORE_Z', '0.001 METERS', 0, 0, 'Shape_Length 0.001', '#', 
    'CONTINUE_COMPARE', r'C:/Workspace/roadcompare.txt')
```

### Example 4

```python
# Name: FeatureCompare.py
# Description: Compare two feature classes and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_features = "C:/Workspace/baseroads.shp"
test_features = "C:/Workspace/newroads.shp"
sort_field = "ROAD_ID"
compare_type = "ALL"
ignore_option = "IGNORE_M;IGNORE_Z"
xy_tolerance = "0.001 METERS"
m_tolerance = 0
z_tolerance = 0
attribute_tolerance = "Shape_Length 0.001"
omit_field = "#"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/roadcompare.txt"
 
# Process: FeatureCompare
compare_result = arcpy.FeatureCompare_management(
    base_features, test_features, sort_field, compare_type, ignore_option, 
    xy_tolerance, m_tolerance, z_tolerance, attribute_tolerance, omit_field, 
    continue_compare, compare_file)
print(compare_result[1])
print(arcpy.GetMessages())
```

### Example 5

```python
# Name: FeatureCompare.py
# Description: Compare two feature classes and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_features = "C:/Workspace/baseroads.shp"
test_features = "C:/Workspace/newroads.shp"
sort_field = "ROAD_ID"
compare_type = "ALL"
ignore_option = "IGNORE_M;IGNORE_Z"
xy_tolerance = "0.001 METERS"
m_tolerance = 0
z_tolerance = 0
attribute_tolerance = "Shape_Length 0.001"
omit_field = "#"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/roadcompare.txt"
 
# Process: FeatureCompare
compare_result = arcpy.FeatureCompare_management(
    base_features, test_features, sort_field, compare_type, ignore_option, 
    xy_tolerance, m_tolerance, z_tolerance, attribute_tolerance, omit_field, 
    continue_compare, compare_file)
print(compare_result[1])
print(arcpy.GetMessages())
```

---

## Feature Envelope To Polygon (Data Management)

## Summary

Creates a feature class containing polygons, each of which represents the envelope of an input feature.

## Usage

- The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.
- Since the envelope of a perfectly horizontal line (parallel to the x-axis) has a zero height and the envelope of a perfectly vertical line (parallel to the y-axis) has a zero width, the resulting polygon from either line would have a zero area; such invalid polygons will be omitted in the output. The same applies to a part in a multipart line feature.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that can be multipoint, line, polygon, or annotation. | Feature Layer |
| Output Feature Class | The output polygon feature class. | Feature Class |
| Create multipart features(Optional) | Specifies whether to use one envelope for each entire multipart feature or one envelope per part of a multipart feature. This parameter will affect the results of multipart input features only.Unchecked—Uses one envelope containing an entire multipart feature; therefore, the resulting polygon will be singlepart. This is the default.Checked—Uses one envelope for each part of a multipart feature; the resulting polygon of the multipart feature will remain multipart. | Boolean |
| in_features | The input features that can be multipoint, line, polygon, or annotation. | Feature Layer |
| out_feature_class | The output polygon feature class. | Feature Class |
| single_envelope(Optional) | Specifies whether to use one envelope for each entire multipart feature or one envelope per part of a multipart feature. This parameter will affect the results of multipart input features only.SINGLEPART—Uses one envelope containing an entire multipart feature; therefore, the resulting polygon will be singlepart. This is the default.MULTIPART— Uses one envelope for each part of a multipart feature; the resulting polygon of the multipart feature will remain multipart. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.FeatureEnvelopeToPolygon(in_features, out_feature_class, {single_envelope})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.FeatureEnvelopeToPolygon_management("urban_analysis.gdb/parks",
                                          "c:/output/output.gdb/parks_extent",
                                          "SINGLEPART")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.FeatureEnvelopeToPolygon_management("urban_analysis.gdb/parks",
                                          "c:/output/output.gdb/parks_extent",
                                          "SINGLEPART")
```

### Example 4

```python
# Name: FeatureEnvelopeToPolygon_Example2.py
# Description: Use FeatureEnvelopeToPolygon function to find 
#              the general extent of features.

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/urban_analysis.gdb"

# Set local variables
inFeatures = "houses"
outFeatureClass = "c:/output/output.gdb/houses_extent"

# Execute FeatureEnvelopeToPolygon
arcpy.FeatureEnvelopeToPolygon_management(inFeatures, outFeatureClass, 
                                          "SINGLEPART")
```

### Example 5

```python
# Name: FeatureEnvelopeToPolygon_Example2.py
# Description: Use FeatureEnvelopeToPolygon function to find 
#              the general extent of features.

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/urban_analysis.gdb"

# Set local variables
inFeatures = "houses"
outFeatureClass = "c:/output/output.gdb/houses_extent"

# Execute FeatureEnvelopeToPolygon
arcpy.FeatureEnvelopeToPolygon_management(inFeatures, outFeatureClass, 
                                          "SINGLEPART")
```

---

## Feature To Line (Data Management)

## Summary

Creates a feature class containing lines generated by converting polygon boundaries to lines, or splitting line, polygon, or both features at their intersections.

## Usage

- Input attributes can optionally be maintained in the output feature class, determined by the Preserve attributes option on the dialog box (the attributes parameter in scripting).
- When multiple feature classes or layers are specified in the list of input features, the order of the entries in the list does not affect the output feature type, but the spatial reference of the top entry on the tool dialog box (the first entry in scripting) in the list will be used during processing and set to the output.
- Where input lines or polygon boundaries touch, cross, or overlap each other at locations other than their start and end vertices, they will be split at those intersections; each of the split lines will become an output line feature. If an input line or polygon boundary is not intersected by another feature, its entire shape will still be written out as a line feature.
- For multipart input features, the output lines will be singlepart.
- For input features that are parametric (true) curves, the output lines will remain true curves even if they are split. This does not apply to shapefile data.
- If the Preserve attributes option on the dialog box is checked (the attributes parameter is set to ATTRIBUTES in scripting), the attributes from all input entries will be maintained in the output in the order they appear in the input list. A new field, FID_xxx, where xxx is the source feature class name of a particular input entry, will be added to the output for each input entry and set to the source feature IDs. The output lines are associated with their attributes in the following ways:For coincident lines or polygon boundaries within the same set of input features, for example, the boundary separating two polygons, two line features with identical geometry will be written to the output: each of them will have the attributes of its source feature.For coincident lines or polygon boundaries from two different sets of input features, for example, a line overlapping a polygon boundary, only one line feature with the attributes of both source features will be written to the output.If an output line does not overlap any feature in a particular input feature set, it will have the value of -1 in the FID_xxx field and zero or null values in the other fields from that feature set.If the Preserve attributes option on the dialog box is unchecked (the attributes parameter is set to NO_ATTRIBUTES in scripting), none of the input attributes will be maintained in the output feature class; a single line feature will be written to the output for each set of coincident lines or polygon boundaries.
- For coincident lines or polygon boundaries within the same set of input features, for example, the boundary separating two polygons, two line features with identical geometry will be written to the output: each of them will have the attributes of its source feature.
- For coincident lines or polygon boundaries from two different sets of input features, for example, a line overlapping a polygon boundary, only one line feature with the attributes of both source features will be written to the output.
- If an output line does not overlap any feature in a particular input feature set, it will have the value of -1 in the FID_xxx field and zero or null values in the other fields from that feature set.
- When input features contain adjacent polygons, to get the shared boundary line with left and right polygon feature IDs as attributes in the output, use the Polygon To Line tool instead.
- For better performance and scalability, this tool uses a tiling process to handle very large datasets. For details, see Tiled processing of large datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that can be line or polygon, or both. | Feature Layer |
| Output Feature Class | The output line feature class. | Feature Class |
| XY Tolerance(Optional) | The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |
| Preserve attributes(Optional) | Specifies whether to preserve or omit the input feature attributes in the output feature class.Checked—Preserves the input attributes in the output features. This is the default.Unchecked—Omits the input attributes in the output features. | Boolean |
| in_features[in_features,...] | The input features that can be line or polygon, or both. | Feature Layer |
| out_feature_class | The output line feature class. | Feature Class |
| cluster_tolerance(Optional) | The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |
| attributes(Optional) | Specifies whether to preserve or omit the input attributes in the output feature class.ATTRIBUTES—Preserves the input attributes in the output features. This is the default.NO_ATTRIBUTES—Omits the input attributes in the output features. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.FeatureToLine(in_features, out_feature_class, {cluster_tolerance}, {attributes})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.FeatureToLine_management(["majorrds.shp", "habitat_analysis.gdb/futrds"],
                               "c:/output/output.gdb/allroads",
                               "0.001 Meters", "ATTRIBUTES")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.FeatureToLine_management(["majorrds.shp", "habitat_analysis.gdb/futrds"],
                               "c:/output/output.gdb/allroads",
                               "0.001 Meters", "ATTRIBUTES")
```

### Example 4

```python
# Name: FeatureToLine_Example2.py
# Description: Use FeatureToLine function to combine features from two 
#                  street feature classes into a single feature class,
#                  then determine an area of impact around all streets
#                  by buffering

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

#  Set local variables
oldStreets = "majorrds.shp"
newStreets = "habitat_analysis.gdb/futrds"
uptodateStreets = "c:/output/output.gdb/allroads"

# Use FeatureToLine function to combine features into single feature class
arcpy.FeatureToLine_management([oldStreets, newStreets], uptodateStreets,
                               "0.001 Meters", "ATTRIBUTES")

# Use Buffer function to determine area of impact around streets
roadsBuffer = "c:/output/output.gdb/buffer_output"
arcpy.Buffer_analysis(uptodateStreets, roadsBuffer, "50 Feet",
                      "FULL", "ROUND", "ALL")
```

### Example 5

```python
# Name: FeatureToLine_Example2.py
# Description: Use FeatureToLine function to combine features from two 
#                  street feature classes into a single feature class,
#                  then determine an area of impact around all streets
#                  by buffering

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

#  Set local variables
oldStreets = "majorrds.shp"
newStreets = "habitat_analysis.gdb/futrds"
uptodateStreets = "c:/output/output.gdb/allroads"

# Use FeatureToLine function to combine features into single feature class
arcpy.FeatureToLine_management([oldStreets, newStreets], uptodateStreets,
                               "0.001 Meters", "ATTRIBUTES")

# Use Buffer function to determine area of impact around streets
roadsBuffer = "c:/output/output.gdb/buffer_output"
arcpy.Buffer_analysis(uptodateStreets, roadsBuffer, "50 Feet",
                      "FULL", "ROUND", "ALL")
```

---

## Feature To Point (Data Management)

## Summary

Creates a feature class containing points generated from the centroids of the input features or placed within the input features.

## Usage

- The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features, which can be multipoint, line, polygon, or annotation. | Feature Layer |
| Output Feature Class | The output point feature class. | Feature Class |
| Inside(Optional) | Specifies whether an output point will be located within the input feature or at the centroid of the input feature.Unchecked—The output point will be located at the centroid of the input feature. The output point may not always be contained by the input feature. This is the default.Checked—The output point will be located within the input feature. If the Inside parameter is not checked, the location of each output point will be determined as follows:Multipoint features—The output point will be located at the average x- and y-coordinate of all the points in the multipoint.Polyline features—The output point will be located at the weighted average x- and y-coordinate of the midpoints of all line segments in the line where the weight of a particular midpoint is the length of the correspondent line segment. Parametric (true) curves will be densified.Polygon features—The output point will be located at the center of gravity (centroid) of the polygon.If the Inside parameter is checked, the location of the representative point of an input feature will be contained by the input feature and determined as follows:Multipoint features—The output point will be coincident with one of the points in the multipoint.Polyline features—The output point will be on the line. If the line is a parametric (true) curve, the output point will be at the midpoint of the line.Polygon features—The output point will be inside the polygon. | Boolean |
| in_features | The input features, which can be multipoint, line, polygon, or annotation. | Feature Layer |
| out_feature_class | The output point feature class. | Feature Class |
| point_location(Optional) | Specifies whether an output point will be located within the input feature or at the centroid of the input feature.CENTROID—The output point will be located at the centroid of the input feature. The output point may not always be contained by the input feature. This is the default.INSIDE—The output point will be located within the input feature.If the point_location parameter is set to CENTROID, the location of each output point will be determined as follows:Multipoint features—The output point will be located at the average x- and y-coordinate of all the points in the multipoint.Polyline features—The output point will be located at the weighted average x- and y-coordinate of the midpoints of all line segments in the line where the weight of a particular midpoint is the length of the correspondent line segment. Parametric (true) curves will be densified.Polygon features—The output point will be located at the center of gravity (centroid) of the polygon.If the point_location parameter is set to INSIDE, the location of the representative point of an input feature will be contained by the input feature and determined as follows:Multipoint features—The output point will be coincident with one of the points in the multipoint.Polyline features—The output point will be on the line. If the line is a parametric (true) curve, the output point will be at the midpoint of the line.Polygon features—The output point will be inside the polygon. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.FeatureToPoint(in_features, out_feature_class, {point_location})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.FeatureToPoint("parcels.shp", "c:/data/output/parcels_center.shp", 
                                "CENTROID")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.FeatureToPoint("parcels.shp", "c:/data/output/parcels_center.shp", 
                                "CENTROID")
```

### Example 4

```python
# Name: FeatureToPoint_Example2.py
# Description: Use FeatureToPoint function to find a point inside each park

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

#  Set local variables
inFeatures = "parks.shp"
outFeatureClass = "c:/output/output.gdb/parks_pt"

# Use FeatureToPoint function to find a point inside each park
arcpy.management.FeatureToPoint(inFeatures, outFeatureClass, "INSIDE")
```

### Example 5

```python
# Name: FeatureToPoint_Example2.py
# Description: Use FeatureToPoint function to find a point inside each park

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

#  Set local variables
inFeatures = "parks.shp"
outFeatureClass = "c:/output/output.gdb/parks_pt"

# Use FeatureToPoint function to find a point inside each park
arcpy.management.FeatureToPoint(inFeatures, outFeatureClass, "INSIDE")
```

---

## Feature To Polygon (Data Management)

## Summary

Creates a feature class containing polygons generated from areas enclosed by input line or polygon features.

## Usage

- Where one or more input features form a closed area, a new polygon feature will be constructed and written to the output feature class. The output attributes will vary depending on the Preserve attributes parameter value and the Label Features parameter value.
- When multiple feature classes or layers are specified in the list of input features, the order of the entries in the list does not affect the output feature type, but the spatial reference of the top entry on the tool dialog box (the first entry in Python) in the list will be used during processing and set to the output.
- Parametric (true) curves in the input features will remain true curves in the output polygons, even if they are split. This does not apply to shapefile data.
- Note:It is recommended that you do not use the Preserve attributes parameter, as it is no longer supported and does not work. It will remain, however, for backward compatibility of scripts and models. While the output attribute schema and field values for certain input combinations may be produced as described below, most of them are unintended.If the Preserve attributes parameter is checked, the output attribute schema and field values will depend on whether the label features (points) are provided in the following ways: If no Label Features parameter values are provided, the attribute schema (field names and properties, not field values) from each input entry will be maintained in the output in the order they appear in the input list. A new field, FID_xxx, in which xxx is the source feature class name of a particular input entry, will be added to the output for each input entry and set to the value of -1. All other fields will have zero or null values.If a Label Features parameter value is provided, no input attribute schemas will be maintained in the output feature class; only the attributes of the label features will be included in the output feature class. If an output polygon contains a label feature, it will have field values from that label feature. If an output polygon contains more than one label feature, it will have field values from one of them; otherwise, it will have zero or null field values.
- If no Label Features parameter values are provided, the attribute schema (field names and properties, not field values) from each input entry will be maintained in the output in the order they appear in the input list. A new field, FID_xxx, in which xxx is the source feature class name of a particular input entry, will be added to the output for each input entry and set to the value of -1. All other fields will have zero or null values.
- If a Label Features parameter value is provided, no input attribute schemas will be maintained in the output feature class; only the attributes of the label features will be included in the output feature class. If an output polygon contains a label feature, it will have field values from that label feature. If an output polygon contains more than one label feature, it will have field values from one of them; otherwise, it will have zero or null field values.
- If the Preserve attributes parameter is unchecked, the input attribute schemas will be written to the output; however, the attribute values will be empty. If you do not want attributes in the output polygon feature class, supply a point feature class that has no attributes for the Label Features parameter.
- When input polygon features are broken into smaller output polygon features, you can use the Identity tool to transfer attributes from the input polygon features to the resulting polygon features.
- For better performance and scalability, this tool uses a tiling process to handle very large datasets. For details, see Tiled processing of large datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features, which can be line, polygon, or both. | Feature Layer |
| Output Feature Class | The output polygon feature class. | Feature Class |
| XY Tolerance(Optional) | The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |
| Preserve attributes(Optional) | Note:This parameter is no longer supported. The parameter remains for backward compatibility of scripts and models. See the Usage section for more information. | Boolean |
| Label Features(Optional) | The optional input point features that contain the attributes to be transferred to the output polygon features. | Feature Layer |
| in_features[in_features,...] | The input features, which can be line, polygon, or both. | Feature Layer |
| out_feature_class | The output polygon feature class. | Feature Class |
| cluster_tolerance(Optional) | The minimum distance separating all feature coordinates, and the distance a coordinate can move in X, Y, or both during spatial computation. The default XY tolerance is set to 0.001 meter or its equivalent in feature units.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |
| attributes(Optional) | Note:This parameter is no longer supported. The parameter remains for backward compatibility of scripts and models. See the Usage section for more information. | Boolean |
| label_features(Optional) | The optional input point features that contain the attributes to be transferred to the output polygon features. | Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.FeatureToPolygon(in_features, out_feature_class, {cluster_tolerance}, {attributes}, {label_features})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.FeatureToPolygon(["mainroads.shp","streets.shp"],
                                  "c:/output/output.gdb/streetblocks",
                                  "", "NO_ATTRIBUTES")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.FeatureToPolygon(["mainroads.shp","streets.shp"],
                                  "c:/output/output.gdb/streetblocks",
                                  "", "NO_ATTRIBUTES")
```

### Example 4

```python
# Name: FeatureToPolygon_Example2.py
# Description: Use FeatureToPolygon function to construct habitat areas
#              from park boundaries and rivers.

# Import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/parks_analysis.gdb"

# Set local parameters
inFeatures = ["park_boundaries", "rivers"]
outFeatureClass = "c:/output/output.gdb/habitat_areas"
clusTol = "0.05 Meters"

# Use the FeatureToPolygon function to form new areas
arcpy.management.FeatureToPolygon(inFeatures, outFeatureClass, clusTol,
                                  "NO_ATTRIBUTES")
```

### Example 5

```python
# Name: FeatureToPolygon_Example2.py
# Description: Use FeatureToPolygon function to construct habitat areas
#              from park boundaries and rivers.

# Import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/parks_analysis.gdb"

# Set local parameters
inFeatures = ["park_boundaries", "rivers"]
outFeatureClass = "c:/output/output.gdb/habitat_areas"
clusTol = "0.05 Meters"

# Use the FeatureToPolygon function to form new areas
arcpy.management.FeatureToPolygon(inFeatures, outFeatureClass, clusTol,
                                  "NO_ATTRIBUTES")
```

---

## Feature Vertices To Points (Data Management)

## Summary

Creates a feature class containing points generated from specified vertices or locations of the input features.

## Usage

- The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.
- For multipart lines or polygons, each part will be treated as a line. Therefore, each part will have its own start, end, and mid points, as well as possible dangle point(s).
- A parametric (true) curve has only the start and end points and will not be densified.
- For the Dangle option of the Point Type parameter on the dialog box (the point_location parameter in Python), an additional field, DANGLE_LEN carrying the dangle length values in feature unit, will be added to the output feature class. For an isolated line, both endpoints are dangle points; therefore, the dangle length is the line length itself. For a dangle line that intersects another line at one of its endpoints, the dangle length is measured from the dangling endpoint to the intersection.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that can be line or polygon. | Feature Layer |
| Output Feature Class | The output point feature class. | Feature Class |
| Point Type(Optional) | Specifies where an output point will be created.All vertices—A point will be created at each input feature vertex. This is the default. Midpoint—A point will be created at the midpoint, not necessarily a vertex, of each input line or polygon boundary. Start vertex—A point will be created at the start point (first vertex) of each input feature. End vertex—A point will be created at the end point (last vertex) of each input feature. Both start and end vertex—Two points will be created, one at the start point and another at the endpoint of each input feature.Dangling vertex—A dangle point will be created for any start or end point of an input line, if that point is not connected to another line at any location along that line. This option does not apply to polygon input. | String |
| in_features | The input features that can be line or polygon. | Feature Layer |
| out_feature_class | The output point feature class. | Feature Class |
| point_location(Optional) | Specifies where an output point will be created.ALL—A point will be created at each input feature vertex. This is the default. MID—A point will be created at the midpoint, not necessarily a vertex, of each input line or polygon boundary. START—A point will be created at the start point (first vertex) of each input feature. END—A point will be created at the end point (last vertex) of each input feature. BOTH_ENDS—Two points will be created, one at the start point and another at the endpoint of each input feature.DANGLE—A dangle point will be created for any start or end point of an input line, if that point is not connected to another line at any location along that line. This option does not apply to polygon input. | String |

## Code Samples

### Example 1

```python
arcpy.management.FeatureVerticesToPoints(in_features, out_feature_class, {point_location})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.FeatureVerticesToPoints_management("parcels.shp",
                                         "c:/output/output.gdb/parcels_corner", 
                                         "ALL")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.FeatureVerticesToPoints_management("parcels.shp",
                                         "c:/output/output.gdb/parcels_corner", 
                                         "ALL")
```

### Example 4

```python
# Name: FeatureVerticesToPoints_Example2.py
# Description: Use FeatureVerticesToPoints function to get the mid-points
#              of input line features

 
# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"
 
# Set local variables
inFeatures = "majorrds.shp"
outFeatureClass = "c:/output/output.gdb/majorrds_midpt"

# Execute FeatureVerticesToPoints
arcpy.FeatureVerticesToPoints_management(inFeatures, outFeatureClass, "MID")
```

### Example 5

```python
# Name: FeatureVerticesToPoints_Example2.py
# Description: Use FeatureVerticesToPoints function to get the mid-points
#              of input line features

 
# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"
 
# Set local variables
inFeatures = "majorrds.shp"
outFeatureClass = "c:/output/output.gdb/majorrds_midpt"

# Execute FeatureVerticesToPoints
arcpy.FeatureVerticesToPoints_management(inFeatures, outFeatureClass, "MID")
```

---

## Field Statistics To Table (Data Management)

## Summary

Creates a table of descriptive statistics for one or more input fields in a table or feature class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table containing the fields that will be used to create the statistics table. | Table View |
| Input Fields | The fields containing the values that will be used to calculate the statistics. | Field |
| Output Location | The location where the output tables will be created. The location can be a geodatabase, folder, or feature dataset. | Workspace |
| Output Tables | The output tables containing the statistics. The Field Types column specifies the field types that will be included in each output table, and the name of each output table is provided in the Output Name column. For example, you can create a single table with summaries of all field types, or you can create separate tables for summaries of Numeric, Text, and Date field types.The following options are available for the Field Types column:Numeric—A table summarizing numeric fields of the input (Short, Long, Big Integer, Float, and Double types) will be created.Text—A table summarizing text fields of the input (Text type) will be created.Date—A table summarizing date fields of the input (Date, Date Only, Time Only and Timestamp Offset types) will be created.All—A table summarizing all numeric, text, and date fields of the input will be created. Output fields containing statistics that apply to multiple field types will be saved as type Text. Output statistics that do not apply to Text and Date type fields will be empty. | Value Table |
| Group By Field (Optional) | The field that will be used to group rows into categories. If a group by field is provided, each field of the input will appear as a row in the output table once per unique value in the group by field. | Field |
| Output Statistics (Optional) | Specifies the statistics that will be summarized and the names of the output fields containing the statistics. The statistic is provided in the Statistic column, and the name of the output field is provided in the Output Field Name column. If no values are provided, all applicable statistics will be calculated for all input fields.The following options are available for the Statistic column (only statistics applicable to the input fields will be available):Field name—The name of the field.Alias—The alias of the field.Field type—The field type of the field (Short, Long, Double, Float, Text, or Date).Nulls—The number of records containing null values of the field.Minimum—The smallest value in the field.Maximum—The largest value in the field.Mean—The mean (sum divided by total count) of all values in the field. To calculate the mean date for date fields, each date is converted to a number by calculating the difference between the date and a reference date (for example, 1900-01-01), calculated in milliseconds.Standard deviation—The standard deviation of the values in the field. It is calculated as the square root of the variance, in which the variance is the average squared difference of each value from the mean of the field.Median—The median for all values in the field. The median is the middle value in the sorted list of values. If there is an even number of values, the median is the mean of the two middle values in the distribution.Count—The number of nonnull values in the field.Number of unique values—The number of unique values in the field.Mode—The most frequently occurring value in the field.Least common—The least common value in the field.Outliers—The number of records with outlier values in the field. Outliers are values that are more than 1.5 times the interquartile range above the third quartile or below the first quartile of the values of the field.Sum—The sum of all values in the field.Range—The difference between the largest and smallest values in the field.Interquartile range—The range between the first quartile and the third quartile of the values in the field. This represents the range of the middle half of the data.First quartile—The value of the first quartile of the field. Quartiles divide the sorted list of values into four groups containing equal numbers of values. The first quartile is the upper limit of the first group in ascending order.Third quartile—The value of the third quartile of the field. Quartiles divide the sorted list of values into four groups containing equal numbers of values. The third quartile is the upper limit of the third group in ascending order.Coefficient of variation—The coefficient of variation of the values in the field. The coefficient of variation is a measure of the relative spread of the values. It is calculated as the standard deviation divided by the mean of the field.Skewness—The skewness of the values in the field. Skewness measures the symmetry of the distribution. The skewness is calculated as the third moment (the average of the cubed data values) divided by the cubed standard deviation.Kurtosis—The kurtosis of the values in the field. Kurtosis describes the heaviness of the tails of a distribution compared to the normal distribution, helping identify the frequency of extreme values. The kurtosis is calculated as the fourth moment (the average of the data values taken to the fourth power) divided by the fourth power of the standard deviation. | Value Table |
| in_table | The input table containing the fields that will be used to create the statistics table. | Table View |
| in_fields[in_fields,...] | The fields containing the values that will be used to calculate the statistics. | Field |
| out_location | The location where the output tables will be created. The location can be a geodatabase, folder, or feature dataset. | Workspace |
| out_tables[[field_type, output_name],...] | The output tables containing the statistics. The field_type column specifies the field types that will be included in each output table, and the name of each output table is provided in the output_name column. For example, you can create a single table with summaries of all field types, or you can create separate tables for summaries of Numeric, Text, and Date field types.The following options are available for the field_type column:NUMERIC—A table summarizing numeric fields of the input (Short, Long, Big Integer, Float, and Double types) will be created.TEXT—A table summarizing text fields of the input (Text type) will be created.DATE—A table summarizing date fields of the input (Date, Date Only, Time Only and Timestamp Offset types) will be created.ALL—A table summarizing all numeric, text, and date fields of the input will be created. Output fields containing statistics that apply to multiple field types will be saved as type Text. Output statistics that do not apply to Text and Date type fields will be empty. | Value Table |
| group_by_field(Optional) | The field that will be used to group rows into categories. If a group by field is provided, each field of the input will appear as a row in the output table once per unique value in the group by field. | Field |
| out_statistics[[out_statistic, output_name],...](Optional) | Specifies the statistics that will be summarized and the names of the output fields containing the statistics. The statistic is provided in the out_statistic column, and the name of the output field is provided in the output_name column. If no values are provided, all applicable statistics will be calculated for all input fields.The following options are available for the out_statistic column (only statistics applicable to the input fields will be available):FIELDNAME—The name of the field.ALIAS—The alias of the field.FIELDTYPE—The field type of the field (Short, Long, Double, Float, Text, or Date).NULLS—The number of records containing null values of the field.MINIMUM—The smallest value in the field.MAXIMUM—The largest value in the field.MEAN—The mean (sum divided by total count) of all values in the field. To calculate the mean date for date fields, each date is converted to a number by calculating the difference between the date and a reference date (for example, 1900-01-01), calculated in milliseconds.STANDARDDEVIATION—The standard deviation of the values in the field. It is calculated as the square root of the variance, in which the variance is the average squared difference of each value from the mean of the field.MEDIAN—The median for all values in the field. The median is the middle value in the sorted list of values. If there is an even number of values, the median is the mean of the two middle values in the distribution.COUNT—The number of nonnull values in the field.NUMBEROFUNIQUEVALUES—The number of unique values in the field.MODE—The most frequently occurring value in the field.LEASTCOMMON—The least common value in the field.OUTLIERS—The number of records with outlier values in the field. Outliers are values that are more than 1.5 times the interquartile range above the third quartile or below the first quartile of the values of the field.SUM—The sum of all values in the field.RANGE—The difference between the largest and smallest values in the field.INTERQUARTILERANGE—The range between the first quartile and the third quartile of the values in the field. This represents the range of the middle half of the data.FIRSTQUARTILE—The value of the first quartile of the field. Quartiles divide the sorted list of values into four groups containing equal numbers of values. The first quartile is the upper limit of the first group in ascending order.THIRDQUARTILE—The value of the third quartile of the field. Quartiles divide the sorted list of values into four groups containing equal numbers of values. The third quartile is the upper limit of the third group in ascending order.COEFFICIENTOFVARIATION—The coefficient of variation of the values in the field. The coefficient of variation is a measure of the relative spread of the values. It is calculated as the standard deviation divided by the mean of the field.SKEWNESS—The skewness of the values in the field. Skewness measures the symmetry of the distribution. The skewness is calculated as the third moment (the average of the cubed data values) divided by the cubed standard deviation.KURTOSIS—The kurtosis of the values in the field. Kurtosis describes the heaviness of the tails of a distribution compared to the normal distribution, helping identify the frequency of extreme values. The kurtosis is calculated as the fourth moment (the average of the data values taken to the fourth power) divided by the fourth power of the standard deviation. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.FieldStatisticsToTable(in_table, in_fields, out_location, out_tables, {group_by_field}, {out_statistics})
```

### Example 2

```python
import arcpy
arcpy.management.FieldStatisticsToTable("SNAP_County_Rates", 
      "LocId;Program_Name;Participants;SNAP_Rate;TimeofYear", r"C:\Output.gdb", 
      "ALL All_Table;DATE Date_Table;NUMERIC Num_Table;TEXT Text_Table")
```

### Example 3

```python
import arcpy
arcpy.management.FieldStatisticsToTable("SNAP_County_Rates", 
      "LocId;Program_Name;Participants;SNAP_Rate;TimeofYear", r"C:\Output.gdb", 
      "ALL All_Table;DATE Date_Table;NUMERIC Num_Table;TEXT Text_Table")
```

### Example 4

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features. 
    arcpy.env.workspace = r"C:\\Statistics\\MyData.gdb" 
    in_table = "County_Data" 
 
    # Set the input fields that will be used to calculate statistics. 
    in_fields = "population_total;unemployment_rate;income;county_name;sample_date" 
 
    # Set the output location.
    out_location = r"C:\\Statistics\\MyData.gdb"

    # Set the output table field type and name.
    out_tables = "ALL AllStats_Table;DATE DateStats_Table;NUMERIC NumStats_Table;TEXT TextStats_Table"
 
    # Run the Field Statistics To Table tool 
    arcpy.management.FieldStatisticsToTable (in_table, in_fields, out_location, out_tables) 
 
except arcpy.ExecuteError: 
    # If an error occurred when running the tool, print the error message. 
    print(arcpy.GetMessages())
```

### Example 5

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features. 
    arcpy.env.workspace = r"C:\\Statistics\\MyData.gdb" 
    in_table = "County_Data" 
 
    # Set the input fields that will be used to calculate statistics. 
    in_fields = "population_total;unemployment_rate;income;county_name;sample_date" 
 
    # Set the output location.
    out_location = r"C:\\Statistics\\MyData.gdb"

    # Set the output table field type and name.
    out_tables = "ALL AllStats_Table;DATE DateStats_Table;NUMERIC NumStats_Table;TEXT TextStats_Table"
 
    # Run the Field Statistics To Table tool 
    arcpy.management.FieldStatisticsToTable (in_table, in_fields, out_location, out_tables) 
 
except arcpy.ExecuteError: 
    # If an error occurred when running the tool, print the error message. 
    print(arcpy.GetMessages())
```

---

## File Compare (Data Management)

## Summary

Compares two files and returns the comparison results..

## Usage

- This tool returns messages showing the comparison result. By default, it will stop executing after encountering the first miscompare. To report all differences, check on the Continue Comparison parameter.
- File Compare can report differences between two ASCII files or two binary files
- This tool supports masking out of characters, words, and lines of text in an ASCII file. For example, files may be identical except they may contain text representing date and time of creation. Thus, the files would miscompare. In addition, small variations occur in the way that each platform stores or manipulates numbers. This leads to differences in numeric precision among platforms. The SunOS platform may report a value of 415.999999999, while the Windows XP platform reports 416.000000000. To handle false character comparisons, File Compare provides several masking capabilities. Before comparing new text files with existing base files, edit the base files to include these special masking symbols."#"—The simplest masking symbol is the "#" symbol. Wherever a # appears in the input base file, the corresponding character in the input test file will be ignored.Base: Y delta = 9048.6# Test: Y delta = 9048.61"??"—Another masking tool is the "??" symbol combination. To mask out an entire "word", add "??" at the beginning of it.Base: Processing ??ESRI1/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE.DGN Test: Processing ESRI2/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE2.DGN"?!"—A single token may have a '.' (period) imbedded in it. An obvious example of this would be the name of a file with an extension—streetnames.dbf. There could be instances where you would want part of the name, either before or after the '.', to be ignored in the comparison of the token.Base: Master table is: streetnames?!.dbf Test: Master table is: streetnames"???"—This allows you to mask the entire line following it.Base: ??? 8 4 1 0 14 10 Test: 12 8 2 1 16 12
- "#"—The simplest masking symbol is the "#" symbol. Wherever a # appears in the input base file, the corresponding character in the input test file will be ignored.Base: Y delta = 9048.6# Test: Y delta = 9048.61
- "??"—Another masking tool is the "??" symbol combination. To mask out an entire "word", add "??" at the beginning of it.Base: Processing ??ESRI1/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE.DGN Test: Processing ESRI2/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE2.DGN
- "?!"—A single token may have a '.' (period) imbedded in it. An obvious example of this would be the name of a file with an extension—streetnames.dbf. There could be instances where you would want part of the name, either before or after the '.', to be ignored in the comparison of the token.Base: Master table is: streetnames?!.dbf Test: Master table is: streetnames
- "???"—This allows you to mask the entire line following it.Base: ??? 8 4 1 0 14 10 Test: 12 8 2 1 16 12
- ASCII is the default file type. If entering binary files, change the file type to Binary (BINARY in Python).
- When ASCII files miscompare, it will report differences, such as the total number of characters are different and report the differences for each line.
- When binary files miscompare, it will report that the file sizes are different and report the differences for each byte.
- The Output Compare File will contain all similarities and differences between the Input Base File and the Input Test File. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS.
- When using this tool in Python, you can get the status of this tool using result.getOutput(1). The value will be 'true' when no differences are found and 'false' when differences are detected. Learn more about using tools in Python

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Base File | The Input Base File is compared with the Input Test File. The Input Base File refers to a file that you have declared valid. This base file has the correct content and information. | File |
| Input Test File | The Input Test File is compared against the Input Base File. The Input Test File refers to a file that you have made changes to by editing or compiling new information. | File |
| File Type(Optional) | The type of files being compared. ASCII—Compare using ASCII characters. This is the default.Binary—Perform a binary compare. | String |
| Continue Comparison(Optional) | Indicates whether to compare all properties after encountering the first mismatch.Unchecked—Stops after encountering the first mismatch. This is the default.Checked—Compares other properties after encountering the first mismatch. | Boolean |
| Output Compare File(Optional) | This file will contain all similarities and differences between the Input Base File and the Input Test File. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. | File |
| in_base_file | The Input Base File is compared with the Input Test File. The Input Base File refers to a file that you have declared valid. This base file has the correct content and information. | File |
| in_test_file | The Input Test File is compared against the Input Base File. The Input Test File refers to a file that you have made changes to by editing or compiling new information. | File |
| file_type(Optional) | The type of files being compared. ASCII—Compare using ASCII characters. This is the default.BINARY—Perform a binary compare. | String |
| continue_compare(Optional) | Indicates whether to compare all properties after encountering the first mismatch.NO_CONTINUE_COMPARE—Stops after encountering the first mismatch. This is the default. CONTINUE_COMPARE—Compares other properties after encountering the first mismatch. | Boolean |
| out_compare_file(Optional) | This file will contain all similarities and differences between the Input Base File and the Input Test File. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. | File |

## Code Samples

### Example 1

```python
Base: Y delta = 9048.6#
Test: Y delta = 9048.61
```

### Example 2

```python
Base: Y delta = 9048.6#
Test: Y delta = 9048.61
```

### Example 3

```python
Base: Processing ??ESRI1/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE.DGN
Test: Processing ESRI2/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE2.DGN
```

### Example 4

```python
Base: Processing ??ESRI1/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE.DGN
Test: Processing ESRI2/ARCIGDS/TESTRUN/CONV/ARCIGDS/CPXSHAPE2.DGN
```

### Example 5

```python
Base: Master table is: streetnames?!.dbf
Test: Master table is: streetnames
```

### Example 6

```python
Base: Master table is: streetnames?!.dbf
Test: Master table is: streetnames
```

### Example 7

```python
Base: ???       8       4       1       0      14      10
Test:        12      8      2       1      16     12
```

### Example 8

```python
Base: ???       8       4       1       0      14      10
Test:        12      8      2       1      16     12
```

### Example 9

```python
arcpy.management.FileCompare(in_base_file, in_test_file, {file_type}, {continue_compare}, {out_compare_file})
```

### Example 10

```python
import arcpy
arcpy.FileCompare_management(
    r'C:/Workspace/well_xycoordinates.txt', 
    r'C:/Workspace/new_well_coordinates.txt', 'ASCII', 'CONTINUE_COMPARE', 
    r'C:/Workspace/well_file_compare.txt')
```

### Example 11

```python
import arcpy
arcpy.FileCompare_management(
    r'C:/Workspace/well_xycoordinates.txt', 
    r'C:/Workspace/new_well_coordinates.txt', 'ASCII', 'CONTINUE_COMPARE', 
    r'C:/Workspace/well_file_compare.txt')
```

### Example 12

```python
# Name: FileCompare.py
# Description: Compare two text files and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_file= "C:/Workspace/well_xycoordinates.txt"
test_file= "C:/Workspace/new_well_coordinates.txt"
file_type = "ASCII"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/well_file_compare.txt"

# Process: FeatureCompare
compare_result = arcpy.FileCompare_management(base_file, test_features, 
                                              file_type, continue_compare, 
                                              compare_file)
print(compare_result)
print(arcpy.GetMessages())
```

### Example 13

```python
# Name: FileCompare.py
# Description: Compare two text files and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_file= "C:/Workspace/well_xycoordinates.txt"
test_file= "C:/Workspace/new_well_coordinates.txt"
file_type = "ASCII"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/well_file_compare.txt"

# Process: FeatureCompare
compare_result = arcpy.FileCompare_management(base_file, test_features, 
                                              file_type, continue_compare, 
                                              compare_file)
print(compare_result)
print(arcpy.GetMessages())
```

---

## Find Identical (Data Management)

## Summary

Reports any records in a feature class or table that have identical values in a list of fields, and generates a table listing the identical records. If the Shape field is specified, feature geometries will be compared.

## Usage

- Records are identical if values in the selected input fields are the same for those records. The values from multiple fields in the input dataset can be compared. If more than one field is specified, records are matched by the values in the first field, then by the values of the second field, and so on.
- With feature class or feature layer input, specify the Shape field in the Field(s) parameter to compare feature geometries to find identical features by location. The XY Tolerance and Z Tolerance parameters are only valid when the Shape field is specified.If the Shape field is specified and the input features have m- or z-values enabled, the m- or z-values will also be used to determine identical features.
- Check the Output only duplicated records parameter if you want only the duplicated records in the output table. If this parameter is unchecked, the output will have the same number of records as the input dataset.
- The output table will contain the following fields:IN_FID—The Object ID field value from the input dataset. This field can be used to join the records of the output table back to the input dataset.FEAT_SEQ—The sequence number. Records from the input that have the same values will have the same FEAT_SEQ value while nonidentical records will have a unique sequential value. The FEAT_SEQ values have no relationship to IDs of input records.
- IN_FID—The Object ID field value from the input dataset. This field can be used to join the records of the output table back to the input dataset.
- FEAT_SEQ—The sequence number. Records from the input that have the same values will have the same FEAT_SEQ value while nonidentical records will have a unique sequential value. The FEAT_SEQ values have no relationship to IDs of input records.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The table or feature class for which identical records will be found. | Table View |
| Output Dataset | The output table reporting identical records. The FEAT_SEQ field in the output table will have the same value for identical records. | Table |
| Field(s) | The field or fields whose values will be compared to find identical records. | Field |
| XY Tolerance(Optional) | The x,y tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature.This parameter is active when the Field(s) parameter value includes the Shape field. | Linear Unit |
| Z Tolerance(Optional) | The z-tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature.This parameter is active when the Field(s) parameter value includes the Shape field. | Double |
| Output only duplicated records (Optional) | Specifies whether only duplicated records will be included in the output table.Unchecked—All input records will have corresponding records in the output table. This is the default.Checked—Only duplicate records will have corresponding records in the output table. The output will be empty if no duplicate is found. | Boolean |
| in_dataset | The table or feature class for which identical records will be found. | Table View |
| out_dataset | The output table reporting identical records. The FEAT_SEQ field in the output table will have the same value for identical records. | Table |
| fields[fields,...] | The field or fields whose values will be compared to find identical records. | Field |
| xy_tolerance(Optional) | The x,y tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature.This parameter is enabled when the fields parameter value includes the Shape field. | Linear Unit |
| z_tolerance(Optional) | The z-tolerance that will be applied to each vertex when evaluating whether there is an identical vertex in another feature.This parameter is enabled when the fields parameter value includes the Shape field. | Double |
| output_record_option(Optional) | Specifies whether only duplicated records will be included in the output table.ALL—All input records will have corresponding records in the output table. This is the default.ONLY_DUPLICATES—Only duplicate records will have corresponding records in the output table. The output will be empty if no duplicate is found. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.FindIdentical(in_dataset, out_dataset, fields, {xy_tolerance}, {z_tolerance}, {output_record_option})
```

### Example 2

```python
import arcpy

# Find identical records based on a text field and a numeric field.
arcpy.FindIdentical_management("C:/data/fireincidents.shp", "C:/output/duplicate_incidents.dbf", ["ZONE", "INTENSITY"])
```

### Example 3

```python
import arcpy

# Find identical records based on a text field and a numeric field.
arcpy.FindIdentical_management("C:/data/fireincidents.shp", "C:/output/duplicate_incidents.dbf", ["ZONE", "INTENSITY"])
```

### Example 4

```python
# Name: FindIdentical_Example2.py
# Description: Finds duplicate features in a dataset based on location (Shape field) and fire intensity

import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/findidentical.gdb"

# Set input feature class
in_dataset = "fireincidents"

# Set the fields upon which the matches are found
fields = ["Shape", "INTENSITY"]

# Set xy tolerance
xy_tol = ".02 Meters"

out_table = "duplicate_incidents"

# Execute Find Identical 
arcpy.FindIdentical_management(in_dataset, out_table, fields, xy_tol)
print(arcpy.GetMessages())
```

### Example 5

```python
# Name: FindIdentical_Example2.py
# Description: Finds duplicate features in a dataset based on location (Shape field) and fire intensity

import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/findidentical.gdb"

# Set input feature class
in_dataset = "fireincidents"

# Set the fields upon which the matches are found
fields = ["Shape", "INTENSITY"]

# Set xy tolerance
xy_tol = ".02 Meters"

out_table = "duplicate_incidents"

# Execute Find Identical 
arcpy.FindIdentical_management(in_dataset, out_table, fields, xy_tol)
print(arcpy.GetMessages())
```

### Example 6

```python
# Name: FindIdentical_Example3.py
# Description: Demonstrates the use of the optional parameter Output only duplicated records.

import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/redlands.gdb"

in_data = "crime"
out_data = "crime_dups"

# Note that XY Tolerance and Z Tolerance parameters are not used
# In that case, any optional parameter after them must assign
# the value with the name of that parameter    
arcpy.FindIdentical_management(in_data, out_data, ["Shape"], output_record_option="ONLY_DUPLICATES")

print(arcpy.GetMessages())
```

### Example 7

```python
# Name: FindIdentical_Example3.py
# Description: Demonstrates the use of the optional parameter Output only duplicated records.

import arcpy

arcpy.env.overwriteOutput = True

# Set workspace environment
arcpy.env.workspace = "C:/data/redlands.gdb"

in_data = "crime"
out_data = "crime_dups"

# Note that XY Tolerance and Z Tolerance parameters are not used
# In that case, any optional parameter after them must assign
# the value with the name of that parameter    
arcpy.FindIdentical_management(in_data, out_data, ["Shape"], output_record_option="ONLY_DUPLICATES")

print(arcpy.GetMessages())
```

### Example 8

```python
import arcpy

from itertools import groupby
from operator import itemgetter

# Set workspace environment
arcpy.env.workspace = r"C:\data\redlands.gdb"

# Run Find Identical on feature geometry only.
result = arcpy.FindIdentical_management("parcels", "parcels_dups", ["Shape"])
    
# List of all output records as IN_FID and FEAT_SEQ pair - a list of lists
out_records = []   
for row in arcpy.SearchCursor(result.getOutput(0), fields="IN_FID; FEAT_SEQ"):
    out_records.append([row.IN_FID, row.FEAT_SEQ])

# Sort the output records by FEAT_SEQ values
# Example of out_records = [[3, 1], [5, 3], [1, 1], [4, 3], [2, 2]]
out_records.sort(key = itemgetter(1))
    
# records after sorted by FEAT_SEQ: [[3, 1], [1, 1], [2, 2], [5, 3], [4, 3]]
# records with same FEAT_SEQ value will be in the same group (i.e., identical)
identicals_iter = groupby(out_records, itemgetter(1))
    
# now, make a list of identical groups - each group in a list.
# example identical groups: [[3, 1], [2], [5, 4]]
# i.e., IN_FID 3, 1 are identical, and 5, 4 are identical.
identical_groups = [[item[0] for item in data] for (key, data) in identicals_iter]

print(identical_groups)
```

### Example 9

```python
import arcpy

from itertools import groupby
from operator import itemgetter

# Set workspace environment
arcpy.env.workspace = r"C:\data\redlands.gdb"

# Run Find Identical on feature geometry only.
result = arcpy.FindIdentical_management("parcels", "parcels_dups", ["Shape"])
    
# List of all output records as IN_FID and FEAT_SEQ pair - a list of lists
out_records = []   
for row in arcpy.SearchCursor(result.getOutput(0), fields="IN_FID; FEAT_SEQ"):
    out_records.append([row.IN_FID, row.FEAT_SEQ])

# Sort the output records by FEAT_SEQ values
# Example of out_records = [[3, 1], [5, 3], [1, 1], [4, 3], [2, 2]]
out_records.sort(key = itemgetter(1))
    
# records after sorted by FEAT_SEQ: [[3, 1], [1, 1], [2, 2], [5, 3], [4, 3]]
# records with same FEAT_SEQ value will be in the same group (i.e., identical)
identicals_iter = groupby(out_records, itemgetter(1))
    
# now, make a list of identical groups - each group in a list.
# example identical groups: [[3, 1], [2], [5, 4]]
# i.e., IN_FID 3, 1 are identical, and 5, 4 are identical.
identical_groups = [[item[0] for item in data] for (key, data) in identicals_iter]

print(identical_groups)
```

---

## Flip (Data Management)

## Summary

Reorients the raster by turning it over, from top to bottom, along the horizontal axis through the center of the raster. This may be useful to correct raster datasets that are upside down.

## Usage

- This tool flips the grid from top to bottom along the horizontal axis through the center of the region.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| in_raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| out_raster | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |

## Code Samples

### Example 1

```python
arcpy.management.Flip(in_raster, out_raster)
```

### Example 2

```python
import arcpy
arcpy.Flip_management("c:/data/image.tif", "c:/data/flip.tif")
```

### Example 3

```python
import arcpy
arcpy.Flip_management("c:/data/image.tif", "c:/data/flip.tif")
```

### Example 4

```python
##====================================
##Flip
##Usage: Flip_management in_raster out_raster
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Flip a TIFF format image
arcpy.Flip_management("image.tif", "flip.tif")
```

### Example 5

```python
##====================================
##Flip
##Usage: Flip_management in_raster out_raster
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Flip a TIFF format image
arcpy.Flip_management("image.tif", "flip.tif")
```

---

## Generate Attachment Match Table (Data Management)

## Summary

Creates a match table to be used with the Add Attachments and Remove Attachments tools.

## Usage

- This tool will evaluate each row in the input target dataset and compare the Key Field parameter values to the names of supported media files in the Input Folder parameter value. For each match that occurs, a record will be created in the output table that contains the Object ID value from the input dataset and the name of the matched files (or optionally the full path to the files). When used with the Add Attachments and Remove Attachments tools, the MATCHID field is used as the key field to link one or more files on disk to records in the input dataset.
- If the Output Match Table parameter value is a folder, the output can be created as a dBASE table by specifying a name with the .dbf extension. If the output location is a geodatabase, the match table will be a geodatabase table (do not specify an extension).

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | A dataset containing records that will have files attached. | Table View |
| Input Folder | A folder containing files to attach. | Folder |
| Output Match Table | The output match table with the MATCHID and FILENAME fields. | Table |
| Key Field | The field from which values will be used to match the names of the files from the input folder. The matching behavior will compare field values with each file name, disregarding the file extension. This allows multiple files with various file extensions to match a single record in the input dataset.For example, a field value of lot5986 will match a file named lot5986.jpg if the Match Pattern parameter value is Exact. | Field |
| Input Data Filter(Optional) | A data filter that will be used to limit the files considered for matching. Wild cards (*) can be used for more flexible filtering options. Multiple filters delimited by semicolons are also supported.For example, you have an input directory with a variety of file types. To limit the possible matches to only .jpg files, , use a value of *.jpg. To limit the possible matches to only .pdf and .doc files, use a value of *.pdf; *.doc.To limit possible matches to only file names that contain the text arc, use a value of *arc*. | String |
| Store Relative Path (Optional) | Specifies whether the output match table field FILENAME will contain full paths or only the file names. Checked—The field will contain only the file names (relative paths). This is the default.Unchecked—The field will contain full paths to the data. | Boolean |
| Match Pattern(Optional) | Specifies the type of match pattern that will be used to match file names with the specified Key Field parameter value.Exact—File names that are an exact match to the values in the key field will be matched. This is the default.Prefix—File names that have the key field value at the beginning of the file name will be matched.Suffix—File names that have the key field value at the end of the file name will be matched.Any—File names that have the key field value anywhere in the file name will be matched. | String |
| in_dataset | A dataset containing records that will have files attached. | Table View |
| in_folder | A folder containing files to attach. | Folder |
| out_match_table | The output match table with the MATCHID and FILENAME fields. | Table |
| in_key_field | The field from which values will be used to match the names of the files from the input folder. The matching behavior will compare field values with each file name, disregarding the file extension. This allows multiple files with various file extensions to match a single record in the input dataset.For example, a field value of lot5986 will match a file named lot5986.jpg if the match_pattern parameter value is EXACT. | Field |
| in_file_filter(Optional) | A data filter that will be used to limit the files considered for matching. Wild cards (*) can be used for more flexible filtering options. Multiple filters delimited by semicolons are also supported.For example, you have an input directory with a variety of file types. To limit the possible matches to only .jpg files, , use a value of *.jpg. To limit the possible matches to only .pdf and .doc files, use a value of *.pdf; *.doc.To limit possible matches to only file names that contain the text arc, use a value of *arc*. | String |
| in_use_relative_paths(Optional) | Specifies whether the output match table field FILENAME will contain full paths or only the file names. RELATIVE—The field will contain only the file names (relative paths). This is the default.ABSOLUTE—The field will contain full paths. | Boolean |
| match_pattern(Optional) | Specifies the type of match pattern that will be used to match file names with the specified key_field parameter value. EXACT—File names that are an exact match to the values in the key field will be matched. This is the default.PREFIX—File names that have the key field value at the beginning of the file name will be matched.SUFFIX—File names that have the key field value at the end of the file name will be matched.ANY—File names that have the key field value anywhere in the file name will be matched. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateAttachmentMatchTable(in_dataset, in_folder, out_match_table, in_key_field, {in_file_filter}, {in_use_relative_paths}, {match_pattern})
```

### Example 2

```python
import arcpy
arcpy.management.GenerateAttachmentMatchTable(
    "C:/data/parcels.gdb/parcels",
    "C:/attachment_folder",
    "C:/data/temp.gdb/matchtable",
    "AttachmentKeyField",
    "*.jpg; *.pdf",
    "ABSOLUTE",
    "EXACT")
```

### Example 3

```python
import arcpy
arcpy.management.GenerateAttachmentMatchTable(
    "C:/data/parcels.gdb/parcels",
    "C:/attachment_folder",
    "C:/data/temp.gdb/matchtable",
    "AttachmentKeyField",
    "*.jpg; *.pdf",
    "ABSOLUTE",
    "EXACT")
```

### Example 4

```python
# Name: GenerateAttachmentMatchTable_Example.py
# Description: Create an attachment match table for all files that contain the string
# 'property' and are of type 'jpg' while looping through multiple folders.

# Import system modules
import arcpy
import os

# Set local variables.
rootFolder = 'c:/work/'

for folder in os.walk(rootFolder):
    # Exclude file geodatabases from the folder list.
    if folder[0].find('.gdb') == -1:
        arcpy.management.GenerateAttachmentMatchTable(
            "C:/data/parcels.gdb/parcels", folder[0], 
            "C:/data/temp.gdb/matchtable", "AttachmentKeyField", 
            "*property*.jpg", "RELATIVE", "EXACT")
```

### Example 5

```python
# Name: GenerateAttachmentMatchTable_Example.py
# Description: Create an attachment match table for all files that contain the string
# 'property' and are of type 'jpg' while looping through multiple folders.

# Import system modules
import arcpy
import os

# Set local variables.
rootFolder = 'c:/work/'

for folder in os.walk(rootFolder):
    # Exclude file geodatabases from the folder list.
    if folder[0].find('.gdb') == -1:
        arcpy.management.GenerateAttachmentMatchTable(
            "C:/data/parcels.gdb/parcels", folder[0], 
            "C:/data/temp.gdb/matchtable", "AttachmentKeyField", 
            "*property*.jpg", "RELATIVE", "EXACT")
```

---

## Generate Block Adjustment Report (Data Management)

## Summary

Generates a report after performing an ortho mapping block adjustment on a mosaic dataset. The report is useful when evaluating the quality and accuracy of the ortho mapping products.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset path. | Mosaic Dataset; Mosaic Layer |
| Input Solution Table | The associated solution point table after block adjustment. | Table View |
| Input Solution Points | The solution point feature class. | Table View |
| Output Report | The output ortho mapping report file path and name. The supported output format for a website is HTML. | File |
| Input Control Points For Adjustment (Optional) | The associated control points table, which may include tie points and ground control points. | Table View |
| Report Format (Optional) | Specifies the output format of the block adjustment report.HTML—The adjustment report will be created in HTML format. This is the default.PDF—The adjustment report will be created in PDF format.JSON—The adjustment report will be created in JSON format.Brief JSON—The adjustment report will be created in an abbreviated JSON format. | String |
| input_mosaic_dataset | The input mosaic dataset path. | Mosaic Dataset; Mosaic Layer |
| input_solution_table | The associated solution point table after block adjustment. | Table View |
| input_solution_point | The solution point feature class. | Table View |
| output_report | The output ortho mapping report file path and name. The supported output format for a website is HTML. | File |
| input_control_point_for_adjustment(Optional) | The associated control points table, which may include tie points and ground control points. | Table View |
| report_format(Optional) | Specifies the output format of the block adjustment report.HTML—The adjustment report will be created in HTML format. This is the default.PDF—The adjustment report will be created in PDF format.JSON—The adjustment report will be created in JSON format.BRIEF JSON—The adjustment report will be created in an abbreviated JSON format. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateBlockAdjustmentReport(input_mosaic_dataset, input_solution_table, input_solution_point, output_report, {input_control_point_for_adjustment}, {report_format})
```

### Example 2

```python
import arcpy
arcpy.management.ComputeBlockAdjustment(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePoints",
     "POLYORDER1", "c:/BD/BD.gdb/redQB_solution")
```

### Example 3

```python
import arcpy
arcpy.management.ComputeBlockAdjustment(
     "c:/BD/BD.gdb/redQB", "c:/BD/BD.gdb/redQB_tiePoints",
     "POLYORDER1", "c:/BD/BD.gdb/redQB_solution")
```

### Example 4

```python
import arcpy
mdname = "c:/omproject/adjustedcollection.gdb/droneimgs"
solutiontbl = "c:/omproject/adjustedcollection.gdb/droneimgs_solutiontbl"
solutionpnt = "c:/omproject/adjustedcollection.gdb/droneimgs_solutionpnt"
controlpnt = "c:/omproject/adjustedcollection.gdb/droneimgs_tiepoints"
arcpy.management.GenerateBlockAdjustmentReport(
        mdname, solutiontbl, solutionpnt, "c:/omproject/adjustmentreport.pdf",
controlpnt, report_format="PDF")
```

### Example 5

```python
import arcpy
mdname = "c:/omproject/adjustedcollection.gdb/droneimgs"
solutiontbl = "c:/omproject/adjustedcollection.gdb/droneimgs_solutiontbl"
solutionpnt = "c:/omproject/adjustedcollection.gdb/droneimgs_solutionpnt"
controlpnt = "c:/omproject/adjustedcollection.gdb/droneimgs_tiepoints"
arcpy.management.GenerateBlockAdjustmentReport(
        mdname, solutiontbl, solutionpnt, "c:/omproject/adjustmentreport.pdf",
controlpnt, report_format="PDF")
```

---

## Generate Contingent Values (Data Management)

## Summary

Generates contingent values from an existing dataset using either the data values in the table or the domain values assigned to the fields. The tool creates two .csv files: one for field groups and one for contingent values.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table whose data or schema will be used to generate the contingent values. | Table View |
| Field Groups File | The output .csv file containing field group information. | File |
| Contingent Values File | The output .csv file containing contingent value information. | File |
| Mode (Optional) | Specifies the method that will be used to generate contingent values.Data— The values in the input table will be used to generate valid contingent values. This is the default.Schema— The product of the coded values of all domains assigned to the field group's fields will be used to generate all possible contingent value combinations. | String |
| Field Groups (Optional) | The field groups for which contingent values will be generated. If no value is provided, all field groups will be used. | String |
| in_table | The table whose data or schema will be used to generate the contingent values. | Table View |
| field_groups_file | The output .csv file containing field group information. | File |
| contingent_values_file | The output .csv file containing contingent value information. | File |
| mode(Optional) | Specifies the method that will be used to generate contingent values.DATA— The values in the input table will be used to generate valid contingent values. This is the default.SCHEMA— The product of the coded values of all domains assigned to the field group's fields will be used to generate all possible contingent value combinations. | String |
| field_groups[field_groups,...](Optional) | The field groups for which contingent values will be generated. If no value is provided, all field groups will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateContingentValues(in_table, field_groups_file, contingent_values_file, {mode}, {field_groups})
```

### Example 2

```python
import arcpy
arcpy.management.GenerateContingentValues(
    'C:\\location\\gdb.gdb\\table_1', 'c:\\temp\\fg.csv', 'c:\\temp\\cv.csv',
    ['field_group_1', 'field_group_3'], 'DATA')
```

### Example 3

```python
import arcpy
arcpy.management.GenerateContingentValues(
    'C:\\location\\gdb.gdb\\table_1', 'c:\\temp\\fg.csv', 'c:\\temp\\cv.csv',
    ['field_group_1', 'field_group_3'], 'DATA')
```

---

## Generate Definition Query From Selection (Data Management)

## Summary

Creates a definition query (in SQL format) from the selected features or rows of the layer or table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The layer or table view from which the definition query will be generated. | Table View |
| Generation Method (Optional) | Specifies the method that will be used to generate the definition query.Match selection—The input table's ObjectID field (or GlobalID field if present) will be used to generate the query with values present in the selection. This is the default.Use field values—The Field parameter value will be used to generate the query with values present in the selection. | String |
| Field (Optional) | The field in the table or table view that will be used to generate values for the query. This parameter is required when the Generation Method parameter is set to Use field values. | Field |
| Query Name (Optional) | The unique name of the query that will be generated. | String |
| Invert Where Clause (Optional) | Specifies whether the generated definition query (where clause) will be inverted and include all unselected values or include all selected values of the input table. Checked—The where clause will be inverted.Unchecked—The where clause will not be inverted. This is the default. | Boolean |
| Append The Active Query (Optional) | Specifies whether the generated definition query will be appended to the active query. This parameter is available when the layer or table has an active definition query and the Generation Method parameter is set to Use field values.Checked—The generated definition query will be appended to the active query.Unchecked—The generated definition query will not be appended to the active query. This is the default. | Boolean |
| Overwrite Where Clause (Optional) | Specifies whether the definition query (where clause) will be displayed and can be modified using the Where Clause parameter before it is generated.Checked—The where clause will be displayed and can be modified before it is generated.Unchecked—The where clause cannot be modified. This is the default. | Boolean |
| Where Clause (Optional) | The generated definition query SQL expression from the other parameter values. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |
| in_table | The layer or table view from which the definition query will be generated. | Table View |
| method(Optional) | Specifies the method that will be used to generate the definition query.MATCH_SELECTION—The input table's ObjectID field (or GlobalID field if present) will be used to generate the query with values present in the selection. This is the default.USE_FIELD_VALUES—The field parameter value will be used to generate the query with values present in the selection. | String |
| field(Optional) | The field in the table or table view that will be used to generate values for the query. This parameter is required when the method parameter is set to USE_FIELD_VALUES. | Field |
| query_name(Optional) | The unique name of the query that will be generated. | String |
| invert_where_clause(Optional) | Specifies whether the generated definition query (where clause) will be inverted and include all unselected values or include all selected values of the input table.INVERT—The where clause will be inverted.NON_INVERT—The where clause will not be inverted. This is the default. | Boolean |
| append_active_query(Optional) | Specifies whether the generated definition query will be appended to the active query. This parameter is available when a layer or table has an active definition query and the method parameter is set to USE_FIELD_VALUES.APPEND—The generated definition query will be appended to the active query.NOT_APPEND—The generated definition query will not be appended to the active query. This is the default. | Boolean |
| overwrite_where_clause(Optional) | Specifies whether the definition query (where clause) can be modified using the where_clause parameter before it is generated.OVERWRITE—The where clause can be modified before it is generated.NOT_OVERWRITE—The where clause cannot be modified. This is the default. | Boolean |
| where_clause(Optional) | The definition query that will override the other parameter values. For more information about SQL syntax, see SQL reference for query expressions used in ArcGIS. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.GenerateDefinitionQueryFromSelection(in_table, {method}, {field}, {query_name}, {invert_where_clause}, {append_active_query}, {overwrite_where_clause}, {where_clause})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"

# Set local variables
in_table = "WisconsinAirports.lyrx"
where_clause = '"TRAFFICCOUNT" >= \'100\''

# Run the tool
arcpy.management.GenerateDefinitionQueryFromSelection(
    in_table,
    "MATCH_SELECTION",
    "MyPythonQueryName",
    "NON_INVERT",
    "NOT_APPEND",
    "OVERWRITE",
    where_clause)
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"

# Set local variables
in_table = "WisconsinAirports.lyrx"
where_clause = '"TRAFFICCOUNT" >= \'100\''

# Run the tool
arcpy.management.GenerateDefinitionQueryFromSelection(
    in_table,
    "MATCH_SELECTION",
    "MyPythonQueryName",
    "NON_INVERT",
    "NOT_APPEND",
    "OVERWRITE",
    where_clause)
```

---

## Generate Exclude Area (Data Management)

## Summary

Masks pixels based on their color or by clipping a range of values. The output of this tool is used as an input to the Color Balance Mosaic Dataset tool to eliminate areas such as clouds and water that can skew the statistics used to color balance multiple images.

## Usage

- This tool is used to exclude areas that will be difficult to color correct, such as water, clouds, and anomalous areas.
- The output of this tool can be used in the Color Balance Mosaic Dataset tool to exclude pixels (and colors) from the algorithm used to color correct the mosaic dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster or mosaic dataset layer that you want to mask. | Mosaic Dataset; Raster Dataset; Raster Layer |
| Output Raster Dataset | The name, location and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a Compression type and Compression Quality within the Environment Settings. | Raster Dataset |
| Pixel Type | Choose the pixel depth of your input raster dataset. 8-bit is the default value; however, raster datasets with a greater bit-depth will need to have the color mask and histogram values scaled accordingly.8 bit—The input raster dataset has values from 0 to 255. This is the default.11 bit—The input raster dataset has values from 0 to 2047.12 bit—The input raster dataset has values from 0 to 4095.16 bit—The input raster dataset has values from 0 to 65535. | String |
| Generate Method | Create your mask based on the color of the pixels or by clipping high and low values.Color mask—Set the maximum color values to include in the output. This is the default.Histogram percentage—Remove a percentage of high and low pixel values. | String |
| Maximum Red (Optional) | The maximum red value to exclude. The default is 255. | Double |
| Maximum Green (Optional) | The maximum green value to exclude. The default is 255. | Double |
| Maximum Blue (Optional) | The maximum blue value to exclude. The default is 255. | Double |
| Maximum White (Optional) | The maximum white value to exclude. The default is 255. | Double |
| Maximum Black (Optional) | The maximum black value to exclude. The default is 0. | Double |
| Maximum Magenta (Optional) | The maximum magenta value to exclude. The default is 255. | Double |
| Maximum Cyan (Optional) | The maximum cyan value to exclude. The default is 255. | Double |
| Maximum Yellow (Optional) | The maximum yellow value to exclude. The default is 255. | Double |
| Low Percentage (Optional) | Exclude this percentage of the lowest pixel values. The default is 0. | Double |
| High Percentage (Optional) | Exclude this percentage of the highest pixel values. The default is 100. | Double |
| in_raster | The raster or mosaic dataset layer that you want to mask. | Mosaic Dataset; Raster Dataset; Raster Layer |
| out_raster | The name, location and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, or a geodatabase, you can specify a Compression type and Compression Quality within the Environment Settings. | Raster Dataset |
| pixel_type | Choose the pixel depth of your input raster dataset. 8-bit is the default value; however, raster datasets with a greater bit-depth will need to have the color mask and histogram values scaled accordingly.8_BIT—The input raster dataset has values from 0 to 255. This is the default.11_BIT—The input raster dataset has values from 0 to 2047.12_BIT—The input raster dataset has values from 0 to 4095.16_BIT—The input raster dataset has values from 0 to 65535. | String |
| generate_method | Create your mask based on the color of the pixels or by clipping high and low values.COLOR_MASK—Set the maximum color values to include in the output. This is the default.HISTOGRAM_PERCENTAGE—Remove a percentage of high and low pixel values. | String |
| max_red(Optional) | The maximum red value to exclude. The default is 255. | Double |
| max_green(Optional) | The maximum green value to exclude. The default is 255. | Double |
| max_blue(Optional) | The maximum blue value to exclude. The default is 255. | Double |
| max_white(Optional) | The maximum white value to exclude. The default is 255. | Double |
| max_black(Optional) | The maximum black value to exclude. The default is 0. | Double |
| max_magenta(Optional) | The maximum magenta value to exclude. The default is 255. | Double |
| max_cyan(Optional) | The maximum cyan value to exclude. The default is 255. | Double |
| max_yellow(Optional) | The maximum yellow value to exclude. The default is 255. | Double |
| percentage_low(Optional) | Exclude this percentage of the lowest pixel values. The default is 0. | Double |
| percentage_high(Optional) | Exclude this percentage of the highest pixel values. The default is 100. | Double |

## Code Samples

### Example 1

```python
arcpy.management.GenerateExcludeArea(in_raster, out_raster, pixel_type, generate_method, {max_red}, {max_green}, {max_blue}, {max_white}, {max_black}, {max_magenta}, {max_cyan}, {max_yellow}, {percentage_low}, {percentage_high})
```

### Example 2

```python
import arcpy
arcpy.GenerateExcludeArea_management("C:/workspace/fgdb.gdb/mosdata",
                               "C:/workspace/excludeArea.tif","8_BIT",
                               "COLOR_MASK","255","255","255","255","15",
                               "255","255","255","0","100")
```

### Example 3

```python
import arcpy
arcpy.GenerateExcludeArea_management("C:/workspace/fgdb.gdb/mosdata",
                               "C:/workspace/excludeArea.tif","8_BIT",
                               "COLOR_MASK","255","255","255","255","15",
                               "255","255","255","0","100")
```

### Example 4

```python
##===========================
##Generate Exclude Area
##Usage: GenerateExcludeArea_management in_raster out_raster 8_BIT | 11_BIT | 
##                                      12_BIT | 16_BIT COLOR_MASK | HISTOGRAM_PERCENTAGE
##                                      {max_red} {max_green} {max_blue} {max_white} 
##                                      {max_black} {max_magenta} {max_cyan}
##                                      {max_yellow} {percentage_low} {percentage_high}

import arcpy
arcpy.env.workspace = "c:/workspace"

# Generate exclude area dataset from raster dataset with Histogram
arcpy.GenerateExcludeArea_management("srcimage.tif", "exarea.tif", "8_BIT",
                                     "HISTOGRAM_PERCENTAGE", "", "", "", "",
                                     "", "", "", "", "10", "100")                                      

# Generate exclude area dataset from mosaic dataset with Color Mask
arcpy.GenerateExcludeArea_management("CC.gdb/srcmd", "exarea.tif", "8_BIT",
                                     "COLOR_MASK", "255", "200", "50", "255",
                                     "10", "210", "100", "255", "", "")
```

### Example 5

```python
##===========================
##Generate Exclude Area
##Usage: GenerateExcludeArea_management in_raster out_raster 8_BIT | 11_BIT | 
##                                      12_BIT | 16_BIT COLOR_MASK | HISTOGRAM_PERCENTAGE
##                                      {max_red} {max_green} {max_blue} {max_white} 
##                                      {max_black} {max_magenta} {max_cyan}
##                                      {max_yellow} {percentage_low} {percentage_high}

import arcpy
arcpy.env.workspace = "c:/workspace"

# Generate exclude area dataset from raster dataset with Histogram
arcpy.GenerateExcludeArea_management("srcimage.tif", "exarea.tif", "8_BIT",
                                     "HISTOGRAM_PERCENTAGE", "", "", "", "",
                                     "", "", "", "", "10", "100")                                      

# Generate exclude area dataset from mosaic dataset with Color Mask
arcpy.GenerateExcludeArea_management("CC.gdb/srcmd", "exarea.tif", "8_BIT",
                                     "COLOR_MASK", "255", "200", "50", "255",
                                     "10", "210", "100", "255", "", "")
```

---

## Generate File Geodatabase License (Data Management)

## Summary

Generates a license file (.sdlic) for displaying the contents in a licensed file geodatabase created by the Generate Licensed File Geodatabase tool.

## Usage

- The Allow Export of Vector Data parameter specifies whether an end user can export vector data in the licensed file geodatabase.
- The Expiration Date parameter defines an expiration date for the license file, after which the file geodatabase’s contents can no longer be displayed. This allows a set term for a licensed geodatabase. The default value is empty (blank), which means the data license file will never expire.
- You cannot individually license a feature class or table to produce a mixed state in which some feature classes or tables are licensed and others are not. However, a licensed file geodatabase allows you to add an unlicensed feature class or table through operations such as creating an empty feature class, copying and pasting, and importing.
- Once you generate a license file using this tool, you must add the license to the project. See Manage file geodatabase data licenses for instructions.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input License Definition File | The license definition file (.licdef) created by the Generate Licensed File Geodatabase tool. | File |
| Output Data License File | The license file (.sdlic) for distribution. | File |
| Allow Export of Vector Data (Optional) | Specifies whether the export of vector data will be allowed.Vector data cannot be exported—Vector data cannot be exported with the data license file (.sdlic) installed. This is the default.Allow export of vector data— Vector data can be exported with the data license file (.sdlic) installed. | String |
| Expiration Date(Optional) | The expiration date of the data license file, after which the file geodatabase’s contents can no longer be displayed. The default value is empty (blank), which means the data license file will never expire. | Date |
| in_lic_def_file | The license definition file (.licdef) created by the Generate Licensed File Geodatabase tool. | File |
| out_lic_file | The license file (.sdlic) for distribution. | File |
| allow_export(Optional) | Specifies whether the export of vector data will be allowed.DENY_EXPORT—Vector data cannot be exported with the data license file (.sdlic) installed. This is the default.ALLOW_EXPORT— Vector data can be exported with the data license file (.sdlic) installed. | String |
| exp_date(Optional) | The expiration date of the data license file, after which the file geodatabase’s contents can no longer be displayed. The default value is empty (blank), which means the data license file will never expire. | Date |

## Code Samples

### Example 1

```python
arcpy.management.GenerateFgdbLicense(in_lic_def_file, out_lic_file, {allow_export}, {exp_date})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.GenerateFgdbLicense("london.licdef", "london.sdlic", "ALLOW_EXPORT", "2013-09-26 18:35:54")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.GenerateFgdbLicense("london.licdef", "london.sdlic", "ALLOW_EXPORT", "2013-09-26 18:35:54")
```

### Example 4

```python
# Name: GenerateFgdbLicense.py
# Description: Use the GenerateFgdbLicense tool to generate a license file (*.sdlic) for a protected file geodatabase.

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
input_licdef = "london.licdef"
export = "DENY_EXPORT"
exp_date = "2013-09-26 18:35:54"
output_sdlic = "london.sdlic"

# Process: generate the license file
arcpy.management.GenerateFgdbLicense(input_licdef, output_sdlic, export, exp_date)
```

### Example 5

```python
# Name: GenerateFgdbLicense.py
# Description: Use the GenerateFgdbLicense tool to generate a license file (*.sdlic) for a protected file geodatabase.

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
input_licdef = "london.licdef"
export = "DENY_EXPORT"
exp_date = "2013-09-26 18:35:54"
output_sdlic = "london.sdlic"

# Process: generate the license file
arcpy.management.GenerateFgdbLicense(input_licdef, output_sdlic, export, exp_date)
```

---

## Generate ID Attribute Rule (Data Management)

## Summary

Creates an attribute rule that generates a unique value for a field from a query. The tool outputs a .csv file containing an ArcGIS Arcade attribute rule based on the inputs provided, a Python file that contains the code for generating sequences in the workspace, and an ID file for visualizing generated IDs.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The dataset that the new rule will be applied to. | Table View |
| Output Folder | The folder where the output .csv file will be saved. The name of the file will be taken from the Input Table parameter value. | Folder |
| Field | The name of the existing field the rule will be applied to. | Field |
| Expression (Optional) | An SQL expression that limits the rows that the attribute rule will be applied to. | SQL Expression |
| Create Sequences (Optional) | Specifies whether database sequences will be created in the specified workspace.Checked—Database sequences will be created. This is the default.Unchecked—Database sequences will not be created. | Boolean |
| Definition Type (Optional) | Specifies the method that will be used to define the format of the ID.Builder— The ID format will be constructed using the ID Builder parameter.Table—The ID format will be defined from a table using the ID Definition Table parameter.Code Values— A configuration entry and sequence will be generated for each combination of coded values from the selected fields.Builder— The ID format will be constructed using the id_builder parameter.Table—The ID format will be defined from a table using the id_table parameter.Code Values— A configuration entry and sequence will be generated for each combination of coded values from the selected fields. | String |
| ID Builder (Optional) | A set of format options that will define the attribute rule. This includes an SQL expression that will be used to apply different formats and sequences per asset, as well as padding, prefix, and suffix values. Filter SQL—An optional SQL expression that will be used to determine whether the sequence will be applied to the row. Description—An optional description of the entry that will be added to the rule settings in the attribute rule. Sequence Name—The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. Starting Value—The starting number of the sequence. If you do not provide a starting number, the sequence will start at 1. If you do provide a starting number, it must be greater than 0. Increment Value—Describes how the sequence numbers will be incremented. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence is 15, and the next value after that is 20. If you do not specify an increment value, sequence values will increment by 1. Prefix—An optional string or value that will be placed before the sequence value. Suffix—An optional string or value that will be placed after the sequence value. Padding—An optional positive number that represents the number of digits, where zeros will be used to fill the unused digits. For example, a value of 5 will represent the sequence value of 1 as 00001. Separator—An optional value that will be used to join the prefix, sequence, and suffix.A preview of the string will display in the Geoprocessing pane from the values entered in the format options. | Value Table |
| ID Definition Table (Optional) | A table or record set that defines the ID format. This includes the SQL expression that will be used to apply different formats and sequences per asset, as well as padding, prefix, and suffix values. The table must include the following fields: WhereClause—An optional SQL expression that will be used to determine whether the sequence will be applied to the row.Description—An optional description of the entry that will be added to the rule settings in the attribute rule. SequenceName—The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. StartingValue—The starting number of the sequence. If you do not provide a starting number, the sequence will start at 1. If specified, the starting number must be greater than 0. IncrementValue—Describes how the sequence numbers will increment. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence will be 15, and the next value after that will be 20. If you do not specify an increment value, sequence values will increment by 1. Prefix—An optional string or value that will be placed before the sequence value. Suffix—An optional string or value that will be placed after the sequence value. Padding—An optional positive number that represents the number of digits, where zeros will be used to fill the unused digits. For example, a value of 5 will represent the sequence value of 1 as 00001. Separator—An optional value that will be used to join the prefix, sequence, and suffix. | Record Set |
| Coded Value Fields (Optional) | The fields that will be used to generate an SQL expression and configuration for an ID. | Field |
| in_table | The dataset that the new rule will be applied to. | Table View |
| out_folder | The folder where the output .csv file will be saved. The name of the file will be taken from the in_table parameter value. | Folder |
| field | The name of the existing field the rule will be applied to. | Field |
| where_clause(Optional) | An SQL expression that limits the rows that the attribute rule will be applied to. | SQL Expression |
| create_seq(Optional) | Specifies whether database sequences will be created in the specified workspace.CREATE—Database sequences will be created. This is the default.NO_CREATE—Database sequences will not be created. | Boolean |
| definition_method(Optional) | Specifies the method that will be used to define the format of the ID. | String |
| id_builder[id_builder,...](Optional) | A set of format options that will define the attribute rule. This includes an SQL expression that will be used to apply different formats and sequences per asset, as well as padding, prefix, and suffix values. Filter SQL—An optional SQL expression that will be used to determine whether the sequence will be applied to the row. Description—An optional description of the entry that will be added to the rule settings in the attribute rule. Sequence Name—The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. Starting Value—The starting number of the sequence. If you do not provide a starting number, the sequence will start at 1. If you do provide a starting number, it must be greater than 0. Increment Value—Describes how the sequence numbers will be incremented. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence is 15, and the next value after that is 20. If you do not specify an increment value, sequence values will increment by 1. Prefix—An optional string or value that will be placed before the sequence value. Suffix—An optional string or value that will be placed after the sequence value. Padding—An optional positive number that represents the number of digits, where zeros will be used to fill the unused digits. For example, a value of 5 will represent the sequence value of 1 as 00001. Separator—An optional value that will be used to join the prefix, sequence, and suffix. | Value Table |
| id_table(Optional) | A table or record set that defines the ID format. This includes the SQL expression that will be used to apply different formats and sequences per asset, as well as padding, prefix, and suffix values. The table must include the following fields: WhereClause—An optional SQL expression that will be used to determine whether the sequence will be applied to the row.Description—An optional description of the entry that will be added to the rule settings in the attribute rule. SequenceName—The name that will be assigned to the database sequence. For enterprise geodatabases, the name must meet sequence name requirements for the database platform you're using and must be unique in the database. For file geodatabases, the name must be unique to the file geodatabase. StartingValue—The starting number of the sequence. If you do not provide a starting number, the sequence will start at 1. If specified, the starting number must be greater than 0. IncrementValue—Describes how the sequence numbers will increment. For example, if the sequence starts at 10 and the increment value is 5, the next value in the sequence will be 15, and the next value after that will be 20. If you do not specify an increment value, sequence values will increment by 1. Prefix—An optional string or value that will be placed before the sequence value. Suffix—An optional string or value that will be placed after the sequence value. Padding—An optional positive number that represents the number of digits, where zeros will be used to fill the unused digits. For example, a value of 5 will represent the sequence value of 1 as 00001. Separator—An optional value that will be used to join the prefix, sequence, and suffix. | Record Set |
| id_coded_value[id_coded_value,...](Optional) | The fields that will be used to generate an SQL expression and configuration for an ID. | Field |

## Code Samples

### Example 1

```python
arcpy.management.GenerateIDAttributeRule(in_table, out_folder, field, {where_clause}, {create_seq}, {definition_method}, {id_builder}, {id_table}, {id_coded_value})
```

### Example 2

```python
# Name: GenerateIDAttributeRule_Example.py
# Description: GenerateIDAttributeRule of a template attribute rule in a file geodatabase

# Import the system modules
import arcpy

# Set local variables
in_table = "C:/data/data.gdb"
out_folder = "C:/out/"
field = "ID"
where_clause = ""
create_seq = "CREATE"
definition_method = "BUILDER"
id_builder = "# # 2 2 # # # #"
id_table = r"in_memory\record_set1"
id_coded_value_fields = None

arcpy.management.GenerateIDAttributeRule(
   in_table,
   out_folder,
   field,
   where_clause,
   create_seq,
   definition_method,
   id_builder,
   id_table,
   coded_value_fields
)
```

### Example 3

```python
# Name: GenerateIDAttributeRule_Example.py
# Description: GenerateIDAttributeRule of a template attribute rule in a file geodatabase

# Import the system modules
import arcpy

# Set local variables
in_table = "C:/data/data.gdb"
out_folder = "C:/out/"
field = "ID"
where_clause = ""
create_seq = "CREATE"
definition_method = "BUILDER"
id_builder = "# # 2 2 # # # #"
id_table = r"in_memory\record_set1"
id_coded_value_fields = None

arcpy.management.GenerateIDAttributeRule(
   in_table,
   out_folder,
   field,
   where_clause,
   create_seq,
   definition_method,
   id_builder,
   id_table,
   coded_value_fields
)
```

---

## Generate Level Of Detail (Data Management)

## Summary

Generates a new scene layer package with properly defined levels of detail.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input integrated mesh scene layer package. | File |
| Output Dataset | The output scene layer package. | File |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package will be used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| in_dataset | The input integrated mesh scene layer package. | File |
| out_dataset | The output scene layer package. | File |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package will be used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the DESKTOP or NONE options. ALL—All texture formats will be optimized including JPEG, DXT, and KTX2 for use in desktop, web, and mobile platforms.DESKTOP—Windows, Linux, and Mac supported textures will be optimized including JPEG and DXT for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.MOBILE—Android and iOS supported textures will be optimized including JPEG and KTX2 for use in ArcGIS Maps SDKs mobile applications.NONE—JPEG textures will be optimized for use in desktop and web platforms. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateLevelOfDetail(in_dataset, out_dataset, {texture_optimization})
```

### Example 2

```python
import arcpy
arcpy.management.GenerateLevelOfDetail(
    in_dataset=r"C:\test\mesh.slpk",
    out_dataset=r"C:\test\meshLOD.slpk",
    texture_optimization="DESKTOP"
)
```

### Example 3

```python
import arcpy
arcpy.management.GenerateLevelOfDetail(
    in_dataset=r"C:\test\mesh.slpk",
    out_dataset=r"C:\test\meshLOD.slpk",
    texture_optimization="DESKTOP"
)
```

---

## Generate Licensed File Geodatabase (Data Management)

## Summary

Generates a license definition file (.licdef) that defines and restricts the display of contents in a file geodatabase. The contents of the licensed file geodatabase can be viewed by creating a license file (*.sdlic) and configuring the ArcGIS clients to recognize it. The license file is created using the Generate File Geodatabase License tool.

## Usage

- Licensing is ideally suited to mature datasets that will be shared or licensed under a use agreement and do not require further editing. The output licensed file geodatabase cannot be unlicensed to return it to its original unlicensed format.
- Once licensed, the output file geodatabase’s contents cannot be displayed in ArcGIS Pro until you create a license file (.sdlic) using the Generate File Geodatabase License tool and apply the license to each ArcGIS client.
- You cannot license individual feature classes or tables to produce a mixed state in which some feature classes or tables are licensed and others are not. However, a licensed file geodatabase allows you to add an unlicensed feature class or table through operations such as creating a new, empty feature class, copying and pasting, or importing a feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input File Geodatabase | The unlicensed file geodatabase that will be licensed. | Workspace |
| Output Licensed File Geodatabase | The name and location of the generated licensed file geodatabase. | Workspace |
| Output License Definition File | The license definition file. | File |
| in_fgdb | The unlicensed file geodatabase that will be licensed. | Workspace |
| out_fgdb | The name and location of the generated licensed file geodatabase. | Workspace |
| out_lic_def | The license definition file. | File |

## Code Samples

### Example 1

```python
arcpy.management.GenerateLicensedFgdb(in_fgdb, out_fgdb, out_lic_def)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/"
arcpy.management.GenerateFgdbLicense("london.gdb", "london_lic.gdb", "london.licdef")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/"
arcpy.management.GenerateFgdbLicense("london.gdb", "london_lic.gdb", "london.licdef")
```

### Example 4

```python
# Name: GenerateLicensedFileGeodatabase.py
# Description: Use the GenerateLicensedFgdb tool to license a file geodatabase

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"
out_geodatabase = "london_lic.gdb"
licdef = "london.licdef"

# Process: Restrict the data
arcpy.management.GenerateFgdbLicense(geodatabase, out_geodatabase, licdef)
```

### Example 5

```python
# Name: GenerateLicensedFileGeodatabase.py
# Description: Use the GenerateLicensedFgdb tool to license a file geodatabase

# import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"
out_geodatabase = "london_lic.gdb"
licdef = "london.licdef"

# Process: Restrict the data
arcpy.management.GenerateFgdbLicense(geodatabase, out_geodatabase, licdef)
```

---

## Generate Mapping Table (Data Management)

## Summary

Generates the Mapping Table based on a configured data loading workspace. The table includes a list of predefined datasets, fields, and attribute domain coded value descriptions. This output table is used as input to the Create Data Loading Workspace tool.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Data Reference Workbook | The Data Reference Workbook that will be used to generate a Mapping Table. | File |
| Output Table | The output table, which will include a list of datasets, fields, and attribute domain coded value descriptions based on the source and target mapping from a Data Loading Workspace. Use this table in the Create Data Loading Workspace tool to refine a future iteration of a Data Loading Workspace. | Table |
| in_workbook | The Data Reference Workbook that will be used to generate a Mapping Table. | File |
| out_table | The output table, which will include a list of datasets, fields, and attribute domain coded value descriptions based on the source and target mapping from a Data Loading Workspace. Use this table in the Create Data Loading Workspace tool to refine a future iteration of a Data Loading Workspace. | Table |

## Code Samples

### Example 1

```python
arcpy.management.GenerateMappingTable(in_workbook, out_table)
```

### Example 2

```python
import arcpy

arcpy.management.GenerateMappingTable(
    "C:/data/DataLoadingWorkspace/DataReference.xlsx", "C:/temp/Default.gdb/DataReference_GenerateMappingTable"
)
```

### Example 3

```python
import arcpy

arcpy.management.GenerateMappingTable(
    "C:/data/DataLoadingWorkspace/DataReference.xlsx", "C:/temp/Default.gdb/DataReference_GenerateMappingTable"
)
```

### Example 4

```python
# Name: GenerateMappingTable.py
# Description: Generate a Mapping Table from the input Data Reference workbook.

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"
mapping = "C:/temp/MappingTable.csv"

arcpy.management.GenerateMappingTable(in_workbook=workbook, out_table=mapping)
```

### Example 5

```python
# Name: GenerateMappingTable.py
# Description: Generate a Mapping Table from the input Data Reference workbook.

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"
mapping = "C:/temp/MappingTable.csv"

arcpy.management.GenerateMappingTable(in_workbook=workbook, out_table=mapping)
```

---

## Generate Point Cloud (Data Management)

## Summary

Generates 3D points from stereo pairs and outputs a point cloud as a set of LAS files.

## Usage

- If this tool is run multiple times with the same input parameters, the output may be slightly different due to random sampling.
- The order of the stereo pairs in the Number of Image Pairs parameter is first based on the values defined for the Adjustment Quality Threshold, GSD Difference Threshold, and Omega/Phi Difference Threshold parameters. Each of these thresholds will give a pair of images a score of 0 if it did not meet the threshold, or a score of 1 for each threshold that was met, for a maximum of 3. The highest scores will be ordered at the top of the priority list. Next, the Overlap Area Threshold value is taken into account for any pairs with the same scores, and the intersection angle between the image pairs will be used as the last criteria to order the list, when the larger intersection angle is ordered higher.
- If you want a specific pair to be used for point cloud generation, set a high value for this pair in the Use field of the stereo table. To open the stereo table, right-click the mosaic layer in the Contents pane and click Open > Stereo.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The input mosaic dataset, which must have completed the block adjustment process and have a stereo model.To block adjust the mosaic dataset, use the Apply Block Adjustment tool. To build a stereo model on the mosaic dataset, use the Build Stereo Model tool. | Mosaic Dataset; Mosaic Layer |
| Matching Method | Specifies the method that will be used to generate 3D points.References: Heiko Hirschmuller et al., "Memory Efficient Semi-Global Matching," ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume 1–3, (2012): 371–376. Hirschmuller, H. "Stereo Processing by Semiglobal Matching and Mutual Information." Pattern Analysis and Machine Intelligence, (2008). Extended terrain matching—Extended terrain matching, a feature-based stereo matching in which the Harris operator is used in detecting feature points, will be used as the matching method. Since fewer feature points are extracted, this method is fast and can be used for data with fewer terrain variations and less detail.Semiglobal matching—Semi-Global Matching, which produces points that are denser and have more detailed terrain information, will be used as the matching method. It can be used for images of urban areas. This method is more computational intensive than ETM.Multi-view image matching—Multi-view image matching, which is based on the SGM matching method followed by a fusion step in which the redundant depth estimations across a single stereo model are merged, will be used as the matching method. It produces dense 3D points and is computationally efficient. | String |
| Output LAS Folder | The folder used to store the output LAS files, including cloud storage.If this tool is run multiple times with the same input parameters, the output may be slightly different due to random sampling. | Folder |
| Output LAS Base Name | A string used as a prefix to formulate the output LAS file names. For example, if name is used as the base, the output files will be named name1.las, name2.las, and so on. | String |
| Maximum Object Size (in meter)(Optional) | A search radius within which surface objects, such as buildings or trees, will be identified. It is the linear size in map units. | Double |
| DSM Ground Spacing (in meter)(Optional) | The ground spacing, in meters, at which the 3D points will be generated.The default is five times the source image pixel size. | Double |
| Number of Image Pairs(Optional) | The maximum number of image pairs that an image can contribute to generate 3D points. The default value is a minimum of 8 image pairs.If the image is involved in more image pairs than specified, those image pairs will not be considered when constructing the 3D points. In this case, the tool will order the pairs based on the various threshold parameters specified in the tool. The pairs with the highest scores will be used to generate the points. | Double |
| Overlap Area Threshold(Optional) | The minimum overlap threshold area that is acceptable, which is a percentage of overlap between a pair of images. Image pairs with overlap areas smaller than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The range of values for the threshold is 0 to 1. The default threshold value is 0.5, which is equal to 50 percent. | Double |
| Adjustment Quality Threshold(Optional) | The minimum adjustment quality that is acceptable. The threshold value will be compared to the adjustment quality value that is stored in the stereo model. Image pairs with an adjustment quality less than the specified threshold will receive a score of 0 for this criteria and will descend in the ordered list. The range of values for the threshold is 0 to 1. The default value is 0.2, which is equal to 20 percent. | Double |
| GSD Difference Threshold(Optional) | The maximum allowable threshold for the ground sample distance (GSD) between two images in a pair. The resolution ratio between the two images will be compared to the threshold value. Image pairs with a ground sample ratio greater than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The default threshold ratio is 2. | Double |
| Omega/Phi Difference Threshold(Optional) | The maximum threshold for the difference between the Omega values and Phi values for the two image pairs. The Omega values and Phi values for the image pairs are compared. Image pairs with an Omega or a Phi difference greater than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The default threshold difference for each comparison is 8. | Double |
| in_mosaic_dataset | The input mosaic dataset, which must have completed the block adjustment process and have a stereo model.To block adjust the mosaic dataset, use the Apply Block Adjustment tool. To build a stereo model on the mosaic dataset, use the Build Stereo Model tool. | Mosaic Dataset; Mosaic Layer |
| matching_method | Specifies the method that will be used to generate 3D points.ETM—Extended terrain matching, a feature-based stereo matching in which the Harris operator is used in detecting feature points, will be used as the matching method. Since fewer feature points are extracted, this method is fast and can be used for data with fewer terrain variations and less detail.SGM—Semi-Global Matching, which produces points that are denser and have more detailed terrain information, will be used as the matching method. It can be used for images of urban areas. This method is more computational intensive than ETM.MVM—Multi-view image matching, which is based on the SGM matching method followed by a fusion step in which the redundant depth estimations across a single stereo model are merged, will be used as the matching method. It produces dense 3D points and is computationally efficient.References: Heiko Hirschmuller et al., "Memory Efficient Semi-Global Matching," ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Volume 1–3, (2012): 371–376. Hirschmuller, H. "Stereo Processing by Semiglobal Matching and Mutual Information." Pattern Analysis and Machine Intelligence, (2008). | String |
| out_folder | The folder used to store the output LAS files, including cloud storage.If this tool is run multiple times with the same input parameters, the output may be slightly different due to random sampling. | Folder |
| out_base_name | A string used as a prefix to formulate the output LAS file names. For example, if name is used as the base, the output files will be named name1.las, name2.las, and so on. | String |
| object_size(Optional) | A search radius within which surface objects, such as buildings or trees, will be identified. It is the linear size in map units. | Double |
| ground_spacing(Optional) | The ground spacing, in meters, at which the 3D points will be generated.The default is five times the source image pixel size. | Double |
| minimum_pairs(Optional) | The maximum number of image pairs that an image can contribute to generate 3D points. The default value is a minimum of 8 image pairs.If the image is involved in more image pairs than specified, those image pairs will not be considered when constructing the 3D points. In this case, the tool will order the pairs based on the various threshold parameters specified in the tool. The pairs with the highest scores will be used to generate the points. | Double |
| minimum_area(Optional) | The minimum overlap threshold area that is acceptable, which is a percentage of overlap between a pair of images. Image pairs with overlap areas smaller than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The range of values for the threshold is 0 to 1. The default threshold value is 0.5, which is equal to 50 percent. | Double |
| minimum_adjustment_quality(Optional) | The minimum adjustment quality that is acceptable. The threshold value will be compared to the adjustment quality value that is stored in the stereo model. Image pairs with an adjustment quality less than the specified threshold will receive a score of 0 for this criteria and will descend in the ordered list. The range of values for the threshold is 0 to 1. The default value is 0.2, which is equal to 20 percent. | Double |
| maximum_diff_gsd(Optional) | The maximum allowable threshold for the ground sample distance (GSD) between two images in a pair. The resolution ratio between the two images will be compared to the threshold value. Image pairs with a ground sample ratio greater than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The default threshold ratio is 2. | Double |
| maximum_diff_OP(Optional) | The maximum threshold for the difference between the Omega values and Phi values for the two image pairs. The Omega values and Phi values for the image pairs are compared. Image pairs with an Omega or a Phi difference greater than this threshold will receive a score of 0 for this criteria and will descend in the ordered list. The default threshold difference for each comparison is 8. | Double |

## Code Samples

### Example 1

```python
arcpy.management.GeneratePointCloud(in_mosaic_dataset, matching_method, out_folder, out_base_name, {object_size}, {ground_spacing}, {minimum_pairs}, {minimum_area}, {minimum_adjustment_quality}, {maximum_diff_gsd}, {maximum_diff_OP})
```

### Example 2

```python
import arcpy
arcpy.management.GeneratePointCloud('c:/data/BD.gdb/SpringMD', 'ETM',
                                    'c:/data/output', 'SpringLAS', '10')
```

### Example 3

```python
import arcpy
arcpy.management.GeneratePointCloud('c:/data/BD.gdb/SpringMD', 'ETM',
                                    'c:/data/output', 'SpringLAS', '10')
```

---

## Generate Points Along Lines (Data Management)

## Summary

Creates point features along lines or polygons.

## Usage

- Points can be placed at a fixed interval for all features, or along features by percentage of the feature's length. Points can also be placed using a field from the input; the field can represent a fixed interval for each feature or specific distances for each feature.Use the Point Placement parameter to specify the method that will be used to place the output points.
- The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.
- If the Add accumulated distance and sequence fields parameter is checked, the following fields will be added:ORIG_LEN—The accumulated distance along the line from the start point of the line to the point. Distance values are added in the units of the Input Features value's spatial reference.ORIG_SEQ—The sequence number for each point in the order of the points created from each input line.
- ORIG_LEN—The accumulated distance along the line from the start point of the line to the point. Distance values are added in the units of the Input Features value's spatial reference.
- ORIG_SEQ—The sequence number for each point in the order of the points created from each input line.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The line or polygon features that will be used to place points. | Feature Layer |
| Output Feature Class | The point feature class that will be created from the input features. | Feature Class |
| Point Placement | Specifies the method that will be used to place points.By Percentage—The Percentage parameter value will be used to place points along the features by percentage.By Distance—The Distance parameter value will be used to place points at fixed distances along the features. This is the default.By Distance Field—Field values from the Distance Field parameter value will be used to place points along the features. | String |
| Distance (Optional) | The interval from the beginning of the feature at which points will be placed.This parameter is active when the Point Placement parameter is set to By Distance. | Linear Unit |
| Percentage (Optional) | The percentage from the beginning of the feature at which points will be placed. For example, if a percentage of 40 is used, points will be placed at 40 percent and 80 percent of the feature's distance.This parameter is active when the Point Placement parameter is set to By Percentage. | Double |
| Include end points (Optional) | Specifies whether additional points will be included at the start point and end point of the feature.Checked—Additional points will be included at the start point and end point of the feature.Unchecked—No additional points will be included at the start point and end point of the feature. This is the default. | Boolean |
| Add accumulated distance and sequence fields(Optional) | Specifies whether the accumulated distance and sequence fields will be added to the output.Checked—The accumulated distance (ORIG_LEN) and sequence (ORIG_SEQ) fields will be added to the output. Distance values are added in the units of the Input Features value's spatial reference.Unchecked—The accumulated distance and sequence fields will not be added to the output. This is the default. | Boolean |
| Distance Field (Optional) | A field from the input features that will be used to place output points.If the field is a numeric type, the field value will be used to place points at that interval.If the field is a string type, the field values must be organized as a semicolon-delimited string of distances. Points will be placed at those distances.Field values that are zero or negative vales will be ignored. Field values that exceed the length of a feature will be ignored for that feature. The distances will be in the linear units of the input's spatial reference.This parameter is active when the Point Placement parameter is set to By Distance Field. | Field |
| Distance Method (Optional) | Specifies the measurement method that will be used to create the points.This parameter is active when the Point Placement parameter is set to By Distance.Planar—Points will be created using a planar method. Planar measurements use 2D Cartesian mathematics. This is the default.Geodesic—Points will be created using a geodesic method. Geodesic measurements calculate the distance between two points on the earth's surface. | String |
| Input_Features | The line or polygon features that will be used to place points. | Feature Layer |
| Output_Feature_Class | The point feature class that will be created from the input features. | Feature Class |
| Point_Placement | Specifies the method that will be used to place points.PERCENTAGE—The Percentage parameter value will be used to place points along the features by percentage.DISTANCE—The Distance parameter value will be used to place points at fixed distances along the features. This is the default.DISTANCE_FIELD—Field values from the Distance_Field parameter value will be used to place points. | String |
| Distance(Optional) | The interval from the beginning of the feature at which points will be placed.This parameter is active when the Point_Placement parameter is set to DISTANCE. | Linear Unit |
| Percentage(Optional) | The percentage from the beginning of the feature at which points will be placed. For example, if a percentage of 40 is used, points will be placed at 40 percent and 80 percent of the feature's distance.This parameter is active when the Point_Placement parameter is set to PERCENTAGE. | Double |
| Include_End_Points(Optional) | Specifies whether additional points will be included at the start point and end point of the feature.END_POINTS—Additional points will be included at the start point and end point of the feature.NO_END_POINTS—No additional points will be included at the start point and end point of the feature. This is the default. | Boolean |
| Add_Chainage_Fields(Optional) | Specifies whether the accumulated distance and sequence fields will be added to the output.ADD_CHAINAGE—The accumulated distance (ORIG_LEN) and sequence (ORIG_SEQ) fields will be added to the output. Distance values are added in the units of the Input_Features value's spatial reference.NO_CHAINAGE—The accumulated distance and sequence fields will not be added to the output. This is the default. | Boolean |
| Distance_Field(Optional) | A field from the input features that will be used to place output points.If the field is a numeric type, the field value will be used to place points at that interval.If the field is a string type, the field values must be organized as a semicolon-delimited string of distances. Points will be placed at those distances.Field values that are zero or negative vales will be ignored. Field values that exceed the length of a feature will be ignored for that feature. The distances will be in the linear units of the input's spatial reference.This parameter is active when the Point_Placement parameter is set to DISTANCE_FIELD. | Field |
| Distance_Method(Optional) | Specifies the measurement method that will be used to create the points.This parameter is active when the Point_Placement parameter is set to DISTANCE.PLANAR—Points will be created using a planar method. Planar measurements use 2D Cartesian mathematics. This is the default.GEODESIC—Points will be created using a geodesic method. Geodesic measurements calculate the distance between two points on the earth's surface. | String |

## Code Samples

### Example 1

```python
arcpy.management.GeneratePointsAlongLines(Input_Features, Output_Feature_Class, Point_Placement, {Distance}, {Percentage}, {Include_End_Points}, {Add_Chainage_Fields}, {Distance_Field}, {Distance_Method})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/data/base.gdb'
arcpy.management.GeneratePointsAlongLines('rivers', 'distance_intervals', 'DISTANCE', Distance='500 meters')
arcpy.management.GeneratePointsAlongLines('rivers', 'percentage_intervals', 'PERCENTAGE', Percentage=10)
arcpy.management.GeneratePointsAlongLines('rivers', 'distance_by_field', 'DISTANCE_FIELD', Distance_Field='distance')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/data/base.gdb'
arcpy.management.GeneratePointsAlongLines('rivers', 'distance_intervals', 'DISTANCE', Distance='500 meters')
arcpy.management.GeneratePointsAlongLines('rivers', 'percentage_intervals', 'PERCENTAGE', Percentage=10)
arcpy.management.GeneratePointsAlongLines('rivers', 'distance_by_field', 'DISTANCE_FIELD', Distance_Field='distance')
```

### Example 4

```python
# Description: Convert point features to line features

import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/data/base.gdb'

# Set local variables
in_features = 'rivers'
out_fc_1 = 'distance_intervals'
out_fc_2 = 'percentage_intervals'

# Run GeneratePointsAlongLines by distance
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_1, 'DISTANCE',
                                          Distance='500 meters')

# Run GeneratePointsAlongLines by percentage
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_2, 'PERCENTAGE',
                                          Percentage=10,
                                          Include_End_Points='END_POINTS')

# Run GeneratePointsAlongLines by distance field
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_2, 'DISTANCE_FIELD',
                                          Distance_Field='distance')
```

### Example 5

```python
# Description: Convert point features to line features

import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/data/base.gdb'

# Set local variables
in_features = 'rivers'
out_fc_1 = 'distance_intervals'
out_fc_2 = 'percentage_intervals'

# Run GeneratePointsAlongLines by distance
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_1, 'DISTANCE',
                                          Distance='500 meters')

# Run GeneratePointsAlongLines by percentage
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_2, 'PERCENTAGE',
                                          Percentage=10,
                                          Include_End_Points='END_POINTS')

# Run GeneratePointsAlongLines by distance field
arcpy.management.GeneratePointsAlongLines(in_features, out_fc_2, 'DISTANCE_FIELD',
                                          Distance_Field='distance')
```

---

## Generate Raster Collection (Data Management)

## Summary

Performs batch analysis or processing on image collections contained in a mosaic dataset. The images in the input mosaic dataset can be processed individually or as groups.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Raster Collection | The full path of the mosaic dataset to be created. The mosaic dataset must be stored in a geodatabase. | Mosaic Dataset |
| Collection Builder | The input image collection. It can be seen as a template that contains arguments such as the source mosaic dataset path, filters to extract a subset from the input data source, and so on.Currently, this tool only supports Simple Collection, which allows you to define a single data source and a query filter, for the data source.Simple collection—Allows you to define a data source and a query filter. | String |
| Collection Builder Arguments | The list of arguments to create a subset collection of the mosaic dataset.This tool only supports the data source and filter to subset the mosaic dataset. The Data Source and Where Clause values must be completed, otherwise the tool cannot be executed.Data Source—The path of the data source.Where Clause—The filter used to subset the mosaic dataset. | Value Table |
| Input Raster Function (Optional) | The path to a raster function template file (.rft.xml or .rft.json). The raster function template will be applied to every item in the input mosaic dataset. The Function Editor can be used to create the template. If no RFT is defined, this tool will create the output mosaic based on the Collection Builder Arguments parameter. | String; File |
| Raster Function Arguments (Optional) | The parameters associated with the function chain. For example, if the function chain applies the NDVI function, set the visible and infrared IDs. The raster variable name of the RFT should be the Tag field value in the input data source. | Value Table |
| Raster Collection Properties (Optional) | The output mosaic dataset key properties.The key metadata properties that are available is based on the type of sensor that captured the imagery. Some examples of key metadata properties include the following:SensorNameProductName AcquisitionDate CloudCoverSunAzimuth SunElevation SensorAzimuth SensorElevation Off-nadirAngle BandNameMinimumWavelength MaximumWavelength RadianceGain RadianceBias SolarIrradiance ReflectanceGain ReflectanceBias | Value Table |
| Generate Rasters (Optional) | Generate raster dataset files of the mosaic dataset items, after the application of the RFT. Unchecked—The processing defined by the raster function template will be appended to the image items from the input data source to produce an image item in the output mosaic dataset. This is the default.Checked—Create raster datasets on disk. You will also need to specify the Output Raster Workspace and Format. | Boolean |
| Output Raster Workspace (Optional) | Defines the output location for the persisted raster datasets, if the Generate Rasters parameter is checked on. The naming convention for the output raster files is oid_<oid#>_<Unique_GUID>. | Folder; String |
| Format(Optional) | The format type of the raster to be generated.Tiff—Tagged Image File Format (TIFF)ERDAS IMAGINE—ERDAS IMAGINE fileCRF—Cloud Raster Format. This is the default.MRF—Meta Raster Format | String |
| Output Base Name(Optional) | Defines the output base name of the persisted raster datasets, if the Generate Rasters parameter is checked on. Multiple raster dataset outputs will have a number alias appended to their base name. The resulting mosaic dataset will reference the CRF directly without maintaining the raster function chain. | String |
| out_raster_collection | The full path of the mosaic dataset to be created. The mosaic dataset must be stored in a geodatabase. | Mosaic Dataset |
| collection_builder | The input image collection. It can be seen as a template that contains arguments such as the source mosaic dataset path, filters to extract a subset from the input data source, and so on.Currently, this tool only supports SIMPLE_COLLECTION, which allows you to define a single data source and a query filter for the data source.SIMPLE_COLLECTION—Allows you to define a data source and a query filter. | String |
| collection_builder_arguments[[Name, Value],...] | The list of arguments to create a subset collection of the mosaic dataset.This tool only supports the data source and filter to subset the mosaic dataset. The DataSource and WhereClause values must be completed, otherwise the tool cannot be executed.DataSource—The path of the data source.WhereClause—The filter used to subset the mosaic dataset. | Value Table |
| raster_function(Optional) | The path to a raster function template file (.rft.xml or .rft.json). The raster function template will be applied to every item in the input mosaic dataset. The Function Editor can be used to create the template. If no RFT is defined, this tool will create the output mosaic based on the collection_builder_arguments parameter. | String; File |
| raster_function_arguments[[Name, Value],...](Optional) | The parameters associated with the function chain. For example, if the function chain applies the NDVI function, set the visible and infrared IDs. The raster variable name of the RFT should be the Tag field value in the input data source. | Value Table |
| collection_properties[[Name, Value],...](Optional) | The output mosaic dataset key properties.The key metadata properties that are available is based on the type of sensor that captured the imagery. Some examples of key metadata properties include the following:SensorNameProductName AcquisitionDate CloudCoverSunAzimuth SunElevation SensorAzimuth SensorElevation Off-nadirAngle BandNameMinimumWavelength MaximumWavelength RadianceGain RadianceBias SolarIrradiance ReflectanceGain ReflectanceBias | Value Table |
| generate_rasters(Optional) | Choose to generate raster dataset files of the mosaic dataset items, after the application of the RFT. NO_GENERATE_RASTERS—The processing defined by the raster function template will be appended to the image items from the input data source to produce an image item in the output mosaic dataset. This is the default.GENERATE_RASTERS—Create raster datasets on disk. You will also need to specify the out_workspace and format. | Boolean |
| out_workspace(Optional) | Defines the output location for the persisted raster datasets, if the generate_rasters parameter is set to GENERATE_RASTERS. The naming convention for the output raster files is oid_<oid#>_<Unique_GUID>. | Folder; String |
| format(Optional) | The format type of the raster to be generated.TIFF—Tagged Image File Format (TIFF)IMAGINE Image—ERDAS IMAGINE fileCRF—Cloud Raster Format. This is the default.MRF—Meta Raster Format | String |
| out_base_name(Optional) | Defines the output base name for the persisted raster datasets, if the generate_rasters parameter is set to GENERATE_RASTERS. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateRasterCollection(out_raster_collection, collection_builder, collection_builder_arguments, {raster_function}, {raster_function_arguments}, {collection_properties}, {generate_rasters}, {out_workspace}, {format}, {out_base_name})
```

### Example 2

```python
import arcpy
arcpy.GenerateRasterCollection_management(
    out_raster_collection="c:/temp/FGDB.gdb/testgencollection", 
    collection_builder="Simple Collection", 
    collection_builder_arguments="# DataSource c:\temp\FGDB.gdb\qb_portland;# 
WhereClause 'Tag = 'MS''", 
    raster_function="C:/temp/NDVI_test.rft.json", 
    raster_function_arguments="# MS #;# VisibleBandID_20171019_7337_958 1;# 
InfraredBandID_20171019_7337_958 4", 
    collection_properties="", 
    generate_rasters="GENERATE_RASTERS", 
    out_workspace="c:/temp/persistedoutput", 
    format="CRF"
)
```

### Example 3

```python
import arcpy
arcpy.GenerateRasterCollection_management(
    out_raster_collection="c:/temp/FGDB.gdb/testgencollection", 
    collection_builder="Simple Collection", 
    collection_builder_arguments="# DataSource c:\temp\FGDB.gdb\qb_portland;# 
WhereClause 'Tag = 'MS''", 
    raster_function="C:/temp/NDVI_test.rft.json", 
    raster_function_arguments="# MS #;# VisibleBandID_20171019_7337_958 1;# 
InfraredBandID_20171019_7337_958 4", 
    collection_properties="", 
    generate_rasters="GENERATE_RASTERS", 
    out_workspace="c:/temp/persistedoutput", 
    format="CRF"
)
```

---

## Generate Raster From Raster Function (Data Management)

## Summary

Generates a raster dataset from an input raster function or function chain.

## Usage

- The tool is designed for raster processing using multiple threads to help speed up the processing.
- The output raster format can be TIFF, GRID, ERDAS IMAGINE, CRF (Cloud Raster Format), or MRF (Meta Raster Format).

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Function | The name of a raster function, raster function JSON object, or function chain (in .rft.xml format). | File; String |
| Output Raster Dataset | The output raster dataset. | Raster Dataset |
| Raster Function Arguments (Optional) | The parameters associated with the function chain. For example, if the function chain applies the Hillshade raster function, set the data source, azimuth, and altitude. | Value Table |
| Raster Properties (Optional) | The output raster dataset key properties, such as the sensor or wavelength. | Value Table |
| Format (Optional) | The output raster format.The default format will be derived from the file extension specified in the Output Raster Dataset value. TIFF—Tagged Image File Format for raster datasets will be used.Cloud Optimized GeoTIFF—Cloud Optimized GeoTIFF format will be used.ERDAS IMAGINE file—ERDAS IMAGINE raster data format will be used.Esri Grid—Esri Grid raster dataset format will be used.CRF—Cloud Raster Format will be used.MRF—Meta Raster Format will be used. | String |
| Process as Multidimensional (Optional) | Specifies whether the input mosaic dataset will be processed as a multidimensional raster dataset.Unchecked—The input will not be processed as a multidimensional raster dataset. If the input is multidimensional, only the slice that is currently displayed will be processed. This is the default. Checked—The input will be processed as a multidimensional raster dataset and all slices will be processed to produce a new multidimensional raster dataset. Set the Format parameter to Cloud raster format to use this option. | Boolean |
| raster_function | The name of a raster function, raster function JSON object, or function chain (in .rft.xml format). | File; String |
| out_raster_dataset | The output raster dataset. | Raster Dataset |
| raster_function_arguments[[Name, Value],...](Optional) | The parameters associated with the function chain. For example, if the function chain applies the Hillshade raster function, set the data source, azimuth, and altitude. | Value Table |
| raster_properties[[Name, Value],...](Optional) | The output raster dataset key properties, such as the sensor or wavelength. | Value Table |
| format(Optional) | The output raster format.The default format will be derived from the file extension specified in the output_raster_dataset value. TIFF—Tagged Image File Format for raster datasets will be used.Cloud Optimized GeoTIFF—Cloud Optimized GeoTIFF will be used.IMAGINE Image—ERDAS IMAGINE raster data format will be used.Esri Grid—Esri Grid raster dataset format will be used.CRF—Cloud Raster Format will be used.MRF—Meta Raster Format will be used. | String |
| process_as_multidimensional(Optional) | Specifies whether the input mosaic dataset will be processed as a multidimensional raster dataset.CURRENT_SLICE—The input will not be processed as a multidimensional raster dataset. If the input is multidimensional, only the slice that is currently displayed will be processed. This is the default.ALL_SLICES—The input will be processed as a multidimensional raster dataset and all slices will be processed to produce a new multidimensional raster dataset. Set the format parameter to CRF to use this option. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.GenerateRasterFromRasterFunction(raster_function, out_raster_dataset, {raster_function_arguments}, {raster_properties}, {format}, {process_as_multidimensional})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"C:\PrjWorkspace"

arcpy.GenerateRasterFromRasterFunction_management(
	raster_function="NDVI",
	out_raster_dataset="c:/temp/ndvitest.crf", 
	raster_function_arguments="Raster \\\\somemachine\\data\\test.tif; VisibleBandID 3;InfraredBandID 4", 
	format="CRF")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"C:\PrjWorkspace"

arcpy.GenerateRasterFromRasterFunction_management(
	raster_function="NDVI",
	out_raster_dataset="c:/temp/ndvitest.crf", 
	raster_function_arguments="Raster \\\\somemachine\\data\\test.tif; VisibleBandID 3;InfraredBandID 4", 
	format="CRF")
```

### Example 4

```python
## Generate raster from Band Arithmetic raster function where method is set to SAVI.

arcpy.management.GenerateRasterFromRasterFunction(
	r"C:\Projects\SAVI.rft.xml", r"C:\Projects\Portland_SAVI.tif", 
	r"Raster C:\Projects\PortlandIKONOS.tif;Method SAVI;'Band Indexes' '4 3 0.33'", 
	None, "TIFF", "CURRENT_SLICE")
```

### Example 5

```python
## Generate raster from Band Arithmetic raster function where method is set to SAVI.

arcpy.management.GenerateRasterFromRasterFunction(
	r"C:\Projects\SAVI.rft.xml", r"C:\Projects\Portland_SAVI.tif", 
	r"Raster C:\Projects\PortlandIKONOS.tif;Method SAVI;'Band Indexes' '4 3 0.33'", 
	None, "TIFF", "CURRENT_SLICE")
```

---

## Generate Rectangles Along Lines (Data Management)

## Summary

Creates a series of rectangular polygons that follow a single linear feature or a group of linear features.

## Usage

- ID—An incremental number assigned to each feature.
- GroupId—An integer assigned to all the features in a connecting chain or group of line features. Generate Rectangles Along Lines results can include multiple groups. Usually, a single group will originate from a connected set of line features or from a single line with multiple, unconnected parts. Features created from individual, unconnected line features will be assigned to separate groups. GroupId values are unique and are derived from the ObjectID of the first line feature in the given chain.
- SeqId—An incremental number assigned to each feature based on the creation order for the feature in each group.
- Previous—A reference to the previous SeqId in each group except for the first record in each group. The first record could reference a neighboring element outside the group.
- Next—A reference to the next SeqId in each group except for the last record in each group. The last record could reference a neighboring element outside the group.
- Angle—Values are calculated against the default orientation of a map where 0 degrees is due North, 90 degrees is due East, 180 degrees is due South and -90 degrees is due West.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Line Features | The input polyline features defining the path of the features. | Feature Layer |
| Output Feature Class | The output polygon feature class. | Feature Class |
| Length Along the Line (Optional) | The length of the output polygon features along the input line features. The default value is determined by the spatial reference of the input line features. This value will be 1/100 of the input feature class extent along the x-axis. | Linear Unit |
| Length Perpendicular to the Line(Optional) | The length of the output polygon features perpendicular to the input line features. The default value is determined by the spatial reference of the input line features. This value will be one-half the number used for the length along the line. | Linear Unit |
| Spatial Sort Method(Optional) | Output features are created in a sequential order and require a spatial starting point. Setting the direction type to upper right will start the output features in the upper right of each input feature.Upper right—Features start in the upper right corner. This is the default.Upper left—Features start in the upper left corner.Lower right—Features starts in the lower right corner.Lower left—Features starts in the lower left corner. | String |
| in_features | The input polyline features defining the path of the features. | Feature Layer |
| out_feature_class | The output polygon feature class. | Feature Class |
| length_along_line(Optional) | The length of the output polygon features along the input line features. The default value is determined by the spatial reference of the input line features. This value will be 1/100 of the input feature class extent along the x-axis. | Linear Unit |
| length_perpendicular_to_line(Optional) | The length of the output polygon features perpendicular to the input line features. The default value is determined by the spatial reference of the input line features. This value will be one-half the number used for the length along the line. | Linear Unit |
| spatial_sort_method(Optional) | Output features are created in a sequential order and require a spatial starting point. Setting the direction type to upper right will start the output features in the upper right of each input feature.UR—Features start in the upper right corner. This is the default.UL—Features start in the upper left corner.LR—Features starts in the lower right corner.LL—Features starts in the lower left corner. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateRectanglesAlongLines(in_features, out_feature_class, {length_along_line}, {length_perpendicular_to_line}, {spatial_sort_method})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"C:\data\US_Streams.gdb"
arcpy.management.GenerateRectanglesAlongLines("StreamReach", "riparian_zones", 
                                              "250 Meters", "180 Meters", "UR")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"C:\data\US_Streams.gdb"
arcpy.management.GenerateRectanglesAlongLines("StreamReach", "riparian_zones", 
                                              "250 Meters", "180 Meters", "UR")
```

---

## Generate Schema Report (Data Management)

## Summary

Generates an Excel, JSON, PDF, or HTML representation of the geodatabase schema. These formats are output to a target destination folder.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The workspace, feature dataset, feature layer, or table view that will be used to generate the schema report. | Workspace; Feature Dataset; Feature Layer; Table View |
| Output Location | The folder where the report will be created. | Folder |
| Name | The name of the file outputs. | String |
| Output Formats | Specifies the file types that will be included in the output folder.JSON—The output folder will include a .json file.PDF—The output folder will include a .pdf file.HTML—The output folder will include an .html file.XLSX—The output folder will include an Excel .xlsx file. | String |
| in_dataset | The workspace, feature dataset, feature layer, or table view that will be used to generate the schema report. | Workspace; Feature Dataset; Feature Layer; Table View |
| out_location | The folder where the report will be created. | Folder |
| name | The name of the file outputs. | String |
| formats[formats,...] | Specifies the file types that will be included in the output folder.JSON—The output folder will include a .json file.PDF—The output folder will include a .pdf file.HTML—The output folder will include an .html file.XLSX—The output folder will include an Excel .xlsx file. | String |

## Code Samples

### Example 1

```python
arcpy.management.GenerateSchemaReport(in_dataset, out_location, name, formats)
```

### Example 2

```python
import arcpy
arcpy.management.GenerateSchemaReport("C:/MyProject/MyGDB.gdb", "C:/MyProject/My_folder", "schema_report", ["JSON", "PDF"])
```

### Example 3

```python
import arcpy
arcpy.management.GenerateSchemaReport("C:/MyProject/MyGDB.gdb", "C:/MyProject/My_folder", "schema_report", ["JSON", "PDF"])
```

### Example 4

```python
# Name: GenerateSchemaReport_Example.py
# Description: GenerateSchemaReport of a file geodatabase

# Import the system modules
import arcpy

# Set local variables
gdbWorkspace = "C:/data/data.gdb"

arcpy.management.GenerateSchemaReport(gdbWorkspace, "C:/MyProject/My_folder", "schema_report", ["JSON", "PDF"])
```

### Example 5

```python
# Name: GenerateSchemaReport_Example.py
# Description: GenerateSchemaReport of a file geodatabase

# Import the system modules
import arcpy

# Set local variables
gdbWorkspace = "C:/data/data.gdb"

arcpy.management.GenerateSchemaReport(gdbWorkspace, "C:/MyProject/My_folder", "schema_report", ["JSON", "PDF"])
```

---

## Generate Spatial Join Attribute Rule (Data Management)

## Summary

Generates a .csv file with an attribute rule for an input based on field values queried from one or more feature classes.

## Usage

- Add and remove fields from the fields list, reorder the fields list, and rename fields.
- The default data type of an output field is the same as the data type of the first input field (of that name) it encounters. You can change the data type to another valid data type.
- Use an action to determine how the values from one or multiple input fields will be merged into a single output field. The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.
- When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters.
- Standard Deviation is not a valid option for single input values.
- Use the Export option to save a field map as a .fieldmap file.
- Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.
- Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the output field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.
- Fields can also be mapped in Python script.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The feature class from which the attribute rule will be created. | Feature Layer |
| Output Folder | The folder where the output .csv file will be saved. The name of the file will use the name of the Input Table parameter value. | Folder |
| Join Classes | The attributes from the join features that will be joined to the attributes of the input features. | Feature Layer |
| Field Map | Controls the transfer or mapping of fields from the Join Classes parameter values to the Input Table parameter value. | Field Mappings |
| Expression(Optional) | An SQL expression that will be used to limit the features from the input that will participate in the attribute rule. | SQL Expression |
| Search Options(Optional) | Defines spatial queries between the features from the Input Table and Join Classes parameter values. The geometry type, spatial operator, and spatial distance can be configured to define matches between each join class and the input table. Join Class—The name of the join class the spatial query will run on.Input Geometry Type—The portion of the input geometry that will be used to query the join class. Spatial Operator—The spatial operation that will be used in the query.Search Distance—The distance from the geometry that will be included in the query. Available input geometry operators are as follows:Geometry—The complete geometry of the input feature. This is default.Start—The first vertex within a polyline feature. This option is only supported for polyline features.End—The last vertex within a polyline feature. This option is only supported for polyline features.Centroid—The geometric center of the input feature. Available spatial operators are as follows:Intersects—Features in the join class will be matched if they intersect with an input feature. This is default.Crosses—Features in the join class will be matched if they cross with an input feature.Contains—Features in the join class will be matched if an input feature contains them. This is the opposite of the Within option.Envelope_Intersects—Features in the join class will be matched if their bounding boxes (envelopes) intersect with the bounding box of an input feature.Overlaps—Features in the join class will be matched if they overlap with an input feature. The join class features are not completely contained by the input features.Touches—Features in the join class will be matched if they have a boundary that touches an input feature. When the input and join features are polylines or polygons, the boundary of the join feature can only touch the boundary of the input feature and no part of the join feature can cross the boundary of the input feature.Within—Features in the join class will be matched if an input feature is within them. This is the opposite of the Contains option. | Value Table |
| in_table | The feature class from which the attribute rule will be created. | Feature Layer |
| out_folder | The folder where the output .csv file will be saved. The name of the file will use the name of the in_table parameter value. | Folder |
| join_classes[join_classes,...] | The attributes from the join features that will be joined to the attributes of the input features. | Feature Layer |
| field_map | Controls the transfer or mapping of fields from the join_classes parameter values to the in_table parameter value. | Field Mappings |
| where_clause(Optional) | An SQL expression that will be used to limit the features from the input that will participate in the attribute rule. | SQL Expression |
| search_options[search_options,...](Optional) | Defines spatial queries between the features from the in_table and join_classes parameter values. The geometry type, spatial operator, and spatial distance can be configured to define matches between each join class and the input table. Join Class—The name of the join class the spatial query will run on.Input Geometry Type—The portion of the input geometry that will be used to query the join class. Spatial Operator—The spatial operation that will be used in the query.Search Distance—The distance from the geometry that will be included in the query. Available input geometry operators are as follows:GEOMETRY—The complete geometry of the input feature. This is default.START—The first vertex within a polyline feature. This option is only supported for polyline features.END—The last vertex within a polyline feature. This option is only supported for polyline features.CENTROID—The geometric center of the input feature. Available spatial operators are as follows:INTERSECTS—Features in the join class will be matched if they intersect with an input feature. This is default.CROSSES—Features in the join class will be matched if they cross with an input feature.CONTAINS—Features in the join class will be matched if an input feature contains them. This is the opposite of the WITHIN option.ENVELOPE_INTERSECTS—Features in the join class will be matched if their bounding boxes (envelopes) intersect with the bounding box of an input feature.OVERLAPS—Features in the join class will be matched if they overlap with an input feature. The join class features are not completely contained by the input features.TOUCHES—Features in the join class will be matched if they have a boundary that touches an input feature. When the input and join features are polylines or polygons, the boundary of the join feature can only touch the boundary of the input feature and no part of the join feature can cross the boundary of the input feature.WITHIN—Features in the join class will be matched if an input feature is within them. This is the opposite of the CONTAINS option. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.GenerateSpatialJoinAttributeRule(in_table, out_folder, join_classes, field_map, {where_clause}, {search_options})
```

### Example 2

```python
import arcpy
arcpy.management.GenerateSpatialJoinAttributeRule( 
    in_table=r"C:\temp\test.gdb\in_featureclass", 
    out_folder=r"C:\temp", 
    join_classes=["layer_one", "layer_two"], 
    where_clause="select * from X" 
)
```

### Example 3

```python
import arcpy
arcpy.management.GenerateSpatialJoinAttributeRule( 
    in_table=r"C:\temp\test.gdb\in_featureclass", 
    out_folder=r"C:\temp", 
    join_classes=["layer_one", "layer_two"], 
    where_clause="select * from X" 
)
```

---

## Generate Symbol Rotation Attribute Rule (Data Management)

## Summary

Generates an attribute rule specific to symbol rotation logic. The tool outputs a .csv file containing an ArcGIS Arcade attribute rule based on the inputs provided.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The point feature class that the new rule will be applied to. | Feature Layer |
| Output Folder | The folder where the output .csv file will be saved. The name of the file will be the same as the Input Table parameter value. | String |
| Rotation Field | The name of an existing field to which the rule will be applied. | Field |
| Intersecting Line Classes | The line feature classes that will be used to intersect the point feature class. The orientation field is a numeric field that is used as a tie breaker when there are multiple intersecting lines. | Value Table |
| Expression (Optional) | A SQL expression that limits the features the attribute rule will be applied to. | SQL Expression |
| Rotation Options (Optional) | The rotation options for the point feature class.Rotation option components are as follows:Point Class Expression—A SQL query that will further limit the features the attribute rule will be applied to. The default is none.Rotation Style—Specifies how the direction will be measured.Arithmetic—The rotation will start at zero in the east direction and is calculated counterclockwise. This is the default.Geographic—The rotation will start at zero in the north direction and is calculated clockwise.Rotate Towards—Specifies whether the feature will be oriented toward the maximum or minimum value. Minimum—The feature will be oriented toward the minimum value. This is the default.Maximum—The feature will be oriented toward the maximum value.Additional Rotation—The rotation value that will be added to the calculated rotation value. The default is 0. | Value Table |
| in_table | The point feature class that the new rule will be applied to. | Feature Layer |
| out_folder | The folder where the output .csv file will be saved. The name of the file will be the same as the in_table parameter value. | String |
| field | The name of an existing field to which the rule will be applied. | Field |
| line_classes[[Line Class, Orientation Field],...] | The line feature classes that will be used to intersect the point feature class. The orientation field is a numeric field that is used as a tie breaker when there are multiple intersecting lines. | Value Table |
| where_clause(Optional) | A SQL expression that limits the features the attribute rule will be applied to. | SQL Expression |
| rotation_options[[Point Class Expression, Rotation Style, Rotate Towards, Additional Rotation],...](Optional) | The rotation options for the point feature class.Rotation option components are as follows:Point Class Expression—A SQL query that will further limit the features the attribute rule will be applied to. The default is none.Rotation Style—Specifies how the direction will be measured.ARITHMETIC—The rotation will start at zero in the east direction and is calculated counterclockwise. This is the default.GEOGRAPHIC—The rotation will start at zero in the north direction and is calculated clockwise.Rotate Towards—Specifies whether the feature will be oriented toward the maximum or minimum value.MIN—The feature will be oriented toward the minimum value. This is the default.MAX—The feature will be oriented toward the maximum value.Additional Rotation—The rotation value that will be added to the calculated rotation value. The default is 0. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.GenerateSymbolRotationAttributeRule(in_table, out_folder, field, line_classes, {where_clause}, {rotation_options})
```

### Example 2

```python
# Description: GenerateSymbolRotationAttributeRule of a template attribute rule in a file geodatabase

# Import the system modules
import arcpy

# Set local variables
in_table = "C:/data/data.gdb"
out_folder = "C:/out/"
field = "symbolRotation"
line_classes = [['C:/data/state.gdb/roads', 'OBJECTID']]

arcpy.management.GenerateSymbolRotationAttributeRule(
   in_table,
   out_folder,
   field,
   line_classes
)
```

### Example 3

```python
# Description: GenerateSymbolRotationAttributeRule of a template attribute rule in a file geodatabase

# Import the system modules
import arcpy

# Set local variables
in_table = "C:/data/data.gdb"
out_folder = "C:/out/"
field = "symbolRotation"
line_classes = [['C:/data/state.gdb/roads', 'OBJECTID']]

arcpy.management.GenerateSymbolRotationAttributeRule(
   in_table,
   out_folder,
   field,
   line_classes
)
```

---

## Generate Table From Raster Function (Data Management)

## Summary

Converts a raster function dataset to a table or feature class. The input raster function should be a raster function designed to output a table or feature class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster Function | The function template or function JSON object that outputs a table or feature class. | String; File |
| Output Table | The path, file name, and type (extension) of the output table or feature class. | Table |
| Raster Function Arguments (Optional) | The function arguments and their values to be set. Each raster function has its own arguments and values, which are listed in the dialog of the tool. | Value Table |
| raster_function | The function template or function JSON object that outputs a table or feature class. | String; File |
| out_table | The path, file name, and type (extension) of the output table or feature class. | Table |
| raster_function_arguments[raster_function_arguments,...](Optional) | The function arguments and their values to be set. Each raster function has its own arguments and values, which are listed in the dialog of the tool. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.GenerateTableFromRasterFunction(raster_function, out_table, {raster_function_arguments})
```

### Example 2

```python
#====================================
# GenerateTableFromRasterFunction
# Usage:
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, { {Name} {Value}; {Name} {Value}...}))
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, {raster_function_arguments})

import arcpy

# Convert Raster to point feature class using build in Pixel to Vector function
arcpy.management.GenerateTableFromRasterFunction(
"P2V", "C:/Workspace/outputdb.sde/pixelpoints", 
"Raster C:/Workspace/data/testimage.tif")
```

### Example 3

```python
#====================================
# GenerateTableFromRasterFunction
# Usage:
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, { {Name} {Value}; {Name} {Value}...}))
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, {raster_function_arguments})

import arcpy

# Convert Raster to point feature class using build in Pixel to Vector function
arcpy.management.GenerateTableFromRasterFunction(
"P2V", "C:/Workspace/outputdb.sde/pixelpoints", 
"Raster C:/Workspace/data/testimage.tif")
```

### Example 4

```python
#====================================
# GenerateTableFromRasterFunction
# Usage:
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, { {Name} {Value}; {Name} {Value}...}))
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, {raster_function_arguments})

import arcpy

rasterfunc = "C:/Workspace/funcs/TestGeometry.rft.xml"
outfc = "C:/Workspace/polygonfeat.shp"
funcargs = "Raster C:/Workspace/data/testgeo.tif"

# Generate polygon feature class using custom python raster function
arcpy.management.GenerateTableFromRasterFunction(
rasterfunc, outfc, funcargs)
```

### Example 5

```python
#====================================
# GenerateTableFromRasterFunction
# Usage:
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, { {Name} {Value}; {Name} {Value}...}))
# arcpy.management.GenerateTableFromRasterFunction(
#     raster_function, out_table, {raster_function_arguments})

import arcpy

rasterfunc = "C:/Workspace/funcs/TestGeometry.rft.xml"
outfc = "C:/Workspace/polygonfeat.shp"
funcargs = "Raster C:/Workspace/data/testgeo.tif"

# Generate polygon feature class using custom python raster function
arcpy.management.GenerateTableFromRasterFunction(
rasterfunc, outfc, funcargs)
```

---

## Generate Tile Cache Tiling Scheme (Data Management)

## Summary

Creates a tiling scheme file based on the information from the source dataset. The tiling scheme file will then be used in the Manage Tile Cache tool when creating cache tiles.

## Usage

- By default, the tiling origin starts at the upper left of the coordinate system used by the source dataset.
- There are several options for loading an existing tiling scheme.Load a tiling scheme from an online mapping service such as ArcGIS Online, Google Maps, Bing Maps, or Yahoo. These tiling schemes are located in the installation directory of ArcGIS Pro, in the <install>/resources/TilingSchemes folder.Load a tiling scheme from an existing image service or map service cache.Load a tiling scheme that you created in a previous run of this tool.
- Load a tiling scheme from an online mapping service such as ArcGIS Online, Google Maps, Bing Maps, or Yahoo. These tiling schemes are located in the installation directory of ArcGIS Pro, in the <install>/resources/TilingSchemes folder.
- Load a tiling scheme from an existing image service or map service cache.
- Load a tiling scheme that you created in a previous run of this tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data Source | The source to be used to generate the tiling scheme. It can be a raster dataset, a mosaic dataset, or a map. | Raster Layer; Mosaic Layer; Map |
| Output Tiling Scheme | The path and file name for the output tiling scheme to be created. | File |
| Generation Method | Choose to use a new or predefined tiling scheme. You can define a new tiling scheme with this tool or browse to a predefined tiling scheme file (.xml).New—Define a new tiling scheme using other parameters in this tool to define scale levels, image format, storage format, and so on. This is the default.Predefined—Use a tiling scheme .xml file that already exists on disk. | String |
| Number of Scales | The number of scale levels to be created in the tiling scheme. | Long |
| Predefined Tiling Scheme(Optional) | Path to a predefined tiling scheme file (usually named conf.xml). This parameter is enabled only when the Predefined option is chosen as the tiling scheme generation method. | File |
| Scales(Optional) | Scale levels to be included in the tiling scheme. By default, these are not represented as fractions. Instead, use 500 to represent a scale of 1:500, and so on. The value entered in the Number of Scales parameter generates a set of default scale levels. | Value Table |
| Cell Size(Optional) | Determines the units of the Scales parameter.Checked—The values of the Scales parameter are pixel sizes. This is the default.Unchecked—The values of the Scales parameter are scale levels. | Boolean |
| Tile Origin in map units(Optional) | The origin (upper left corner) of the tiling scheme in the coordinates of the spatial reference of the source dataset. The extent of the source dataset must be within (but does not need to coincide) this region. | Point |
| Dots (Pixels) Per Inch (Optional) | The dots per inch of the intended output device. If a DPI is chosen that does not match the resolution of the output device, typically a display monitor, the scale of the tile will appear incorrect. The default value is 96. | Long |
| Tile Size (in pixels) (Optional) | The width and height of the cache tiles in pixels. The default is 256 by 256.For the best balance between performance and manageability, avoid deviating from widths of 256 or 512.128 by 128 pixels—Tile width and height of 128 pixels.256 by 256 pixels—Tile width and height of 256 pixels.512 by 512 pixels—Tile width and height of 512 pixels.1024 by 1024 pixels—Tile width and height of 1024 pixels. | String |
| Tile Format(Optional) | The file format for the tiles in the cache. PNG—Creates PNG format with varying bit depths. The bit depths are optimized according to the color variation and transparency values in each tile.PNG-8—A lossless, 8-bit color, image format that uses an indexed color palette and an alpha table. Each pixel stores a value (0 to 255) that is used to look up the color in the color palette and the transparency in the alpha table. 8-bit PNGs are similar to GIF images and provide the best support for a transparent background by most web browsers.PNG-24—A lossless, three-channel image format that supports large color variations (16 million colors) and has limited support for transparency. Each pixel contains three 8-bit color channels, and the file header contains the single color that represents the transparent background. The color representing the transparent background color can be set in the application. Versions of Internet Explorer prior to version 7 do not support this type of transparency. Caches using PNG24 are significantly larger than those using PNG8 or JPEG, so will take more disk space and require greater bandwidth to serve clients.PNG-32—A lossless, four-channel image format that supports large color variations (16 million colors) and transparency. Each pixel contains three 8-bit color channels and one 8-bit alpha channel that represents the level of transparency for each pixel. While the PNG32 format allows for partially transparent pixels in the range from 0 to 255, the ArcGIS Server cache generation tool only writes fully transparent (0) or fully opaque (255) values in the transparency channel. Caches using PNG32 are significantly larger than the other supported formats, so will take more disk space and require greater bandwidth to serve clients.JPEG—A lossy, three-channel image format that supports large color variations (16 million colors) but does not support transparency. Each pixel contains three 8-bit color channels. Caches using JPEG provide control over output quality and size.Mixed compression—Creates PNG32 anywhere that transparency is detected (in other words, anyplace where the data frame background is visible), but creates JPEG for the remaining tiles. This keeps the average file size down while providing you with a clean overlay on top of other caches. This is the default.LERC compression—Limited Error Raster Compression (LERC) is an efficient lossy compression method recommended for single-band or elevation data with a large pixel depth (12 bit to 32 bit). Compresses between 10:1 and 20:1. | String |
| Tile Compression Quality(Optional) | Enter a value between 1 and 100 for the JPEG or Mixed compression quality. The default value is 75.Compression is supported only for Mixed and JPEG format. Choosing a higher value will result in higher-quality images, but the file sizes will be larger. Using a lower value will result in lower-quality images with smaller file sizes. | Long |
| Storage Format (Optional) | Determines the storage format of tiles. Compact—Group tiles into large files called bundles. This storage format is more efficient in terms of storage and mobility. This is the default.Exploded—Each tile is stored as an individual file.Note that this format cannot be used with tile packages. | String |
| LERC Error (Optional) | Set the maximum tolerance in pixel values when compressing with LERC. | Double |
| in_dataset | The source to be used to generate the tiling scheme. It can be a raster dataset, a mosaic dataset, or a map. | Raster Layer; Mosaic Layer; Map |
| out_tiling_scheme | The path and file name for the output tiling scheme to be created. | File |
| tiling_scheme_generation_method | Choose to use a new or predefined tiling scheme. You can define a new tiling scheme with this tool or browse to a predefined tiling scheme file (.xml).NEW—Define a new tiling scheme using other parameters in this tool to define scale levels, image format, storage format, and so on. This is the default.PREDEFINED—Use a tiling scheme .xml file that already exists on disk. | String |
| number_of_scales | The number of scale levels to be created in the tiling scheme. | Long |
| predefined_tiling_scheme(Optional) | Path to a predefined tiling scheme file (usually named conf.xml). This parameter is enabled only when the Predefined option is chosen as the tiling scheme generation method. | File |
| scales[scale,...](Optional) | Scale levels to be included in the tiling scheme. By default, these are not represented as fractions. Instead, use 500 to represent a scale of 1:500, and so on. The value entered in the Number of Scales parameter generates a set of default scale levels. | Value Table |
| scales_type(Optional) | Determines the units of the scales parameter.CELL_SIZE—Indicates the values of the scales parameter are pixel sizes. This is the default.SCALE—Indicates the values of the scales parameter are scale levels. | Boolean |
| tile_origin(Optional) | The origin (upper left corner) of the tiling scheme in the coordinates of the spatial reference of the source dataset. The extent of the source dataset must be within (but does not need to coincide) this region. | Point |
| dpi(Optional) | The dots per inch of the intended output device. If a DPI is chosen that does not match the resolution of the output device, typically a display monitor, the scale of the tile will appear incorrect. The default value is 96. | Long |
| tile_size(Optional) | The width and height of the cache tiles in pixels. The default is 256 by 256.For the best balance between performance and manageability, avoid deviating from widths of 256 or 512.128 x 128—Tile width and height of 128 pixels.256 x 256—Tile width and height of 256 pixels.512 x 512—Tile width and height of 512 pixels.1024 x 1024—Tile width and height of 1024 pixels. | String |
| tile_format(Optional) | The file format for the tiles in the cache. PNG—Creates PNG format with varying bit depths. The bit depths are optimized according to the color variation and transparency values in each tile.PNG8—A lossless, 8-bit color, image format that uses an indexed color palette and an alpha table. Each pixel stores a value (0 to 255) that is used to look up the color in the color palette and the transparency in the alpha table. 8-bit PNGs are similar to GIF images and provide the best support for a transparent background by most web browsers.PNG24—A lossless, three-channel image format that supports large color variations (16 million colors) and has limited support for transparency. Each pixel contains three 8-bit color channels, and the file header contains the single color that represents the transparent background. The color representing the transparent background color can be set in the application. Versions of Internet Explorer prior to version 7 do not support this type of transparency. Caches using PNG24 are significantly larger than those using PNG8 or JPEG, so will take more disk space and require greater bandwidth to serve clients.PNG32—A lossless, four-channel image format that supports large color variations (16 million colors) and transparency. Each pixel contains three 8-bit color channels and one 8-bit alpha channel that represents the level of transparency for each pixel. While the PNG32 format allows for partially transparent pixels in the range from 0 to 255, the ArcGIS Server cache generation tool only writes fully transparent (0) or fully opaque (255) values in the transparency channel. Caches using PNG32 are significantly larger than the other supported formats, so will take more disk space and require greater bandwidth to serve clients.JPEG—A lossy, three-channel image format that supports large color variations (16 million colors) but does not support transparency. Each pixel contains three 8-bit color channels. Caches using JPEG provide control over output quality and size.MIXED—Creates PNG32 anywhere that transparency is detected (in other words, anyplace where the data frame background is visible), but creates JPEG for the remaining tiles. This keeps the average file size down while providing you with a clean overlay on top of other caches. This is the default.LERC—Limited Error Raster Compression (LERC) is an efficient lossy compression method recommended for single-band or elevation data with a large pixel depth (12 bit to 32 bit). Compresses between 10:1 and 20:1. | String |
| tile_compression_quality(Optional) | Enter a value between 1 and 100 for the JPEG or Mixed compression quality. The default value is 75.Compression is supported only for Mixed and JPEG format. Choosing a higher value will result in higher-quality images, but the file sizes will be larger. Using a lower value will result in lower-quality images with smaller file sizes. | Long |
| storage_format(Optional) | Determines the storage format of tiles. COMPACT—Group tiles into large files called bundles. This storage format is more efficient in terms of storage and mobility. This is the default.EXPLODED—Each tile is stored as an individual file.Note that this format cannot be used with tile packages. | String |
| lerc_error(Optional) | Set the maximum tolerance in pixel values when compressing with LERC. | Double |

## Code Samples

### Example 1

```python
arcpy.management.GenerateTileCacheTilingScheme(in_dataset, out_tiling_scheme, tiling_scheme_generation_method, number_of_scales, {predefined_tiling_scheme}, {scales}, {scales_type}, {tile_origin}, {dpi}, {tile_size}, {tile_format}, {tile_compression_quality}, {storage_format}, {lerc_error})
```

### Example 2

```python
import arcpy

arcpy.GenerateTileCacheTilingScheme_management(
     "C:/Data/Cache.gdb/Md","C:/TilingSchemes/scheme.xml",
     "NEW","8","#","#","#","#","96","256 x 256","MIXED",
     "75","COMPACT")
```

### Example 3

```python
import arcpy

arcpy.GenerateTileCacheTilingScheme_management(
     "C:/Data/Cache.gdb/Md","C:/TilingSchemes/scheme.xml",
     "NEW","8","#","#","#","#","96","256 x 256","MIXED",
     "75","COMPACT")
```

### Example 4

```python
#Generate tiling scheme for a mosaic dataset
#Generate 5 default scales



import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "C:/Workspace/Cache.gdb/md"
outScheme = "C:/Workspace/Schemes/Tilingscheme.xml"
method = "NEW"
numscales = "5"
predefScheme = "#"
scales = "#"
scaleType = "SCALE"
tileOrigin = "-20037700 30198300"
dpi = "96"
tileSize ="256 x 256"
tileFormat = "MIXED"
compQuality = "75"
storageFormat = "COMPACT"

arcpy.GenerateTileCacheTilingScheme_management(
     mdName, outScheme, method, numscales, predefScheme, scales,
     scaleType, tileOrigin, dpi, tileSize, compQuality, storageFormat)
```

### Example 5

```python
#Generate tiling scheme for a mosaic dataset
#Generate 5 default scales



import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "C:/Workspace/Cache.gdb/md"
outScheme = "C:/Workspace/Schemes/Tilingscheme.xml"
method = "NEW"
numscales = "5"
predefScheme = "#"
scales = "#"
scaleType = "SCALE"
tileOrigin = "-20037700 30198300"
dpi = "96"
tileSize ="256 x 256"
tileFormat = "MIXED"
compQuality = "75"
storageFormat = "COMPACT"

arcpy.GenerateTileCacheTilingScheme_management(
     mdName, outScheme, method, numscales, predefScheme, scales,
     scaleType, tileOrigin, dpi, tileSize, compQuality, storageFormat)
```

---

## Generate Transects Along Lines (Data Management)

## Summary

Creates perpendicular transect lines at a regular interval along lines.

## Usage

- This tool works best when the input features have a projected coordinate system appropriate for distance measurements in a local area. Equidistant, UTM, or other localized coordinates systems are well suited for distance measurement. Coordinate systems that distort distance to provide more cartographic shapes, such as Web Mercator, or geographic coordinate systems may not have accurate results.
- The output of this tool has only one attribute, ORIG_FID, which stores the Object ID of the input feature along which each transect line was generated. You can use this field to add additional attributes from the input features to the output transect lines by running the Add Join or Join Field tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The line features along which perpendicular transect lines will be generated. | Feature Layer |
| Output Feature Class | The output perpendicular transect lines generated along the input features. | Feature Class |
| Distance Between Transects | The interval from the beginning of the feature at which transects will be placed. | Linear Unit |
| Transect Length | The length or width of the transect line. Each transect will be placed in such a way along the input line that half its length falls on one side of the line, and half its length falls on the other side of the line. This is the overall length of each transect line, not the distance that the transect extends from the input line. To specify how far the transect line should extend from the input line—for example, 100 meters—double this value to specify the transect length (200 meters). | Linear Unit |
| Generate transects at line start and end(Optional) | Specifies whether transects will be generated at the start and end of the input line.Checked—Transects will be generated at the start and end of the input line.Unchecked—Transects will not be generated at the start and end of the input line. This is the default. | Boolean |
| in_features | The line features along which perpendicular transect lines will be generated. | Feature Layer |
| out_feature_class | The output perpendicular transect lines generated along the input features. | Feature Class |
| interval | The interval from the beginning of the feature at which transects will be placed. | Linear Unit |
| transect_length | The length or width of the transect line. Each transect will be placed in such a way along the input line that half its length falls on one side of the line, and half its length falls on the other side of the line. This is the overall length of each transect line, not the distance that the transect extends from the input line. To specify how far the transect line should extend from the input line—for example, 100 meters—double this value to specify the transect length (200 meters). | Linear Unit |
| include_ends(Optional) | Specifies whether transects will be generated at the start and end of the input line.END_POINTS—Transects will be generated at the start and end of the input line.NO_END_POINTS—Transects will not be generated at the start and end of the input line. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.GenerateTransectsAlongLines(in_features, out_feature_class, interval, transect_length, {include_ends})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/data/base.gdb'
arcpy.GenerateTransectsAlongLines_management('rivers', 'river_sample_transects',
                                             '100 Meters', '100 Meters', 
                                             'NO_END_POINTS')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/data/base.gdb'
arcpy.GenerateTransectsAlongLines_management('rivers', 'river_sample_transects',
                                             '100 Meters', '100 Meters', 
                                             'NO_END_POINTS')
```

### Example 4

```python
# Description: Add sampling perpendicular transect lines along a river

import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/data/base.gdb'

# Set local variables
in_features = 'rivers'
out_fc_1 = 'river_samples_transects'

# Execute GeneratePointsAlongLines by distance
arcpy.GenerateTransectsAlongLines_management(in_features, out_fc_1, '100 Meters',
                                             '100 meters', 'END_POINTS')
```

### Example 5

```python
# Description: Add sampling perpendicular transect lines along a river

import arcpy

# Set environment settings
arcpy.env.workspace = 'C:/data/base.gdb'

# Set local variables
in_features = 'rivers'
out_fc_1 = 'river_samples_transects'

# Execute GeneratePointsAlongLines by distance
arcpy.GenerateTransectsAlongLines_management(in_features, out_fc_1, '100 Meters',
                                             '100 meters', 'END_POINTS')
```

---

## Generate Tessellation (Data Management)

## Summary

Generates a tessellated grid of regular polygon features to cover a given extent. The tessellation can be of triangles, squares, diamonds, hexagons, H3 hexagons, or transverse hexagons.

## Usage

- To ensure that the entire input extent is covered by the tessellated grid, the output features purposely extend beyond the input extent. This occurs because the edges of the tessellated grid will not always be straight lines, and gaps would be present if the grid was limited by the input extent.
- The GRID_ID field will be added to the output. When the Shape Type parameter value is H3 hexagon, the GRID_ID field values will be a unique hierarchical index for each cell. For all other Shape Type parameter values, the GRID_ID field will be a unique ID for each feature. The format for the IDs is A-1, A-2, B-1, B-2, and so on. This allows for easy selection of rows and columns by query using the Select Layer By Attribute tool. For example, select all features in column A with GRID_ID like 'A-%', or select all features in row 1 with GRID_ID like '%-1'.
- To generate a grid that excludes tessellation features that do not intersect features in another dataset, use the Select Layer By Location tool to select output polygons that contain the source features, and use the Copy Features tool to make a permanent copy of the selected output features to a new feature class.
- The tool generates shapes by areal units. To determine the area of a shape based on the length of a side, use one of the following formulas to calculate the value of the Size parameter:ShapeFormulaExampleHexagon or Transverse hexagonTo generate hexagons with a side length of 100 meters, specify the Size parameter value of 25980.76211353316 square meters (100 raised to the power of 2 multiplied by 3 multiplied by the square root of 3 divided by 2).SquareTo generate squares with a side length of 100 meters, specify the Size parameter value of 10,000 square meters (100 raised to the power of 2).DiamondTo generate diamonds with a side length of 100 meters, specify the Size parameter value of 10,000 square meters (100 raised to the power of 2).TriangleTo generate triangles with a side length of 100 meters, specify the Size parameter value of 4330.127018922193 square meters (100 raised to the power of 2 multiplied by the square root of 3 divided by 4).The Shape Type parameter value's H3 hexagon option ignores the Size parameter. The area of the hexagon will be based on the H3 Resolution parameter value. Learn more about H3 resolutions

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Output Feature Class | The path and name of the output feature class containing the tessellated grid. | Feature Class |
| Extent | The extent that the tessellation will cover. This can be the currently visible area, the extent of a dataset, or manually entered values.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Shape Type (Optional) | Specifies the shape that will be generated.Hexagon—Hexagon-shaped features will be generated. The top and bottom side of each hexagon will be parallel with the x-axis of the coordinate system (the top and bottom are flat).Transverse hexagon—Transverse hexagon-shaped features will be generated. The right and left side of each hexagon will be parallel with the y-axis of the dataset's coordinate system (the top and bottom are pointed).Square—Square-shaped features will be generated. The top and bottom side of each square will be parallel with the x-axis of the coordinate system, and the right and left sides will be parallel with the y-axis of the coordinate system.Diamond—Diamond-shaped features will be generated. The sides of each polygon will be rotated 45 degrees away from the x-axis and y-axis of the coordinate system.Triangle—Triangular-shaped features will be generated. Each triangle will be a regular three-sided equilateral polygon.H3 hexagon—Hexagon-shaped features will be generated based on the H3 Hexagonal hierarchical geospatial indexing system. | String |
| Size (Optional) | The area of each individual shape that comprises the tessellation. | Areal Unit |
| Spatial Reference (Optional) | The spatial reference to which the output dataset will be projected. If a spatial reference is not provided, the output will be projected to the spatial reference of the input extent. If neither has a spatial reference, the output will be projected in GCS_WGS_1984. | Spatial Reference |
| H3 Resolution(Optional) | Specifies the H3 resolution of the hexagons.With each increasing resolution value, the area of the polygons will be one seventh the size. 0—Hexagons will be created at the H3 resolution of 0, with an average area of 4,357,449.416078381 square kilometers.1—Hexagons will be created at the H3 resolution of 1, with an average area of 609,788.441794133 square kilometers. 2—Hexagons will be created at the H3 resolution of 2, with an average area of 86,801.780398997 square kilometers.3—Hexagons will be created at the H3 resolution of 3, with an average area of 12,393.434655088 square kilometers.4—Hexagons will be created at the H3 resolution of 4, with an average area of 1,770.347654491 square kilometers.5—Hexagons will be created at the H3 resolution of 5, with an average area of 252.903858182 square kilometers.6—Hexagons will be created at the H3 resolution of 6, with an average area of 36.129062164 square kilometers.7—Hexagons will be created at the H3 resolution of 7, with an average area of 5.161293360 square kilometers. This is the default.8—Hexagons will be created at the H3 resolution of 8, with an average area of 0.737327598 square kilometers.9—Hexagons will be created at the H3 resolution of 9, with an average area of 0.105332513 square kilometers.10—Hexagons will be created at the H3 resolution of 10, with an average area of 0.015047502 square kilometers.11—Hexagons will be created at the H3 resolution of 11, with an average area of 0.002149643 square kilometers.12—Hexagons will be created at the H3 resolution of 12, with an average area of 0.000307092 square kilometers.13—Hexagons will be created at the H3 resolution of 13, with an average area of 0.000043870 square kilometers.14—Hexagons will be created at the H3 resolution of 14, with an average area of 0.000006267 square kilometers.15—Hexagons will be created at the H3 resolution of 15, with an average area of 0.000000895 square kilometers.This parameter is active when the Shape Type parameter is set to H3 hexagon. | Long |
| Output_Feature_Class | The path and name of the output feature class containing the tessellated grid. | Feature Class |
| Extent | The extent that the tessellation will cover. This can be the currently visible area, the extent of a dataset, or manually entered values.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| Shape_Type(Optional) | Specifies the shape that will be generated.HEXAGON—Hexagon-shaped features will be generated. The top and bottom side of each hexagon will be parallel with the x-axis of the coordinate system (the top and bottom are flat).TRANSVERSE_HEXAGON—Transverse hexagon-shaped features will be generated. The right and left side of each hexagon will be parallel with the y-axis of the dataset's coordinate system (the top and bottom are pointed).SQUARE—Square-shaped features will be generated. The top and bottom side of each square will be parallel with the x-axis of the coordinate system, and the right and left sides will be parallel with the y-axis of the coordinate system.DIAMOND—Diamond-shaped features will be generated. The sides of each polygon will be rotated 45 degrees away from the x-axis and y-axis of the coordinate system.TRIANGLE—Triangular-shaped features will be generated. Each triangle will be a regular three-sided equilateral polygon.H3_HEXAGON—Hexagon-shaped features will be generated based on the H3 Hexagonal hierarchical geospatial indexing system. | String |
| Size(Optional) | The area of each individual shape that comprises the tessellation. | Areal Unit |
| Spatial_Reference(Optional) | The spatial reference to which the output dataset will be projected. If a spatial reference is not provided, the output will be projected to the spatial reference of the input extent. If neither has a spatial reference, the output will be projected in GCS_WGS_1984. | Spatial Reference |
| H3_Resolution(Optional) | Specifies the H3 resolution of the hexagons.With each increasing resolution value, the area of the polygons will be one seventh the size. 0—Hexagons will be created at the H3 resolution of 0, with an average area of 4,357,449.416078381 square kilometers.1—Hexagons will be created at the H3 resolution of 1, with an average area of 609,788.441794133 square kilometers. 2—Hexagons will be created at the H3 resolution of 2, with an average area of 86,801.780398997 square kilometers.3—Hexagons will be created at the H3 resolution of 3, with an average area of 12,393.434655088 square kilometers.4—Hexagons will be created at the H3 resolution of 4, with an average area of 1,770.347654491 square kilometers.5—Hexagons will be created at the H3 resolution of 5, with an average area of 252.903858182 square kilometers.6—Hexagons will be created at the H3 resolution of 6, with an average area of 36.129062164 square kilometers.7—Hexagons will be created at the H3 resolution of 7, with an average area of 5.161293360 square kilometers. This is the default.8—Hexagons will be created at the H3 resolution of 8, with an average area of 0.737327598 square kilometers.9—Hexagons will be created at the H3 resolution of 9, with an average area of 0.105332513 square kilometers.10—Hexagons will be created at the H3 resolution of 10, with an average area of 0.015047502 square kilometers.11—Hexagons will be created at the H3 resolution of 11, with an average area of 0.002149643 square kilometers.12—Hexagons will be created at the H3 resolution of 12, with an average area of 0.000307092 square kilometers.13—Hexagons will be created at the H3 resolution of 13, with an average area of 0.000043870 square kilometers.14—Hexagons will be created at the H3 resolution of 14, with an average area of 0.000006267 square kilometers.15—Hexagons will be created at the H3 resolution of 15, with an average area of 0.000000895 square kilometers.This parameter is enabled when the Shape_Type parameter is set to H3_HEXAGON. | Long |

## Code Samples

### Example 1

```python
arcpy.management.GenerateTessellation(Output_Feature_Class, Extent, {Shape_Type}, {Size}, {Spatial_Reference}, {H3_Resolution})
```

### Example 2

```python
import arcpy
tessellation_extent = arcpy.Extent(0.0, 0.0, 10.0, 10.0)
spatial_ref = arcpy.SpatialReference(4326)
arcpy.management.GenerateTessellation(r"C:\data\project.gdb\hex_tessellation", 
                                      tessellation_extent, "HEXAGON", 
                                      "100 SquareMiles", spatial_ref)
```

### Example 3

```python
import arcpy
tessellation_extent = arcpy.Extent(0.0, 0.0, 10.0, 10.0)
spatial_ref = arcpy.SpatialReference(4326)
arcpy.management.GenerateTessellation(r"C:\data\project.gdb\hex_tessellation", 
                                      tessellation_extent, "HEXAGON", 
                                      "100 SquareMiles", spatial_ref)
```

### Example 4

```python
# Import modules
import arcpy

# Create some variables
out_gdb = r"C:\temp\project.gdb\h3_hexagon"
extent = arcpy.Extent(0.0, 0.0, 10.0, 10.0)
sr = arcpy.SpatialReference(4326)

# Generate H3 hexagons
arcpy.management.GenerateTessellation(out_gdb, Extent=extent, Shape_Type="H3_HEXAGON",
                                      H3_Resolution=5, Spatial_Reference=sr)
```

### Example 5

```python
# Import modules
import arcpy

# Create some variables
out_gdb = r"C:\temp\project.gdb\h3_hexagon"
extent = arcpy.Extent(0.0, 0.0, 10.0, 10.0)
sr = arcpy.SpatialReference(4326)

# Generate H3 hexagons
arcpy.management.GenerateTessellation(out_gdb, Extent=extent, Shape_Type="H3_HEXAGON",
                                      H3_Resolution=5, Spatial_Reference=sr)
```

### Example 6

```python
# Name: GenerateDynamicTessellation.py
# Purpose: Generate a grid of squares over the envelope of a provided feature 
# class.

# Import modules
import arcpy 

# Set paths of features
my_feature = r"C:\data\project.gdb\myfeature"
output_feature = r"C:\data\project.gdb\sqtessellation"

# Describe the input feature and extract the extent
description = arcpy.Describe(my_feature)
extent = description.extent

# Find the width, height, and linear unit used by the input feature class' extent
# Divide the width and height value by three.
# Multiply the divided values together and specify an area unit from the linear 
# unit.
# Should result in a 4x4 grid covering the extent. (Not 3x3 since the squares 
# hang over the extent.)
w = extent.width
h = extent.height
u = extent.spatialReference.linearUnitName
area = "{size} Square{unit}s".format(size=w/3 * h/3, unit=u)

# Use the extent's spatial reference to project the output
spatial_ref = extent.spatialReference

arcpy.management.GenerateTessellation(output_feature, extent, "SQUARE", area, 
                                      spatial_ref)
```

### Example 7

```python
# Name: GenerateDynamicTessellation.py
# Purpose: Generate a grid of squares over the envelope of a provided feature 
# class.

# Import modules
import arcpy 

# Set paths of features
my_feature = r"C:\data\project.gdb\myfeature"
output_feature = r"C:\data\project.gdb\sqtessellation"

# Describe the input feature and extract the extent
description = arcpy.Describe(my_feature)
extent = description.extent

# Find the width, height, and linear unit used by the input feature class' extent
# Divide the width and height value by three.
# Multiply the divided values together and specify an area unit from the linear 
# unit.
# Should result in a 4x4 grid covering the extent. (Not 3x3 since the squares 
# hang over the extent.)
w = extent.width
h = extent.height
u = extent.spatialReference.linearUnitName
area = "{size} Square{unit}s".format(size=w/3 * h/3, unit=u)

# Use the extent's spatial reference to project the output
spatial_ref = extent.spatialReference

arcpy.management.GenerateTessellation(output_feature, extent, "SQUARE", area, 
                                      spatial_ref)
```

---

## Geodetic Densify (Data Management)

## Summary

Creates new features by replacing the segments of the input features with densified approximations of geodesic segments. The following types of geodesic segments can be constructed: geodesic, great elliptic, loxodrome, and normal section.

## Usage

- The end points of the segments will be connected using a geodetic approximation. The input segment type is irrelevant; linear and nonlinear (circular arc, elliptic arc, and Bezier curve) segments with common end points will produce the same output.
- No output feature will be written for an input feature that consists of a single segment with coincident start and end points. For example, a polygon feature made of a single circular arc will not be written to the output.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input line or polygon features. | Feature Layer |
| Output Feature Class | The output feature class containing the densified geodesic features. | Feature Class |
| Geodetic Type | Specifies the type of geodetic segments that will be created. Geodetic calculations are performed on the ellipsoid associated with the input data's coordinate system.Geodesic—The segments will be the shortest distance between two points on the surface of the spheroid (ellipsoid).Loxodrome—The segments will be the line of equal azimuth (from a pole) connecting the two points.Great elliptic— The segments will be the line made by the intersection of a plane that contains the center of the spheroid and the two points.Normal section—The segments will be the line made by the intersection of a plane that contains the center of the spheroid and is perpendicular to the surface at the first point. | String |
| Distance (Optional) | The distance between vertices along the output geodesic segment. The default value is 50 kilometers. | Linear Unit |
| in_features | The input line or polygon features. | Feature Layer |
| out_feature_class | The output feature class containing the densified geodesic features. | Feature Class |
| geodetic_type | Specifies the type of geodetic segments that will be created. Geodetic calculations are performed on the ellipsoid associated with the input data's coordinate system.GEODESIC—The segments will be the shortest distance between two points on the surface of the spheroid (ellipsoid).LOXODROME—The segments will be the line of equal azimuth (from a pole) connecting the two points.GREAT_ELLIPTIC— The segments will be the line made by the intersection of a plane that contains the center of the spheroid and the two points.NORMAL_SECTION—The segments will be the line made by the intersection of a plane that contains the center of the spheroid and is perpendicular to the surface at the first point. | String |
| distance(Optional) | The distance between vertices along the output geodesic segment. The default value is 50 kilometers. | Linear Unit |

## Code Samples

### Example 1

```python
arcpy.management.GeodeticDensify(in_features, out_feature_class, geodetic_type, {distance})
```

### Example 2

```python
import arcpy
arcpy.management.GeodeticDensify(r"C:\data.gdb\flight_lines", 
                                 r"C:\data.gdb\flight_lines_geodesic", 
                                 "GEODESIC")
```

### Example 3

```python
import arcpy
arcpy.management.GeodeticDensify(r"C:\data.gdb\flight_lines", 
                                 r"C:\data.gdb\flight_lines_geodesic", 
                                 "GEODESIC")
```

---

## GeoTagged Photos To Points (Data Management)

## Summary

Creates points from the x-, y-, and z-coordinates stored in the metadata of geotagged photo files (.jpg or .tif). You can add the photo files to the output features as geodatabase attachments.

## Usage

- This tool reads the longitude, latitude, and altitude coordinates from photo files (.jpg or .tif) and writes the coordinates and associated attributes to an output point feature class.
- The output feature class will include the following attribute fields: Path—The full path to the photo file used to generate the point, for example, C:\data\photos\Pic0001.jpg.Name—The short name of the photo file, for example, Pic0001.jpg.DateTime—The original capture date and time of the photo file. When the output feature class is a shapefile, this field will be of type string. When the output feature class is in a geodatabase, this field will be of type date.If the DateTime field contains null or blank values, it may be an indication that the device did not capture a useable time stamp with the photo. Photo files may have a date created or date modified property, but these may not represent the date and time the photo was captured.Direction—The direction the device was pointing when the photo was captured. Values range from 0 to 359.99 in which 0 indicates north, 90 indicates east, and so on. If no direction is recorded by the device, this field will have a value of Null, 0, or -999999, depending on the device and the output location you specified. The direction value may refer to degrees from true north or magnetic north. Refer to the device documentation for more information.Note:Use a geotagged photo's direction information with caution, as the direction recorded by the device may not be accurate. Refer to the device documentation for information on directional accuracy.X—The x-coordinate where the photo was captured.Y—The y-coordinate where the photo was captured.Z—The altitude in meters where the photo was captured. If an altitude was not recorded by the device, the field will have a value of Null, 0, or -999999, depending on the device and the output location you specified.
- Path—The full path to the photo file used to generate the point, for example, C:\data\photos\Pic0001.jpg.
- Name—The short name of the photo file, for example, Pic0001.jpg.
- DateTime—The original capture date and time of the photo file. When the output feature class is a shapefile, this field will be of type string. When the output feature class is in a geodatabase, this field will be of type date.If the DateTime field contains null or blank values, it may be an indication that the device did not capture a useable time stamp with the photo. Photo files may have a date created or date modified property, but these may not represent the date and time the photo was captured.
- Direction—The direction the device was pointing when the photo was captured. Values range from 0 to 359.99 in which 0 indicates north, 90 indicates east, and so on. If no direction is recorded by the device, this field will have a value of Null, 0, or -999999, depending on the device and the output location you specified. The direction value may refer to degrees from true north or magnetic north. Refer to the device documentation for more information.Note:Use a geotagged photo's direction information with caution, as the direction recorded by the device may not be accurate. Refer to the device documentation for information on directional accuracy.
- X—The x-coordinate where the photo was captured.
- Y—The y-coordinate where the photo was captured.
- Z—The altitude in meters where the photo was captured. If an altitude was not recorded by the device, the field will have a value of Null, 0, or -999999, depending on the device and the output location you specified.
- The output DateTime field can be used to analyze and map the output feature class through time.
- The tool output includes a line chart showing the timeline of photos using a count of the time stamps in the DateTime field in date and time bins that are automatically calculated but can be customized.
- If the x- and y-coordinates of a photo are 0,0, no point will be generated for that photo. Empty coordinates may occur when the device does not have an adequate signal to capture coordinates. If the Include Non-GeoTagged Photos parameter is checked, the photo will be added as an output feature with null geometry.
- The output feature class will have a GCS_WGS_1984 x,y and vertical coordinate system, since that is the coordinate system used by GPS receivers.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Folder | The folder where photo files (.jpg or .tif) are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. | Folder |
| Output Feature Class | The output point feature class. | Feature Class |
| Invalid Photos Table (Optional) | An output table that will list any photo files in the input folder with invalid Exif metadata or empty or invalid coordinates.If no value is specified, the table will not be created. | Table |
| Include Non-GeoTagged Photos (Optional) | Specifies whether all photo files will be included in the output feature class or only those with valid coordinates.Checked—All photos will be included as records in the output feature class. If a photo file does not have coordinate information, it will be included as a feature with null geometry. This is the default.Unchecked—Only photos with valid coordinate information will be included in the output feature class. | Boolean |
| Add Photos As Attachments(Optional) | Specifies whether the input photos will be added to the output features as geodatabase attachments.Note:Adding attachments requires that the output feature class be in a version 10 or later geodatabase.Checked—Photos will be added to the output features as geodatabase attachments copied internally to the geodatabase. This is the default.Unchecked—Photos will not be added to the output features as geodatabase attachments. | Boolean |
| Input_Folder | The folder where photo files (.jpg or .tif) are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. | Folder |
| Output_Feature_Class | The output point feature class. | Feature Class |
| Invalid_Photos_Table(Optional) | An output table that will list any photo files in the input folder with invalid Exif metadata or empty or invalid coordinates.If no value is specified, the table will not be created. | Table |
| Include_Non-GeoTagged_Photos(Optional) | Specifies whether all photo files will be included in the output feature class or only those with valid coordinates.ALL_PHOTOS— All photos will be included as records in the output feature class. If a photo file does not have coordinate information, it will be included as a feature with null geometry. This is the default.ONLY_GEOTAGGED— Only photos with valid coordinate information will be included in the output feature class. | Boolean |
| Add_Photos_As_Attachments(Optional) | Specifies whether the input photos will be added to the output features as geodatabase attachments.Note:Adding attachments requires that the output feature class be in a version 10 or later geodatabase. ADD_ATTACHMENTS— Photos will be added to the output features as geodatabase attachments copied internally to the geodatabase. This is the default.NO_ATTACHMENTS—Photos will not be added to the output features as geodatabase attachments. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.GeoTaggedPhotosToPoints(Input_Folder, Output_Feature_Class, {Invalid_Photos_Table}, {Include_Non-GeoTagged_Photos}, {Add_Photos_As_Attachments})
```

### Example 2

```python
import arcpy
arcpy.management.GeoTaggedPhotosToPoints("c:/data/photos", 
                                         "c:/data/city.gdb/photo_points", "", 
                                         "ONLY_GEOTAGGED", "ADD_ATTACHMENTS")
```

### Example 3

```python
import arcpy
arcpy.management.GeoTaggedPhotosToPoints("c:/data/photos", 
                                         "c:/data/city.gdb/photo_points", "", 
                                         "ONLY_GEOTAGGED", "ADD_ATTACHMENTS")
```

### Example 4

```python
"""Name: GeoTaggedPhotosToPoints example
Description: Convert a folder of photos to points, then perform a buffer
""" 

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFolder = "photos"
outFeatures = "city.gdb/photos_points"
badPhotosList = "city.gdb/photos_noGPS"
photoOption = "ONLY_GEOTAGGED"
attachmentsOption = "ADD_ATTACHMENTS"

buffers = "city.gdb/photos_points_buffer"
bufferDist = "1 Miles"

arcpy.management.GeoTaggedPhotosToPoints(inFolder, outFeatures, badPhotosList, 
                                         photoOption, attachmentsOption)
arcpy.analysis.Buffer(outFeatures, buffers, bufferDist)
```

### Example 5

```python
"""Name: GeoTaggedPhotosToPoints example
Description: Convert a folder of photos to points, then perform a buffer
""" 

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFolder = "photos"
outFeatures = "city.gdb/photos_points"
badPhotosList = "city.gdb/photos_noGPS"
photoOption = "ONLY_GEOTAGGED"
attachmentsOption = "ADD_ATTACHMENTS"

buffers = "city.gdb/photos_points_buffer"
bufferDist = "1 Miles"

arcpy.management.GeoTaggedPhotosToPoints(inFolder, outFeatures, badPhotosList, 
                                         photoOption, attachmentsOption)
arcpy.analysis.Buffer(outFeatures, buffers, bufferDist)
```

---

## Get Cell Value (Data Management)

## Summary

Retrieves the value of a given pixel using its x, y coordinates.

## Usage

- This tool is used when you need the pixel value for a geoprocessing model. In ArcGIS Pro, select the navigation button from the Map tab and click on a pixel. Values for each visible band will be returned.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster that you want to query. | Mosaic Dataset; Mosaic Layer; Raster Layer |
| Location | The X and Y coordinates of the pixel location. | Point |
| Bands(Optional) | Specify the bands that you want to query. Leave blank to query all bands in a multiband dataset. | Value Table |
| in_raster | The raster that you want to query. | Mosaic Dataset; Mosaic Layer; Raster Layer |
| location_point | The X and Y coordinates of the pixel location. | Point |
| band_index[band_index,...](Optional) | Specify the bands that you want to query. Leave blank to query all bands in a multiband dataset. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.GetCellValue(in_raster, location_point, {band_index})
```

### Example 2

```python
import arcpy
result = arcpy.GetCellValue_management("C:/data/rgb.img", "480785 3807335", "2;3")
cellvalue = int(result.getOutput(0))
print(cellvalue)
```

### Example 3

```python
import arcpy
result = arcpy.GetCellValue_management("C:/data/rgb.img", "480785 3807335", "2;3")
cellvalue = int(result.getOutput(0))
print(cellvalue)
```

### Example 4

```python
'''====================================
Get Cell Value
Usage: GetCellValue_management in_raster location_point {ID;ID...}
'''   
   
import arcpy
arcpy.env.workspace = "C:/Workspace"

# Get the Band_2 and Band_3 cell value of certain point in a RGB image
result = arcpy.GetCellValue_management("rgb.img", "480785 3807335", "2;3")
cellvalue = int(result.getOutput(0))

# View the result in execution log
print(cellvalue)
```

### Example 5

```python
'''====================================
Get Cell Value
Usage: GetCellValue_management in_raster location_point {ID;ID...}
'''   
   
import arcpy
arcpy.env.workspace = "C:/Workspace"

# Get the Band_2 and Band_3 cell value of certain point in a RGB image
result = arcpy.GetCellValue_management("rgb.img", "480785 3807335", "2;3")
cellvalue = int(result.getOutput(0))

# View the result in execution log
print(cellvalue)
```

---

## Get Count (Data Management)

## Summary

Returns the total number of rows for a table.

## Usage

- If the input is a layer or table view containing a selected set of records, only the selected records will be counted.
- This tool honors the Extent environment. Only those features that are within or intersect the Extent environment setting will be counted.
- You can view the returned row count in Geoprocessing history.
- In ModelBuilder, this tool can be used to set up a precondition, as illustrated below. In this model, the Get Count tool counts the number of records returned by the Select tool. If the count is zero, the Buffer tool will not run due to the precondition.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rows | The input table view or raster layer. If a selection is defined on the input, the count of the selected rows will be returned. | Table View; Raster Layer |
| in_rows | The input table view or raster layer. If a selection is defined on the input, the count of the selected rows will be returned. | Table View; Raster Layer |

## Code Samples

### Example 1

```python
arcpy.management.GetCount(in_rows)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
result = arcpy.management.GetCount("roads")
count = int(result[0])
print(f'roads has {count} records')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
result = arcpy.management.GetCount("roads")
count = int(result[0])
print(f'roads has {count} records')
```

### Example 4

```python
# Purpose: Calculate the number of features in a feature class

# Import system modules
import arcpy
 
lyrfile = r"C:\data\streets.lyr"
result = arcpy.management.GetCount(lyrfile)

count = int(result[0])

# Print the number of features using the Result object
print(f'{lyrfile} has {count} records')
```

### Example 5

```python
# Purpose: Calculate the number of features in a feature class

# Import system modules
import arcpy
 
lyrfile = r"C:\data\streets.lyr"
result = arcpy.management.GetCount(lyrfile)

count = int(result[0])

# Print the number of features using the Result object
print(f'{lyrfile} has {count} records')
```

---

## Get Raster Properties (Data Management)

## Summary

Retrieves information from the metadata and descriptive statistics about a raster dataset.

## Usage

- The property value returned is displayed in the Geoprocessing history item created by running the tool.
- The Python result of this tool is a geoprocessing Result object. In order to obtain the string value, use the Result object's getOutput method.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster containing the properties to retrieve. | Composite Geodataset |
| Property type(Optional) | The property to be obtained from the input raster.Minimum cell value—Smallest value of all cells in the input raster. Maximum cell value—Largest value of all cells in the input raster. Mean of all cells—Average of all cells in the input raster. Standard Deviation of all cells—Standard deviation of all cells in the input raster. Unique value count—Number of unique values in the input raster. Maximum Y coordinate (top)—Top (maximum y-coordinate) value of the extent. Minimum X coordinate (left)—Left (minimum x-coordinate) value of the extent. Maximum X coordinate (right)—Right (maximum x-coordinate) value of the extent. Minimum Y coordinate (bottom)—Bottom (minimum y-coordinate) value of the extent. Cell size x-direction—Cell size in the x-direction. Cell size y-direction—Cell size in the y-direction. Cell value type—Type of the cell value in the input raster.Number of columns—Number of columns in the input raster. Number of rows—Number of rows in the input raster. Number of bands—Number of bands in the input raster. Contains NoData cells—Returns whether there is NoData in the raster.All cells contain NoData—Returns whether all the pixels are NoData. This is also known as ISNULL.Sensor name—Name of the sensor.Product name—Product name related to the sensor.Acquisition date—Date that the data was captured.Source type—Source type.Cloud cover—Amount of cloud cover as a percentage.Sun azimuth—Sun azimuth, in degrees.Sun elevation—Sun elevation, in degrees.Sensor azimuth—Sensor azimuth, in degrees.Sensor elevation—Sensor elevation, in degrees.Off nadir—Off-nadir angle, in degrees.Wavelength—Wavelength range of the band, in nanometers. | String |
| Band Name (Optional) | Choose the band name from the drop-down box. If no band is chosen, then the first band will be used. | String |
| in_raster | The raster containing the properties to retrieve. | Composite Geodataset |
| property_type(Optional) | The property to be obtained from the input raster.MINIMUM—Smallest value of all cells in the input raster. MAXIMUM—Largest value of all cells in the input raster. MEAN—Average of all cells in the input raster. STD—Standard deviation of all cells in the input raster. UNIQUEVALUECOUNT—Number of unique values in the input raster. TOP—Top or YMax value of the extent. LEFT—Left or XMin value of the extent. RIGHT—Right or XMax value of the extent. BOTTOM—Bottom or YMin value of the extent. CELLSIZEX—Cell size in the x-direction. CELLSIZEY—Cell size in the y-direction. VALUETYPE—Type of the cell value in the input raster: 0 = 1-bit1 = 2-bit2 = 4-bit3 = 8-bit unsigned integer4 = 8-bit signed integer5 = 16-bit unsigned integer6 = 16-bit signed integer7 = 32-bit unsigned integer8 = 32-bit signed integer9 = 32-bit floating point10 = 64-bit double precision11 = 8-bit complex12 = 16-bit complex13 = 32-bit complex 14 = 64-bit complex COLUMNCOUNT—Number of columns in the input raster. ROWCOUNT—Number of rows in the input raster. BANDCOUNT—Number of bands in the input raster. ANYNODATA—Returns whether there is NoData in the raster.ALLNODATA—Returns whether all the pixels are NoData. This is the same as ISNULL.SENSORNAME—Name of the sensor.PRODUCTNAME—Product name related to the sensor.ACQUISITIONDATE—Date that the data was captured.SOURCETYPE—Source type.CLOUDCOVER—Amount of cloud cover as a percentage.SUNAZIMUTH—Sun azimuth, in degrees.SUNELEVATION—Sun elevation, in degrees.SENSORAZIMUTH—Sensor azimuth, in degrees.SENSORELEVATION—Sensor elevation, in degrees.OFFNADIR—Off-nadir angle, in degrees.WAVELENGTH—Wavelength range of the band, in nanometers. | String |
| band_index(Optional) | Choose the band name from which to get the properties. If no band is chosen, then the first band will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.GetRasterProperties(in_raster, {property_type}, {band_index})
```

### Example 2

```python
import arcpy
#Get the geoprocessing result object
elevSTDResult = arcpy.GetRasterProperties_management("c:/data/elevation", "STD")
#Get the elevation standard deviation value from geoprocessing result object
elevSTD = elevSTDResult.getOutput(0)
```

### Example 3

```python
import arcpy
#Get the geoprocessing result object
elevSTDResult = arcpy.GetRasterProperties_management("c:/data/elevation", "STD")
#Get the elevation standard deviation value from geoprocessing result object
elevSTD = elevSTDResult.getOutput(0)
```

### Example 4

```python
import arcpy
#Get the geoprocessing result object
elevSTDResult = arcpy.GetRasterProperties_management("c:/data/elevation", "STD")
#Get the elevation standard deviation value from geoprocessing result object
elevSTD = elevSTDResult.getOutput(0)
```

### Example 5

```python
import arcpy
#Get the geoprocessing result object
elevSTDResult = arcpy.GetRasterProperties_management("c:/data/elevation", "STD")
#Get the elevation standard deviation value from geoprocessing result object
elevSTD = elevSTDResult.getOutput(0)
```

---

## Geoprocessing workflow for subtypes

## Code Samples

### Example 1

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("Water/Fittings", "TYPECODE")
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("Water/Fittings", "TYPECODE")
```

### Example 3

```python
arcpy.AddSubtype_management ("Water/Fittings","0", "unknown")
arcpy.AddSubtype_management ("Water/Fittings", "1", "bend")
arcpy.AddSubtype_management ("Water/Fittings", "2", "cap")
arcpy.AddSubtype_management ("Water/Fittings", "3", "cross")
arcpy.AddSubtype_management ("Water/Fittings", "4", "coupling")
arcpy.AddSubtype_management ("Water/Fittings", "5", "expansion joint")
arcpy.AddSubtype_management ("Water/Fittings", "6", "offset")
arcpy.AddSubtype_management ("Water/Fittings", "7", "plug")
arcpy.AddSubtype_management ("Water/Fittings", "8", "reducer")
arcpy.AddSubtype_management ("Water/Fittings", "9", "saddle")
arcpy.AddSubtype_management ("Water/Fittings", "10", "sleeve")
arcpy.AddSubtype_management ("Water/Fittings", "11", "tap")
arcpy.AddSubtype_management ("Water/Fittings", "12", "tee")
arcpy.AddSubtype_management ("Water/Fittings", "13", "weld")
arcpy.AddSubtype_management ("Water/Fittings", "14", "riser")
```

### Example 4

```python
arcpy.AddSubtype_management ("Water/Fittings","0", "unknown")
arcpy.AddSubtype_management ("Water/Fittings", "1", "bend")
arcpy.AddSubtype_management ("Water/Fittings", "2", "cap")
arcpy.AddSubtype_management ("Water/Fittings", "3", "cross")
arcpy.AddSubtype_management ("Water/Fittings", "4", "coupling")
arcpy.AddSubtype_management ("Water/Fittings", "5", "expansion joint")
arcpy.AddSubtype_management ("Water/Fittings", "6", "offset")
arcpy.AddSubtype_management ("Water/Fittings", "7", "plug")
arcpy.AddSubtype_management ("Water/Fittings", "8", "reducer")
arcpy.AddSubtype_management ("Water/Fittings", "9", "saddle")
arcpy.AddSubtype_management ("Water/Fittings", "10", "sleeve")
arcpy.AddSubtype_management ("Water/Fittings", "11", "tap")
arcpy.AddSubtype_management ("Water/Fittings", "12", "tee")
arcpy.AddSubtype_management ("Water/Fittings", "13", "weld")
arcpy.AddSubtype_management ("Water/Fittings", "14", "riser")
```

### Example 5

```python
arcpy.SetDefaultSubtype_management ("Water/Fittings", "2")
```

### Example 6

```python
arcpy.SetDefaultSubtype_management ("Water/Fittings", "2")
```

---

## How Create Fishnet works

## Code Samples

### Example 1

```python
x-coordinate = tan(60) * 10
```

### Example 2

```python
x-coordinate = tan(60) * 10
```

---

## Import 3D Objects (Data Management)

## Summary

Imports 3D models from one or more 3D file formats and creates or updates a 3D object feature layer.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input 3D Files and Folders | The 3D files or folders containing 3D files that will be imported. When a folder is provided, all supported 3D models contained in it and its subdirectories will be imported. The following models are supported:COLLADA (.dae)Drawing (.dwg)Autodesk Filmbox (.fbx)Graphics Library Transmission (.glb)JSON Graphics Library Transmission (.gltf)Industry Foundation Class (.ifc)Wavefront Object (.obj)Universal Scene Description (.usdc)Compressed Universal Scene Description (.usdz) | File; Folder |
| Output 3D Object Feature Class | The 3D object feature layer that will be created or updated. | Feature Layer |
| Update Strategy (Optional) | Specifies how an existing 3D object feature class will be updated.Replace All Existing Models—All existing features in the 3D object feature class will be removed and only the 3D models that are specified as input will be added.Only Update Existing Models—3D models that exist in the 3D object feature class will be updated. New models will be skipped.Update Existing Models and Add New—3D models that exist in the 3D object feature class will be updated and new models will be appended. This is the default.Add All Models—All 3D models will be added without replacing any that currently exist in the 3D object feature class. | String |
| XY Offset (Optional) | The x- and y-coordinate offset that will be applied to the imported models. | Point |
| Elevation Offset (Optional) | The height offset that will be applied to the imported models. | Double |
| Scale Factor (Optional) | The scale factor that will be used to resize the 3D models being imported. | Double |
| Rotation Angle (Optional) | The degree rotation angle that will be applied to the imported models. Rotation is applied with the assumption of zero degrees (0°) representing north and angular values incrementing in the clockwise direction. | Double |
| Y is up if unspecified by data | Specifies whether y coordinates will be interpreted as height or along the horizontal plane. This parameter is only supported for Wavefront Object files (.obj).Checked—Y coordinates will be interpreted as height. This is the default.Unchecked—Z coordinates will be interpreted as height. | Boolean |
| files_and_folders[files_and_folders,...] | The 3D files or folders containing 3D files that will be imported. When a folder is provided, all supported 3D models contained in it and its subdirectories will be imported. The following models are supported:COLLADA (.dae)Drawing (.dwg)Autodesk Filmbox (.fbx)Graphics Library Transmission (.glb)JSON Graphics Library Transmission (.gltf)Industry Foundation Class (.ifc)Wavefront Object (.obj)Universal Scene Description (.usdc)Compressed Universal Scene Description (.usdz) | File; Folder |
| updated_features | The 3D object feature layer that will be created or updated. | Feature Layer |
| update(Optional) | Specifies how an existing 3D object feature class will be updated.REPLACE_ALL—All existing features in the 3D object feature class will be removed and only the 3D models that are specified as input will be added.UPDATE_EXISTING—3D models that exist in the 3D object feature class will be updated. New models will be skipped.UPDATE_EXISTING_ADD_NEW—3D models that exist in the 3D object feature class will be updated and new models will be appended. This is the default.ADD_ALL—All 3D models will be added without replacing any that currently exist in the 3D object feature class. | String |
| translate(Optional) | The x- and y-coordinate offset that will be applied to the imported models. | Point |
| elevation(Optional) | The height offset that will be applied to the imported models. | Double |
| scale(Optional) | The scale factor that will be used to resize the 3D models being imported. | Double |
| rotate(Optional) | The degree rotation angle that will be applied to the imported models. Rotation is applied with the assumption of zero degrees (0°) representing north and angular values incrementing in the clockwise direction. | Double |
| y_is_up | Specifies whether y coordinates will be interpreted as height or along the horizontal plane. This parameter is only supported for Wavefront Object files (.obj).Y_IS_UP—Y coordinates will be interpreted as height. This is the default.Z_IS_UP—Z coordinates will be interpreted as height. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.Import3DObjects(files_and_folders, updated_features, {update}, {translate}, {elevation}, {scale}, {rotate}, y_is_up)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

arcpy.management.Import3DObjects("import_models", 
                                 "city_models.gdb/Downtown_Buildings", 
                                 update="ADD_ALL", translate="50 100.5", 
                                 elevation=-52.73, scale=0.3048, rotate=15.25)
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

arcpy.management.Import3DObjects("import_models", 
                                 "city_models.gdb/Downtown_Buildings", 
                                 update="ADD_ALL", translate="50 100.5", 
                                 elevation=-52.73, scale=0.3048, rotate=15.25)
```

### Example 4

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

# Export the feature class or layer to model files on disk
arcpy.management.Export3DObjects("city_models.gdb/Downtown_Buildings", 
                                 "exported_models", ["FMT3D_IFC"])

# Optionally, edit the exported model files in other software, or replace the 
# files with a new version. 
# Keep the file names the same to update existing features. New file names are
# interpreted as new features.

# The input folder and feature class or layer in Import 3D Objects are the same
# values used in Export 3D Objects.
arcpy.management.Import3DObjects("exported_models", 
                                 "city_models.gdb/Downtown_Buildings", 
                                 update="UPDATE_EXISTING_ADD_NEW", 
                                 translate="350 150",
                                 elevation=100, scale=2.54, rotate=-90)
```

### Example 5

```python
import arcpy
arcpy.env.workspace = 'C:/project_directory'

# Export the feature class or layer to model files on disk
arcpy.management.Export3DObjects("city_models.gdb/Downtown_Buildings", 
                                 "exported_models", ["FMT3D_IFC"])

# Optionally, edit the exported model files in other software, or replace the 
# files with a new version. 
# Keep the file names the same to update existing features. New file names are
# interpreted as new features.

# The input folder and feature class or layer in Import 3D Objects are the same
# values used in Export 3D Objects.
arcpy.management.Import3DObjects("exported_models", 
                                 "city_models.gdb/Downtown_Buildings", 
                                 update="UPDATE_EXISTING_ADD_NEW", 
                                 translate="350 150",
                                 elevation=100, scale=2.54, rotate=-90)
```

---

## Import Attribute Rules (Data Management)

## Summary

Imports attribute rules from a comma-separated values file (.csv) to a dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The table or feature class to which the attribute rules will be applied. The dataset must have all the features specified in the rule definition. | Table View |
| Input File | The .csv file containing the rules to import. | File |
| target_table | The table or feature class to which the attribute rules will be applied. The dataset must have all the features specified in the rule definition. | Table View |
| csv_file | The .csv file containing the rules to import. | File |

## Code Samples

### Example 1

```python
arcpy.management.ImportAttributeRules(target_table, csv_file)
```

### Example 2

```python
import arcpy
arcpy.management.ImportAttributeRules("C:\\MyProject\\MyDatabase.sde\\pro.USER1.Building",
                                      "C:\\MyProject\\expAttrRules.csv")
```

### Example 3

```python
import arcpy
arcpy.management.ImportAttributeRules("C:\\MyProject\\MyDatabase.sde\\pro.USER1.Building",
                                      "C:\\MyProject\\expAttrRules.csv")
```

---

## Import Contingent Values (Data Management)

## Summary

Imports multiple contingent values and field groups from a comma-separated values file (.csv) to a dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase table or feature class to which the field groups and contingent values will be imported. | Table View |
| Field Groups Input File (.csv) | A .csv file with specific column names that contains information about the field groups. | File |
| Contingent Values Input File (.csv) | A .csv file with specific column names that contains information about the contingent values. | File |
| Replace existing contingent values (Optional) | Specifies whether existing values will be replaced or merged when imported.Checked—Existing values for the target table will be replaced with the values in the input .csv files.Unchecked—Existing values will be merged with the values in the input .csv files. Any duplicates will be excluded. This is the default. | Boolean |
| target_table | The input geodatabase table or feature class to which the field groups and contingent values will be imported. | Table View |
| field_group_file | A .csv file with specific column names that contains information about the field groups. | File |
| contingent_value_file | A .csv file with specific column names that contains information about the contingent values. | File |
| import_type(Optional) | Specifies whether existing values will be replaced or merged when imported.REPLACE—Existing values for the target table will be replaced with the values in the input .csv files.UNION—Existing values will be merged with the values in the input .csv files. Any duplicates will be excluded. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ImportContingentValues(target_table, field_group_file, contingent_value_file, {import_type})
```

### Example 2

```python
import arcpy
arcpy.management.ImportContingentValues("C:\\MyProject\\myConn.sde\\pro.USER1.Animals",
                                        "C:\\MyProject\\MyFieldGroups.csv",
                                        "C:\\MyProject\\MyContingentValues.csv",
                                        "REPLACE")
```

### Example 3

```python
import arcpy
arcpy.management.ImportContingentValues("C:\\MyProject\\myConn.sde\\pro.USER1.Animals",
                                        "C:\\MyProject\\MyFieldGroups.csv",
                                        "C:\\MyProject\\MyContingentValues.csv",
                                        "REPLACE")
```

---

## Import Geodatabase Configuration Keywords (Data Management)

## Summary

Defines data storage parameters for an enterprise geodatabase by importing a file containing configuration keywords, parameters, and values.

## Usage

- This tool only works with enterprise geodatabases.
- Typically, you will run the Export Geodatabase Configuration Keywords tool first to obtain a file containing the geodatabase's existing configuration keyword and parameter values. Alter this file and import the changes using the Import Geodatabase Configuration Keywords tool.
- Only the geodatabase administrator can run the Import Geodatabase Configuration Keywords tool.
- This tool is not supported for geodatabases in SAP HANA.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The connection file for the enterprise geodatabase to which you want to import the configuration file. You must connect as the geodatabase administrator. | Workspace |
| Input File | The path to and name of the ASCII text file containing configuration keywords, parameters, and values to import. | File |
| input_database | The connection file for the enterprise geodatabase to which you want to import the configuration file. You must connect as the geodatabase administrator. | Workspace |
| in_file | The path to and name of the ASCII text file containing configuration keywords, parameters, and values to import. | File |

## Code Samples

### Example 1

```python
arcpy.management.ImportGeodatabaseConfigurationKeywords(input_database, in_file)
```

### Example 2

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
input_file = "C:\\temp\\keyword.txt"
arcpy.ImportGeodatabaseConfigurationKeywords_management(ent_gdb,input_file)
```

### Example 3

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
input_file = "C:\\temp\\keyword.txt"
arcpy.ImportGeodatabaseConfigurationKeywords_management(ent_gdb,input_file)
```

### Example 4

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
input_file = "C:\\temp\\keyword.txt"

# Process: Import the text file containing configuration keywords
arcpy.ImportGeodatabaseConfigurationKeywords_management(ent_gdb,input_file)
```

### Example 5

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
input_file = "C:\\temp\\keyword.txt"

# Process: Import the text file containing configuration keywords
arcpy.ImportGeodatabaseConfigurationKeywords_management(ent_gdb,input_file)
```

---

## Import Message (Data Management)

## Summary

Imports changes from a delta file into a replica geodatabase or imports an acknowledgment message into a replica geodatabase.

## Usage

- Use this tool when synchronizing a replica while disconnected. First run the Export Data Change Message tool, which creates a delta file with changes to synchronize. Then copy and import the delta file to the relative replica using the Import Message tool. If the delta file gets lost and you want to resend, use the Re-Export Unacknowledged Messages tool to regenerate the delta file. After the changes are imported, you can export an acknowledgment file from the relative replica using the Export Acknowledgement Message tool. Copy and import the acknowledgment file using the Import Message tool. If the acknowledgment is not received, the next time changes are sent, they will include the new changes and the previously sent changes.
- The geodatabase can be a local geodatabase or a geodata service.
- The tool accepts acknowledgment messages or data change messages. Acknowledgment files are .xml files. Data change messages can be delta file geodatabase (.gdb) or delta .xml files.
- After importing a data change message, you can immediately export an acknowledgment message. The output acknowledgment file must be an .xml file.
- To synchronize replicas in a connected mode, see the Synchronize Changes tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Import To Replica Geodatabase | The replica geodatabase that will receive the imported message. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| Import from Delta file | The file from which the message will be imported. | Workspace ; File |
| Output Acknowledgement File(Optional) | The file that will contain the acknowledgement message. When importing data changes, you can also export a message to acknowledge the import of a data change message. This parameter is only supported for a data change message. | File |
| Conflict Resolution Policy(Optional) | Specifies how conflicts will be resolved when they are encountered while importing a data change message.Manually resolve conflicts—Conflicts must be manually resolved in the versioning reconcile environment. In favor of the database—Conflicts will be automatically resolved in favor of the database receiving the changes. In favor of imported changes—Conflicts will be automatically resolved in favor of the imported changes. | String |
| Conflict Definition(Optional) | Specifies whether the conditions required for a conflict to occur will be detected by object (row) or by attribute (column).By object—Conflicts will be detected by row.By attribute—Conflicts will be detected by column. | String |
| Reconcile with the Parent Version (Check-out replicas)(Optional) | Specifies whether data changes will be automatically reconciled once they are sent to the parent replica if no conflicts are present. This parameter is only active for check-out/check-in replicas.Unchecked—Changes will not be reconciled with the parent version. This is the default.Checked—Changes will be reconciled with the parent version. | Boolean |
| in_geodatabase | The replica geodatabase that will receive the imported message. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| source_delta_file | The file from which the message will be imported. | Workspace ; File |
| output_acknowledgement_file(Optional) | The file that will contain the acknowledgement message. When importing data changes, you can also export a message to acknowledge the import of a data change message. This parameter is only supported for a data change message. | File |
| conflict_policy(Optional) | Specifies how conflicts will be resolved when they are encountered while importing a data change message.MANUAL—Conflicts must be manually resolved in the versioning reconcile environment. IN_FAVOR_OF_DATABASE—Conflicts will be automatically resolved in favor of the database receiving the changes. IN_FAVOR_OF_IMPORTED_CHANGES—Conflicts will be automatically resolved in favor of the imported changes. | String |
| conflict_definition(Optional) | Specifies whether the conditions required for a conflict to occur will be detected by object (row) or by attribute (column).BY_OBJECT—Conflicts will be detected by row.BY_ATTRIBUTE—Conflicts will be detected by column. | String |
| reconcile_with_parent_version(Optional) | Specifies whether data changes will be automatically reconciled once they are sent to the parent replica if no conflicts are present. This parameter is only enabled for check-out/check-in replicas.DO_NOT_RECONCILE—Changes will not be reconciled with the parent version. This is the default.RECONCILE—Changes will be reconciled with the parent version. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ImportMessage(in_geodatabase, source_delta_file, {output_acknowledgement_file}, {conflict_policy}, {conflict_definition}, {reconcile_with_parent_version})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ImportMessage("MySDEdata.sde", "DataChanges.gdb", 
                               "acknowledgement.xml", 
                               "IN_FAVOR_OF_IMPORTED_CHANGES", "BY_OBJECT")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ImportMessage("MySDEdata.sde", "DataChanges.gdb", 
                               "acknowledgement.xml", 
                               "IN_FAVOR_OF_IMPORTED_CHANGES", "BY_OBJECT")
```

### Example 4

```python
# Name: ImportMessage_Example2.py
# Description: Import a data change message (from a delta gdb) into a replica workspace.   

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_workspace = "MySDEdata.sde"
in_message = "DataChanges.gdb" # data changes file (delta gdb)
output_acknowledgement = "acknowledgement.xml" # optional
conflict_policy = "IN_FAVOR_OF_IMPORTED_CHANGES"
conflict_detection = "BY_OBJECT"
reconcile = "" # Only applicable for checkout replicas

# Run Import Message
arcpy.management.ImportMessage(replica_workspace, in_message, 
                               output_acknowledgement, conflict_policy, 
                               conflict_detection, reconcile)
```

### Example 5

```python
# Name: ImportMessage_Example2.py
# Description: Import a data change message (from a delta gdb) into a replica workspace.   

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_workspace = "MySDEdata.sde"
in_message = "DataChanges.gdb" # data changes file (delta gdb)
output_acknowledgement = "acknowledgement.xml" # optional
conflict_policy = "IN_FAVOR_OF_IMPORTED_CHANGES"
conflict_detection = "BY_OBJECT"
reconcile = "" # Only applicable for checkout replicas

# Run Import Message
arcpy.management.ImportMessage(replica_workspace, in_message, 
                               output_acknowledgement, conflict_policy, 
                               conflict_detection, reconcile)
```

### Example 6

```python
# Name: ImportMessage_Example3.py
# Description: Import an acknowledgement message into a replica workspace.  

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_workspace = "MySDEdata.sde"
in_message = "acknowledgement.xml" # Acknowledgement file 
output_acknowledgement = "" 	# not applicable when importing an acknowledgement file
conflict_policy = ""        	# not applicable when importing an acknowledgement file 
conflict_detection = ""     	# not applicable when importing an acknowledgement file
reconcile = ""              	# not applicable when importing an acknowledgement file

# Run Import Message
arcpy.management.ImportMessage(replica_workspace, dc_Message, 
                               output_acknowledgement, conflict_policy, 
                               conflict_detection, reconcile)
```

### Example 7

```python
# Name: ImportMessage_Example3.py
# Description: Import an acknowledgement message into a replica workspace.  

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_workspace = "MySDEdata.sde"
in_message = "acknowledgement.xml" # Acknowledgement file 
output_acknowledgement = "" 	# not applicable when importing an acknowledgement file
conflict_policy = ""        	# not applicable when importing an acknowledgement file 
conflict_detection = ""     	# not applicable when importing an acknowledgement file
reconcile = ""              	# not applicable when importing an acknowledgement file

# Run Import Message
arcpy.management.ImportMessage(replica_workspace, dc_Message, 
                               output_acknowledgement, conflict_policy, 
                               conflict_detection, reconcile)
```

---

## Import Mosaic Dataset Geometry (Data Management)

## Summary

Modifies the geometry for the footprints, boundary, or seamlines in a mosaic dataset to match those in a feature class.

## Usage

- This tool matches the feature in the mosaic dataset with the feature in the feature class based on a common attribute field.
- The footprint is not always used to clip the image in the mosaic dataset. You can change the Always Clip The Image To Its Footprint property in the Mosaic Dataset Properties dialog box on the Defaults tab.
- If the Input Feature Class has more than 1,000 records, you should add an index on the Input Join Field by running the Add Attribute Index tool. If your mosaic dataset is very large and the join field is not indexed, the tool will take much longer to complete.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset whose geometries you want to edit. | Mosaic Layer |
| Target Feature Class | The geometry that you want to change.Footprint—The footprint polygons in the mosaic datasetSeamline—The seamline polygons in the mosaic datasetBoundary—The boundary polygon in the mosaic dataset | String |
| Target Join Field | The field in the mosaic dataset to use as a basis for the join. | Field |
| Input Feature Class | The feature class with the new geometry. | Feature Layer |
| Input Join Field | The field in the Input Feature Class to use as a basis for the join.If the Input Feature Class has more than 1,000 records, add an index on this field by running the Add_Attribute_Index tool. If your mosaic dataset is very large and the join field is not indexed, the tool will take much longer to complete. | Field |
| in_mosaic_dataset | The mosaic dataset whose geometries you want to edit. | Mosaic Layer |
| target_featureclass_type | The geometry that you want to change.FOOTPRINT—The footprint polygons in the mosaic datasetSEAMLINE—The seamline polygons in the mosaic datasetBOUNDARY—The boundary polygon in the mosaic dataset | String |
| target_join_field | The field in the mosaic dataset to use as a basis for the join. | Field |
| input_featureclass | The feature class with the new geometry. | Feature Layer |
| input_join_field | The field in the input_featureclass to use as a basis for the join.If the input_featureclass has more than 1,000 records, add an index on this field by running the Add_Attribute_Index tool. If your mosaic dataset is very large and the join field is not indexed, the tool will take much longer to complete. | Field |

## Code Samples

### Example 1

```python
arcpy.management.ImportMosaicDatasetGeometry(in_mosaic_dataset, target_featureclass_type, target_join_field, input_featureclass, input_join_field)
```

### Example 2

```python
import arcpy
arcpy.ImportMosaicDatasetGeometry_management("c:/workspace/fgdb.gdb/md",
                                                   "FOOTPRINT", "OBJECTID",
                                                   "infootprint.shp", "FTID")
```

### Example 3

```python
import arcpy
arcpy.ImportMosaicDatasetGeometry_management("c:/workspace/fgdb.gdb/md",
                                                   "FOOTPRINT", "OBJECTID",
                                                   "infootprint.shp", "FTID")
```

### Example 4

```python
##===========================
##Import Mosaic Dataset Geometry
##Usage: ImportMosaicDatasetGeometry_management in_mosaic_dataset FOOTPRINT | SEAMLINE
##                                       | BOUNDARY target_join_field 
##                                       input_featureclass input_join_field 

import arcpy
arcpy.env.workspace = "c:/PrjWorkspace/RasGP"

# Import shape file geometry as Mosaic Dataset Footprints
# Note: Feature class FID starts with 0
arcpy.ImportMosaicDatasetGeometry_management("Geometry.gdb/md",
                                            "FOOTPRINT", "OBJECTID",
                                            "infootprint.shp", "FTID")                                      

# Import GDB feature class as Mosaic Dataset Boundary
arcpy.ImportMosaicDatasetGeometry_management("Geometry.gdb/md", "BOUNDARY",
                                      "OBJECTID", "Geometry.gdb/inboundary",
                                      "OBJECTID")
```

### Example 5

```python
##===========================
##Import Mosaic Dataset Geometry
##Usage: ImportMosaicDatasetGeometry_management in_mosaic_dataset FOOTPRINT | SEAMLINE
##                                       | BOUNDARY target_join_field 
##                                       input_featureclass input_join_field 

import arcpy
arcpy.env.workspace = "c:/PrjWorkspace/RasGP"

# Import shape file geometry as Mosaic Dataset Footprints
# Note: Feature class FID starts with 0
arcpy.ImportMosaicDatasetGeometry_management("Geometry.gdb/md",
                                            "FOOTPRINT", "OBJECTID",
                                            "infootprint.shp", "FTID")                                      

# Import GDB feature class as Mosaic Dataset Boundary
arcpy.ImportMosaicDatasetGeometry_management("Geometry.gdb/md", "BOUNDARY",
                                      "OBJECTID", "Geometry.gdb/inboundary",
                                      "OBJECTID")
```

---

## Import Replica Schema (Data Management)

## Summary

Applies replica schema differences using an input replica geodatabase and an XML schema file.

## Usage

- The input replica schema changes file must be XML.
- Modifying the schema of a replica to match the schema of a relative replica is a separate process from data synchronization. Use the following tools for this purpose:Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.Import the changes using the Import Replica Schema tool.To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.
- Use the Compare Replica Schema tool to generate an .xml file containing the schema changes.
- Import the changes using the Import Replica Schema tool.
- To apply replica schema changes, run the Export Replica Schema tool to export the schema of the replica with the changes to an .xml file. Then use the .xml file as input to the Compare Replica Schema tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Import to Replica Geodatabase | The replica geodatabase to which the replica schema will be imported. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| Replica Schema Changes File | The file that contains the replica schema differences that will be imported. | File |
| in_geodatabase | The replica geodatabase to which the replica schema will be imported. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| in_source | The file that contains the replica schema differences that will be imported. | File |

## Code Samples

### Example 1

```python
arcpy.management.ImportReplicaSchema(in_geodatabase, in_source)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ImportReplicaSchema("Countries.gdb", "schemaDifferences.xml")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ImportReplicaSchema("Countries.gdb", "schemaDifferences.xml")
```

### Example 4

```python
# Description: Import a schema changes file into a replica geodatabase

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_geodatabase = "Countries.gdb"
schema_file = "schemaDifferences.xml"

# Run ImportReplicaSchema
arcpy.management.ImportReplicaSchema(replica_geodatabase, schema_file)
```

### Example 5

```python
# Description: Import a schema changes file into a replica geodatabase

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_geodatabase = "Countries.gdb"
schema_file = "schemaDifferences.xml"

# Run ImportReplicaSchema
arcpy.management.ImportReplicaSchema(replica_geodatabase, schema_file)
```

---

## Import Tile Cache (Data Management)

## Summary

Imports tiles from an existing tile cache or a tile package. The target cache must have the same tiling scheme, spatial reference, and storage format as the source tile cache.

## Usage

- Use this tool to import all or portions of a cache from one tile cache to another.
- This tool supports the Parallel Processing environment setting.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Tile Cache | An existing tile cache to which the tiles will be imported. | Raster Layer |
| Source Tile Cache | An existing tile cache or a tile package from which the tiles are imported. | Raster Layer; File |
| Scales [Pixel Size] (Estimated Disk Space) (Optional) | A list of scale levels at which tiles will be imported. | Double |
| Area of Interest (Optional) | An area of interest will spatially constrain where tiles are imported into the cache.This parameter is useful if you want to import tiles for irregularly shaped areas. | Feature Set |
| Overwrite Tiles (Optional) | Determines whether the images in the destination cache will be merged with the tiles from the originating cache or overwritten by them.Unchecked—When the tiles are imported, transparent pixels in the originating cache are ignored by default. This results in a merged or blended image in the destination cache. This is the default.Checked—The import replaces all pixels in the area of interest, effectively overwriting tiles in the destination cache with tiles from the originating cache. | Boolean |
| in_cache_target | An existing tile cache to which the tiles will be imported. | Raster Layer |
| in_cache_source | An existing tile cache or a tile package from which the tiles are imported. | Raster Layer; File |
| scales[scales,...](Optional) | A list of scale levels at which tiles will be imported. | Double |
| area_of_interest(Optional) | An area of interest will spatially constrain where tiles are imported into the cache.This parameter is useful if you want to import tiles for irregularly shaped areas. | Feature Set |
| overwrite(Optional) | Determines whether the images in the destination cache will be merged with the tiles from the originating cache or overwritten by them.MERGE—When the tiles are imported, transparent pixels in the originating cache are ignored by default. This results in a merged or blended image in the destination cache. This is the default.OVERWRITE—The import replaces all pixels in the area of interest, effectively overwriting tiles in the destination cache with tiles from the originating cache. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ImportTileCache(in_cache_target, in_cache_source, {scales}, {area_of_interest}, {overwrite})
```

### Example 2

```python
import arcpy

arcpy.ImportTileCache_management("C:/Data/CacheDatasets/Target", 
                                 "C:/Data/CacheDatasets/Source", 
                                 "4000;2000;1000", "#","MERGE")
```

### Example 3

```python
import arcpy

arcpy.ImportTileCache_management("C:/Data/CacheDatasets/Target", 
                                 "C:/Data/CacheDatasets/Source", 
                                 "4000;2000;1000", "#","MERGE")
```

### Example 4

```python
#Import tile cache for some levels from a pre-existing tile cache

import arcpy

    
cacheTarget = "C:/Data/CacheDatasets/Target"
cacheSource = "C:/Data/CacheDatasets/Source"
scales = "4000;2000;1000"
areaofinterest = "#"
overwrite = "MERGE"

arcpy.ImportTileCache_management(cacheTarget, cacheSource, scales, 
                                 areaofinterest, overwrite)
```

### Example 5

```python
#Import tile cache for some levels from a pre-existing tile cache

import arcpy

    
cacheTarget = "C:/Data/CacheDatasets/Target"
cacheSource = "C:/Data/CacheDatasets/Source"
scales = "4000;2000;1000"
areaofinterest = "#"
overwrite = "MERGE"

arcpy.ImportTileCache_management(cacheTarget, cacheSource, scales, 
                                 areaofinterest, overwrite)
```

---

## Import XML Workspace Document (Data Management)

## Summary

Imports the contents of an XML workspace document into an existing geodatabase.

## Usage

- The Target Geodatabase parameter value must already exist and can be a file, mobile, or enterprise geodatabase. To create an empty geodatabase, use the Create File Geodatabase, Create Mobile Geodatabase, or Create Enterprise Geodatabase tool.
- If you are importing to a file or enterprise geodatabase and want to use a configuration keyword, you can choose one from the Configuration Keyword drop-down list. In a Python script, you need to know the name of the configuration keyword to use.
- If the Allow geoprocessing tools to overwrite existing datasets option is unchecked and a data element from the input XML workspace document has the same name as a data element in the Target Geodatabase parameter value, the data element will be imported with a new unique name. If this option is checked, existing datasets will be overwritten. For more information about overwriting tool output, see Geoprocessing options in ArcGIS Pro.
- The tool messages will include the list of data element names that were imported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Geodatabase | An existing geodatabase where the contents of the XML workspace document will be imported. | Workspace |
| Import File | The input XML workspace document file containing geodatabase contents to be imported. The file can be an .xml file or a compressed .zip or .z file containing the .xml file. | File |
| Import Options (Optional) | Specifies whether both data (feature class and table records, including geometry) and schema will be imported, or only the schema will be imported. Import data and schema—Data and schema will be imported. This is the default.Import schema only—Only the schema will be imported. | String |
| Configuration Keyword (Optional) | The geodatabase configuration keyword to be applied if the Target Geodatabase parameter value is an enterprise or file geodatabase. | String |
| target_geodatabase | An existing geodatabase where the contents of the XML workspace document will be imported. | Workspace |
| in_file | The input XML workspace document file containing geodatabase contents to be imported. The file can be an .xml file or a compressed .zip or .z file containing the .xml file. | File |
| import_type(Optional) | Specifies whether both data (feature class and table records, including geometry) and schema will be imported, or only the schema will be imported. DATA—Data and schema will be imported. This is the default.SCHEMA_ONLY—Only the schema will be imported. | String |
| config_keyword(Optional) | The geodatabase configuration keyword to be applied if the target_geodatabase parameter value is an enterprise or file geodatabase. | String |

## Code Samples

### Example 1

```python
arcpy.management.ImportXMLWorkspaceDocument(target_geodatabase, in_file, {import_type}, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.management.ImportXMLWorkspaceDocument("C:/Data/Target.gdb", 
                                            "C:/Data/StJohnsData.xml", 
                                            "SCHEMA_ONLY", "DEFAULTS")
```

### Example 3

```python
import arcpy
arcpy.management.ImportXMLWorkspaceDocument("C:/Data/Target.gdb", 
                                            "C:/Data/StJohnsData.xml", 
                                            "SCHEMA_ONLY", "DEFAULTS")
```

### Example 4

```python
# Name: ImportXMLWorkspaceDocument.py
# Description: Import the contents of an XML workspace document into a target 
#              geodatabase. 

# Import system modules
import arcpy

# Set local variables
target_gdb = "c:/data/Target.gdb"
in_file = "c:/data/StJohnsData.xml"
import_type = "SCHEMA_ONLY"
config_keyword = "DEFAULTS"

# Run ImportXMLWorkspaceDocument
arcpy.management.ImportXMLWorkspaceDocument(target_gdb, in_file, import_type, 
                                            config_keyword)
```

### Example 5

```python
# Name: ImportXMLWorkspaceDocument.py
# Description: Import the contents of an XML workspace document into a target 
#              geodatabase. 

# Import system modules
import arcpy

# Set local variables
target_gdb = "c:/data/Target.gdb"
in_file = "c:/data/StJohnsData.xml"
import_type = "SCHEMA_ONLY"
config_keyword = "DEFAULTS"

# Run ImportXMLWorkspaceDocument
arcpy.management.ImportXMLWorkspaceDocument(target_gdb, in_file, import_type, 
                                            config_keyword)
```

---

## Integrate (Data Management)

## Summary

Analyzes the coordinate locations of feature vertices among features in one or more feature classes. Those that fall within a specified distance of one another are assumed to represent the same location and are assigned a common coordinate value (in other words, they are colocated). The tool also adds vertices where feature vertices are within the x,y tolerance of an edge and where line segments intersect.

## Usage

- If input features are selected, this tool will run only on those selected features.
- This tool performs the same kind of work as a topology in that it moves features within an x,y tolerance and inserts vertices where features intersect. Consider using a topology to perform this type of operation, because a topology allows you to specify rules and conditions about how features relate to each other.Use this tool rather than a topology in the following circumstances:You do not need to specify rules about how features are moved and you want all features to coalesce within a specified tolerance.You want lines to have vertices wherever they intersect. You are working with nongeodatabase features, such as shapefiles, or with features from different geodatabases (features in a topology must all come from the same feature dataset).
- You do not need to specify rules about how features are moved and you want all features to coalesce within a specified tolerance.
- You want lines to have vertices wherever they intersect.
- You are working with nongeodatabase features, such as shapefiles, or with features from different geodatabases (features in a topology must all come from the same feature dataset).
- Potential problems present in the data may be resolved during integration. Handling of extremely small overshoots or undershoots, automatic sliver removal of duplicate segments, and coordinate thinning along boundary lines may be resolved.
- It is recommended that you do not provide a value for the XY Tolerance parameter. When no value is provided, the tool will check the input feature classes' spatial reference to determine the x,y tolerance to use during integration. Ensure that the spatial reference of the input data is set to its default x,y resolution and x,y tolerance.The XY Tolerance parameter is not intended to be used to generalize geometry shapes. It is intended to integrate line work and boundaries within the context of a properly set input feature class spatial reference. Setting the XY Tolerance parameter to a value other than the default for the input spatial reference may cause features to move too much or too little, resulting in geometry issues. If the correct spatial reference properties are used, running the Integrate tool can minimize the amount of movement in the data during subsequent topological operations (such as overlay and dissolve). Learn more about cluster processing
- This tool only accepts simple feature classes as input (point, multipoint, line, or polygon).
- To undo changes to the input features, use this tool in an edit session.
- When processing datasets that contain individual features with a very large number of vertices (for example, hundreds of thousands to millions of vertices in a single feature), some geometric processing operations may run out of memory. For details, see Geoprocessing with large datasets.
- The output data element of this tool is a derived multivalue output. To use this tool's output with another tool, use its input directly and set its output as a precondition of the other tool. Learn more about setting a precondition

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature classes that will be integrated. When the distance between features is small in comparison to the tolerance, the vertices or points will be clustered (moved to be coincident).The feature class or layer with the lower rank will snap to the feature from the feature class or layer with the higher rank (with 1 being a higher rank than 2). Features in the feature class with a rank of 1 may move when a large x,y tolerance is used. For more information, see Priority ranks and geoprocessing tools. | Value Table |
| XY Tolerance(Optional) | The distance that determines the range in which feature vertices are made coincident. To minimize undesired movement of vertices, the x,y tolerance should be small. If no value is provided, the x,y tolerance from the first dataset in the list of inputs will be used.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |
| in_features[[Feature Layer, Long],...] | The feature classes that will be integrated. When the distance between features is small in comparison to the tolerance, the vertices or points will be clustered (moved to be coincident).The feature class or layer with the lower rank will snap to the feature from the feature class or layer with the higher rank (with 1 being a higher rank than 2). Features in the feature class with a rank of 1 may move when a large x,y tolerance is used. For more information, see Priority ranks and geoprocessing tools. | Value Table |
| cluster_tolerance(Optional) | The distance that determines the range in which feature vertices are made coincident. To minimize undesired movement of vertices, the x,y tolerance should be small. If no value is provided, the x,y tolerance from the first dataset in the list of inputs will be used.Caution: Changing this parameter's value may cause failure or unexpected results. It is recommended that you do not modify this parameter. It has been removed from view on the tool dialog box. By default, the input feature class's spatial reference x,y tolerance property is used. | Linear Unit |

## Code Samples

### Example 1

```python
arcpy.management.Integrate(in_features, {cluster_tolerance})
```

### Example 2

```python
import arcpy

arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("Habitat_Analysis.gdb/vegtype", "C:/output/output.gdb/vegtype")
arcpy.management.Integrate("C:/output/output.gdb/vegtype")
```

### Example 3

```python
import arcpy

arcpy.env.workspace = "C:/data"
arcpy.management.CopyFeatures("Habitat_Analysis.gdb/vegtype", "C:/output/output.gdb/vegtype")
arcpy.management.Integrate("C:/output/output.gdb/vegtype")
```

### Example 4

```python
# Description: Run Integrate on a feature class
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
 
# Set local variables
inFeatures = "vegtype"
integrateFeatures = "C:/output/output.gdb/vegtype"
 
# Run CopyFeatures (since Integrate modifies the original data,
#  this ensures the original is preserved)
arcpy.management.CopyFeatures(inFeatures, integrateFeatures)
 
# Run Integrate
arcpy.management.Integrate(integrateFeatures)
```

### Example 5

```python
# Description: Run Integrate on a feature class
 
# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
 
# Set local variables
inFeatures = "vegtype"
integrateFeatures = "C:/output/output.gdb/vegtype"
 
# Run CopyFeatures (since Integrate modifies the original data,
#  this ensures the original is preserved)
arcpy.management.CopyFeatures(inFeatures, integrateFeatures)
 
# Run Integrate
arcpy.management.Integrate(integrateFeatures)
```

---

## Interpolate From Point Cloud (Data Management)

## Summary

Interpolates a digital terrain model (DTM) or a digital surface model (DSM) from a point cloud.

## Usage

- The form of the point cloud can be either LAS files or a solution point table.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Folder or Point Table | The path and name of the input file, folder, or feature layer. The input can be a folder of LAS files or a solution point table from orthomapping tools. For cloud storage, provide the cloud storage path, such as C:\Temp\Cloud.acs\lasfolder. The LAS files can be the output from the Generate Point Cloud tool, in which LAS points are categorized as ground and above ground. The solution point table is output from either the Compute Block Adjustments tool or the Compute Camera Model tool. | Folder; File; Feature Class; Feature Layer |
| Output Raster | The output raster dataset location, name, and file extension. You can also save the output raster dataset by providing a cloud storage path such as C:\Temp\Cloud.acs\lasfolder.The output can be created in most writable raster formats, such as TIFF, CRF, or IMG. | Raster Dataset |
| Cellsize | The cell size of the output raster dataset. | Double |
| Interpolation Method(Optional) | Specifies the method that will be used to interpolate the output raster dataset from the point cloud. TIN linear interpolation— The triangulation method will be used. It is also known as triangulated irregular network (TIN) linear interpolation and is designed for irregularly distributed sparse points, such as solution points from block adjustment computation.TIN natural neighbor interpolation—The natural neighbor method will be used. It is similar to triangulation but generates a smoother surface and is more computationally intensive.Inverse distance weighted average interpolation—The inverse distance weighted (IDW) average method will be used. It is used for regularly distributed dense points, such as point cloud LAS files from the Generate Point Cloud tool. The IDW search radius is automatically computed based on average point density. | String |
| Smoothing Method(Optional) | Specifies the filter that will be used to smooth the output raster dataset. Gaussian 3 by 3—A Gaussian filter with a 3 by 3 window will be used.Gaussian 5 by 5—A Gaussian filter with a 5 by 5 window will be used.Gaussian 7 by 7—A Gaussian filter with a 7 by 7 window will be used.Gaussian 9 by 9—A Gaussian filter with a 9 by 9 window will be used.No smoothing—No smoothing filter will be used. | String |
| Surface Type(Optional) | Specifies whether a digital terrain model or a digital surface model will be created.Digital terrain model—A digital terrain model will be created by interpolating only the ground points.Digital surface model—A digital surface model will be created by interpolating all the points. | String |
| Input Fill DEM(Optional) | A DEM raster input that is used to fill NoData areas. Areas of NoData may exist where pixels do not have enough information from the input to generate values. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| Classify Ground Options (Optional) | Classify ground points from the input LAS data. This parameter is active when the Surface Type parameter is set to Digital terrain model.Note:To set an option in the Geoprocessing pane, type the Name keyword method that will be used to detect ground points, and the corresponding value in the list box.Classify—Classify the ground using different options depending on the type of terrain. The options are not case sensitive. standard—This method has a tolerance for slope variation that allows it to capture gradual undulations in the ground's topography that would typically be missed by the conservative option, but not capture the type of sharp relief that would be captured by the aggressive option. This is the default.conservative—When compared to other options, this method uses a tighter restriction on the variation of the ground's slope that allows it to differentiate the ground from low-lying vegetation such as grass and shrubbery. It is best suited for topography with minimal curvature.aggressive—This method detects ground areas with sharp relief, such as ridges and hilltops, that may be ignored by the standard option. This method is best used in a second iteration of this tool with the ReuseGround option set to 1. Avoid using this method in urban areas or flat, rural areas, since it may result in the misclassification of taller objects—such as utility towers, vegetation, and portions of buildings—as ground.LowNoise—The distance below the ground that will be used to classify the point to be low-noise points. The unit is meters. The default value is 0.25 meter.HighNoise—The distance above the ground that will be used to classify the point to be high-noise points. The unit is meters. The default value is 100 meters.ReuseGround—Specifies whether existing ground points will be reclassified or reused. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0.ReuseLowNoise—Specifies whether existing low-noise points will be reused or reclassified. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0.ReuseHighNoise—Specifies whether existing high-noise points will be reused or reclassified. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0. | Value Table |
| in_container | The path and name of the input file, folder, or feature layer. The input can be a folder of LAS files or a solution point table from orthomapping tools. For cloud storage, provide the cloud storage path, such as C:\Temp\Cloud.acs\lasfolder. The LAS files can be the output from the Generate Point Cloud tool, in which LAS points are categorized as ground and above ground. The solution point table is output from either the Compute Block Adjustments tool or the Compute Camera Model tool. | Folder; File; Feature Class; Feature Layer |
| out_raster | The output raster dataset location, name, and file extension. You can also save the output raster dataset by providing a cloud storage path such as C:\Temp\Cloud.acs\lasfolder.The output can be created in most writable raster formats, such as TIFF, CRF, or IMG. | Raster Dataset |
| cell_size | The cell size of the output raster dataset. | Double |
| interpolation_method(Optional) | Specifies the method that will be used to interpolate the output raster dataset from the point cloud. TRIANGULATION— The triangulation method will be used. It is also known as triangulated irregular network (TIN) linear interpolation and is designed for irregularly distributed sparse points, such as solution points from block adjustment computation.NATURAL_NEIGHBOR—The natural neighbor method will be used. It is similar to triangulation but generates a smoother surface and is more computationally intensive.IDW—The inverse distance weighted (IDW) average method will be used. It is used for regularly distributed dense points, such as point cloud LAS files from the Generate Point Cloud tool. The IDW search radius is automatically computed based on average point density. | String |
| smooth_method(Optional) | Specifies the filter that will be used to smooth the output raster dataset. GAUSS3x3—A Gaussian filter with a 3 by 3 window will be used.GAUSS5x5—A Gaussian filter with a 5 by 5 window will be used.GAUSS7x7—A Gaussian filter with a 7 by 7 window will be used.GAUSS9x9—A Gaussian filter with a 9 by 9 window will be used.NONE—No smoothing filter will be used. | String |
| surface_type(Optional) | Specifies whether a digital terrain model or a digital surface model will be created.DTM—A digital terrain model will be created by interpolating only the ground points.DSM—A digital surface model will be created by interpolating all the points. | String |
| fill_dem(Optional) | A DEM raster input that is used to fill NoData areas. Areas of NoData may exist where pixels do not have enough information from the input to generate values. | Raster Dataset; Raster Layer; Mosaic Dataset; Mosaic Layer |
| options[[name, value],...](Optional) | Classify ground points from the input LAS data.This parameter is active when the surface_type parameter is set to DTM.Classify—Classify the ground using different options depending on the type of terrain. The options are not case sensitive. standard—This method has a tolerance for slope variation that allows it to capture gradual undulations in the ground's topography that would typically be missed by the conservative option, but not capture the type of sharp relief that would be captured by the aggressive option. This is the default.conservative—When compared to other options, this method uses a tighter restriction on the variation of the ground's slope that allows it to differentiate the ground from low-lying vegetation such as grass and shrubbery. It is best suited for topography with minimal curvature.aggressive—This method detects ground areas with sharp relief, such as ridges and hilltops, that may be ignored by the standard option. This method is best used in a second iteration of this tool with the ReuseGround option set to 1. Avoid using this method in urban areas or flat, rural areas, since it may result in the misclassification of taller objects—such as utility towers, vegetation, and portions of buildings—as ground.LowNoise—The distance below the ground that will be used to classify the point to be low-noise points. The unit is meters. The default value is 0.25 meter.HighNoise—The distance above the ground that will be used to classify the point to be high-noise points. The unit is meters. The default value is 100 meters.ReuseGround—Specifies whether existing ground points will be reclassified or reused. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0.ReuseLowNoise—Specifies whether existing low-noise points will be reused or reclassified. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0.ReuseHighNoise—Specifies whether existing high-noise points will be reused or reclassified. A value of 0 specifies reclassify, and a value of 1 specifies reuse. The default value is 0. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.InterpolateFromPointCloud(in_container, out_raster, cell_size, {interpolation_method}, {smooth_method}, {surface_type}, {fill_dem}, {options})
```

### Example 2

```python
# Import system modules 
import arcpy
 
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container=r"C:\data\LASFoler", out_raster=r"C:\data\dtm.crf", cell_size=0.2, interpolation_method="IDW", smooth_method="GAUSS5x5", surface_type="DTM", fill_dem=None, options="Classify standard;LowNoise 0.25;HighNoise 110;ReuseGround 0;ReuseLowNoise 1;ReuseHighNoise 1")
```

### Example 3

```python
# Import system modules 
import arcpy
 
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container=r"C:\data\LASFoler", out_raster=r"C:\data\dtm.crf", cell_size=0.2, interpolation_method="IDW", smooth_method="GAUSS5x5", surface_type="DTM", fill_dem=None, options="Classify standard;LowNoise 0.25;HighNoise 110;ReuseGround 0;ReuseLowNoise 1;ReuseHighNoise 1")
```

### Example 4

```python
# Define input parameters 
import arcpy
in_container="C:/data/LASFoler"
out_raster="C:/data/dsm.crf"
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container,out_raster, 0.2, "TRIANGULATION", "GAUSS5x5", "DSM")
```

### Example 5

```python
# Define input parameters 
import arcpy
in_container="C:/data/LASFoler"
out_raster="C:/data/dsm.crf"
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container,out_raster, 0.2, "TRIANGULATION", "GAUSS5x5", "DSM")
```

### Example 6

```python
# Define input parameters 
import arcpy
in_container="C:/data/LASFoler"
out_raster="C:/data/Azure.acs/ProductFolder/dsm.crf"
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container,out_raster, 0.2, "TRIANGULATION", "GAUSS5x5", "DSM")
```

### Example 7

```python
# Define input parameters 
import arcpy
in_container="C:/data/LASFoler"
out_raster="C:/data/Azure.acs/ProductFolder/dsm.crf"
# Execute 
arcpy.management.InterpolateFromPointCloud(in_container,out_raster, 0.2, "TRIANGULATION", "GAUSS5x5", "DSM")
```

---

## Join Field (Data Management)

## Summary

Permanently joins the contents of a table to another table based on a common attribute field. The input table is updated to contain the fields from the join table. You can select which fields from the join table will be added to the input table.

## Usage

- The records in the Input Table value are matched to the records in the Join Table value based on the values of the Input Join Field and Join Table Field parameters. You can also select specific fields from the Join Table value to be appended to the Input Table value during the join.
- The Input Table value can be a feature class (including a shapefile) or a table.
- All fields in the Input Table value will be kept during the join. You can also select specific fields from the Join Table value to be added to the output. Use the Transfer Fields parameter to add these fields.
- Records from the Join Table value can be matched to more than one record in the Input Table value.
- If no fields are selected for the optional Transfer Fields parameter, all fields from the Join Table value will be joined to the output. To alter field names, aliases, or properties, set the Transfer Method parameter to Use field mapping.
- Joins can be based on fields of type text, date, or number.
- Joins based on text fields are case sensitive.
- Fields of different number formats can be joined as long as the values are equal. For example, a field of type float can be joined to a short integer field.
- When joined to an input table, fields from the join table with a Global ID type or an Object ID type will not be transferred.The Input Join Field value and the Join Table Field value can have different field names.
- If a join field has the same name as a field from the input table, the joined field will be appended with _1 (or _2, or _3, and so on) to make it unique.
- If the Input Table and Join Table parameter values have the same name, clicking the Validate Join button will fail with an error. This is a known limitation. However, the tool will run successfully when you click the Run button.
- If the Select transfer fields option is specified for the Transfer Method parameter and field values in the Join Table Field parameter value are not unique, only the first occurrence of each value will be used. To account for values other than the first occurrence (a one-to-many join), set the Transfer Method parameter to Use field mapping. To perform a one-to-many join, the Input Table parameter value must have an Object ID field and be in the same workspace as the Join Table parameter value.
- Use the Field Map parameter to manage the fields and their content in the output dataset. Add and remove fields from the fields list, reorder the fields list, and rename fields. The default data type of an output field is the same as the data type of the first input field (of that name) it encounters. You can change the data type to another valid data type.Use an action to determine how the values from one or multiple input fields will be merged into a single output field. The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters. Standard Deviation is not a valid option for single input values.Use the Export option to save a field map as a .fieldmap file.Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the output field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.Fields can also be mapped in Python script.
- Add and remove fields from the fields list, reorder the fields list, and rename fields.
- The default data type of an output field is the same as the data type of the first input field (of that name) it encounters. You can change the data type to another valid data type.
- Use an action to determine how the values from one or multiple input fields will be merged into a single output field. The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.
- When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters.
- Standard Deviation is not a valid option for single input values.
- Use the Export option to save a field map as a .fieldmap file.
- Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.
- Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the output field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.
- Fields can also be mapped in Python script.
- If the Field Map parameter is specified with the Join merge rule, there is no way to guarantee that the order of the joined values will be consistent with the row order of the Join Table Field parameter if the values are not unique. For example, if three features with the ANIMAL attribute values of mouse, cat, and dog are joined, the result will not necessarily be in the order mouse, cat, and dog.
- The Validate Join tool can be used to validate a join between two layers or tables to determine if the layers or tables have valid field names and Object ID fields, if the join produces matching records, if the join is a one-to-one or one-to-many join, and other properties of the join. A button to validate the join is available on the tool dialog box for ease of use.
- Indexing the input field and join field can improve performance. Use the Index Join Fields parameter to add or replace indexes.
- If the join results are unexpected or incomplete, review whether the input field and join field are indexed. If the fields are not indexed, try adding an index. If the fields are already indexed, try deleting and re-adding the index to correct any problems with the index. Use the Index Join Fields parameter to manage indexes while running the tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table or feature class to which the join table will be joined. | Mosaic Layer; Raster Layer; Table View |
| Input Field | The field in the input table on which the join will be based. | Field |
| Join Table | The table that will be joined to the input table. | Mosaic Layer; Raster Layer; Table View |
| Join Field | The field in the join table that contains the values on which the join will be based. | Field |
| Transfer Fields(Optional) | The fields from the join table that will be transferred to the input table based on a join between the input table and the join table. | Field |
| Transfer Method(Optional) | Specifies how joining fields and field types will be transferred to the output.Select transfer fields—Fields and field types from the joined table will be transferred to the output. This is the default.Use field mapping—The transfer of fields and field types from the joined table to the output will be controlled by the Field Map parameter. | String |
| Field Map(Optional) | The fields that will be joined to the input table with their respective properties and source fields. All fields from the join table will be included by default. Use the field map to add, delete, rename, and reorder fields, as well as change other field properties. The field map can be used to combine values from two or more input fields into a single output field. | Field Mappings |
| Index Join Fields (Optional) | Specifies whether attribute indexes will be added or replaced for the input field and join field.Do not add indexes—Attribute indexes will not be added. This is the default.Add an attribute index for fields that do not have an existing index—An attribute index will be added for any field that does not have an index. Existing attribute indexes will be retained.Replace indexes for all fields—An attribute index will be added for any field that does not have an index. Existing attribute indexes will be replaced. | String |
| in_data | The table or feature class to which the join table will be joined. | Mosaic Layer; Raster Layer; Table View |
| in_field | The field in the input table on which the join will be based. | Field |
| join_table | The table that will be joined to the input table. | Mosaic Layer; Raster Layer; Table View |
| join_field | The field in the join table that contains the values on which the join will be based. | Field |
| fields[fields,...](Optional) | The fields from the join table that will be transferred to the input table based on a join between the input table and the join table. | Field |
| fm_option(Optional) | Specifies how joining fields and field types will be transferred to the output.NOT_USE_FM—Fields and field types from the joined table will be transferred to the output. This is the default.USE_FM—The transfer of fields and field types from the joined table to the output will be controlled by the field_mapping parameter. | String |
| field_mapping(Optional) | The fields that will be joined to the input table with their respective properties and source fields. All fields from the join table will be included by default. Use the field map to add, delete, rename, and reorder fields, as well as change other field properties. The field map can be used to combine values from two or more input fields into a single output field.In Python, use the FieldMappings class to define this parameter. | Field Mappings |
| index_join_fields(Optional) | Specifies whether attribute indexes will be added or replaced for the input field and join field.NO_INDEXES—Attribute indexes will not be added. This is the default.NEW_INDEXES—An attribute index will be added for any field that does not have an index. Existing attribute indexes will be retained.REPLACE_INDEXES—An attribute index will be added for any field that does not have an index. Existing attribute indexes will be replaced. | String |

## Code Samples

### Example 1

```python
arcpy.management.JoinField(in_data, in_field, join_table, join_field, {fields}, {fm_option}, {field_mapping}, {index_join_fields})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
arcpy.management.JoinField("zion_park", "zonecode", "zion_zoning", "zonecode", 
                           ["land_use", "land_cover"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
arcpy.management.JoinField("zion_park", "zonecode", "zion_zoning", "zonecode", 
                           ["land_use", "land_cover"])
```

### Example 4

```python
# PermanentJoin.py
# Purpose: Join two fields from a table to a feature class 

# Import system modules
import arcpy

# Set the current workspace 
arcpy.env.workspace = "c:/data/data.gdb"

# Set the local parameters
inFeatures = "zion_park"
joinField = "zonecode"
joinTable = "zion_zoning"
fieldList = ["land_use", "land_cover"]

# Join two feature classes by the zonecode field and only carry 
# over the land use and land cover fields
arcpy.management.JoinField(inFeatures, joinField, joinTable, joinField, 
                           fieldList)
```

### Example 5

```python
# PermanentJoin.py
# Purpose: Join two fields from a table to a feature class 

# Import system modules
import arcpy

# Set the current workspace 
arcpy.env.workspace = "c:/data/data.gdb"

# Set the local parameters
inFeatures = "zion_park"
joinField = "zonecode"
joinTable = "zion_zoning"
fieldList = ["land_use", "land_cover"]

# Join two feature classes by the zonecode field and only carry 
# over the land use and land cover fields
arcpy.management.JoinField(inFeatures, joinField, joinTable, joinField, 
                           fieldList)
```

---

## LAS Dataset Statistics (Data Management)

## Summary

Calculates or updates statistics for a LAS dataset and generates an optional statistics report.

## Usage

- Computing statistics will create a spatial and attribute index that helps optimize display and analysis performance. Statistics also provide a more accurate estimate for average point spacing, and additional details about point attributes. The statistics are stored in a file with the same name as its corresponding LAS file, but ending in the .lasx extension.
- Statistics enable filtering options for a LAS dataset layer to automatically display the available class codes and return values in the LAS files. The LAS dataset layer can be filtered through its Layer Properties dialog box or the Make LAS Dataset Layer tool.
- The optional statistics report file provides an overview of LAS properties of all files in the LAS dataset or each individual LAS file in a text format that can be imported into any number of applications.
- If a LAS file's classification codes or flags are modified, its statistics get invalidated and will need to be recomputed. Similarly, if a LAS dataset has new LAS file references added, it also will result in the statistics being invalidated.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| Skip Existing (Optional) | Specifies whether statistics will be calculated for all lidar files or only for those that do not have statistics: Checked—LAS files with up-to-date statistics will be skipped, and statistics will only be calculated for newly added LAS files or ones that were updated since the initial calculation. This is the default.Unchecked—Statistics will be calculated for all LAS files, including ones that have up-to-date statistics. This is useful if the LAS files were modified in an external application that went undetected by ArcGIS. | Boolean |
| Output Statistics Report Text File (Optional) | The output text file that will contain the summary of the LAS dataset statistics. | Text File |
| Summary Level (Optional) | Specify the type of summary contained in the report. Aggregate Statistics for All Files—The report will summarize statistics for the entire LAS dataset. This is the default.Statistics for Each LAS File—The report will summarize statistics for the LAS files referenced by the LAS dataset. | String |
| Delimiter(Optional) | The delimiter that will be used to indicate the separation of entries in the columns of the text file table.Space—A space will be used to delimit field values. This is the default.Comma—A comma will be used to delimit field values. This option is not applicable if the decimal separator is also a comma. | String |
| Decimal Separator(Optional) | The decimal character that will be used in the text file to differentiate the integer of a number from its fractional part.Point—A point will be used as the decimal character. This is the default.Comma—A comma will be used as the decimal character. | String |
| in_las_dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| calculation_type(Optional) | Specifies whether statistics will be calculated for all lidar files or only for those that do not have statistics: SKIP_EXISTING_STATS—LAS files with up-to-date statistics will be skipped, and statistics will only be calculated for newly added LAS files or ones that were updated since the initial calculation. This is the default.OVERWRITE_EXISTING_STATS—Statistics will be calculated for all LAS files, including ones that have up-to-date statistics. This is useful if the LAS files were modified in an external application that went undetected by ArcGIS. | Boolean |
| out_file(Optional) | The output text file that will contain the summary of the LAS dataset statistics. | Text File |
| summary_level(Optional) | Specify the type of summary contained in the report. DATASET—The report will summarize statistics for the entire LAS dataset. This is the default.LAS_FILES—The report will summarize statistics for the LAS files referenced by the LAS dataset. | String |
| delimiter(Optional) | The delimiter that will be used to indicate the separation of entries in the columns of the text file table. SPACE—A space will be used to delimit field values. This is the default.COMMA—A comma will be used to delimit field values. This option is not applicable if the decimal separator is also a comma. | String |
| decimal_separator(Optional) | The decimal character that will be used in the text file to differentiate the integer of a number from its fractional part. DECIMAL_POINT—A point will be used as the decimal character. This is the default.DECIMAL_COMMA—A comma will be used as the decimal character. | String |

## Code Samples

### Example 1

```python
arcpy.management.LasDatasetStatistics(in_las_dataset, {calculation_type}, {out_file}, {summary_level}, {delimiter}, {decimal_separator})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.LASDatasetStatistics_3d("test.lasd", "NO_FORCE", "LAS_FILE", 
                            "DECIMAL_POINT", "SPACE", "LAS_summary.txt")
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.LASDatasetStatistics_3d("test.lasd", "NO_FORCE", "LAS_FILE", 
                            "DECIMAL_POINT", "SPACE", "LAS_summary.txt")
```

### Example 4

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

### Example 5

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

---

## LAS Point Statistics As Raster (Data Management)

## Summary

Creates a raster whose cell values reflect statistical information about LAS points.

## Usage

- You can filter the points that are processed by this tool using any combination of classification codes, classification flags, and return values through the LAS dataset layer's point filters. The filters can be defined on the Layer Properties dialog box or using the Make LAS Dataset Layer tool. LAS point filters will reflect in the results obtained for all statistical outputs except the Pulse Count method, which evaluates all last return points.
- The Point Count option of the Method parameter evaluates the point density of the LAS dataset. This information is helpful when detecting irregular hot spots of high density points, because this type of data can produce degraded performance when processing the LAS files using other tools. If high density hot spots are encountered, the Thin LAS tool can be used to produce a new point cloud with more consistent spatial distribution.
- The Most Frequent Class Code option of the Method parameter can be used to identify the number of unique objects defined by a specific class code, particularly if those objects are spatially separated in the horizontal direction. To do this, first apply a point filter to isolate the specific class code. Then create the statistics raster using a cell size that is large enough to ensure that the connectivity of cells representing points belong to the same object and that gaps between points from different objects are preserved. For example, to determine the cell size appropriate for connecting points belonging to a street light, consider the overall spacing of the points and the distance between nearby street lights. The resulting raster can be used to assess the footprint of each object. The number of unique objects can be obtained by either converting the raster to a polygon feature using the Raster To Polygon tool or assigning a unique value to each cluster of cells using the Region Group tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| Output Raster | The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, do not add a file extension to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder, such as .tif to generate a GeoTIFF or .img to generate an ERDAS IMAGINE format file. If the raster is stored as a .tif file or in a geodatabase, the raster compression type and quality can be specified using geoprocessing environment settings. | Raster Dataset |
| Method (Optional) | Specifies the type of statistics that will be collected about the LAS points in each cell of the output raster. Pulse Count—The number of last return points will be collected.Point Count—The number of points from all returns will be collected.Most Frequent Last Return—The most frequent last return value will be collected.Most Frequent Class Code—The most frequent class code will be collected.Range of Intensity Values—The range of intensity values will be collected.Range of Elevation Values—The range of elevation values will be collected. | String |
| Sampling Type (Optional) | Specifies how the Sampling Value parameter will be interpreted to define the output raster's cell size.Observations—The Sampling Value will define the number of columns or rows in the output raster based on whichever is longest. The cell size will be derived by dividing the longest side of the output's extent with the input in the Sampling Value parameter. If an observation value of 3000 is used on a dataset whose longest side is 23.67 kilometers, the output raster's resolution will be 7.89 meters. This method offers a helpful way of creating an output with a predictable size that can be generated rapidly.Cell Size—The cell size will be directly defined by the Sampling Value parameter. This is the default. | String |
| Sampling Value (Optional) | The value used in conjunction with the Sampling Type parameter to define the output raster's cell size. | Double |
| in_las_dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| out_raster | The location and name of the output raster. When storing a raster dataset in a geodatabase or in a folder such as an Esri Grid, do not add a file extension to the name of the raster dataset. A file extension can be provided to define the raster's format when storing it in a folder, such as .tif to generate a GeoTIFF or .img to generate an ERDAS IMAGINE format file. If the raster is stored as a .tif file or in a geodatabase, the raster compression type and quality can be specified using geoprocessing environment settings. | Raster Dataset |
| method(Optional) | Specifies the type of statistics that will be collected about the LAS points in each cell of the output raster. PULSE_COUNT—The number of last return points will be collected.POINT_COUNT—The number of points from all returns will be collected.PREDOMINANT_LAST_RETURN—The most frequent last return value will be collected.PREDOMINANT_CLASS—The most frequent class code will be collected.INTENSITY_RANGE—The range of intensity values will be collected.Z_RANGE—The range of elevation values will be collected. | String |
| sampling_type(Optional) | Specifies how the Sampling Value parameter will be interpreted to define the output raster's cell size. OBSERVATIONS—The Sampling Value will define the number of columns or rows in the output raster based on whichever is longest. The cell size will be derived by dividing the longest side of the output's extent with the input in the Sampling Value parameter. If an observation value of 3000 is used on a dataset whose longest side is 23.67 kilometers, the output raster's resolution will be 7.89 meters. This method offers a helpful way of creating an output with a predictable size that can be generated rapidly.CELLSIZE—The cell size will be directly defined by the Sampling Value parameter. This is the default. | String |
| sampling_value(Optional) | The value used in conjunction with the Sampling Type parameter to define the output raster's cell size. | Double |

## Code Samples

### Example 1

```python
arcpy.management.LasPointStatsAsRaster(in_las_dataset, out_raster, {method}, {sampling_type}, {sampling_value})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.LasPointStatsAsRaster_3d("test.lasd", "lidar_intensity.img", 
                             "INTENSITY_RANGE", "CELLSIZE", 15)
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.LasPointStatsAsRaster_3d("test.lasd", "lidar_intensity.img", 
                             "INTENSITY_RANGE", "CELLSIZE", 15)
```

### Example 4

```python
'''**********************************************************************
Name: LAS Point Statistics As Raster
Description: Identifies the most frequently occurring return value for
             each pulse in a given set of LAS files.
             Designed for use as a script tool.
**********************************************************************'''
# Import system modules
import arcpy

# Set Local Variables
lasD = arcpy.GetParameterAsText(0)
inLas = arcpy.GetParameterAsText(1) #input las files
sr = arcpy.GetParameter(2) #spatial reference of las dataset
statsRaster = arcpy.GetParameterAsText(3)

# Execute CreateLasDataset
arcpy.management.CreateLasDataset(inLas, lasD, 'RECURSION', '', sr)
# Execute LasPointStatsAsRaster
arcpy.management.LasPointStatsAsRaster(lasD, statsRaster,
                                       "PREDOMINANT_RETURNS_PER_PULSE",
                                       "CELLSIZE", 15)
```

### Example 5

```python
'''**********************************************************************
Name: LAS Point Statistics As Raster
Description: Identifies the most frequently occurring return value for
             each pulse in a given set of LAS files.
             Designed for use as a script tool.
**********************************************************************'''
# Import system modules
import arcpy

# Set Local Variables
lasD = arcpy.GetParameterAsText(0)
inLas = arcpy.GetParameterAsText(1) #input las files
sr = arcpy.GetParameter(2) #spatial reference of las dataset
statsRaster = arcpy.GetParameterAsText(3)

# Execute CreateLasDataset
arcpy.management.CreateLasDataset(inLas, lasD, 'RECURSION', '', sr)
# Execute LasPointStatsAsRaster
arcpy.management.LasPointStatsAsRaster(lasD, statsRaster,
                                       "PREDOMINANT_RETURNS_PER_PULSE",
                                       "CELLSIZE", 15)
```

---

## Load Data To Preview (Data Management)

## Summary

Uses a Data Loading Workspace to load data from a source to a preview geodatabase. Use this tool to preview the results before loading data to the target schema.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Data Reference Workbook | The path to the Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |
| Preview Output Folder | The output folder where the preview geodatabase will be created. | Folder |
| in_workbook | The path to the Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |
| out_folder | The output folder where the preview geodatabase will be created. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.LoadDataToPreview(in_workbook, out_folder)
```

### Example 2

```python
import arcpy

arcpy.management.LoadDataToPreview("C:/data/DataLoadingWorkspace/DataReference.xlsx", "C:/temp")
```

### Example 3

```python
import arcpy

arcpy.management.LoadDataToPreview("C:/data/DataLoadingWorkspace/DataReference.xlsx", "C:/temp")
```

### Example 4

```python
# Name: LoadDataToPreview.py
# Description: Load the source-target mapping defined in DataReference.xlsx to a preview geodatabase

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"
folder = "C:/temp"

arcpy.management.LoadDataToPreview(in_workbook=workbook, out_folder=folder)
```

### Example 5

```python
# Name: LoadDataToPreview.py
# Description: Load the source-target mapping defined in DataReference.xlsx to a preview geodatabase

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"
folder = "C:/temp"

arcpy.management.LoadDataToPreview(in_workbook=workbook, out_folder=folder)
```

---

## Load Data Using Workspace (Data Management)

## Summary

Uses the Data Reference Workbook from the Data Loading Workspace to load data from a source to a target dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Data Reference Workbook | The path to the Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |
| in_workbook | The path to the Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |

## Code Samples

### Example 1

```python
arcpy.management.LoadDataUsingWorkspace(in_workbook)
```

### Example 2

```python
import arcpy

arcpy.management.LoadDataUsingWorkspace("C:/data/DataLoadingWorkspace/DataReference.xlsx")
```

### Example 3

```python
import arcpy

arcpy.management.LoadDataUsingWorkspace("C:/data/DataLoadingWorkspace/DataReference.xlsx")
```

### Example 4

```python
# Name: LoadDataUsingWorkspace.py
# Description: Load the source-target mapping defined in DataReference.xlsx

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"

arcpy.management.LoadDataUsingWorkspace(in_workbook=workbook)
```

### Example 5

```python
# Name: LoadDataUsingWorkspace.py
# Description: Load the source-target mapping defined in DataReference.xlsx

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"

arcpy.management.LoadDataUsingWorkspace(in_workbook=workbook)
```

---

## Make Aggregation Query Layer (Data Management)

## Summary

Creates a query layer that summarizes, aggregates, and filters DBMS tables dynamically based on time, range, and attribute queries from a related table, and joins the result to a feature layer.

## Usage

- Query layers will only work with enterprise databases. File geodatabases are not a valid input workspace for this tool.
- Aggregate results are always computed dynamically at the database level.
- The Output Layer value will consist of fields containing the result of the statistical operation. The count statistic is included in the ROW_COUNT field by default
- The statistical operations that are available with this tool are count, sum, average, minimum, maximum, and standard deviation.
- A field will be created for each statistic type using the following naming convention: COUNT_<field>, SUM_<field>, AVG_<field>, MIN_<field>, MAX_<field>, and STDDEV_<field>, (where <field> is the name of the input field for which the statistic is computed).
- The Related Join Field value is used in the Group By clause in the SQL statement generated by this tool. Statistics will be calculated separately for each unique attribute value from the Related Join Field value.
- The layer that is created by the tool is temporary and will not persist after the session ends unless the project is saved, the layer is saved to a layer file, or the data is persisted by making a copy using Copy Rows or Copy Features.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Feature Class | The feature class or spatial table from an enterprise database. | Feature Class |
| Target Join Field | The field in the target feature class on which the join will be based. | Field |
| Related Table | The input table containing the fields that will be used to calculate statistics. Statistics are joined to the Output Layer value. | Table; Feature Class |
| Related Join Field | A field in the summary table that contains the values on which the join will be based. Aggregation or summary statistics are also calculated separately for each unique attribute value from this field. | Field |
| Output Layer | The output name of the query layer that will be created. | Feature Layer |
| Summary Field(s)(Optional) | Specifies the numeric field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all statistical calculations.The output layer will include a ROW_COUNT field showing total count (or frequency) of each unique value from the Related Join Field value. The difference between the ROW_COUNT field and the Count statistic type is that ROW_COUNT includes null values while Count excludes null values. Available statistics types are as follows:Count—The number of values included in the statistical calculations will be found. Each value will be counted except null values.Sum—The values for the specified field will be added together.Average—The average for the specified field will be calculated.Minimum—The smallest value for all records of the specified field will be found.Maximum—The largest value for all records of the specified field will be found. Standard deviation—The standard deviation of values in the specified field will be calculated. | Value Table |
| Parameter Definitions(Optional) | Specifies one or more query parameters for criteria or conditions; records matching these criteria are used while computing aggregated results. A query parameter is similar to an SQL statement variable for which the value is defined when the query is run. This allows you to dynamically change query filters for the output layer. You can think of a parameter as a predicate or condition in a SQL where clause. For example Country_Name = 'Nigeria' in a SQL where clause is called a predicate in which the = is a comparison operator, Country_Name is a field name on the left, and 'Nigeria' is a value on the right. When you define more than one parameter, you must specify a logical operator between them (such as AND, OR, and so on).Learn more about defining parameters in a query layerWhen not specified, all records from the related table will be used in computing aggregated or summary results.The two parameter definition types are the following:Range—Connect numeric or temporal values dynamically to the range and time sliders.Discrete—Update a query with literal values when the query is run.The following properties are available: Parameter Type—The parameter type can be Range or Discrete.Name—The name of the parameter, which is similar to a variable name. A name cannot contain spaces or special characters. Once the output query layer is created and the layer source SQL statement is checked, this name in the SQL statement that defines the output query layer source,will be prefixed with either ::r: (for range parameter) or :: (for discrete parameter).Alias—The alias for the parameter name. The alias can include spaces and special characters.Field or Expression—A field name or a valid SQL expression that will be used in the left side of a predicate or condition in a where clause.Data type—The data type of the field or expression specified in the Field or Expression column. When the Parameter Type value is Range, the Data type column value cannot be String.Date—The data type of the field or expression will be Date (date time).String—The data type of the field or expression will be String (text).Integer—The data type of the field or expression will be Integer (whole numbers).Double—The data type of the field or expression will be Double (fractional numbers).Start Value—The default start value for the Range column. This is the value that will be used when the time or range slider is not enabled. When the Start Value and End Value column values are omitted and the time or range slider is disabled, all records from the related table will be used to compute aggregated results. This value is ignored when the Parameter Type column is set to Discrete.End Value—The default end value for the Range parameter. This is the value that will be used when the time or range slider is not enabled. When the Start Value and End Value column values are omitted and the time or range slider is disabled, all records from the related table will be used to compute aggregated results. This value is ignored when the Parameter Type column is set to Discrete.Operator for Discrete Parameter—The comparison operator that will be used between the Field or Expression column value and a value in an SQL predicate or condition.None—Choose None, when Parameter Type is set to Range.Equal to—Compare the equality of a field or expression to a value.Not equal to—Test whether a field or expression is not equal to a value.Greater than—Test whether a field or expression is higher than a value.Less than—Test whether a field or expression is lower than a value.Include values—Determine whether a value from a field or expression matches any value in a list. Default Discrete Values—When the Parameter Type value is Discrete, you must provide a default value. When Operator for Discrete Parameter is Include values, you can provide multiple values separated by commas, for example VANDALISM,BURGLARY/THEFT. Operator for Next Parameter—The logical operator between this operator and the next one. This column is only applicable when you have more than one parameter definition.None—Choose None when there are no more parameters.And—Combine two conditions and select a record if both conditions are true.Or—Combine two conditions and select a record if at least one condition is true. | Value Table |
| Unique Identifier Field(s) (Optional) | The unique identifier fields that will be used to uniquely identify each row in the table. | String |
| Shape Type (Optional) | Specifies the shape type of the query layer. Only those records from the result set of the query that match the specified shape type will be used in the output query layer. By default, the shape type of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field.Point—The output query layer will use point geometry.Multipoint—The output query layer will use multipoint geometry.Polygon—The output query layer will use polygon geometry.Polyline—The output query layer will use polyline geometry. | String |
| Spatial Reference ID (SRID)(Optional) | The spatial reference identifier (SRID) value for queries that return geometry. Only those records from the result set of the query that match the specified SRID value will be used in the output query layer. By default, the SRID value of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field. | String |
| Coordinate System (Optional) | The coordinate system that will be used by the output query layer. By default, the spatial reference of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field. | Spatial Reference |
| Coordinates include M values (Optional) | Specifies whether the output layer will include linear measurements (m-values). Checked—The layer will include m-values.Unchecked—The layer will not include m-values. This is the default. | Boolean |
| Coordinates include Z values (Optional) | Specifies whether the output layer will include elevation values (z-values). Checked—The layer will include z-values.Unchecked—The layer will not include z-values. This is the default. | Boolean |
| Extent (Optional) | Specifies the extent of the layer. The extent must include all features in the table.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| target_feature_class | The feature class or spatial table from an enterprise database. | Feature Class |
| target_join_field | The field in the target feature class on which the join will be based. | Field |
| related_table | The input table containing the fields that will be used to calculate statistics. Statistics are joined to the out_layer value. | Table; Feature Class |
| related_join_field | A field in the summary table that contains the values on which the join will be based. Aggregation or summary statistics are also calculated separately for each unique attribute value from this field. | Field |
| out_layer | The output name of the query layer that will be created. | Feature Layer |
| statistics[[statistic_type, field],...](Optional) | Specifies the numeric field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all statistical calculations.The output layer will include a ROW_COUNT field showing total count (or frequency) of each unique value from the related_join_field value. The difference between the ROW_COUNT field and the COUNT statistic type is that ROW_COUNT includes null values while COUNT excludes null values. COUNT—The number of values included in the statistical calculations will be found. Each value will be counted except null values. SUM—The values for the specified field will be added together.AVG—The average for the specified field will be calculated.MIN—The smallest value for all records of the specified field will be found.MAX—The largest value for all records of the specified field will be found. STDDEV—The standard deviation of values in the specified field will be calculated. | Value Table |
| parameter_definitions[[parameter_def_type, name, alias, field_or_expression, data_type, start_value, end_value, operator, default_value, operator_for_next_parameter],...](Optional) | Specifies one or more query parameters for criteria or conditions; records matching these criteria are used while computing aggregated results. A query parameter is similar to an SQL statement variable for which the value is defined when the query is run. This allows you to dynamically change query filters for the output layer. You can think of a parameter as a predicate or condition in a SQL where clause. For example Country_Name = 'Nigeria' in a SQL where clause is called a predicate in which the = is a comparison operator, Country_Name is a field name on the left, and 'Nigeria' is a value on the right. When you define more than one parameter, you must specify a logical operator between them (such as AND, OR, and so on).Learn more about defining parameters in a query layerWhen not specified, all records from the related table will be used in computing aggregated or summary results.The two parameter definition types are the following:Range—Connect numeric or temporal values dynamically to the range and time sliders.Discrete—Update a query with literal values when the query is run.The following properties are available: Parameter Type—The parameter type can be RANGE or DISCRETE.Name—The name of the parameter, which is similar to a variable name. A name cannot contain spaces or special characters. Once the output query layer is created and the layer source SQL statement is checked, this name in the SQL statement that defines the output query layer source,will be prefixed with either ::r: (for range parameter) or :: (for discrete parameter).Alias—The alias for the parameter name. The alias can include spaces and special characters.Field or Expression—A field name or a valid SQL expression that will be used in the left side of a predicate or condition in a where clause.Data type—The data type of the field or expression specified in the Field or Expression column. When the Parameter Type value is RANGE, the Data type column value cannot be STRING.DATE—The data type of the field or expression will be Date (date time).STRING—The data type of the field or expression will be String (text).INTEGER—The data type of the field or expression will be Integer (whole numbers).DOUBLE—The data type of the field or expression will be Double (fractional numbers).Start Value—The default start value for the RANGE column. This is the value that will be used when the time or range slider is not enabled. When the Start Value and End Value column values are omitted and the time or range slider is disabled, all records from the related table will be used to compute aggregated results. This value is ignored when the Parameter Type column is set to DISCRETE.End Value—The default end value for the RANGE parameter. This is the value that will be used when the time or range slider is not enabled. When the Start Value and End Value column values are omitted and the time or range slider is disabled, all records from the related table will be used to compute aggregated results. This value is ignored when the Parameter Type column is set to DISCRETE.Operator for Discrete Parameter—The comparison operator that will be used between the Field or Expression column value and a value in an SQL predicate or condition.NONE—Choose NONE when Parameter Type is set to RANGE.EQUAL TO—Compare the equality of a field or expression to a value.NOT EQUAL TO—Test whether a field or expression is not equal to a value.GREATER THAN—Test whether a field or expression is higher than a value.LESS THAN—Test whether a field or expression is lower than a value.INCLUDE VALUES—Determine whether a value from a field or expression matches any value in a list. Default Discrete Values—When the Parameter Type value is DISCRETE, you must provide a default value. When Operator for Discrete Parameter is INCLUDE VALUES, you can provide multiple values separated by commas, for example VANDALISM,BURGLARY/THEFT. Operator for Next Parameter—The logical operator between this operator and the next one. This column is only applicable when you have more than one parameter definition.NONE—Choose NONE when there are no more parameters.AND—Combine two conditions and select a record if both conditions are true.OR—Combine two conditions and select a record if at least one condition is true. | Value Table |
| oid_fields[oid_fields,...](Optional) | The unique identifier fields that will be used to uniquely identify each row in the table. | String |
| shape_type(Optional) | Specifies the shape type of the query layer. Only those records from the result set of the query that match the specified shape type will be used in the output query layer. By default, the shape type of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field.POINT—The output query layer will use point geometry.MULTIPOINT—The output query layer will use multipoint geometry.POLYGON—The output query layer will use polygon geometry.POLYLINE—The output query layer will use polyline geometry. | String |
| srid(Optional) | The spatial reference identifier (SRID) value for queries that return geometry. Only those records from the result set of the query that match the specified SRID value will be used in the output query layer. By default, the SRID value of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field. | String |
| spatial_reference(Optional) | The coordinate system that will be used by the output query layer. By default, the spatial reference of the first record in the result set will be used. This parameter is ignored if the result set of the query does not return a geometry field. | Spatial Reference |
| m_values(Optional) | Specifies whether the output layer will include linear measurements (m-values).INCLUDE_M_VALUES—The layer will include m-values.DO_NOT_INCLUDE_M_VALUES—The layer will not include m-values. This is the default. | Boolean |
| z_values(Optional) | Specifies whether the output layer will include elevation values (z-values).INCLUDE_Z_VALUES—The layer will include z-values.DO_NOT_INCLUDE_Z_VALUES—The layer will not include z-values. This is the default. | Boolean |
| extent(Optional) | Specifies the extent of the layer. The extent must include all features in the table.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |

## Code Samples

### Example 1

```python
arcpy.management.MakeAggregationQueryLayer(target_feature_class, target_join_field, related_table, related_join_field, out_layer, {statistics}, {parameter_definitions}, {oid_fields}, {shape_type}, {srid}, {spatial_reference}, {m_values}, {z_values}, {extent})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "PoliceDistricts", "district", "Crime_locations", "PdDistrict", "SF_Crimes")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "PoliceDistricts", "district", "Crime_locations", "PdDistrict", "SF_Crimes")
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "PoliceDistricts", "district", "Crime_locations", "PdDistrict", "SF_Crimes", 
    None, 
    "DISCRETE crime_type # Category STRING # # 'INCLUDE VALUES' 'VANDALISM, BURGLARY/THEFT' NONE")
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "PoliceDistricts", "district", "Crime_locations", "PdDistrict", "SF_Crimes", 
    None, 
    "DISCRETE crime_type # Category STRING # # 'INCLUDE VALUES' 'VANDALISM, BURGLARY/THEFT' NONE")
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "weather_stations", "station_id", "observed_rainfall", "station_id", 
    "Total_Rainfall", [["SUM", "rainfall_inch"], ["MIN", "rainfall_inch"]])
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "weather_stations", "station_id", "observed_rainfall", "station_id", 
    "Total_Rainfall", [["SUM", "rainfall_inch"], ["MIN", "rainfall_inch"]])
```

### Example 8

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "weather_stations", "station_id", "observed_rainfall", "station_id", 
    "Total_Rainfall", [["SUM", "rainfall_inch"], ["MIN", "rainfall_inch"]], 
    "RANGE TimeVar # collection_date DATE 1/1/2020 12/1/2020 NONE # NONE")
```

### Example 9

```python
import arcpy
arcpy.env.workspace = "C:/data/localhost.sde"
arcpy.management.MakeAggregationQueryLayer(
    "weather_stations", "station_id", "observed_rainfall", "station_id", 
    "Total_Rainfall", [["SUM", "rainfall_inch"], ["MIN", "rainfall_inch"]], 
    "RANGE TimeVar # collection_date DATE 1/1/2020 12/1/2020 NONE # NONE")
```

---

## Make Building Layer (Data Management)

## Summary

Creates a composite building layer from a dataset, either a BIM file workspace or a geodatabase dataset, such as the output of the BIM File To Geodatabase tool.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Dataset | The input dataset from which the new building feature layers will be made. The building layer keeps the structure and the symbology grouped together. | Feature Dataset; BIM File Workspace |
| Output Layer | The name of the feature layer that will be created. The layer can be used as input to any geoprocessing tool that accepts a feature layer as input. | Building Layer |
| in_feature_dataset | The input dataset from which the new building feature layers will be made. The building layer keeps the structure and the symbology grouped together. | Feature Dataset; BIM File Workspace |
| out_layer | The name of the feature layer that will be created. The layer can be used as input to any geoprocessing tool that accepts a feature layer as input. | Building Layer |

## Code Samples

### Example 1

```python
arcpy.management.MakeBuildingLayer(in_feature_dataset, out_layer)
```

### Example 2

```python
# Name: makebuildinglayer.py
# Description: Create a feature dataset

# Import system modules
import arcpy

# Set overwrite option
arcpy.env.overwriteOutput = True

# Make a building layer from a Dataset
arcpy.MakeBuildingLayer_management("C:/data/facilities/University.gdb/BuildingA",
                                   "Bld_A")

# Create a building Scene layer package
arcpy.CreateBuildingSceneLayerPackage_management(BLD_A, output_BLD_A.slpk)
```

### Example 3

```python
# Name: makebuildinglayer.py
# Description: Create a feature dataset

# Import system modules
import arcpy

# Set overwrite option
arcpy.env.overwriteOutput = True

# Make a building layer from a Dataset
arcpy.MakeBuildingLayer_management("C:/data/facilities/University.gdb/BuildingA",
                                   "Bld_A")

# Create a building Scene layer package
arcpy.CreateBuildingSceneLayerPackage_management(BLD_A, output_BLD_A.slpk)
```

---

## Make Feature Layer (Data Management)

## Summary

Creates a feature layer from a feature class or layer file. The layer that is created is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved.

## Usage

- The temporary feature layer can be saved as a layer file using the Save To Layer File tool or as a new feature class using the Copy Features tool.
- Complex feature classes, such as annotation and dimensions, are not supported.
- If an SQL expression is used but returns nothing, the output will be empty.
- A split policy can be set using the Ratio option for the Field Info parameter. The split policy can be used when the feature layer is being used as an input to a tool, and a geometry of the input feature layer is split during processing. When the split geometry is sent to the output, a ratio of the input attribute value is calculated for the output attribute value. When the Ratio option is used, whenever a feature in an overlay operation is split, the attributes of the resulting features are a ratio of the attribute value of the input feature. The output value is based on the ratio in which the input feature geometry was divided. For example, if the input geometry was divided equally, each new feature's attribute value is assigned one-half the value of the input feature's attribute value. The split policy only applies to numeric field types.The default is none (unchecked). This means the attribute of the two resulting features is a copy of the original object's attribute value.Caution:Geoprocessing tools do not support geodatabase feature class or table field split policies.
- When using ModelBuilder to create a tool, ensure that the input data variable to this tool is not flagged as intermediate. If the input is flagged as intermediate, it will be deleted after the model is run and the output layer will not be added to the display.
- In a model, the output variable for this tool can be assigned a layer file containing the symbology that will be applied to the layer being created. When the layer being created is returned as a model or script tool output parameter to a map, the symbology from the layer file is preserved but the label properties are not. However, if the layer created by this tool in a model is saved as permanent data (a feature class or shapefile) and that permanent data is returned to the map as an output parameter, the label properties from the layer file will be correctly applied.
- If the Input Features parameter value is a layer, the input's symbology, selection, and definition query will be transferred to the output layer

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input feature class or layer from which the new layer will be made. Complex feature classes, such as annotation and dimensions, are not valid inputs. | Feature Layer |
| Output Layer | The name of the feature layer to be created. The layer can be used as input to any geoprocessing tool that accepts a feature layer as input. | Feature Layer |
| Expression(Optional) | An SQL expression used to select a subset of features. If the input is a layer with an existing definition query and a where clause is specified with this parameter, both where clauses will be combined with an AND operator for the output layer. For example, if the input layer has a where clause of ID > 10 and this parameter is set to ID < 20, the resulting layer's where clause will be ID > 10 AND ID < 20. | SQL Expression |
| Workspace or Feature Dataset(Optional) | Legacy:This parameter is not used.In ArcGIS Desktop, output field names are validated based on this workspace. In ArcGIS Pro, this tool does not support changing the field names because layers do not support field names that differ from the underlying data source. | Workspace; Feature Dataset |
| Field Info(Optional) | The fields from the input features that will be included in the output layer. You can remove input fields by making them not visible, and you can set numeric fields to have a ratio split policy. Renaming fields is not supported. | Field Info |
| in_features | The input feature class or layer from which the new layer will be made. Complex feature classes, such as annotation and dimensions, are not valid inputs. | Feature Layer |
| out_layer | The name of the feature layer to be created. The layer can be used as input to any geoprocessing tool that accepts a feature layer as input. | Feature Layer |
| where_clause(Optional) | An SQL expression used to select a subset of features. For more information on SQL syntax see the help topic SQL reference for query expressions used in ArcGIS. If the input is a layer with an existing definition query and a where clause is specified with this parameter, both where clauses will be combined with an AND operator for the output layer. For example, if the input layer has a where clause of ID > 10 and this parameter is set to ID < 20, the resulting layer's where clause will be ID > 10 AND ID < 20. | SQL Expression |
| workspace(Optional) | Legacy:This parameter is not used.In ArcGIS Desktop, output field names are validated based on this workspace. In ArcGIS Pro, this tool does not support changing the field names because layers do not support field names that differ from the underlying data source. | Workspace; Feature Dataset |
| field_info(Optional) | The fields from the input features that will be included in the output layer. You can remove input fields by making them not visible, and you can set numeric fields to have a ratio split policy. Renaming fields is not supported. | Field Info |

## Code Samples

### Example 1

```python
arcpy.management.MakeFeatureLayer(in_features, out_layer, {where_clause}, {workspace}, {field_info})
```

### Example 2

```python
import arcpy

arcpy.env.workspace = "C:/data/input"
arcpy.management.MakeFeatureLayer("parcels.shp", "parcels_lyr")
```

### Example 3

```python
import arcpy

arcpy.env.workspace = "C:/data/input"
arcpy.management.MakeFeatureLayer("parcels.shp", "parcels_lyr")
```

### Example 4

```python
# Name: makefeaturelayer_example_2.py
# Description:  Uses MakeFeatureLayer with custom field info as input to Intersect

# Import system modules
import arcpy

# Set overwrite option
arcpy.env.overwriteOutput = True

# Set data path
cityboundaries = "C:/data/City.gdb/boundaries"
countyboundaries = "C:/data/City.gdb/counties"

# Get the fields from the input
fields = arcpy.ListFields(cityboundaries)

# Create a fieldinfo object
fieldinfo = arcpy.FieldInfo()

# Iterate through the input fields and add them to fieldinfo
for field in fields:
    if field.name == "POPULATION":
        # Set the Population to have a ratio split policy
        fieldinfo.addField(field.name, field.name, "VISIBLE", "RATIO")
    else:
        fieldinfo.addField(field.name, field.name, "VISIBLE", "NONE")

# Make a layer from the feature class
arcpy.management.MakeFeatureLayer(cityboundaries, "city_boundaries_lyr", fieldinfo)

# Intersect cities and counties, splitting city population proportionally by county
arcpy.analysis.Intersect([["city_boundaries_lyr"],[countyboundaries]], "memory/intersected_city_counties")
```

### Example 5

```python
# Name: makefeaturelayer_example_2.py
# Description:  Uses MakeFeatureLayer with custom field info as input to Intersect

# Import system modules
import arcpy

# Set overwrite option
arcpy.env.overwriteOutput = True

# Set data path
cityboundaries = "C:/data/City.gdb/boundaries"
countyboundaries = "C:/data/City.gdb/counties"

# Get the fields from the input
fields = arcpy.ListFields(cityboundaries)

# Create a fieldinfo object
fieldinfo = arcpy.FieldInfo()

# Iterate through the input fields and add them to fieldinfo
for field in fields:
    if field.name == "POPULATION":
        # Set the Population to have a ratio split policy
        fieldinfo.addField(field.name, field.name, "VISIBLE", "RATIO")
    else:
        fieldinfo.addField(field.name, field.name, "VISIBLE", "NONE")

# Make a layer from the feature class
arcpy.management.MakeFeatureLayer(cityboundaries, "city_boundaries_lyr", fieldinfo)

# Intersect cities and counties, splitting city population proportionally by county
arcpy.analysis.Intersect([["city_boundaries_lyr"],[countyboundaries]], "memory/intersected_city_counties")
```

---

## Make Image Server Layer (Data Management)

## Summary

Creates a temporary raster layer from an image service. The layer that is created will not persist after the session ends unless the document is saved.

## Usage

- Use this tool to create an image layer from an image service or a URL that references an image service.
- The output can be the entire image service or a portion of it.
- Use this tool to add an image service to a Python script or model or when creating a geoprocessing service.
- You can clip out a portion of the image service by choosing an output extent layer or by specifying the rectangle extent. If you choose an output extent layer, the clip extent will be based on the extent of that layer.
- The output can be created with only a subset of the bands. This will help save on time and disk space.
- The mosaicking options are only available when the image service it contains is generated from a mosaic definition or an image service definition.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Image Service | The name of the input image service or the SOAP URL that references the image service. Browse to or type the input image service. This tool can also accept a SOAP URL that references the image service.An example of using the image service name called ProjectX is: C:\MyProject\ServerConnection.ags\ProjectX.ImageServer.An example of a URL is http://AGSServer:8399/arcgis/services/ISName/ImageServer. | Image Service; String |
| Output Image Server Layer | The name of the output image layer. | Raster Layer |
| Template Extent (Optional) | The output extent of the image layer.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Bands(Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| Mosaic Method(Optional) | The mosaic method defines how the mosaic is created from different rasters.Seamline—Smooth transitions between images using seamlines.Northwest—Display imagery that is closest to the northwest corner of the mosaic dataset boundary.Center—Display imagery that is closest to the center of the screen.Lock raster—Select specific raster datasets to display.By attribute—Display and prioritize imagery based on a field in the attribute table.Nadir—Display the rasters with viewing angles closest to zero.Viewpoint—Display imagery that is closest to a selected viewing angle. None—Order rasters based on the ObjectID in the mosaic dataset attribute table. | String |
| Order Field (Optional) | The default field to use to order the rasters when the mosaic method is By_Attribute. The list of fields is defined as those in the service table that are of type metadata and are integer (for example, the values can represent dates or cloud cover percentage). | String |
| Order Base Value(Optional) | The images are sorted based on the difference between this input value and the attribute value in the specified field. | String |
| Lock Raster ID(Optional) | The raster ID or raster name to which the service should be locked, such that only the specified rasters are displayed. If left blank (undefined), it will be similar to the system default. Multiple IDs can be defined as a semicolon-delimited list. | String |
| Output Cell Size (Optional) | The cell size for the output image service layer. | Double |
| Expression (Optional) | Define a query using SQL or use the Query Builder to build a query. | SQL Expression |
| Processing Template (Optional) | The raster function processing template that can be applied on the output image service layer. None—No processing template. | String |
| in_image_service | The name of the input image service or the SOAP URL that references the image service.An example of using the image service name called ProjectX is: C:\MyProject\ServerConnection.ags\ProjectX.ImageServer.An example of a URL is http://AGSServer:8399/arcgis/services/ISName/ImageServer. | Image Service; String |
| out_imageserver_layer | The name of the output image layer. | Raster Layer |
| template(Optional) | The output extent of the image layer.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| band_index[ID,...](Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| mosaic_method(Optional) | The mosaic method defines how the mosaic is created from different rasters.SEAMLINE—Smooth transitions between images using seamlines.NORTH_WEST—Display imagery that is closest to the northwest corner of the mosaic dataset boundary.CLOSEST_TO_CENTER—Display imagery that is closest to the center of the screen.LOCK_RASTER—Select specific raster datasets to display.BY_ATTRIBUTE—Display and prioritize imagery based on a field in the attribute table.CLOSEST_TO_NADIR—Display the rasters with viewing angles closest to zero.CLOSEST_TO_VIEWPOINT—Display imagery that is closest to a selected viewing angle. NONE—Order rasters based on the ObjectID in the mosaic dataset attribute table. | String |
| order_field(Optional) | The default field to use to order the rasters when the mosaic method is By_Attribute. The list of fields is defined as those in the service table that are of type metadata and are integer (for example, the values can represent dates or cloud cover percentage). | String |
| order_base_value(Optional) | The images are sorted based on the difference between this input value and the attribute value in the specified field. | String |
| lock_rasterid(Optional) | The raster ID or raster name to which the service should be locked, such that only the specified rasters are displayed. If left blank (undefined), it will be similar to the system default. Multiple IDs can be defined as a semicolon-delimited list. | String |
| cell_size(Optional) | The cell size for the output image service layer. | Double |
| where_clause(Optional) | Define a query using SQL. | SQL Expression |
| processing_template(Optional) | The raster function processing template that can be applied on the output image service layer. None—No processing template. | String |

## Code Samples

### Example 1

```python
arcpy.management.MakeImageServerLayer(in_image_service, out_imageserver_layer, {template}, {band_index}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {cell_size}, {where_clause}, {processing_template})
```

### Example 2

```python
arcpy.MakeImageServerLayer_management(
        input2, "mdlayer", "feature.shp", "1;2;3",
        "LockRaster", "#", "#", "4", "#", processing_template="Hillshade")
```

### Example 3

```python
arcpy.MakeImageServerLayer_management(
        input2, "mdlayer", "feature.shp", "1;2;3",
        "LockRaster", "#", "#", "4", "#", processing_template="Hillshade")
```

### Example 4

```python
arcpy.MakeImageServerLayer_management(
        input2, "mdlayer", "feature.shp", "1;2;3",
        "LockRaster", "#", "#", "4", "#", processing_template="Custom_func")
```

### Example 5

```python
arcpy.MakeImageServerLayer_management(
        input2, "mdlayer", "feature.shp", "1;2;3",
        "LockRaster", "#", "#", "4", "#", processing_template="Custom_func")
```

---

## Make LAS Dataset Layer (Data Management)

## Summary

Creates a LAS dataset layer that can apply filters to LAS points and control the enforcement of surface constraint features.

## Usage

- A LAS dataset layer can be used to filter LAS points and control which surface constraint features are enforced when triangulating a surface from the LAS dataset. LAS points can be filtered using the classification codes, classification flags, and return values associated with each point. The filters are honored by various tools that process the LAS dataset. For example, a raster surface modeling the bare earth can be constructed by filtering for ground classified points and using the resulting layer as input for the LAS Dataset To Raster tool. Note:The layer produced by this tool can be preserved as a layer file using the Save To Layer File tool.
- You can use the LAS dataset layer properties dialog box to filter LAS points and surface constraints when working with a LAS dataset layer in a map or scene. It provides a convenient mechanism for managing the filter options. This tool is useful for enforcing LAS dataset filters in the context of an automated solution authored through ModelBuilder or Python.
- The classification codes, classification flags, and return values supported in a given LAS file will depend on that file's version and point record format. When no values present in the input LAS files are specified to define a filter, the resulting layer will yield no points. The classification codes, classification flags, and return values present in a LAS dataset can be established by calculating statistics. Learn more about working with LAS dataset statistics

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| Output Layer | The name of the resulting LAS dataset layer. A backslash or forward slash can be used to denote a group layer. | LAS Dataset Layer |
| Class Codes (Optional) | Specifies the classification codes that will be used to filter LAS points. All class codes will be selected by default.0—Never processed by a classification method1—Processed by a classification method but could not be determined2—Bare earth measurements3—Vegetation whose height is considered to be low for the area4—Vegetation whose height is considered to be intermediate for the area5—Vegetation whose height is considered to be high for the area6—Structure with roof and walls7—Erroneous or undesirable data that is closer to the ground8—Reserved for later use, but used for model key points in LAS 1.1 - 1.39—Water10—Railway tracks used by trains11—Road surfaces12—Reserved for later use, but used for overlap points in LAS 1.1 - 1.313—Shielding around electrical wires 14—Power lines15—A lattice tower used to support an overhead power line 16—A mechanical assembly that joins an electrical circuit17—The surface of a bridge18—Erroneous or undesirable data that is far from the ground19 - 63—Reserved class codes for ASPRS designation64 - 255—User-definable class codes | String |
| Return Values (Optional) | Specifies the ordinal pulse return values that will be used to filter LAS points. All returns will be used when no value is specified. Return information is only available for LAS point clouds collected from a lidar scanner. The return number reflects the order of discrete points obtained from the lidar pulse, whereby the first return is closest to the scanner and the last return is farthest from the scanner.Single Return—First of Many Returns—Last of Many Returns—Last Return—1st Return—All points with a return value of 1 will be used.2nd Return—All points with a return value of 2 will be used.3rd Return—All points with a return value of 3 will be used.4th Return—All points with a return value of 4 will be used.5th Return—All points with a return value of 5 will be used.6th Return—All points with a return value of 6 will be used. 7th Return—All points with a return value of 7 will be used. 8th Return—All points with a return value of 8 will be used. 9th Return—All points with a return value of 9 will be used. 10th Return—All points with a return value of 10 will be used.11th Return—All points with a return value of 11 will be used.12th Return—All points with a return value of 12 will be used.13th Return—All points with a return value of 13 will be used.14th Return—All points with a return value of 14 will be used.15th Return—All points with a return value of 15 will be used. | String |
| Unflagged Points(Optional) | Specifies whether data points that do not have classification flags assigned will be included.Checked—Unflagged points will be included. This is the default.Unchecked—Unflagged points will be excluded. | Boolean |
| Synthetic Points(Optional) | Specifies whether data points flagged as synthetic will be included. Synthetic points refer to LAS points that originated from a data source other than a lidar scanner.Checked—Synthetic points will be included. This is the default.Unchecked—Synthetic points will be excluded. | Boolean |
| Model Key-Point (Optional) | Specifies whether data points flagged as model key points will be included. Model key points refer to LAS points that are significant for modeling the object they are associated with. Checked—Model key points will be included. This is the default.Unchecked—Model key points will be excluded. | Boolean |
| Withheld Points(Optional) | Specifies whether data points flagged as withheld will be included. Withheld points represent erroneous or undesired measurements captured in the LAS points.Checked—Withheld points will be included.Unchecked—Withheld points will be excluded. This is the default. | Boolean |
| Surface Constraints (Optional) | The name of the surface constraint features that will be enabled in the layer. All constraints are enabled by default. | String |
| Overlap Points(Optional) | Specifies whether data points flagged as overlap will be included. Overlap points refer to points collected in overlapping scans that typically have a larger scan angle. Filtering overlap points can help ensure a regular distribution of LAS points is achieved across the extent of the data.Checked—Overlap points will be included. This is the default.Unchecked—Overlap points will be excluded. | Boolean |
| in_las_dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| out_layer | The name of the resulting LAS dataset layer. A backslash or forward slash can be used to denote a group layer. | LAS Dataset Layer |
| class_code[class_code,...](Optional) | Specifies the classification codes that will be used to filter LAS points. All class codes will be selected by default.0—Never processed by a classification method1—Processed by a classification method but could not be determined2—Bare earth measurements3—Vegetation whose height is considered to be low for the area4—Vegetation whose height is considered to be intermediate for the area5—Vegetation whose height is considered to be high for the area6—Structure with roof and walls7—Erroneous or undesirable data that is closer to the ground8—Reserved for later use, but used for model key points in LAS 1.1 - 1.39—Water10—Railway tracks used by trains11—Road surfaces12—Reserved for later use, but used for overlap points in LAS 1.1 - 1.313—Shielding around electrical wires 14—Power lines15—A lattice tower used to support an overhead power line 16—A mechanical assembly that joins an electrical circuit17—The surface of a bridge18—Erroneous or undesirable data that is far from the ground19 - 63—Reserved class codes for ASPRS designation64 - 255—User-definable class codes | String |
| return_values[return_values,...](Optional) | Specifies the ordinal pulse return values that will be used to filter LAS points. All returns will be used when no value is specified. Return information is only available for LAS point clouds collected from a lidar scanner. The return number reflects the order of discrete points obtained from the lidar pulse, whereby the first return is closest to the scanner and the last return is farthest from the scanner.LAST—The last point from all lidar pulses will be used.FIRST_OF_MANY—The first point from each lidar pulse with multiple returns will be used.LAST_OF_MANY—The last point from each lidar pulse with multiple returns will be used.SINGLE—All points from lidar pulses with only one return will be used.1—All points with a return value of 1 will be used.2—All points with a return value of 2 will be used.3—All points with a return value of 3 will be used.4—All points with a return value of 4 will be used.5—All points with a return value of 5 will be used.6—All points with a return value of 6 will be used. 7—All points with a return value of 7 will be used. 8—All points with a return value of 8 will be used. 9—All points with a return value of 9 will be used. 10—All points with a return value of 10 will be used.11—All points with a return value of 11 will be used.12—All points with a return value of 12 will be used.13—All points with a return value of 13 will be used.14—All points with a return value of 14 will be used.15—All points with a return value of 15 will be used. | String |
| no_flag(Optional) | Specifies whether data points that do not have classification flags assigned will be included for display and analysis.INCLUDE_UNFLAGGED—Unflagged points will be included. This is the default.EXCLUDE_UNFLAGGED—Unflagged points will be excluded. | Boolean |
| synthetic(Optional) | Specifies whether data points flagged as synthetic will be included. Synthetic points refer to LAS points that originated from a data source other than a lidar scanner.INCLUDE_SYNTHETIC—Synthetic points will be included. This is the default.EXCLUDE_SYNTHETIC—Synthetic points will be excluded. | Boolean |
| keypoint(Optional) | Specifies whether data points flagged as model key points will be included. Model key points refer to LAS points that are significant for modeling the object they are associated with. INCLUDE_KEYPOINT—Model key points will be included. This is the default.EXCLUDE_KEYPOINT—Model key points will be excluded. | Boolean |
| withheld(Optional) | Specifies whether data points flagged as withheld will be included. Withheld points represent erroneous or undesired measurements captured in the LAS points.INCLUDE_WITHHELD—Withheld points will be included.EXCLUDE_WITHHELD—Withheld points will be excluded. This is the default. | Boolean |
| surface_constraints[surface_constraints,...](Optional) | The name of the surface constraint features that will be enabled in the layer. All constraints are enabled by default. | String |
| overlap(Optional) | Specifies whether data points flagged as overlap will be included. Overlap points refer to points collected in overlapping scans that typically have a larger scan angle. Filtering overlap points can help ensure a regular distribution of LAS points is achieved across the extent of the data.INCLUDE_OVERLAP—Overlap points will be included. This is the default.EXCLUDE_OVERLAP—Overlap points will be excluded. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.MakeLasDatasetLayer(in_las_dataset, out_layer, {class_code}, {return_values}, {no_flag}, {synthetic}, {keypoint}, {withheld}, {surface_constraints}, {overlap})
```

### Example 2

```python
arcpy.env.workspace = 'C:/data'
arcpy.management.MakeLasDatasetLayer('Baltimore.lasd', 'Baltimore Layer',
                                     class_code=[2, 6], return_values=['LAST', 'SINGLE'])
```

### Example 3

```python
arcpy.env.workspace = 'C:/data'
arcpy.management.MakeLasDatasetLayer('Baltimore.lasd', 'Baltimore Layer',
                                     class_code=[2, 6], return_values=['LAST', 'SINGLE'])
```

### Example 4

```python
'''*********************************************************************
Name: Export Elevation Raster from Ground LAS Measurements
Description: This script demonstrates how to export
             ground measurements from LAS files to a raster using a
             LAS dataset. This sample is designed to be used as a script
             tool.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Set Local Variables
    inLas = arcpy.GetParameterAsText(0)
    recursion = arcpy.GetParameterAsText(1)
    surfCons = arcpy.GetParameterAsText(2)
    classCode = arcpy.GetParameterAsText(3)
    returnValue = arcpy.GetParameterAsText(4)
    spatialRef = arcpy.GetParameterAsText(5)
    lasD = arcpy.GetParameterAsText(6)
    outRaster = arcpy.GetParameterAsText(7)
    cellSize = arcpy.GetParameter(8)
    zFactor = arcpy.GetParameter(9)

    # Execute CreateLasDataset
    arcpy.management.CreateLasDataset(inLas, lasD, recursion, surfCons, sr)
    # Execute MakeLasDatasetLayer
    lasLyr = arcpy.CreateUniqueName('Baltimore')
    arcpy.management.MakeLasDatasetLayer(lasD, lasLyr, classCode, returnValue)
    # Execute LasDatasetToRaster
    arcpy.conversion.LasDatasetToRaster(lasLyr, outRaster, 'ELEVATION',
                              'TRIANGULATION LINEAR WINDOW_SIZE 10', 'FLOAT',
                              'CELLSIZE', cellSize, zFactor)
    print(arcpy.GetMessages())

except arcpy.ExecuteError:
    print(arcpy.GetMessages())

except Exception as err:
    print(err.args[0])

finally:
    arcpy.management.Delete(lasLyr)
```

### Example 5

```python
'''*********************************************************************
Name: Export Elevation Raster from Ground LAS Measurements
Description: This script demonstrates how to export
             ground measurements from LAS files to a raster using a
             LAS dataset. This sample is designed to be used as a script
             tool.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Set Local Variables
    inLas = arcpy.GetParameterAsText(0)
    recursion = arcpy.GetParameterAsText(1)
    surfCons = arcpy.GetParameterAsText(2)
    classCode = arcpy.GetParameterAsText(3)
    returnValue = arcpy.GetParameterAsText(4)
    spatialRef = arcpy.GetParameterAsText(5)
    lasD = arcpy.GetParameterAsText(6)
    outRaster = arcpy.GetParameterAsText(7)
    cellSize = arcpy.GetParameter(8)
    zFactor = arcpy.GetParameter(9)

    # Execute CreateLasDataset
    arcpy.management.CreateLasDataset(inLas, lasD, recursion, surfCons, sr)
    # Execute MakeLasDatasetLayer
    lasLyr = arcpy.CreateUniqueName('Baltimore')
    arcpy.management.MakeLasDatasetLayer(lasD, lasLyr, classCode, returnValue)
    # Execute LasDatasetToRaster
    arcpy.conversion.LasDatasetToRaster(lasLyr, outRaster, 'ELEVATION',
                              'TRIANGULATION LINEAR WINDOW_SIZE 10', 'FLOAT',
                              'CELLSIZE', cellSize, zFactor)
    print(arcpy.GetMessages())

except arcpy.ExecuteError:
    print(arcpy.GetMessages())

except Exception as err:
    print(err.args[0])

finally:
    arcpy.management.Delete(lasLyr)
```

---

## Make Mosaic Layer (Data Management)

## Summary

Creates a mosaic layer from a mosaic dataset or layer file. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved as a layer file or the map is saved.

## Usage

- To persist your layer, right-click the layer in the Contents Pane and click Save As Layer File, or use the Save To Layer File tool.
- The output can be created with only a subset of the bands. This will save time and disk space.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The path and name of the input mosaic dataset. | Mosaic Layer |
| Output Mosaic Layer | The name of the output mosaic layer. | Mosaic Layer |
| Expression (Optional) | Define a query using SQL or use the Query Builder to build a query. | SQL Expression |
| Template Extent (Optional) | The output extent can be specified by defining the four coordinates or by using the extent of an existing layer. Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Bands (Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| Mosaic Method (Optional) | Choose the mosaic method. The mosaic method defines how the layer is created from different rasters in the mosaic dataset.Closest to center—Sorts rasters based on an order where rasters that have their center closest to the view center are placed on top.Northwest—Sorts rasters based on an order where rasters that have their center closest to the northwest are placed on top.Lock raster—Enables a user to lock the display of single or multiple rasters, based on an ID or name. When you choose this option, you need to specify the Lock Raster ID.By attribute—Sorts rasters based on an attribute field and its difference from the base value. When this option is chosen, the order field and order base value parameters also need to be set.Closest to nadir—Sorts rasters based on an order where rasters that have their nadir position closest to the view center are placed on top. The nadir point can be different from the center point, especially in oblique imagery.Closest to view point—Sorts rasters based on an order where the nadir position is closest to the user-defined viewpoint location and are placed on top.Seamline—Cuts the rasters using the predefined seamline shape for each raster using optional feathering along the seams. The ordering is predefined during seamline generation. The LAST mosaic operator is not valid with this mosaic method. | String |
| Order Field (Optional) | Choose the order field. When the mosaic method is By attribute, the default field to use when ordering rasters needs to be set. The list of fields is defined as those in the service table that are of type metadata. | String |
| Order Base Value (Optional) | The order base value. The images are sorted based on the difference between this value and the attribute value in the specified field. | String |
| Lock Raster ID (Optional) | The Raster ID or raster name to which the service should be locked so that only the specified rasters are displayed. If left undefined, it will be similar to the system default. Multiple IDs can be defined as a semicolon-delimited list. | String |
| Sort Order (Optional) | Choose whether the sort order is ascending or descending. Ascending—The sort order will be ascending. This is the default.Descending—The sort order will be descending. | String |
| Mosaic Operator (Optional) | Choose the mosaic operator to use. When two or more rasters have the same sort priority, this parameter is used to further refine the sort order.First—The first raster in the list will be on top. This is the default.Last—The last raster in the list will be on top.Minimum—The raster with the lowest value will be on top.Maximum—The raster with the highest value will be on top.Mean—The average pixel value will be on top.Blend—The output cell value will be a blend of values; this blend value relies on an algorithm that is weight based and dependent on the distance from the pixel to the edge within the overlapping area.Sum—The output cell value will be the aggregate of all overlapping cells. | String |
| Output Cell Size (Optional) | The cell size of the output mosaic layer. | Double |
| Processing Template (Optional) | The raster function processing template that can be applied on the output mosaic layer. None—No processing template. | String |
| in_mosaic_dataset | The path and name of the input mosaic dataset. | Mosaic Layer |
| out_mosaic_layer | The name of the output mosaic layer. | Mosaic Layer |
| where_clause(Optional) | Define a query using SQL. | SQL Expression |
| template(Optional) | The output extent can be specified by defining the four coordinates or by using the extent of an existing layer. MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| band_index[ID,...](Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| mosaic_method(Optional) | Choose the mosaic method. The mosaic method defines how the layer is created from different rasters in the mosaic dataset.CLOSEST_TO_CENTER—Sorts rasters based on an order where rasters that have their center closest to the view center are placed on top.NORTH_WEST—Sorts rasters based on an order where rasters that have their center closest to the northwest are placed on top.LOCK_RASTER—Enables a user to lock the display of single or multiple rasters, based on an ID or name. When you choose this option, you need to specify the Lock Raster ID.BY_ATTRIBUTE—Sorts rasters based on an attribute field and its difference from the base value. When this option is chosen, the order field and order base value parameters also need to be set.CLOSEST_TO_NADIR—Sorts rasters based on an order where rasters that have their nadir position closest to the view center are placed on top. The nadir point can be different from the center point, especially in oblique imagery.CLOSEST_TO_VIEWPOINT—Sorts rasters based on an order where the nadir position is closest to the user-defined viewpoint location and are placed on top.SEAMLINE—Cuts the rasters using the predefined seamline shape for each raster using optional feathering along the seams. The ordering is predefined during seamline generation. The LAST mosaic operator is not valid with this mosaic method. | String |
| order_field(Optional) | Choose the order field. When the mosaic method is BY_ATTRIBUTE, the default field to use when ordering rasters needs to be set. The list of fields is defined as those in the service table that are of type metadata. | String |
| order_base_value(Optional) | The order base value. The images are sorted based on the difference between this value and the attribute value in the specified field. | String |
| lock_rasterid(Optional) | The Raster ID or raster name to which the service should be locked so that only the specified rasters are displayed. If left undefined, it will be similar to the system default. Multiple IDs can be defined as a semicolon-delimited list. | String |
| sort_order(Optional) | Choose whether the sort order is ascending or descending. ASCENDING—The sort order will be ascending. This is the default.DESCENDING—The sort order will be descending. | String |
| mosaic_operator(Optional) | Choose the mosaic operator to use. When two or more rasters have the same sort priority, this parameter is used to further refine the sort order.FIRST—The first raster in the list will be on top. This is the default.LAST—The last raster in the list will be on top.MIN—The raster with the lowest value will be on top.MAX—The raster with the highest value will be on top.MEAN—The average pixel value will be on top.BLEND—The output cell value will be a blend of values; this blend value relies on an algorithm that is weight based and dependent on the distance from the pixel to the edge within the overlapping area.SUM—The output cell value will be the aggregate of all overlapping cells. | String |
| cell_size(Optional) | The cell size of the output mosaic layer. | Double |
| processing_template(Optional) | The raster function processing template that can be applied on the output mosaic layer. None—No processing template. | String |

## Code Samples

### Example 1

```python
arcpy.management.MakeMosaicLayer(in_mosaic_dataset, out_mosaic_layer, {where_clause}, {template}, {band_index}, {mosaic_method}, {order_field}, {order_base_value}, {lock_rasterid}, {sort_order}, {mosaic_operator}, {cell_size}, {processing_template})
```

### Example 2

```python
arcpy.MakeMosaicLayer_management(
        "fgdb.gdb/mdsrc", "mdlayer2", "", "clipmd.shp", "3;2;1", 
		"BY_ATTRIBUTE", "Tag", "Dataset", "", "DESCENDING", "LAST", "10", 
		processing_template="NDVI")
```

### Example 3

```python
arcpy.MakeMosaicLayer_management(
        "fgdb.gdb/mdsrc", "mdlayer2", "", "clipmd.shp", "3;2;1", 
		"BY_ATTRIBUTE", "Tag", "Dataset", "", "DESCENDING", "LAST", "10", 
		processing_template="NDVI")
```

### Example 4

```python
arcpy.MakeMosaicLayer_management(
        "fgdb.gdb/mdsrc", "mdlayer2", "", "clipmd.shp", "3;2;1", 
		"BY_ATTRIBUTE", "Tag", "Dataset", "", "DESCENDING", "LAST", "10", 
		processing_template="Custom_func")
```

### Example 5

```python
arcpy.MakeMosaicLayer_management(
        "fgdb.gdb/mdsrc", "mdlayer2", "", "clipmd.shp", "3;2;1", 
		"BY_ATTRIBUTE", "Tag", "Dataset", "", "DESCENDING", "LAST", "10", 
		processing_template="Custom_func")
```

---

## Make Query Layer (Data Management)

## Summary

Creates a query layer from a DBMS table based on an input SQL select statement.

## Usage

- Query layers only work with enterprise databases. File geodatabases are not a valid input workspace for this tool.
- The layer that is created by the tool is temporary and will not persist after the session ends unless the project is saved or the data is persisted by making a copy using Copy Rows or Copy Features.
- If the result of the SQL query entered returns a spatial column, the output will be a feature layer. If the SQL query does not return a spatial column, the output will be a stand-alone table.
- The connection files necessary for this tool can be created using the Create Database Connection tool.
- If the result of the SQL query does not return any rows, the output query layer will be empty, only containing the schema of the columns returned by the query. In this case, if the columns returned contain a spatial column, the tool will use the following defaults to create the query layer:Geometry type—POINTSRID—1Spatial Reference—NAD1983Then you need to determine whether any of these values should be changed before running the tool.
- Geometry type—POINT
- SRID—1
- Spatial Reference—NAD1983
- For geographic data, each record in the result returned from the SQL statement should have an associated spatial reference identifier (SRID). The SRID value is used by the database to determine the spatial reference for the data. The specific functional differences for the SRID will vary between each DBMS platform. Some DBMS platforms support multiple SRID values within the same table. ArcGIS only supports one value. This tool allows you to choose the SRID value or it will default to the SRID from the first record in the result set.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection file that contains the data to be queried. | Workspace |
| Output Layer Name | The output name of the feature layer or table view to be created. | String |
| Query | The SQL statement that defines the select query to be run in the database.Note:This string must pass validation before the remaining controls will be enabled. Validation will be triggered when you click outside this input box. The validation process runs the query in the database and verifies whether the result of the SQL query meets the data modeling standards enforced by ArcGIS. If the validation fails, the tool will return a warning. The only exception is for ModelBuilder, in which case validation will not be triggered if the input is derived data.Rules for validation are as follows:The result of the SQL query must have only one spatial field.The result of the SQL query must have only one spatial reference.The result of the SQL query must have only one entity type, such as point, multipoint, line, or polygon.The result of the SQL query cannot have any field types that are not supported by ArcGIS. ArcGIS field data types describes the field types supported in ArcGIS.Validation is especially important when working with data in spatial databases that do not enforce the same standards as ArcGIS. | String |
| Unique Identifier Field(s) (Optional) | One or more fields from the SELECT statement SELECT list that will generate a dynamic, unique row identifier. | String |
| Shape Type (Optional) | Specifies the shape type of the query layer. Only those records from the result set of the query that match the specified shape type will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output shape type. This parameter is ignored if the result set of the query does not return a geometry field.Point—The output query layer will use point geometry.Multipoint—The output query layer will use multipoint geometry.Polygon—The output query layer will use polygon geometry.Polyline—The output query layer will use polyline geometry. | String |
| SRID(Optional) | The spatial reference identifier (SRID) value for queries that return geometry. Only those records from the result set of the query that match the specified SRID value will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output SRID value. This parameter is ignored if the result set of the query does not return a geometry field. | String |
| Coordinate System (Optional) | The coordinate system that will be used by the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output coordinate system. This parameter is ignored if the result set of the query does not return a geometry field. | Spatial Reference |
| Define the spatial properties of the layer (Optional) | Specifies how the spatial properties for the layer will be defined. During the validation process, dimensionality, geometry type, spatial reference, SRID, and unique identifier properties will be set on the query layer. These values are based on the first row returned in the query. To manually define these properties instead of the tool querying the table to get them, this parameter is checked by default. Checked—Manually define the spatial properties of the layer. This is the default.Unchecked—Layer properties will be determined based on the first row returned in the query. | Boolean |
| Coordinates include M values (Optional) | Specifies whether the layer will have m-values. Checked—The layer will have m-values.Unchecked—The layer will not have m-values. This is the default. | Boolean |
| Coordinates include Z values (Optional) | Specifies whether the layer will have z-values. Checked—The layer will have z-values.Unchecked—The layer will not have z-values. This is the default. | Boolean |
| Extent (Optional) | The extent of the layer. This parameter is only used if the Define the spatial properties of the layer parameter is checked (spatial_properties = DEFINE_SPATIAL_PROPERTIES in Python). The extent must include all features in the table.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| input_database | The database connection file that contains the data to be queried. | Workspace |
| out_layer_name | The output name of the feature layer or table view to be created. | String |
| query | The SQL statement that defines the select query to be issued to the database. | String |
| oid_fields[oid_fields,...](Optional) | One or more fields from the SELECT statement SELECT list that will generate a dynamic, unique row identifier. | String |
| shape_type(Optional) | Specifies the shape type of the query layer. Only those records from the result set of the query that match the specified shape type will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output shape type. This parameter is ignored if the result set of the query does not return a geometry field.POINT—The output query layer will use point geometry.MULTIPOINT—The output query layer will use multipoint geometry.POLYGON—The output query layer will use polygon geometry.POLYLINE—The output query layer will use polyline geometry. | String |
| srid(Optional) | The spatial reference identifier (SRID) value for queries that return geometry. Only those records from the result set of the query that match the specified SRID value will be used in the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output SRID value. This parameter is ignored if the result set of the query does not return a geometry field. | String |
| spatial_reference(Optional) | The coordinate system that will be used by the output query layer. Tool validation will attempt to set this property based on the first record in the result set. This can be changed before running the tool if it is not the correct output coordinate system. This parameter is ignored if the result set of the query does not return a geometry field. | Spatial Reference |
| spatial_properties(Optional) | Specifies how the spatial properties for the layer will be defined. During the validation process, dimensionality, geometry type, spatial reference, SRID, and unique identifier properties will be set on the query layer. These values are based on the first row returned in the query. To manually define these properties instead of the tool querying the table to get them, use the default value for this parameter.DEFINE_SPATIAL_PROPERTIES—Manually define the spatial properties of the layer. This is the default.DO_NOT_DEFINE_SPATIAL_PROPERTIES—Layer properties will be determined based on the first row returned in the query. | Boolean |
| m_values(Optional) | Specifies whether the layer will have m-values.INCLUDE_M_VALUES—The layer will have m-values.DO_NOT_INCLUDE_M_VALUES—The layer will not have m-values. This is the default. | Boolean |
| z_values(Optional) | Specifies whether the layer will have z-values.INCLUDE_Z_VALUES—The layer will have z-values.DO_NOT_INCLUDE_Z_VALUES—The layer will not have z-values. This is the default. | Boolean |
| extent(Optional) | The extent of the layer. This parameter is only used if the Define the spatial properties of the layer parameter is checked (spatial_properties = DEFINE_SPATIAL_PROPERTIES in Python). The extent must include all features in the table.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |

## Code Samples

### Example 1

```python
arcpy.management.MakeQueryLayer(input_database, out_layer_name, query, {oid_fields}, {shape_type}, {srid}, {spatial_reference}, {spatial_properties}, {m_values}, {z_values}, {extent})
```

### Example 2

```python
import arcpy

sr = arcpy.SpatialReference("WGS 1984 UTM Zone 12N")

arcpy.MakeQueryLayer_management("Connections/moab.sde",
                                "Slickrock",
                                "select * from moabtrails where name = 'slickrock'",
                                "OBJECTID",
                                "POLYLINE",
                                "32611",
                                sr)
```

### Example 3

```python
import arcpy

sr = arcpy.SpatialReference("WGS 1984 UTM Zone 12N")

arcpy.MakeQueryLayer_management("Connections/moab.sde",
                                "Slickrock",
                                "select * from moabtrails where name = 'slickrock'",
                                "OBJECTID",
                                "POLYLINE",
                                "32611",
                                sr)
```

### Example 4

```python
# Name: MakeQueryLayer.py
# Description: Creates an output query layer based on a where clause.
#   This example shows how to create a spatial reference object using the
#   name of a coordinate system. It also demonstrates how to use two fields
#   to generate a dynamic unique row identifier for the query layer.


# Import system modules
import arcpy

# Create the spatial reference for the output layer.
sr = arcpy.SpatialReference("WGS 1984 UTM Zone 12N")

# Run the tool
arcpy.MakeQueryLayer_management("Connections/moab.sde",
                                "Single Track",
                                "select * from moabtrails where type = 'single'",
                                "UID;name",
                                "POLYLINE",
                                "32611",
                                sr)
```

### Example 5

```python
# Name: MakeQueryLayer.py
# Description: Creates an output query layer based on a where clause.
#   This example shows how to create a spatial reference object using the
#   name of a coordinate system. It also demonstrates how to use two fields
#   to generate a dynamic unique row identifier for the query layer.


# Import system modules
import arcpy

# Create the spatial reference for the output layer.
sr = arcpy.SpatialReference("WGS 1984 UTM Zone 12N")

# Run the tool
arcpy.MakeQueryLayer_management("Connections/moab.sde",
                                "Single Track",
                                "select * from moabtrails where type = 'single'",
                                "UID;name",
                                "POLYLINE",
                                "32611",
                                sr)
```

---

## Make Query Table (Data Management)

## Summary

Applies an SQL query to a database, and the results are represented in either a layer or table view. The query can be used to join several tables or return a subset of fields or rows from the original data in the database.

## Usage

- The layer that is created by the tool is temporary and will not persist after the session ends unless the document is saved.
- All input feature classes or tables must be from the same input workspace.
- If a Shape field is added to the field list, the result is a layer; otherwise, it is a table view.
- If the output result is a layer, it can be persisted to a layer file using the Save To Layer File tool or to a feature class using the Copy Features tool.
- The order of the fields in the field list is the order the fields will appear in the output layer or table view.
- You can provide a key field option and key fields list. This information defines how rows are uniquely identified and is used to add a dynamically generated Object ID field to the data. Without an Object ID field, selections will not be supported.
- You can choose several fields from the key fields list if a combination of the fields is needed to define unique values.
- If an SQL expression is used but returns no matching records, the output feature class will be empty.
- Feature classes can be joined, but the fields list must contain at most one field of type geometry.
- For details on the syntax for the Expression parameter, see SQL mode.
- When input tables are from a file geodatabase, tables generally join in the order listed in the Input Tables parameter. For example, if Table1 is listed before Table2, Table2 will be joined by getting a row from Table1, then getting matching rows from Table2. However, if this would result in querying Table2 on a nonindexed field, and reversing the order would result in querying Table1 on an indexed field, the order will be reversed in an attempt to maximize performance. This is the sole query optimization logic used when you're using file geodatabase data with this tool. In general, joins in file geodatabases perform best when they are one-to-many or one-to-one.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Tables | The name of the table or tables to be used in the query. If several tables are listed, the Expression parameter can be used to define how they will be joined.The input table can be from a geodatabase or a database connection. | Table View; Raster Layer |
| Table Name | The name of the layer or table view that will be created. | Table View; Raster Layer |
| Key Field Options | Specifies how an Object ID field will be generated (if at all) for the query. Layers and table views in ArcGIS require an Object ID field. An Object ID field is an integer field that uniquely identifies rows in the data being used.Use key fields—Specified fields in the Key Fields parameter will be used to uniquely identify a row in the output table. This can be a single field or multiple fields, which, when combined, uniquely identify a row in the output table. If no fields are specified in the key fields list, the Generate a key field option will be applied. This is the default.Generate a key field—If no key fields have been specified, an Object ID field that uniquely identifies each row in the output table will be generated. No key field—No Object ID field will be generated. Selections will not be supported for the table view. Note:If there is an existing Object ID field, it will be used even if this option is chosen. Use key fields—Specified fields in the in_key_field parameter will be used to uniquely identify a row in the output table. This can be a single field or multiple fields, which, when combined, uniquely identify a row in the output table. If no fields are specified in the key fields list, the ADD VIRTUAL_KEY_FIELD option will be applied. Generate a key field—If no key fields have been specified, an Object ID field that uniquely identifies each row in the output table will be generated. No key field—No Object ID field will be generated. Selections will not be supported for the table view. Note:If there is an existing Object ID field, it will be used even if this option is chosen. | String |
| Key Fields(Optional) | A field or combination of fields that will be used to uniquely identify a row in the query. This parameter is used only when the Key Field Options parameter is set to Use key fields. | Field |
| Fields(Optional) | The fields that will be included in the layer or table view. If an alias is set for a field, this is the name that appears. If no fields are specified, all fields from all tables are included. If a Shape field is added to the field list, the result is a layer; otherwise it is a table view. | Value Table |
| Expression(Optional) | An SQL expression used to select a subset of records. | SQL Expression |
| in_table[in_table,...] | The name of the table or tables to be used in the query. If several tables are listed, the where_clause parameter can be used to define how they will be joined.The input table can be from a geodatabase or a database connection. | Table View; Raster Layer |
| out_table | The name of the layer or table view that will be created. | Table View; Raster Layer |
| in_key_field_option | Specifies how an Object ID field will be generated (if at all) for the query. Layers and table views in ArcGIS require an Object ID field. An Object ID field is an integer field that uniquely identifies rows in the data being used.USE_KEY_FIELDS—Specified fields in the in_key_field parameter will be used to uniquely identify a row in the output table. This can be a single field or multiple fields, which, when combined, uniquely identify a row in the output table. If no fields are specified in the key fields list, the ADD VIRTUAL_KEY_FIELD option will be applied. ADD_VIRTUAL_KEY_FIELD—If no key fields have been specified, an Object ID field that uniquely identifies each row in the output table will be generated. NO_KEY_FIELD—No Object ID field will be generated. Selections will not be supported for the table view. Note:If there is an existing Object ID field, it will be used even if this option is chosen. | String |
| in_key_field[in_key_field,...](Optional) | A field or combination of fields that will be used to uniquely identify a row in the query. This parameter is used only when the in_key_field_option parameter is set to USE_KEY_FIELDS. | Field |
| in_field[[field, {alias}],...](Optional) | The fields that will be included in the layer or table view. If an alias is set for a field, this is the name that appears. If no fields are specified, all fields from all tables are included. If a Shape field is added to the field list, the result is a layer; otherwise it is a table view. | Value Table |
| where_clause(Optional) | An SQL expression used to select a subset of records. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.MakeQueryTable(in_table, out_table, in_key_field_option, {in_key_field}, {in_field}, {where_clause})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
arcpy.management.MakeQueryTable(["Counties","codemog"], "queryout", "ADD_VIRTUAL_KEY_FIELD", "",
                                [["Counties.OBJECTID", 'ObjectID'], ["Counties.NAME", 'Name'],
                                 ["codemog.Males", 'Males'], ["codemog.Females", 'Females']],
                                "Counties.FIPS = codemog.Fips and Counties.STATE_NAME = 'California'")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/data.gdb"
arcpy.management.MakeQueryTable(["Counties","codemog"], "queryout", "ADD_VIRTUAL_KEY_FIELD", "",
                                [["Counties.OBJECTID", 'ObjectID'], ["Counties.NAME", 'Name'],
                                 ["codemog.Males", 'Males'], ["codemog.Females", 'Females']],
                                "Counties.FIPS = codemog.Fips and Counties.STATE_NAME = 'California'")
```

### Example 4

```python
# MakeQueryTableOLEDB.py
# Description: Create a query table from two OLE DB tables using a limited set of
#               fields and establishing an equal join.
 
# Import system modules
import arcpy
 
# Local variables...
tableList = ["c:/Connections/balrog.odc/vtest.COUNTIES",\
             "c:/Connections/balrog.odc/vtest.CODEMOG"]

fieldList = [["vtest.COUNTIES.OBJECTID", 'ObjectID'], ["vtest.COUNTIES.NAME", 'Name']\
             ["vtest.CODEMOG.Males", 'Males'], ["vtest.CODEMOG.Females", 'Females']]
whereClause = "vtest.COUNTIES.FIPS = vtest.CODEMOG.Fips" +\
              "and vtest.COUNTIES.STATE_NAME = 'California'"
keyField = "vtest.COUNTIES.OBJECTID"
lyrName = "CountyCombined"

# Make Query Table...
arcpy.management.MakeQueryTable(tableList, lyrName,"USE_KEY_FIELDS", keyField, fieldList, whereClause)

# Print the total rows
print(arcpy.management.GetCount(lyrName))
 
# Print the fields
fields = arcpy.ListFields(lyrName)
for field in fields:
    print(field.name)

# Save as a dBASE file
arcpy.management.CopyRows(lyrName, "C:/temp/calinfo.dbf")
```

### Example 5

```python
# MakeQueryTableOLEDB.py
# Description: Create a query table from two OLE DB tables using a limited set of
#               fields and establishing an equal join.
 
# Import system modules
import arcpy
 
# Local variables...
tableList = ["c:/Connections/balrog.odc/vtest.COUNTIES",\
             "c:/Connections/balrog.odc/vtest.CODEMOG"]

fieldList = [["vtest.COUNTIES.OBJECTID", 'ObjectID'], ["vtest.COUNTIES.NAME", 'Name']\
             ["vtest.CODEMOG.Males", 'Males'], ["vtest.CODEMOG.Females", 'Females']]
whereClause = "vtest.COUNTIES.FIPS = vtest.CODEMOG.Fips" +\
              "and vtest.COUNTIES.STATE_NAME = 'California'"
keyField = "vtest.COUNTIES.OBJECTID"
lyrName = "CountyCombined"

# Make Query Table...
arcpy.management.MakeQueryTable(tableList, lyrName,"USE_KEY_FIELDS", keyField, fieldList, whereClause)

# Print the total rows
print(arcpy.management.GetCount(lyrName))
 
# Print the fields
fields = arcpy.ListFields(lyrName)
for field in fields:
    print(field.name)

# Save as a dBASE file
arcpy.management.CopyRows(lyrName, "C:/temp/calinfo.dbf")
```

---

## Make Raster Layer (Data Management)

## Summary

Creates a raster layer from an input raster dataset or layer file. The layer created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved.

## Usage

- To persist your layer, right-click the layer in the Contents pane and click Save As Layer File, or use the Save To Layer File tool.
- The output can be created with only a subset of the bands. This will save time and disk space.
- You can use a raster layer from a GeoPackage as the input. To reference a raster within a GeoPackage, type the name of the path, followed by the name of the GeoPackage and the name of the raster. For example, c:\data\sample.gpkg\raster_tile would be your input raster, where sample.gpkg is the name of the GeoPackage and raster_tile is the raster dataset within the package.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input raster | The path and name of the input raster dataset.You can use a raster layer from a GeoPackage as the input. To reference a raster within a GeoPackage, type the name of the path, followed by the name of the GeoPackage and the name of the raster. For example, c:\data\sample.gpkg\raster_tile would be your input raster, where sample.gpkg is the name of the GeoPackage and raster_tile is the raster dataset within the package. | Composite Geodataset |
| Output raster layer name | The name of the layer to create. | Raster Layer |
| Where clause(Optional) | Define a query using SQL or use the Query Builder to build a query. | SQL Expression |
| Envelope(Optional) | The output extent can be specified by defining the four coordinates or by using the extent of an existing layer.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Bands(Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| in_raster | The path and name of the input raster dataset.You can use a raster layer from a GeoPackage as the input. To reference a raster within a GeoPackage, type the name of the path, followed by the name of the GeoPackage and the name of the raster. For example, c:\data\sample.gpkg\raster_tile would be your input raster, where sample.gpkg is the name of the GeoPackage and raster_tile is the raster dataset within the package. | Composite Geodataset |
| out_rasterlayer | The name of the layer to create. | Raster Layer |
| where_clause(Optional) | Define a query using SQL. | SQL Expression |
| envelope(Optional) | The output extent can be specified by defining the four coordinates or by using the extent of an existing layer.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| band_index[band_index,...](Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.MakeRasterLayer(in_raster, out_rasterlayer, {where_clause}, {envelope}, {band_index})
```

### Example 2

```python
import arcpy
arcpy.MakeRasterLayer_management("c:/workspace/image.tif", "rdlayer", "#", "feature.shp", "1")
```

### Example 3

```python
import arcpy
arcpy.MakeRasterLayer_management("c:/workspace/image.tif", "rdlayer", "#", "feature.shp", "1")
```

### Example 4

```python
##====================================
##Make Raster Layer
##Usage: MakeRasterLayer_management in_raster out_rasterlayer {where_clause} {envelope}
##                                  {Index;Index...}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Create raster layer from single raster dataset with clipping feature
arcpy.MakeRasterLayer_management("image.tif", "rdlayer", "#", "feature.shp", "1")
```

### Example 5

```python
##====================================
##Make Raster Layer
##Usage: MakeRasterLayer_management in_raster out_rasterlayer {where_clause} {envelope}
##                                  {Index;Index...}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Create raster layer from single raster dataset with clipping feature
arcpy.MakeRasterLayer_management("image.tif", "rdlayer", "#", "feature.shp", "1")
```

---

## Make Scene Layer (Data Management)

## Summary

Creates a scene layer from a scene layer package (.slpk) or scene service.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input scene layer package (.slpk) or scene service from which the new scene layer will be created. | Scene Layer; Building Scene Layer; File |
| Output Layer | The name of the scene layer to be created. | Scene Layer |
| in_dataset | The input scene layer package (.slpk) or scene service from which the new scene layer will be created. | Scene Layer; Building Scene Layer; File |
| out_layer | The name of the scene layer to be created. | Scene Layer |

## Code Samples

### Example 1

```python
arcpy.management.MakeSceneLayer(in_dataset, out_layer)
```

### Example 2

```python
import arcpy
arcpy.management.MakeSceneLayer(r"c:\temp\buildings.slpk", "buildings_Layer")
```

### Example 3

```python
import arcpy
arcpy.management.MakeSceneLayer(r"c:\temp\buildings.slpk", "buildings_Layer")
```

### Example 4

```python
import arcpy
arcpy.management.MakeSceneLayer("https://MyServer.com/server/rest/services/Hosted/City_WSL1/SceneServer/layers/0", 
                                "City_Layer")
```

### Example 5

```python
import arcpy
arcpy.management.MakeSceneLayer("https://MyServer.com/server/rest/services/Hosted/City_WSL1/SceneServer/layers/0", 
                                "City_Layer")
```

### Example 6

```python
# Name: GetSceneLayerCount.py
# Description: Gets the number of features from a scene service

# Import system modules
import arcpy

out_layer = 'Hamburg_Buildings'

# Make a layer from a scene service
arcpy.management.MakeSceneLayer('http://scene.arcgis.com/arcgis/rest/services/Hosted/Building_Hamburg/SceneServer/layers/0',
                                out_layer)
print("Created Scene Layer")

# Get the number of features from the scene service
result = arcpy.management.GetCount(out_layer)
print('{} has {} records'.format(out_layer, result[0]))
```

### Example 7

```python
# Name: GetSceneLayerCount.py
# Description: Gets the number of features from a scene service

# Import system modules
import arcpy

out_layer = 'Hamburg_Buildings'

# Make a layer from a scene service
arcpy.management.MakeSceneLayer('http://scene.arcgis.com/arcgis/rest/services/Hosted/Building_Hamburg/SceneServer/layers/0',
                                out_layer)
print("Created Scene Layer")

# Get the number of features from the scene service
result = arcpy.management.GetCount(out_layer)
print('{} has {} records'.format(out_layer, result[0]))
```

---

## Make Table View (Data Management)

## Summary

Creates a table view from an input table or feature class. The table view that is created is temporary and will not persist after the session ends unless the document is saved.

## Usage

- This tool is commonly used to create a table view based on an SQL expression with a selected set of attribute fields.
- If an SQL expression is used but returns nothing, the output will be empty.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class. | Table View; Raster Layer |
| Table Name | The name of the table view to be created. | Table View; Raster Layer |
| Expression(Optional) | An SQL expression used to select a subset of records. | SQL Expression |
| Output Workspace(Optional) | This parameter is not used.In ArcGIS Desktop, output field names are validated based on this workspace. In ArcGIS Pro, this tool does not support changing the field names because table views do not support field names that differ from the underlying data source. | Workspace |
| Field Info(Optional) | The fields from the input table that will be included in the output layer. You can remove input fields by setting them to not visible. Renaming fields and the use of split policies are not supported. | Field Info |
| in_table | The input table or feature class. | Table View; Raster Layer |
| out_view | The name of the table view to be created. | Table View; Raster Layer |
| where_clause(Optional) | An SQL expression used to select a subset of features. For more information on SQL syntax see the help topic SQL reference for query expressions used in ArcGIS. | SQL Expression |
| workspace(Optional) | This parameter is not used.In ArcGIS Desktop, output field names are validated based on this workspace. In ArcGIS Pro, this tool does not support changing the field names because table views do not support field names that differ from the underlying data source. | Workspace |
| field_info(Optional) | The fields from the input table that will be included in the output layer. You can remove input fields by setting them to not visible. Renaming fields and the use of split policies are not supported. | Field Info |

## Code Samples

### Example 1

```python
arcpy.management.MakeTableView(in_table, out_view, {where_clause}, {workspace}, {field_info})
```

### Example 2

```python
import arcpy

arcpy.management.MakeTableView("C:/data/input/crimefrequency.dbf", "crimefreq_tview")
```

### Example 3

```python
import arcpy

arcpy.management.MakeTableView("C:/data/input/crimefrequency.dbf", "crimefreq_tview")
```

### Example 4

```python
# Name: MakeTableView_Example2.py
# Description: Uses a FieldInfo object to select a subset of fields and use with MakeTableView

# Import system modules
import arcpy

# Set data path
intable = "C:/data/tables.gdb/crimefreq"

# Get the fields from the input
fields= arcpy.ListFields(intable)

# Create a fieldinfo object
fieldinfo = arcpy.FieldInfo()

# Iterate through the input fields and add them to fieldinfo
for field in fields:
    if field.name == "CRIME_ADDRESS":
        # Make the CRIME_ADDRESS field hidden
        fieldinfo.addField(field.name, field.name, "HIDDEN", "")
    else:
        fieldinfo.addField(field.name, field.name, "VISIBLE", "")

# The created crime_view layer will have fields as set in fieldinfo object
arcpy.management.MakeTableView(intable, "crime_view", "", "", fieldinfo)

# Persist the view to a table
arcpy.management.CopyRows("crime_view", "C:/data/tables.gdb/crime_copy")
```

### Example 5

```python
# Name: MakeTableView_Example2.py
# Description: Uses a FieldInfo object to select a subset of fields and use with MakeTableView

# Import system modules
import arcpy

# Set data path
intable = "C:/data/tables.gdb/crimefreq"

# Get the fields from the input
fields= arcpy.ListFields(intable)

# Create a fieldinfo object
fieldinfo = arcpy.FieldInfo()

# Iterate through the input fields and add them to fieldinfo
for field in fields:
    if field.name == "CRIME_ADDRESS":
        # Make the CRIME_ADDRESS field hidden
        fieldinfo.addField(field.name, field.name, "HIDDEN", "")
    else:
        fieldinfo.addField(field.name, field.name, "VISIBLE", "")

# The created crime_view layer will have fields as set in fieldinfo object
arcpy.management.MakeTableView(intable, "crime_view", "", "", fieldinfo)

# Persist the view to a table
arcpy.management.CopyRows("crime_view", "C:/data/tables.gdb/crime_copy")
```

---

## Make TIN Layer (Data Management)

## Summary

Creates a triangulated irregular network (TIN) layer from an input TIN dataset or layer file. The layer that is created by the tool is temporary and will not persist after the session ends unless the layer is saved to disk or the map document is saved.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input TIN | The input TIN dataset or layer from which the new layer will be created. | TIN Layer |
| Output TIN Layer | The name of the TIN layer to be created. The output layer can be used as an input to any geoprocessing tool that accepts a TIN layer as input. | TIN Layer |
| in_tin | The input TIN dataset or layer from which the new layer will be created. | TIN Layer |
| out_layer | The name of the TIN layer to be created. The output layer can be used as an input to any geoprocessing tool that accepts a TIN layer as input. | TIN Layer |

## Code Samples

### Example 1

```python
arcpy.management.MakeTinLayer(in_tin, out_layer)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/gis_data/input'
arcpy.management.MakeTinLayer('dtm', 'Elevation Layer')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/gis_data/input'
arcpy.management.MakeTinLayer('dtm', 'Elevation Layer')
```

---

## Make Trajectory Layer (Data Management)

## Summary

Generates a feature layer from selected variables in a trajectory file.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Trajectory File | The input trajectory file. | File; Trajectory Layer |
| Output Trajectory Layer | The output feature layer that contains the selected variables. | Trajectory Layer |
| Dimension Name (Optional) | The dimension name. The first dimension is used by default. | String |
| Predefined Variables (Optional) | Specifies the predefined variables that will be used for measurement for different sensor types.Surface Backscatter Coefficient—A variable that contains Surface Backscatter Coefficient will be used.Sea Surface Height—A variable that contains Sea Surface Height will be used.Sea Surface Height Anomaly—A variable that contains Sea Surface Height Anomaly will be used.Significant Wave Height—A variable that contains Significant Wave Height will be used.Ocean Surface Backscatter Coefficient—A variable that contains Ocean Surface Backscatter Coefficient will be used.Sea Ice Surface Elevation—A variable that contains Sea Ice Surface Elevation will be used.Sea Ice Surface Height Anomaly—A variable that contains Sea Ice Surface Height Anomaly will be used.Sea Ice Concentration—A variable that contains Sea Ice Concentration will be used.Sea Ice Surface Backscatter Coefficient—A variable that contains Sea Ice Surface Backscatter Coefficient will be used.Ice Sheet Surface Elevation—A variable that contains Ice Sheet Surface Elevation will be used.Ice Sheet Surface Backscatter Coefficient—A variable that contains Ice Sheet Surface Backscatter Coefficient will be used.Ice Surface Elevation—A variable that contains Ice Surface Elevation will be used.Ice Surface Backscatter Coefficient—A variable that contains Ice Surface Backscatter Coefficient will be used.Wind Speed—A variable that contains Wind Speed will be used.Mean Sea Surface Elevation—A variable that contains Mean Sea Surface Elevation will be used. | String |
| Variables (Optional) | The variables that will be included in the output layer. All variables are selected by default. | String |
| in_trajectory_file | The input trajectory file. | File; Trajectory Layer |
| out_trajectory_layer | The output feature layer that contains the selected variables. | Trajectory Layer |
| dimension(Optional) | The dimension name. The first dimension is used by default. | String |
| predefined_variables[predefined_variables,...](Optional) | Specifies the predefined variables that will be used for measurement for different sensor types.SIGMA0—A variable that contains Surface Backscatter Coefficient will be used.SSH—A variable that contains Sea Surface Height will be used.SSHA—A variable that contains Sea Surface Height Anomaly will be used.SWH—A variable that contains Significant Wave Height will be used.SIGMA0_OCEAN—A variable that contains Ocean Surface Backscatter Coefficient will be used.H_SEA_ICE—A variable that contains Sea Ice Surface Elevation will be used.H_SEA_ICE_ANOMALY—A variable that contains Sea Ice Surface Height Anomaly will be used.SIC—A variable that contains Sea Ice Concentration will be used.SIGMA0_SEA_ICE—A variable that contains Sea Ice Surface Backscatter Coefficient will be used.H_ICE_SHEET—A variable that contains Ice Sheet Surface Elevation will be used.SIGMA0_ICE_SHEET—A variable that contains Ice Sheet Surface Backscatter Coefficient will be used.H_ICE—A variable that contains Ice Surface Elevation will be used.SIGMA0_ICE—A variable that contains Ice Surface Backscatter Coefficient will be used.WS—A variable that contains Wind Speed will be used.H_MSS—A variable that contains Mean Sea Surface Elevation will be used. | String |
| variables[variables,...](Optional) | The variables that will be included in the output layer. All variables are selected by default. | String |

## Code Samples

### Example 1

```python
arcpy.management.MakeTrajectoryLayer(in_trajectory_file, out_trajectory_layer, {dimension}, {predefined_variables}, {variables})
```

### Example 2

```python
# Import system modules
import arcpy
from arcpy import *

# Set local variables
in_trajectory_file = r"C:\data\Cryosat\CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001.nc"
out_trajectory_layer = r"C:\data\Cryosat\trajectory_layer
dimension = "CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001_time_20_ku"
predefined_variables = "SSH;H_SEA_ICE"
variables = "height_1_20_ku"

# Execute
trajectory_output = arcpy.management.MakeTrajectoryLayer(in_trajectory_file, out_trajectory_layer, 
		    	dimension, predefined_variables, variables)
```

### Example 3

```python
# Import system modules
import arcpy
from arcpy import *

# Set local variables
in_trajectory_file = r"C:\data\Cryosat\CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001.nc"
out_trajectory_layer = r"C:\data\Cryosat\trajectory_layer
dimension = "CS_OFFL_SIR_LRM_2__20210301T000738_20210301T001611_D001_time_20_ku"
predefined_variables = "SSH;H_SEA_ICE"
variables = "height_1_20_ku"

# Execute
trajectory_output = arcpy.management.MakeTrajectoryLayer(in_trajectory_file, out_trajectory_layer, 
		    	dimension, predefined_variables, variables)
```

---

## Make WCS Layer (Data Management)

## Summary

Creates a temporary raster layer from a WCS service.

## Usage

- This is one of the few tools that can accept a WCS service as an input; therefore, this tool can be used to convert a WCS service to a raster layer, which can then be used by a geoprocessing tool. The layer created by the tool is temporary and will not persist after the session ends unless the document is saved.
- The input can also be a URL to a WCS server. The WCS server URL should also include the coverage and version information. If only URL is entered, the tool will automatically take the first coverage and use the default version (1.0.0) to create the WCS layer.
- The output can be the entire image service or a portion of it.
- You can clip out a portion of the image service by choosing an output extent layer or by specifying the rectangle extent. If you choose an output extent layer, the clip extent will be based on the extent of that layer.
- The output can be created with only a subset of the bands. This will save time and disk space.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input WCS Coverage | Browse to the input WCS service. This tool can also accept a URL that references the WCS service.If a WCS server URL is used, the URL should include the coverage name and version information. If only the URL is entered, the tool will automatically take the first coverage and use the default version (1.0.0) to create the WCS layer.An example of a URL that includes the coverage name and version is http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?coverage=rasterDRGs&version=1.1.1. In this example, http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer? is the URL. The coverage specified is coverage=rasterDRGs, and the version is &version=1.1.1.To get the coverage names on a WCS server, use the WCS GetCapabilities request. An example of WCS request is http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?request=getcapabilities&service=wcs. | WCS Coverage; String |
| Output WCS Layer | The name of the output WCS layer. | Raster Layer |
| Template Extent(Optional) | The output extent of the WCS layer.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Bands(Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |
| in_wcs_coverage | The name of the input WCS service, or the URL that references the WCS service.If a WCS server URL is used, the URL should include the coverage name and version information. If only the URL is entered, the tool will automatically take the first coverage and use the default version (1.0.0) to create the WCS layer.An example of a URL that includes the coverage name and version is http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?coverage=rasterDRGs&version=1.1.1.In this example, http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer? is the URL. The coverage specified is coverage=rasterDRGs, and the version is &version=1.1.1.To get the coverage names on a WCS server, use the WCS GetCapabilities request. An example of WCS request is http://ServerName/arcgis/services/serviceName/ImageServer/WCSServer?request=getcapabilities&service=wcs. | WCS Coverage; String |
| out_wcs_layer | The name of the output WCS layer. | Raster Layer |
| template(Optional) | The output extent of the WCS layer.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| band_index[band_index,...](Optional) | The bands that will be exported for the layer. If no bands are specified, all the bands will be used in the output. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.MakeWCSLayer(in_wcs_coverage, out_wcs_layer, {template}, {band_index})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/Workspace"
input1 = "GIS Servers/File_TIFF_Amberg on server3/090160_1"
arcpy.MakeWCSLayer_management(input1, "wcslayer1", "11.844983 49.445367 11.858321 49.453887",
                              "1;2;3")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/Workspace"
input1 = "GIS Servers/File_TIFF_Amberg on server3/090160_1"
arcpy.MakeWCSLayer_management(input1, "wcslayer1", "11.844983 49.445367 11.858321 49.453887",
                              "1;2;3")
```

### Example 4

```python
##====================================
##Make WCS Layer
##Usage: MakeWCSLayer_management in_wcs_coverage out_wcs_layer {template} {ID;ID...}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"
input1 = r"GIS Servers\File_TIFF_Amberg on server3\090160_1"
input2 = "http://server3/arcgis/services/File_TIFF_Amberg/ImageServer/WCSServer"

##Create WCS layer from WCS connection file
arcpy.MakeWCSLayer_management(input1, "wcslayer1", "11.844983 49.445367 11.858321 49.453887",
                              "1;2;3")

##Create WCS layer from URL with clipping feature
arcpy.MakeWCSLayer_management(input2, "wcslayer2", "clip.shp", "1;2;3")
```

### Example 5

```python
##====================================
##Make WCS Layer
##Usage: MakeWCSLayer_management in_wcs_coverage out_wcs_layer {template} {ID;ID...}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"
input1 = r"GIS Servers\File_TIFF_Amberg on server3\090160_1"
input2 = "http://server3/arcgis/services/File_TIFF_Amberg/ImageServer/WCSServer"

##Create WCS layer from WCS connection file
arcpy.MakeWCSLayer_management(input1, "wcslayer1", "11.844983 49.445367 11.858321 49.453887",
                              "1;2;3")

##Create WCS layer from URL with clipping feature
arcpy.MakeWCSLayer_management(input2, "wcslayer2", "clip.shp", "1;2;3")
```

---

## Make XY Event Layer (Data Management)

## Summary

Creates a point event layer from a table containing fields with x- and y-coordinate values, and optionally, z-coordinate (elevation) values.

## Usage

- The output event layer created by this tool is temporary and is not stored on disk or in a geodatabase. You can export an event layer to a feature class to permanently store it using the Copy Features, Feature To Point, or Export Features tool.
- If you are working with tabular data that updates often, you can create an event layer that updates automatically when the source table updates. This may be more efficient than repetitively converting the table to a new point feature class.
- Event layer features are not editable. Copy or export the event layer to a feature class if editing is required.
- The standard delimiter for tabular text files with a .csv or .txt extension is a comma, and for files with a .tab extension, a tab. To use an input table with a nonstandard delimiter, you must first specify the correct delimiter used in the table using a schema.ini file.
- If the input table is from a file format that does not have an Object ID field or a database table without a primary key, you cannot make selections, apply definition queries or other filters, or add joins or relates to the event layer.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the x- and y-coordinates that define the locations of the point features that will be created. | Table View |
| X Field | The field in the input table that contains the x-coordinates (longitude). | Field |
| Y Field | The field in the input table that contains the y-coordinates (latitude). | Field |
| Output Layer Name | The name of the output event layer. | Feature Layer |
| Coordinate System(Optional) | The coordinate system of the coordinates specified in the X Field and Y Field parameters. This will be the output event layer's coordinate system. | Spatial Reference |
| Z Field(Optional) | The field in the input table that contains the z-coordinates. | Field |
| table | The table containing the x- and y-coordinates that define the locations of the point features that will be created. | Table View |
| in_x_field | The field in the input table that contains the x-coordinates (longitude). | Field |
| in_y_field | The field in the input table that contains the y-coordinates (latitude). | Field |
| out_layer | The name of the output event layer. | Feature Layer |
| spatial_reference(Optional) | The coordinate system of the coordinates specified in the in_x_field and in_y_field parameters. This will be the output event layer's coordinate system. | Spatial Reference |
| in_z_field(Optional) | The field in the input table that contains the z-coordinates. | Field |

## Code Samples

### Example 1

```python
arcpy.management.MakeXYEventLayer(table, in_x_field, in_y_field, out_layer, {spatial_reference}, {in_z_field})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.MakeXYEventLayer("firestations.dbf", "POINT_X", "POINT_Y", 
                                  "firestations_points", "", "POINT_Z")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.MakeXYEventLayer("firestations.dbf", "POINT_X", "POINT_Y", 
                                  "firestations_points", "", "POINT_Z")
```

### Example 4

```python
# Description: Create an XY layer and export it to a layer file.

# Import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set the local variables
in_table = "firestations.dbf"
x_coords = "POINT_X"
y_coords = "POINT_Y"
z_coords = "POINT_Z"
out_layer = "firestations_layer"
saved_layer = r"c:\output\firestations.lyr"

# Set the spatial reference
spatial_ref = r"NAD_1983_UTM_Zone_11N"

# Make the XY event layer...
arcpy.management.MakeXYEventLayer(in_table, x_coords, y_coords, out_layer, 
                                  spatial_ref, z_coords)

# Save to a layer file
arcpy.management.SaveToLayerFile(out_layer, saved_layer)
```

### Example 5

```python
# Description: Create an XY layer and export it to a layer file.

# Import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set the local variables
in_table = "firestations.dbf"
x_coords = "POINT_X"
y_coords = "POINT_Y"
z_coords = "POINT_Z"
out_layer = "firestations_layer"
saved_layer = r"c:\output\firestations.lyr"

# Set the spatial reference
spatial_ref = r"NAD_1983_UTM_Zone_11N"

# Make the XY event layer...
arcpy.management.MakeXYEventLayer(in_table, x_coords, y_coords, out_layer, 
                                  spatial_ref, z_coords)

# Save to a layer file
arcpy.management.SaveToLayerFile(out_layer, saved_layer)
```

---

## Manage Feature Bin Cache (Data Management)

## Summary

Manages the feature binning cache for data that has database computed feature binning enabled.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The binned feature class that will have its static cache updated. | Feature Layer |
| Bin Type (Optional) | Specifies the type of feature binning visualization that will be enabled.Flat hexagon—The flat hexagon binning scheme, also known as flat geohex or flat hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a flat edge of the hexagon on top. This is the default for Microsoft SQL Server, Oracle, and PostgreSQL data.Pointy hexagon—The pointy hexagon binning scheme, also known as pointy geohex or pointy hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a point of the hexagon on top.Square—The square binning scheme in which the tiles are a tessellation of squares, also known as geosquare or squarebinning, will be enabled. This is the default for Db2 data.Geohash—The geohash binning scheme in which the tiles are a tessellation of rectangles will be enabled. Because geohash bins always use the WGS84 geographic coordinate system (GCS WGS84, EPSG WKID 4326), you cannot specify a bin coordinate system for geohash bins. | String |
| Level of Detail (Optional) | Specifies the maximum level of detail that will be used for the cache.Tiling schemes are a continuum of scale ranges. Depending on the map, you may want to forego caching of some of the extremely large or small scales in the tiling scheme. This tool examines the scale dependencies in the map and attempts to provide a maximum range of scale for caching. Choose a level of detail that most closely matches the intended use of the map in which the data will be shown.World—A world scale will be used as the maximum level of detail.Continents—Multiple continents scale will be used as the maximum level of detail.Continent—A single continent scale will be used as the maximum level of detail.Countries—Multiple countries scale will be used as the maximum level of detail.Country—A single country scale will be used as the maximum level of detail.States—Multiple states scale will be used as the maximum level of detail.State—A single state scale will be used as the maximum level of detail.Counties—Multiple counties scale will be used as the maximum level of detail.County—A single county scale will be used as the maximum level of detail.Cities—Multiple cities scale will be used as the maximum level of detail.City—A single city scale will be used as the maximum level of detail. | String |
| Add Statistic to Cache (Optional) | Specifies the statistics that will be summarized and stored in the bin cache. Statistics are used to symbolize bins and provide aggregate information for all the points in a bin. One summary statistic, shape_count (which is the total feature count), is always available.Field—The field on which the summary statistics will be calculated. Supported field types are short integer, long integer, big integer, float, and double.Statistic Type—The type of statistic that will be calculated for the specified field. Statistics are calculated for all features in the bin. Available statistics types are as follows: Mean (AVG)—The mean (average value) for the specified field will be calculated.Minimum (MIN)—The minimum (smallest value) of all records for the specified field will be identified.Maximum (MAX)—The maximum (largest value) of all records for the specified field will be identified.Standard deviation (STDDEV)—The standard deviation value for the field will be calculated.Sum (SUM)—The sum of the values for the specified field will be calculated. | Value Table |
| Delete Statistic from Cache (Optional) | The summary statistic that will be deleted from the cache. You cannot delete the default COUNT summary statistic. | String |
| in_features | The binned feature class that will have its static cache updated. | Feature Layer |
| bin_type(Optional) | Specifies the type of feature binning visualization that will be enabled.FLAT_HEXAGON—The flat hexagon binning scheme, also known as flat geohex or flat hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a flat edge of the hexagon on top. This is the default for Microsoft SQL Server, Oracle, and PostgreSQL data.POINTY_HEXAGON—The pointy hexagon binning scheme, also known as pointy geohex or pointy hexbinning, will be enabled. The tiles are a tessellation of hexagons in which the orientation of the hexagons has a point of the hexagon on top.SQUARE—The square binning scheme in which the tiles are a tessellation of squares, also known as geosquare or squarebinning, will be enabled. This is the default for Db2 data.GEOHASH—The geohash binning scheme in which the tiles are a tessellation of rectangles will be enabled. Because geohash bins always use the WGS84 geographic coordinate system (GCS WGS84, EPSG WKID 4326), you cannot specify a bin coordinate system for geohash bins. | String |
| max_lod(Optional) | Specifies the maximum level of detail that will be used for the cache.Tiling schemes are a continuum of scale ranges. Depending on the map, you may want to forego caching of some of the extremely large or small scales in the tiling scheme. This tool examines the scale dependencies in the map and attempts to provide a maximum range of scale for caching. Choose a level of detail that most closely matches the intended use of the map in which the data will be shown.WORLD—A world scale will be used as the maximum level of detail.CONTINENTS—Multiple continents scale will be used as the maximum level of detail.CONTINENT—A single continent scale will be used as the maximum level of detail.COUNTRIES—Multiple countries scale will be used as the maximum level of detail.COUNTRY—A single country scale will be used as the maximum level of detail.STATES—Multiple states scale will be used as the maximum level of detail.STATE—A single state scale will be used as the maximum level of detail.COUNTIES—Multiple counties scale will be used as the maximum level of detail.COUNTY—A single county scale will be used as the maximum level of detail.CITIES—Multiple cities scale will be used as the maximum level of detail.CITY—A single city scale will be used as the maximum level of detail. | String |
| add_cache_statistics[[Field, Statistic Type],...](Optional) | Specifies the statistics that will be summarized and stored in the bin cache. Statistics are used to symbolize bins and provide aggregate information for all the points in a bin. One summary statistic, shape_count (which is the total feature count), is always available.Field—The field on which the summary statistics will be calculated. Supported field types are short integer, long integer, big integer, float, and double.Statistic Type—The type of statistic that will be calculated for the specified field. Statistics are calculated for all features in the bin. Available statistics types are as follows: Mean (AVG)—The mean (average value) for the specified field will be calculated.Minimum (MIN)—The minimum (smallest value) of all records for the specified field will be identified.Maximum (MAX)—The maximum (largest value) of all records for the specified field will be identified.Standard deviation (STDDEV)—The standard deviation value for the field will be calculated.Sum (SUM)—The sum of the values for the specified field will be calculated. | Value Table |
| delete_cache_statistics[delete_cache_statistics,...](Optional) | The summary statistic that will be deleted from the cache. You cannot delete the default COUNT summary statistic. | String |

## Code Samples

### Example 1

```python
arcpy.management.ManageFeatureBinCache(in_features, {bin_type}, {max_lod}, {add_cache_statistics}, {delete_cache_statistics})
```

### Example 2

```python
import arcpy
arcpy.management.ManageFeatureBinCache("lod_gdb.elec.Earthquakes", "SQUARE", 
                                       "STATE", "depth_km MAX")
```

### Example 3

```python
import arcpy
arcpy.management.ManageFeatureBinCache("lod_gdb.elec.Earthquakes", "SQUARE", 
                                       "STATE", "depth_km MAX")
```

---

## Manage Tile Cache (Data Management)

## Summary

Creates a tile cache or updates tiles in an existing tile cache. You can use this tool to create tiles, replace missing tiles, overwrite outdated tiles, and delete tiles.

## Usage

- When creating a tile cache using this tool, if no Area of Interest parameter value is specified, the cache will be created with the full extent of the source raster or mosaic dataset or the current extent of the map file.
- The extent of the generated cache may be larger than the Area of Interest parameter value. In this case, the tool divides the data into large areas, known as supertiles, before cutting it into tiles of the size specified. When the Input Data Source parameter value uses antialiasing, the supertile is 2,048 by 2,048 pixels; otherwise, it is 4,096 by 4,096. When the specified Area of Interest feature intersects the boundary of a supertile, that entire supertile will be created. This means that you may need to zoom in before caching by a feature class can have an effect in saving time or disk space. If you have multiple areas of interest and they cover most of the Input Data Source parameter value, you may not get much benefit from caching by a feature class, because most of the features will intersect a supertile.
- To create cache in a custom tiling scheme, ensure that you have a tiling scheme defined for the source dataset before using this tool. You can use the Generate Tile Cache Tiling Scheme tool to create the tiling scheme.
- To create a cache in an ArcGIS Online tiling scheme, specify ArcGIS Online scheme for the Input Tiling Scheme parameter.
- This tool may take a long time to run for caches that cover a large geographic extent or very large scales. If the tool is canceled, tile creation is stopped, but the existing tiles are not deleted. This means that you can cancel the tool at any time, and if you rerun it later on the same cache and specify Recreate empty tiles for the Manage Mode parameter, it will continue from where it left off.
- This tool supports the Parallel Processing environment setting.
- When the Ready to serve format parameter is checked, the cache content is generated using the open tile package specification. The cache format is Compact V2, but the cache schema is stored in JSON instead of XML. To learn more about the ready to learn format, see Publish a tile layer from a cache dataset.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Cache Location | The folder in which the cache dataset will be created, the raster layer, or the path to an existing tile cache. | Folder; Raster Layer |
| Manage Mode | Specifies the mode that will be used to manage the cache. Recreate all tiles—Existing tiles will be replaced and new tiles will be added if the extent has changed or if layers have been added to a multilayer cache.Recreate empty tiles—Only tiles that are empty will be created. Existing tiles will be left unchanged.Delete tiles—Tiles will be deleted from the cache. The cache folder structure will not be deleted. | String |
| Cache Name(Optional) | The name of the cache dataset that will be created in the cache location. | String |
| Input Data Source (Optional) | A raster dataset, mosaic dataset, or map file. This parameter is not required when the Manage Mode parameter is set to Delete tiles.A map file (.mapx) cannot contain a map service or image service. | Mosaic Layer; Raster Layer; Map |
| Input Tiling Scheme(Optional) | Specifies the tiling scheme that will be used.ArcGIS Online scheme—The default ArcGIS Online tiling scheme will be used.Import scheme—An existing tiling scheme will be imported and used.Elevation tiling scheme—The elevation services tiling scheme will be used.WGS84 Version 2 tiling scheme—The WGS84 version 2 tiling scheme will be used.WGS84 Version 2 elevation tiling scheme— The WGS84 version 2 tiling scheme will be used to build a tile cache for elevation data. | String |
| Import Tiling Scheme(Optional) | The path to an existing scheme file (.xml) or to a tiling scheme imported from an existing image service or map service. | Image Service; Map Server; File |
| Scales [Pixel Size] (Estimated Disk Space)(Optional) | The scale levels at which tiles will be created or deleted, depending on the value of the Manage Mode parameter. The pixel size is based on the spatial reference of the tiling scheme.By default, only the values for Minimum Cached Scale and Maximum Cached Scale will be used.Altering the value of either the Minimum Cached Scale or the Maximum Cached Scale parameter will check on or off the appropriate scale values.Scales that are checked on and are not within the range of the Minimum Cached Scale or Maximum Cached Scale parameter values will be ignored when generating the cache. | Double |
| Area of Interest (Optional) | An area of interest that will be used to constrain where tiles will be created or deleted.It can be a feature class, or it can be a feature set that you interactively define.This parameter is useful if you want to manage tiles for irregularly shaped areas. It's also useful when you want to precache some areas and leave less-visited areas uncached. | Feature Set |
| Maximum Source Cell Size(Optional) | The value that defines the visibility of the data source for which the cache will be generated. By default, the value is empty.If the value is empty, the following apply:For levels of cache that lie within the visibility ranges of the data source, the cache will be generated from the data source.For levels of cache that fall outside the visibility of the data source, the cache will be generated from the previous level of cache.If the value is greater than zero, the following apply:For levels with cell sizes smaller than or equal to this parameter value, the cache will be generated from the data source.For levels with cell sizes greater than this parameter value, the cache will be generated from the previous level of cache.The measurement unit of this parameter value should be the same as the measurement unit of the cell size of the source dataset. | Double |
| Minimum Cached Scale(Optional) | The minimum scale at which tiles will be created. This value does not need to be the smallest scale in the tiling scheme. The minimum cache scale will determine which scales are used when generating cache. | Double |
| Maximum Cached Scale(Optional) | The maximum scale at which tiles will be created. This value does not need to be the largest scale in the tiling scheme. The maximum cache scale will determine which scales are used when generating cache. | Double |
| Ready to serve format(Optional) | Specifies whether the cache content will be generated using the open tile package specification and also specifies the file format of the cache schema. Checked—The cache content will be generated using the open tile package specification. A tile package can be packaged to a zip archive as a tile package for offline workflows. The cache format is Compact V2, and the cache schema is stored in JSON format.Unchecked—The cache content will be generated using a schema stored in XML format. This is the default. | Boolean |
| in_cache_location | The folder in which the cache dataset will be created, the raster layer, or the path to an existing tile cache. | Folder; Raster Layer |
| manage_mode | Specifies the mode that will be used to manage the cache. RECREATE_ALL_TILES—Existing tiles will be replaced and new tiles will be added if the extent has changed or if layers have been added to a multilayer cache.RECREATE_EMPTY_TILES—Only tiles that are empty will be created. Existing tiles will be left unchanged.DELETE_TILES—Tiles will be deleted from the cache. The cache folder structure will not be deleted. | String |
| in_cache_name(Optional) | The name of the cache dataset that will be created in the cache location. | String |
| in_datasource(Optional) | A raster dataset, mosaic dataset, or map file. This parameter is not required when the manage_mode parameter is set to DELETE_TILES.A map file (.mapx) cannot contain a map service or image service. | Mosaic Layer; Raster Layer; Map |
| tiling_scheme(Optional) | Specifies the tiling scheme that will be used.ARCGISONLINE_SCHEME—The default ArcGIS Online tiling scheme will be used.IMPORT_SCHEME—An existing tiling scheme will be imported and used.ARCGISONLINE_ELEVATION_SCHEME—The elevation services tiling scheme will be used.WGS84_V2_SCHEME—The WGS84 version 2 tiling scheme will be used.WGS84_V2_ELEVATION_SCHEME— The WGS84 version 2 tiling scheme will be used to build a tile cache for elevation data. | String |
| import_tiling_scheme(Optional) | The path to an existing scheme file (.xml) or to a tiling scheme imported from an existing image service or map service. | Image Service; Map Server; File |
| scales[scale,...](Optional) | The scale levels at which tiles will be created or deleted , depending on the value of the manage_mode parameter. The pixel size is based on the spatial reference of the tiling scheme.By default, only the values for min_cached_scale and max_cached_scale will be used when generating cache.Altering the value of either the min_cached_scale or the max_cached_scale parameter will change which scales will be used when generating cache.Scales that exist but are not within the range of the min_cached_scale or max_cached_scale parameter values will be ignored when generating the cache. | Double |
| area_of_interest(Optional) | An area of interest that will be used to constrain where tiles will be created or deleted.It can be a feature class, or it can be a feature set that you interactively define.This parameter is useful if you want to manage tiles for irregularly shaped areas. It's also useful when you want to precache some areas and leave less-visited areas uncached. | Feature Set |
| max_cell_size(Optional) | The value that defines the visibility of the data source for which the cache will be generated. By default, the value is empty.If the value is empty, the following apply:For levels of cache that lie within the visibility ranges of the data source, the cache will be generated from the data source.For levels of cache that fall outside the visibility of the data source, the cache will be generated from the previous level of cache.If the value is greater than zero, the following apply:For levels with cell sizes smaller than or equal to this parameter value, the cache will be generated from the data source.For levels with cell sizes greater than this parameter value, the cache will be generated from the previous level of cache.The measurement unit of this parameter value should be the same as the measurement unit of the cell size of the source dataset. | Double |
| min_cached_scale(Optional) | The minimum scale at which tiles will be created. This value does not need to be the smallest scale in the tiling scheme. The minimum cache scale will determine which scales are used when generating cache. | Double |
| max_cached_scale(Optional) | The maximum scale at which tiles will be created. This value does not need to be the largest scale in the tiling scheme. The maximum cache scale will determine which scales are used when generating cache. | Double |
| ready_to_serve_format(Optional) | Specifies whether the cache content will be generated using the open tile package specification and also specifies the file format of the cache schema.READY_TO_SERVE_FORMAT—The cache content will be generated using the open tile package specification (https://github.com/Esri/tile-package-spec). A tile package can be packaged to a zip archive as a tile package for offline workflows. The cache format is Compact V2, and the cache schema is stored in JSON format.NON_READY_TO_SERVE_FORMAT—The cache content will be generated using a schema stored in XML format. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ManageTileCache(in_cache_location, manage_mode, {in_cache_name}, {in_datasource}, {tiling_scheme}, {import_tiling_scheme}, {scales}, {area_of_interest}, {max_cell_size}, {min_cached_scale}, {max_cached_scale}, {ready_to_serve_format})
```

### Example 2

```python
import arcpy
            
arcpy.ManageTileCache_management(
      "C:/CacheDatasets/Manage", "RECREATE_ALL_TILES", "Test",
      "C:/Data/Cache.gdb/Md", "IMPORT_SCHEME", "C:/Data/Cache.gdb/Md",
      "#", "#", "#", "40000", "2000", "NON_READY_TO_SERVE_FORMAT")
```

### Example 3

```python
import arcpy
            
arcpy.ManageTileCache_management(
      "C:/CacheDatasets/Manage", "RECREATE_ALL_TILES", "Test",
      "C:/Data/Cache.gdb/Md", "IMPORT_SCHEME", "C:/Data/Cache.gdb/Md",
      "#", "#", "#", "40000", "2000", "NON_READY_TO_SERVE_FORMAT")
```

### Example 4

```python
#Generate tile cache for 3 out of 5 levels defined in tiling scheme

import arcpy

folder = "C:/Workspace/CacheDatasets/Manage"
mode = "RECREATE_ALL_TILES"
cacheName = "Test"
dataSource = "C:/Workspace/Cache.gdb/md"
method = "IMPORT_SCHEME"
tilingScheme = "C:/Workspace/Schemes/Tilingscheme.xml"
scales = "16000;8000;4000;2000;1000"
areaofinterest = "#"
maxcellsize = "#"
mincachedscale = "8000"
maxcachedscale = "2000"
ready_to_serve_format="NON_READY_TO_SERVE_FORMAT"

arcpy.ManageTileCache_management(
       folder, mode, cacheName, dataSource, method, tilingScheme,
       scales, areaofinterest, maxcellsize, mincachedscale, maxcachedscale, ready_to_serve_format)
```

### Example 5

```python
#Generate tile cache for 3 out of 5 levels defined in tiling scheme

import arcpy

folder = "C:/Workspace/CacheDatasets/Manage"
mode = "RECREATE_ALL_TILES"
cacheName = "Test"
dataSource = "C:/Workspace/Cache.gdb/md"
method = "IMPORT_SCHEME"
tilingScheme = "C:/Workspace/Schemes/Tilingscheme.xml"
scales = "16000;8000;4000;2000;1000"
areaofinterest = "#"
maxcellsize = "#"
mincachedscale = "8000"
maxcachedscale = "2000"
ready_to_serve_format="NON_READY_TO_SERVE_FORMAT"

arcpy.ManageTileCache_management(
       folder, mode, cacheName, dataSource, method, tilingScheme,
       scales, areaofinterest, maxcellsize, mincachedscale, maxcachedscale, ready_to_serve_format)
```

---

## Match Control Points (Data Management)

## Summary

Creates matching tie points for a given ground control point and initial tie point in one of the overlapping images.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The mosaic dataset that contains the source imagery from which the tie points will be created. | Mosaic Dataset; Mosaic Layer |
| Input Control Points | The input control point set that contains a list of ground control point features and at least one initial tie point for each ground control point. | File; Feature Class; Feature Layer; String |
| Output Control Point Table | The output control point features that contain ground control points. | Feature Class |
| Similarity (Optional) | Specifies the similarity level that will be used for matching tie points.Low similarity—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.Medium similarity—The similarity criteria for the matching points will be medium.High similarity—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |
| in_mosaic_dataset | The mosaic dataset that contains the source imagery from which the tie points will be created. | Mosaic Dataset; Mosaic Layer |
| in_control_points | The input control point set that contains a list of ground control point features and at least one initial tie point for each ground control point. | File; Feature Class; Feature Layer; String |
| out_control_points | The output control point features that contain ground control points. | Feature Class |
| similarity(Optional) | Specifies the similarity level that will be used for matching tie points.LOW—The similarity criteria for the two matching points will be low. This option will produce the most matching points, but some of the matches may have a higher level of error.MEDIUM—The similarity criteria for the matching points will be medium.HIGH—The similarity criteria for the matching points will be high. This option will produce the fewest matching points, but each match will have a lower level of error. | String |

## Code Samples

### Example 1

```python
[
{
"poinId": <id>,
"x": 
"y":
"z":
"xyAccuracy":
"zAccuracy":
"spatialReference":{<spatialReference>}, // default WGS84
"imagePointSpatialReference": {}, // default ICS
"imagePoints": [
{"imageId": 
 "x":
     "y": 
    },
   …
   ]
}
]
```

### Example 2

```python
[
{
"poinId": <id>,
"x": 
"y":
"z":
"xyAccuracy":
"zAccuracy":
"spatialReference":{<spatialReference>}, // default WGS84
"imagePointSpatialReference": {}, // default ICS
"imagePoints": [
{"imageId": 
 "x":
     "y": 
    },
   …
   ]
}
]
```

### Example 3

```python
arcpy.management.MatchControlPoints(in_mosaic_dataset, in_control_points, out_control_points, {similarity})
```

### Example 4

```python
import arcpy
mdpath = "c:/omproject/dronecollection.gdb/droneimgs"
initpointset = "c:/omproject/initialgcpset.json"
arcpy.MatchControlPoints_management(mdpath, initpointset, out_control_points="c:/omproject/matchedpointsets.shp", similarity="HIGH")
```

### Example 5

```python
import arcpy
mdpath = "c:/omproject/dronecollection.gdb/droneimgs"
initpointset = "c:/omproject/initialgcpset.json"
arcpy.MatchControlPoints_management(mdpath, initpointset, out_control_points="c:/omproject/matchedpointsets.shp", similarity="HIGH")
```

### Example 6

```python
import arcpy
import json
mdpath = "c:/omproject/dronecollection.gdb/droneimgs"
initpointset = [
    {
        "x": -117.21684675264804,
        "y": 34.052400694386705,
        "z": 123,
        "pointId": 1,
        "imagePoints": [
            {
                "imageID": 7,
                "x": -5635883367.549803,
                "y": -26485513430.170017,
                "u": -5635883367.549803,
                "v": -26485513430.170017
            }
        ]
    }
]
arcpy.MatchControlPoints_management(
        mdpath, in_control_points=json.dumps(initpointset), out_control_points="c:/omproject/matchedpointsets.shp", similarity="HIGH")
```

### Example 7

```python
import arcpy
import json
mdpath = "c:/omproject/dronecollection.gdb/droneimgs"
initpointset = [
    {
        "x": -117.21684675264804,
        "y": 34.052400694386705,
        "z": 123,
        "pointId": 1,
        "imagePoints": [
            {
                "imageID": 7,
                "x": -5635883367.549803,
                "y": -26485513430.170017,
                "u": -5635883367.549803,
                "v": -26485513430.170017
            }
        ]
    }
]
arcpy.MatchControlPoints_management(
        mdpath, in_control_points=json.dumps(initpointset), out_control_points="c:/omproject/matchedpointsets.shp", similarity="HIGH")
```

---

## Match Layer Symbology To A Style (Data Management)

## Summary

Creates unique value symbology for the input layer based on the input field or expression by matching input field or expression strings to symbol names from the input style.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The input layer or layer file to which matched symbols are applied as unique values symbol classes. The input layer can contain point, line, polygon, multipoint, or multipatch symbology. Existing symbology on the layer is overwritten. | Feature Layer |
| Match Values (Field or Expression) | The field or expression on which the input layer is symbolized. The field values or resultant expression values are matched to symbol names in the specified style to assign symbols to the resulting symbol classes. | Calculator Expression |
| Style | The style containing symbols with names matching the field or expression values. | String |
| in_layer | The input layer or layer file to which matched symbols are applied as unique values symbol classes. The input layer can contain point, line, polygon, multipoint, or multipatch symbology. Existing symbology on the layer is overwritten. | Feature Layer |
| match_values | The field or expression on which the input layer is symbolized. The field values or resultant expression values are matched to symbol names in the specified style to assign symbols to the resulting symbol classes. | Calculator Expression |
| in_style | The style containing symbols with names matching the field or expression values. | String |

## Code Samples

### Example 1

```python
arcpy.management.MatchLayerSymbologyToAStyle(in_layer, match_values, in_style)
```

### Example 2

```python
import arcpy
arcpy.management.MatchLayerSymbologyToAStyle("Streets", "$feature.RoadClass", 
                                             r"C:\RoadClasses.stylx")
```

### Example 3

```python
import arcpy
arcpy.management.MatchLayerSymbologyToAStyle("Streets", "$feature.RoadClass", 
                                             r"C:\RoadClasses.stylx")
```

---

## Match Photos To Rows By Time (Data Management)

## Summary

Matches photo files to table or feature class rows according to the photo and row time stamps. The row with the time stamp closest to the capture time of a photo will be matched to that photo. A new table is created that contains the object ID values from the input rows and their matching photo paths. You can also use this tool to add matching photo files to the rows of the input table as geodatabase attachments.

## Usage

- This tool can be used to match GPS-captured features to digital photographs taken at the same time the GPS feature was captured.
- The output table will contain the following attribute fields:IN_FID—The object ID of an input row whose time stamp matches the time stamp of a photo.Photo_Path—The full path to a photo file whose time stamp matches the time stamp of the input row identified in the IN_FID field.Photo_Name—The short name of the photo file.Match_Diff—The difference between the time stamps of a photo file and the matching input row. This numeric value is in the unit specified by the Time Difference Unit parameter.
- IN_FID—The object ID of an input row whose time stamp matches the time stamp of a photo.
- Photo_Path—The full path to a photo file whose time stamp matches the time stamp of the input row identified in the IN_FID field.
- Photo_Name—The short name of the photo file.
- Match_Diff—The difference between the time stamps of a photo file and the matching input row. This numeric value is in the unit specified by the Time Difference Unit parameter.
- While shapefile and dBASE data are supported for the input table, it is recommended that you use geodatabase data, because a date field in a shapefile or dBASE table cannot store both date and time information. Learn more about shapefile limitations
- Since a single input row may have a time stamp that matches the time stamp of multiple photographs, the output table may have multiple rows that have the same IN_FID value (each row in the output refers to a match between one photo and one input row).
- The output table can be joined to the input table using the output IN_FID field and the object ID value of the input. When the output table has multiple rows with the same IN_FID field value (one input row matches multiple photos), use a relate or a relationship class to link the output to the input.
- The time field must be of type date. To convert text or numeric fields to a field of type date, use the Convert Time Field tool.
- Even if a GPS point and a digital photograph are captured at exactly the same time, the times recorded by the devices may be in different time zones. For example, GPS devices often record times in coordinated universal time (UTC), or Greenwich mean time, while digital cameras often record times in a local time zone. To reconcile time-stamp differences resulting from different time zones, use the Convert Time Zone tool to change the time field of the input table to match the time zone of the photo file time stamp.Likewise, the clock of the GPS may be out of sync with the clock of the digital camera. To make a good match between the photo and GPS point when these clocks are out of sync, determine the difference between the two clocks, and use this value with the Clock Offset parameter.
- The Time Tolerance and Clock Offset parameters must be specified in seconds. There are a number of utilities on the internet for calculating the number of seconds a different unit of time equals. For example, 3 minutes and 12 seconds is equal to 192 seconds.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Folder | The folder where photo files (.jpg or .tif) are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. | Folder |
| Input Table | The table or feature class whose rows will be matched with photo files. The input table will typically be a point feature class representing GPS recordings. | Table View |
| Time Field | The date field from the input table that indicates when each row was captured or created. The field must be of type date; it cannot be of type string or numeric. | Field |
| Output Table | The output table containing the object ID values from the input table that match a photo, and the matching photo path. Only object ID values from the input table that match a photo are included in the output table. | Table |
| Unmatched Photos Table (Optional) | The optional output table that will list any photo files in the input folder with an invalid time stamp or any photos that cannot be matched because there is no input row within the time tolerance.If no path is specified, this table will not be created. | Table |
| Add Photos As Attachments(Optional) | Checked—Photo files will be added to the rows of the input table as geodatabase attachments. Geodatabase attachments are copied internally to the geodatabase. This is the default.Unchecked—Photo files will not be added to the rows of the input table as geodatabase attachments. | Boolean |
| Time Tolerance (Optional) | The maximum difference (in seconds) between the date/time of an input row and a photo file that will be matched. If an input row and a photo file have time stamps that are different by more than this tolerance, no match will occur. To match a photo file to a row with the closest time stamp, regardless of how large the date/time difference might be, set the tolerance to 0. The sign of this value (- or +) is irrelevant; the absolute value of the number specified will be used.Do not use this parameter to adjust for consistent shifts or offsets between the times recorded by the GPS and the digital camera. Use the Clock Offset parameter or the Convert Time Zone tool to shift the time stamps of the input rows to match those of the photos. | Double |
| Clock Offset (Optional) | The difference (in seconds) between the internal clock of the digital camera used to capture the photos and the GPS unit. If the clock of the digital camera is behind the clock of the GPS unit, use a positive value; if the clock of the digital camera is ahead of the clock of the GPS unit, use a negative value.For example, if a photo with a time stamp of 11:35:17 should match a row with a time stamp of 11:35:32, use a Clock Offset value of 15. | Double |
| Input_Folder | The folder where photo files (.jpg or .tif) are located. This folder is scanned recursively for photo files; any photos in the base level of the folder, as well as in any subfolders, will be added to the output. | Folder |
| Input_Table | The table or feature class whose rows will be matched with photo files. The input table will typically be a point feature class representing GPS recordings. | Table View |
| Time_Field | The date field from the input table that indicates when each row was captured or created. The field must be of type date; it cannot be of type string or numeric. | Field |
| Output_Table | The output table containing the object ID values from the input table that match a photo, and the matching photo path. Only object ID values from the input table that match a photo are included in the output table. | Table |
| Unmatched_Photos_Table(Optional) | The optional output table that will list any photo files in the input folder with an invalid time stamp or any photos that cannot be matched because there is no input row within the time tolerance.If no path is specified, this table will not be created. | Table |
| Add_Photos_As_Attachments(Optional) | Specifies whether the photo files will be added to the rows of the input table as geodatabase attachments. Note:Adding attachments requires that the output feature class be in a version 10 or later geodatabase. ADD_ATTACHMENTS— Photo files will be added to the rows of the input table as geodatabase attachments. Geodatabase attachments are copied internally to the geodatabase. This is the default.NO_ATTACHMENTS— Photo files will not be added to the rows of the input table as geodatabase attachments. | Boolean |
| Time_Tolerance(Optional) | The maximum difference (in seconds) between the date/time of an input row and a photo file that will be matched. If an input row and a photo file have time stamps that are different by more than this tolerance, no match will occur. To match a photo file to a row with the closest time stamp, regardless of how large the date/time difference might be, set the tolerance to 0. The sign of this value (- or +) is irrelevant; the absolute value of the number specified will be used.Do not use this parameter to adjust for consistent shifts or offsets between the times recorded by the GPS and the digital camera. Use the Clock Offset parameter or the Convert Time Zone tool to shift the time stamps of the input rows to match those of the photos. | Double |
| Clock_Offset(Optional) | The difference (in seconds) between the internal clock of the digital camera used to capture the photos and the GPS unit. If the clock of the digital camera is behind the clock of the GPS unit, use a positive value; if the clock of the digital camera is ahead of the clock of the GPS unit, use a negative value.For example, if a photo with a time stamp of 11:35:17 should match a row with a time stamp of 11:35:32, use a Clock Offset value of 15. | Double |

## Code Samples

### Example 1

```python
arcpy.management.MatchPhotosToRowsByTime(Input_Folder, Input_Table, Time_Field, Output_Table, {Unmatched_Photos_Table}, {Add_Photos_As_Attachments}, {Time_Tolerance}, {Clock_Offset})
```

### Example 2

```python
import arcpy
arcpy.management.MatchPhotosToRowsByTime(
    "c:/data/photos", "c:/data/city.gdb/gps_points", "DateTime", 
    "c:/data/city.gdb/output_table", "", "ADD_ATTACHMENTS", "", 20)
```

### Example 3

```python
import arcpy
arcpy.management.MatchPhotosToRowsByTime(
    "c:/data/photos", "c:/data/city.gdb/gps_points", "DateTime", 
    "c:/data/city.gdb/output_table", "", "ADD_ATTACHMENTS", "", 20)
```

### Example 4

```python
"""
Name: GeoTaggedPhotosToPoints example
Description: Find the points that match photo time stamps, then join the output table 
             to the input to see which photos match which points
""" 

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFolder = "photos"
inFC = "city.gdb/gps_points"
timeField = "DateTime"
outTable = "city.gdb/output_table"
outUnmatched = "city.gdb/unmatched_photos"
attachmentsOption = "ADD_ATTACHMENTS"
timeDiff = 0
timeOffset = 20

# Run MatchPhotosToRowsByTime and JoinField
arcpy.management.MatchPhotosToRowsByTime(inFolder, inFC, timeField, outTable, 
                                         outUnmatched, attachmentsOption, 
                                         timeDiff, timeOffset)
arcpy.management.JoinField(inFC, "OBJECTID", outTable, "IN_FID", 
                           ["Photo_Path", "Photo_Name", "Match_Diff"])
```

### Example 5

```python
"""
Name: GeoTaggedPhotosToPoints example
Description: Find the points that match photo time stamps, then join the output table 
             to the input to see which photos match which points
""" 

# Import system modules
import arcpy
 
# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFolder = "photos"
inFC = "city.gdb/gps_points"
timeField = "DateTime"
outTable = "city.gdb/output_table"
outUnmatched = "city.gdb/unmatched_photos"
attachmentsOption = "ADD_ATTACHMENTS"
timeDiff = 0
timeOffset = 20

# Run MatchPhotosToRowsByTime and JoinField
arcpy.management.MatchPhotosToRowsByTime(inFolder, inFC, timeField, outTable, 
                                         outUnmatched, attachmentsOption, 
                                         timeDiff, timeOffset)
arcpy.management.JoinField(inFC, "OBJECTID", outTable, "IN_FID", 
                           ["Photo_Path", "Photo_Name", "Match_Diff"])
```

---

## Merge Mosaic Dataset Items (Data Management)

## Summary

Groups multiple items in a mosaic dataset together as one item.

## Usage

- Selections and queries should be used to choose the mosaic dataset items to merge. If a block field is also specified, the tool will take the results of the selection and merge all the similar block fields into separate rows.
- The default number of maximum allowed items per merge is 1,000. If the maximum is exceeded, the tool will insert additional merged items for the selection or query. For example, if 2,000 items are selected, the tool will create two merged items.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that has the items that you want to merge. | Mosaic Layer |
| Query Definition (Optional) | An SQL expression to select specific rasters to merge in the mosaic dataset. | SQL Expression |
| Block Field(Optional) | The field in the attribute table that you want to use to group images. Only date, numeric, and string fields can be specified as Block fields. | Field |
| Maximum Allowed Rows Per Merged Item(Optional) | Limits the number of items to merge. If the maximum is exceeded, the tool will create multiple merged items. The default is 1,000 rows. | Long |
| in_mosaic_dataset | The mosaic dataset that has the items that you want to merge. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select specific rasters to merge in the mosaic dataset. | SQL Expression |
| block_field(Optional) | The field in the attribute table that you want to use to group images. Only date, numeric, and string fields can be specified as Block fields. | Field |
| max_rows_per_merged_items(Optional) | Limits the number of items to merge. If the maximum is exceeded, the tool will create multiple merged items. The default is 1,000 rows. | Long |

## Code Samples

### Example 1

```python
arcpy.management.MergeMosaicDatasetItems(in_mosaic_dataset, {where_clause}, {block_field}, {max_rows_per_merged_items})
```

### Example 2

```python
import arcpy
arcpy.MergeMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md",
                                         "", "Year", "2000")
```

### Example 3

```python
import arcpy
arcpy.MergeMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md",
                                         "", "Year", "2000")
```

### Example 4

```python
#Merge items with items that are newer than year 1999

import arcpy
    
arcpy.MergeMosaicDatasetItems_management(
    "c:/data/merge_md_items.gdb/md", "Year>1999", "", "1000")
```

### Example 5

```python
#Merge items with items that are newer than year 1999

import arcpy
    
arcpy.MergeMosaicDatasetItems_management(
    "c:/data/merge_md_items.gdb/md", "Year>1999", "", "1000")
```

---

## Merge (Data Management)

## Summary

Combines multiple input datasets into a single, new output dataset. This tool can combine point, line, or polygon feature classes or tables.

## Usage

- Use this tool to combine datasets from multiple sources into a new, single output dataset. All input feature classes must be of the same geometry type. For example, several point feature classes can be merged, but a line feature class cannot be merged with a polygon feature class.Tables and feature classes can be combined in a single output dataset. The output type is determined by the first input. If the first input is a feature class, the output will be a feature class. If the first input is a table, the output will be a table. If a table is merged into a feature class, the rows from the input table will have null geometry.
- Use the Field Matching Mode parameter to specify how fields from the input datasets will be transferred to the output dataset. By default, the tool will map fields of the same name together, while fields that are unique to the inputs will be retained in the output. Use the Field Map parameter when the Use the field map to reconcile field differences option is specified. If all input datasets have the same schema, use the Use the schema of the first dataset only option for better performance.
- Use the Field Map parameter to manage the fields and their content in the output dataset. Add and remove fields from the fields list, reorder the fields list, and rename fields. The default data type of an output field is the same as the data type of the first input field (of that name) it encounters. You can change the data type to another valid data type.Use an action to determine how the values from one or multiple input fields will be merged into a single output field. The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters. Standard Deviation is not a valid option for single input values.Use the Export option to save a field map as a .fieldmap file.Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the output field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.Fields can also be mapped in Python script.
- Add and remove fields from the fields list, reorder the fields list, and rename fields.
- The default data type of an output field is the same as the data type of the first input field (of that name) it encounters. You can change the data type to another valid data type.
- Use an action to determine how the values from one or multiple input fields will be merged into a single output field. The available actions are First, Last, Concatenate, Sum, Mean, Median, Mode, Minimum, Maximum, Standard Deviation, and Count.
- When using the Concatenate action, you can specify a delimiter such as a comma or other characters. Click the start of the Delimiter text box to add the delimiter characters.
- Standard Deviation is not a valid option for single input values.
- Use the Export option to save a field map as a .fieldmap file.
- Use the Load option to load a .fieldmap file. The feature layer or dataset specified in the file must match the dataset used in the tool. Otherwise, the Field Map parameter will be reset.
- Use the Slice Text button on text source fields to choose which characters from an input value will be extracted to the output field. To access the Slice Text button, hover over a text field in the input fields list; then specify the start and end character positions.
- Fields can also be mapped in Python script.
- This tool will not split or alter the geometries from the input datasets. All features from the input datasets will remain intact in the output dataset, even if the features overlap. To combine, or planarize, feature geometries, use the Union tool.
- If feature classes are being merged, the output dataset will be in the coordinate system of the first feature class specified for the Input Datasets parameter, unless the Output Coordinate System environment is set.
- This tool does not support annotation feature classes. Use the Append Annotation Feature Classes tool to combine annotation feature classes.
- This tool does not support raster datasets. Use the Mosaic To New Raster tool to combine multiple rasters into a new output raster.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | The input datasets that will be merged into a new output dataset. Input datasets can be point, line, or polygon feature classes or tables. Input feature classes must all be of the same geometry type.Tables and feature classes can be combined in a single output dataset. The output type is determined by the first input. If the first input is a feature class, the output will be a feature class. If the first input is a table, the output will be a table. If a table is merged into a feature class, the rows from the input table will have null geometry. | Table View |
| Output Dataset | The output dataset that will contain all combined input datasets. | Feature Class;Table |
| Field Map(Optional) | Use the field map to reconcile schema differences and match attribute fields between multiple datasets. The output includes all fields from the input datasets by default.Use the field map to add, delete, rename, and reorder fields, as well as change other field properties.The field map can also be used to combine values from two or more input fields into a single output field. | Field Mappings |
| Add source information to output (Optional) | Specifies whether source information will be added to the output dataset in a new MERGE_SRC text field. The values in the MERGE_SRC field will indicate the input dataset path or layer name that is the source of each record in the output.Unchecked—Source information will not be added to the output dataset in a MERGE_SRC field. This is the default.Checked—Source information will be added to the output dataset in a MERGE_SRC field. | Boolean |
| Field Matching Mode (Optional) | Specifies how fields from the input dataset will be transferred to the output dataset.Automatically generate fields consolidated from all inputs—Fields of the same name will be automatically mapped together in the output. Fields that are unique to the inputs will be retained in the output. This is the default.Use the field map to reconcile field differences—The output fields will determined by the Field Map parameter value.Use the schema of the first dataset only—The schema from the first input dataset will be used. This mode is suitable when all input datasets have the same schema. | String |
| inputs[inputs,...] | The input datasets that will be merged into a new output dataset. Input datasets can be point, line, or polygon feature classes or tables. Input feature classes must all be of the same geometry type.Tables and feature classes can be combined in a single output dataset. The output type is determined by the first input. If the first input is a feature class, the output will be a feature class. If the first input is a table, the output will be a table. If a table is merged into a feature class, the rows from the input table will have null geometry. | Table View |
| output | The output dataset that will contain all combined input datasets. | Feature Class;Table |
| field_mappings(Optional) | Use the field map to reconcile schema differences and match attribute fields between multiple datasets. The output includes all fields from the input datasets by default.Use the field map to add, delete, rename, and reorder fields, as well as change other field properties.The field map can also be used to combine values from two or more input fields into a single output field.In Python, use the FieldMappings class to define this parameter. | Field Mappings |
| add_source(Optional) | Specifies whether source information will be added to the output dataset in a new MERGE_SRC text field. The values in the MERGE_SRC field will indicate the input dataset path or layer name that is the source of each record in the output.NO_SOURCE_INFO— Source information will not be added to the output dataset in a MERGE_SRC field. This is the default.ADD_SOURCE_INFO—Source information will be added to the output dataset in a MERGE_SRC field. | Boolean |
| field_match_mode(Optional) | Specifies how fields from the input dataset will be transferred to the output datasetAUTOMATIC—Fields of the same name will be automatically mapped together in the output. Fields that are unique to the inputs will be retained in the output. This is the default.MANUAL_EDIT—The output fields will determined by the field_mappings parameter value.USE_FIRST_SCHEMA—The schema from the first input dataset will be used. This mode is suitable when all input datasets have the same schema. | String |

## Code Samples

### Example 1

```python
arcpy.management.Merge(inputs, output, {field_mappings}, {add_source}, {field_match_mode})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Merge(["majorrds.shp", "Habitat_Analysis.gdb/futrds"], 
                       "C:/output/Output.gdb/allroads", "", "ADD_SOURCE_INFO")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Merge(["majorrds.shp", "Habitat_Analysis.gdb/futrds"], 
                       "C:/output/Output.gdb/allroads", "", "ADD_SOURCE_INFO")
```

### Example 4

```python
# Name: Merge.py
# Description: Use Merge to move features from two street
#              feature classes into a single dataset with field mapping

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Street feature classes to be merged
oldStreets = "majorrds.shp"
newStreets = "Habitat_Analysis.gdb/futrds"
addSourceInfo = "ADD_SOURCE_INFO"

# Create FieldMappings object to manage merge output fields
fieldMappings = arcpy.FieldMappings()

# Add all fields from both oldStreets and newStreets
fieldMappings.addTable(oldStreets)
fieldMappings.addTable(newStreets)

# Add input fields "STREET_NAM" & "NM" into new output field
fldMap_streetName = arcpy.FieldMap()
fldMap_streetName.addInputField(oldStreets, "STREET_NAM")
fldMap_streetName.addInputField(newStreets, "NM")

# Set name of new output field "Street_Name"
streetName = fldMap_streetName.outputField
streetName.name = "Street_Name"
fldMap_streetName.outputField = streetName

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap_streetName)

# Add input fields "CLASS" & "IFC" into new output field
fldMap_streetClass = arcpy.FieldMap()
fldMap_streetClass.addInputField(oldStreets, "CLASS")
fldMap_streetClass.addInputField(newStreets, "IFC")

# Set name of new output field "Street_Class"
streetClass = fldMap_streetClass.outputField
streetClass.name = "Street_Class"
fldMap_streetClass.outputField = streetClass  

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap_streetClass)  

# Remove all output fields from the field mappings, except fields 
# "Street_Class", "Street_Name", & "Distance"
for field in fieldMappings.fields:
    if field.name not in ["Street_Class", "Street_Name", "Distance"]:
        fieldMappings.removeFieldMap(fieldMappings.findFieldMapIndex(field.name))

# Since both oldStreets and newStreets have field "Distance", no field mapping 
# is required

# Use Merge tool to move features into single dataset
uptodateStreets = "C:/output/Output.gdb/allroads"
arcpy.management.Merge([oldStreets, newStreets], uptodateStreets, fieldMappings, 
                       addSourceInfo)
```

### Example 5

```python
# Name: Merge.py
# Description: Use Merge to move features from two street
#              feature classes into a single dataset with field mapping

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Street feature classes to be merged
oldStreets = "majorrds.shp"
newStreets = "Habitat_Analysis.gdb/futrds"
addSourceInfo = "ADD_SOURCE_INFO"

# Create FieldMappings object to manage merge output fields
fieldMappings = arcpy.FieldMappings()

# Add all fields from both oldStreets and newStreets
fieldMappings.addTable(oldStreets)
fieldMappings.addTable(newStreets)

# Add input fields "STREET_NAM" & "NM" into new output field
fldMap_streetName = arcpy.FieldMap()
fldMap_streetName.addInputField(oldStreets, "STREET_NAM")
fldMap_streetName.addInputField(newStreets, "NM")

# Set name of new output field "Street_Name"
streetName = fldMap_streetName.outputField
streetName.name = "Street_Name"
fldMap_streetName.outputField = streetName

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap_streetName)

# Add input fields "CLASS" & "IFC" into new output field
fldMap_streetClass = arcpy.FieldMap()
fldMap_streetClass.addInputField(oldStreets, "CLASS")
fldMap_streetClass.addInputField(newStreets, "IFC")

# Set name of new output field "Street_Class"
streetClass = fldMap_streetClass.outputField
streetClass.name = "Street_Class"
fldMap_streetClass.outputField = streetClass  

# Add output field to field mappings object
fieldMappings.addFieldMap(fldMap_streetClass)  

# Remove all output fields from the field mappings, except fields 
# "Street_Class", "Street_Name", & "Distance"
for field in fieldMappings.fields:
    if field.name not in ["Street_Class", "Street_Name", "Distance"]:
        fieldMappings.removeFieldMap(fieldMappings.findFieldMapIndex(field.name))

# Since both oldStreets and newStreets have field "Distance", no field mapping 
# is required

# Use Merge tool to move features into single dataset
uptodateStreets = "C:/output/Output.gdb/allroads"
arcpy.management.Merge([oldStreets, newStreets], uptodateStreets, fieldMappings, 
                       addSourceInfo)
```

---

## Migrate Relationship Class (Data Management)

## Summary

Migrates an object ID-based relationship class to a global ID-based relationship class.

## Usage

- This tool will modify an existing relationship class that is object ID-based to a global ID-based relationship class to comply with runtime geodatabase requirements.
- The origin feature class or table that is used in the Input Relationship Class parameter value must have a GlobalID field prior to using this tool. The destination class must also have a GlobalID field.
- The existing input relationship class must be based on the ObjectID field. The data cannot be archive enabled.
- This tool supports versioned and nonversioned data as input.
- Feature-linked annotation created in ArcGIS Desktop must first be upgraded using the Upgrade Dataset tool.
- It is recommended that you make a backup copy of the data used as input for this tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Relationship Class | An object ID-based relationship class that will be migrated to a global ID-based relationship class. The origin and destination feature classes or tables must have an existing GlobalID field. | Relationship Class |
| in_relationship_class | An object ID-based relationship class that will be migrated to a global ID-based relationship class. The origin and destination feature classes or tables must have an existing GlobalID field. | Relationship Class |

## Code Samples

### Example 1

```python
arcpy.management.MigrateRelationshipClass(in_relationship_class)
```

### Example 2

```python
arcpy.MigrateRelationshipClass_management(r'C:\Data\Relationships.gdb\OneToMany')
```

### Example 3

```python
arcpy.MigrateRelationshipClass_management(r'C:\Data\Relationships.gdb\OneToMany')
```

### Example 4

```python
# Name: MigrateRelationshipClass_Example.py
# Description: Migrate an ObjectID-based relationship class to a GlobalID-based
#       relationship class. This script lists the ObjectID-based relationships classes
#       in a workspace, checks for GlobalIDs in the origin, then runs the tool

# Import system modules
import arcpy

# Set local variables
workspace = r'C:\Data\Relationships.gdb'

# List all of the relationship classes within the given workspace
rc_list = [c.name for c in arcpy.Describe(workspace).children if c.datatype == "RelationshipClass"]

# Build a list of relationship classes which have an OBJECTID based origin class key
rc_migrate = []
for rc in rc_list:
    rc_path = workspace + "\\" + rc
    rc_desc = arcpy.Describe(rc_path)
    for item in rc_desc.OriginClassKeys:
        if "OBJECTID" in item:
            rc_migrate.append(rc_path)

# Check that the origin feature classes have Global Ids
rc_final = []
for rel in rc_migrate:
    originfc = workspace + "\\" + arcpy.Describe(rel).originClassNames[0]
    if arcpy.ListFields(originfc,"","GlobalID"):
        rc_final.append(rel)
        print("Adding {0}  to the list to migrate. \n".format(rel.rsplit("\\",1)[1]))
    else:
        print("{0} must have Global Ids to migrate relationship class.\n".format(originfc.rsplit("\\",1)[1]))

# Pass the list of valid relationship classes into the Migrate Relationship tool
print("Passing valid relationship classes into the Migrate Relationship Class tool.\n")
for rel_class in rc_final:
    print("Migrating {0}... \n".format(rel_class.rsplit("\\",1)[1]))
    arcpy.MigrateRelationshipClass_management(rel_class)
    print(arcpy.GetMessages() + "\n")
```

### Example 5

```python
# Name: MigrateRelationshipClass_Example.py
# Description: Migrate an ObjectID-based relationship class to a GlobalID-based
#       relationship class. This script lists the ObjectID-based relationships classes
#       in a workspace, checks for GlobalIDs in the origin, then runs the tool

# Import system modules
import arcpy

# Set local variables
workspace = r'C:\Data\Relationships.gdb'

# List all of the relationship classes within the given workspace
rc_list = [c.name for c in arcpy.Describe(workspace).children if c.datatype == "RelationshipClass"]

# Build a list of relationship classes which have an OBJECTID based origin class key
rc_migrate = []
for rc in rc_list:
    rc_path = workspace + "\\" + rc
    rc_desc = arcpy.Describe(rc_path)
    for item in rc_desc.OriginClassKeys:
        if "OBJECTID" in item:
            rc_migrate.append(rc_path)

# Check that the origin feature classes have Global Ids
rc_final = []
for rel in rc_migrate:
    originfc = workspace + "\\" + arcpy.Describe(rel).originClassNames[0]
    if arcpy.ListFields(originfc,"","GlobalID"):
        rc_final.append(rel)
        print("Adding {0}  to the list to migrate. \n".format(rel.rsplit("\\",1)[1]))
    else:
        print("{0} must have Global Ids to migrate relationship class.\n".format(originfc.rsplit("\\",1)[1]))

# Pass the list of valid relationship classes into the Migrate Relationship tool
print("Passing valid relationship classes into the Migrate Relationship Class tool.\n")
for rel_class in rc_final:
    print("Migrating {0}... \n".format(rel_class.rsplit("\\",1)[1]))
    arcpy.MigrateRelationshipClass_management(rel_class)
    print(arcpy.GetMessages() + "\n")
```

---

## Migrate Storage (Data Management)

## Summary

Migrates the data from a binary, spatial, or spatial attribute column of one data type to a new column of a different data type in geodatabases in Oracle and SQL Server. The configuration keyword you specify when migrating determines the data type used for the new column.

## Usage

- You can use this tool to migrate existing binary, spatial, or spatial attribute columns from one storage type to another. See more about data migration from one storage type to another.
- Add datasets you want to migrate to the Input Datasets parameter and specify the Configuration Keyword value that will get you the data type you require. If you are not sure which configuration keyword to use, contact your geodatabase administrator for that information.
- You must own the input datasets.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | The datasets to be migrated. The connection you use to access the datasets must be connecting as the dataset owner. | Table View; Raster Layer; Feature Dataset |
| Configuration Keyword | The configuration keyword containing the appropriate parameter values for the migration. Parameter values are set by the geodatabase administrator. Contact your geodatabase administrator if you are unsure which configuration keyword to use. | String |
| in_datasets[in_datasets,...] | The datasets to be migrated. The connection you use to access the datasets must be connecting as the dataset owner. | Table View; Raster Layer; Feature Dataset |
| config_keyword | The configuration keyword containing the appropriate parameter values for the migration. Parameter values are set by the geodatabase administrator. Contact your geodatabase administrator if you are unsure which configuration keyword to use. | String |

## Code Samples

### Example 1

```python
arcpy.management.MigrateStorage(in_datasets, config_keyword)
```

### Example 2

```python
# Name: MigrateStorage_Example.py
# Description: Migrates the input dataset to the ST_Geometry geometry storage type. 

# Import arcpy module
import arcpy

# Local variables:
inputDataset = "f:\\Connections\\Oracle on khyber.sde\\MAP.SBMigrate"

# Process: Migrate Storage
arcpy.management.MigrateStorage(inputDataset, "ST_GEOMETRY")
```

### Example 3

```python
# Name: MigrateStorage_Example.py
# Description: Migrates the input dataset to the ST_Geometry geometry storage type. 

# Import arcpy module
import arcpy

# Local variables:
inputDataset = "f:\\Connections\\Oracle on khyber.sde\\MAP.SBMigrate"

# Process: Migrate Storage
arcpy.management.MigrateStorage(inputDataset, "ST_GEOMETRY")
```

---

## Migrate Date Field To High Precision (Data Management)

## Summary

Migrates date fields in a table to high precision.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The geodatabase table or feature class containing the date fields that will be migrated to high precision. | Table View |
| Date Fields | The date fields that will be migrated to high precision. | Field |
| in_table | The geodatabase table or feature class containing the date fields that will be migrated to high precision. | Table View |
| date_fields[date_fields,...] | The date fields that will be migrated to high precision. | Field |

## Code Samples

### Example 1

```python
arcpy.management.MigrateDateFieldToHighPrecision(in_table, date_fields)
```

### Example 2

```python
import arcpy
arcpy.management.MigrateDateFieldToHighPrecision("C:/MyProject/MyGDB.gdb/Table1", "date_field1")
```

### Example 3

```python
import arcpy
arcpy.management.MigrateDateFieldToHighPrecision("C:/MyProject/MyGDB.gdb/Table1", "date_field1")
```

### Example 4

```python
import arcpy
arcpy.management.MigrateDateFieldToHighPrecision("C:/MyProject/MyGDB.gdb/Table1", ["date_field1", "date_field2"])
```

### Example 5

```python
import arcpy
arcpy.management.MigrateDateFieldToHighPrecision("C:/MyProject/MyGDB.gdb/Table1", ["date_field1", "date_field2"])
```

---

## Migrate Object ID To 64 Bit (Data Management)

## Summary

Migrates a dataset's or multiple datasets' ObjectID field to 64-bit object IDs.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | The datasets that will have their ObjectID field migrated to 64 bit. | Table View; Feature Dataset; Layer |
| in_datasets[in_datasets,...] | The datasets that will have their ObjectID field migrated to 64 bit. | Table View; Feature Dataset; Layer |

## Code Samples

### Example 1

```python
arcpy.management.MigrateObjectIDTo64Bit(in_datasets)
```

### Example 2

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit("C:\\Projects\\MyProject\\myGDBConnection.sde\\user1.COUNTIES")
```

### Example 3

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit("C:\\Projects\\MyProject\\myGDBConnection.sde\\user1.COUNTIES")
```

### Example 4

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit(['Cities', 'Roads', 'Rivers', 'Lakes', 'Counties', 'States'])
```

### Example 5

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit(['Cities', 'Roads', 'Rivers', 'Lakes', 'Counties', 'States'])
```

### Example 6

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit("C:\\Projects\\MyProject\\myGDBConnection.sde\\user1.MyTopologyDataset")
```

### Example 7

```python
import arcpy
arcpy.management.MigrateObjectIDTo64Bit("C:\\Projects\\MyProject\\myGDBConnection.sde\\user1.MyTopologyDataset")
```

---

## Minimum Bounding Geometry (Data Management)

## Summary

Creates a feature class containing polygons which represent a specified minimum bounding geometry enclosing each input feature or each group of input features.

## Usage

- The output polygon features and their attributes will vary depending on the specified geometry type and grouping choices.
- The Group Option parameter (group_option parameter in Python) will affect the output polygons and attributes in the following ways: Using None, none of the input features will be grouped. An output polygon feature will be created for each input feature; the resulting polygons may overlap. The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.Using All, one output polygon feature will be created for all input features. The attributes of the input features will not be maintained in the output feature class.Using List, each set of input features with the same field values in the specified group field(s) will be treated as a group. An output polygon feature will be created for each group; the resulting polygons may overlap. The attributes of the input features used as the group field or fields will be maintained in the output feature class.
- Using None, none of the input features will be grouped. An output polygon feature will be created for each input feature; the resulting polygons may overlap. The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.
- Using All, one output polygon feature will be created for all input features. The attributes of the input features will not be maintained in the output feature class.
- Using List, each set of input features with the same field values in the specified group field(s) will be treated as a group. An output polygon feature will be created for each group; the resulting polygons may overlap. The attributes of the input features used as the group field or fields will be maintained in the output feature class.
- Each geometry type can be characterized by one or more unique measurements; these measurements can optionally be added to the output as new fields as described below. The width, length, and diameter values are in feature units; the orientation angles are in decimal degrees clockwise from north. The prefix, MBG_, indicates minimum bounding geometry field. For Rectangle by area and Rectangle by width, the new fields and measurements are:MBG_Width—The length of the shorter side of the resulting rectangle.MBG_Length—The length of the longer side of the resulting rectangle.MBG_Orientation—The orientation of the longer side of the resulting rectangle. For Envelope, the new fields and measurements are:MBG_Width—The length of the shorter side of the resulting rectangle. MBG_Length—The length of the longer side of the resulting rectangle. For Convex hull, the new fields and measurements are:MBG_Width—The shortest distance between any two vertices of the convex hull. (It may be found between more than one pair of vertices, but the first found will be used.)MBG_Length—The longest distance between any two vertices of the convex hull; these vertices are called antipodal pairs or antipodal points. (It may be found between more than one pair of vertices, but the first found will be used.)MBG_APodX1—The x coordinate of the first point of the antipodal pairs. MBG_APodY1—The y coordinate of the first point of the antipodal pairs. MBG_APodX2—The x coordinate of the second point of the antipodal pairs. MBG_APodY2—The y coordinate of the second point of the antipodal pairs. MBG_Orientation—The orientation of the imagined line connecting the antipodal pairs. For Circle, the new field and measurement are:MBG_Diameter—The diameter of the resulting circle.
- For Rectangle by area and Rectangle by width, the new fields and measurements are:MBG_Width—The length of the shorter side of the resulting rectangle.MBG_Length—The length of the longer side of the resulting rectangle.MBG_Orientation—The orientation of the longer side of the resulting rectangle.
- MBG_Width—The length of the shorter side of the resulting rectangle.
- MBG_Length—The length of the longer side of the resulting rectangle.
- MBG_Orientation—The orientation of the longer side of the resulting rectangle.
- For Envelope, the new fields and measurements are:MBG_Width—The length of the shorter side of the resulting rectangle. MBG_Length—The length of the longer side of the resulting rectangle.
- MBG_Width—The length of the shorter side of the resulting rectangle.
- MBG_Length—The length of the longer side of the resulting rectangle.
- For Convex hull, the new fields and measurements are:MBG_Width—The shortest distance between any two vertices of the convex hull. (It may be found between more than one pair of vertices, but the first found will be used.)MBG_Length—The longest distance between any two vertices of the convex hull; these vertices are called antipodal pairs or antipodal points. (It may be found between more than one pair of vertices, but the first found will be used.)MBG_APodX1—The x coordinate of the first point of the antipodal pairs. MBG_APodY1—The y coordinate of the first point of the antipodal pairs. MBG_APodX2—The x coordinate of the second point of the antipodal pairs. MBG_APodY2—The y coordinate of the second point of the antipodal pairs. MBG_Orientation—The orientation of the imagined line connecting the antipodal pairs.
- MBG_Width—The shortest distance between any two vertices of the convex hull. (It may be found between more than one pair of vertices, but the first found will be used.)
- MBG_Length—The longest distance between any two vertices of the convex hull; these vertices are called antipodal pairs or antipodal points. (It may be found between more than one pair of vertices, but the first found will be used.)
- MBG_APodX1—The x coordinate of the first point of the antipodal pairs.
- MBG_APodY1—The y coordinate of the first point of the antipodal pairs.
- MBG_APodX2—The x coordinate of the second point of the antipodal pairs.
- MBG_APodY2—The y coordinate of the second point of the antipodal pairs.
- MBG_Orientation—The orientation of the imagined line connecting the antipodal pairs.
- For Circle, the new field and measurement are:MBG_Diameter—The diameter of the resulting circle.
- MBG_Diameter—The diameter of the resulting circle.
- There are special cases of input features that would result in invalid (zero-area) output polygons. In these cases, a small value derived from the input feature XY Tolerance will be used as the width, length, or diameter to create output polygons. These polygons serve as 'place holders' for keeping track of features. If the resulting polygons appear 'invisible' in ArcMap using the default polygon outline width, change to a thicker outline line symbol to display them. The examples of these cases include: If a multipoint feature contains only one point or a group of such features are coincident, a very small square polygon will be created around the point for geometry types Rectangle by area, Rectangle by width, Convex hull, and Envelope; and a very small circle for geometry type Circle. The MBG_Width, MBG_Length, MBG_Orientation, and MBG_Diameter values will be set to zero to indicate these cases. If an input feature or a group of input features are perfectly aligned, for example, a horizontal or vertical line or a two-point multipoint feature, a rectangle polygon with a very small width will be created around the feature. This applies to geometry types Rectangle by area, Rectangle by width, Convex hull, and Envelope; the resulting MBG_Width value will be set to zero to indicate these cases.
- If a multipoint feature contains only one point or a group of such features are coincident, a very small square polygon will be created around the point for geometry types Rectangle by area, Rectangle by width, Convex hull, and Envelope; and a very small circle for geometry type Circle. The MBG_Width, MBG_Length, MBG_Orientation, and MBG_Diameter values will be set to zero to indicate these cases.
- If an input feature or a group of input features are perfectly aligned, for example, a horizontal or vertical line or a two-point multipoint feature, a rectangle polygon with a very small width will be created around the feature. This applies to geometry types Rectangle by area, Rectangle by width, Convex hull, and Envelope; the resulting MBG_Width value will be set to zero to indicate these cases.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that can be point, multipoint, line, polygon, or multipatch. | Feature Layer |
| Output Feature Class | The output polygon feature class. | Feature Class |
| Geometry Type(Optional) | Specifies what type of minimum bounding geometry the output polygons will represent.Rectangle by area—The rectangle of the smallest area enclosing an input feature. This is the default.Rectangle by width—The rectangle of the smallest width enclosing an input feature. Convex hull—The smallest convex polygon enclosing an input feature. Circle—The smallest circle enclosing an input feature envelope. Envelope—The envelope of an input feature. | String |
| Group Option(Optional) | Specifies how the input features will be grouped; each group will be enclosed with one output polygon.None—Input features will not be grouped. This is the default. This option is not available for point input. All—All input features will be treated as one group. List—Input features will be grouped based on their common values in the specified field or fields in the group field parameter. | String |
| Group Field(s)(Optional) | The field or fields in the input features that will be used to group features, when List is specified as Group Option. At least one group field is required for List option. All features that have the same value in the specified field or fields will be treated as a group. | Field |
| Add geometry characteristics as attributes to output(Optional) | Specifies whether to add the geometric attributes in the output feature class or omit them in the output feature class.Unchecked—Omits the geometric attributes in the output feature class. This is the default.Checked—Adds the geometric attributes in the output feature class. | Boolean |
| in_features | The input features that can be point, multipoint, line, polygon, or multipatch. | Feature Layer |
| out_feature_class | The output polygon feature class. | Feature Class |
| geometry_type(Optional) | Specifies what type of minimum bounding geometry the output polygons will represent.RECTANGLE_BY_AREA—The rectangle of the smallest area enclosing an input feature. This is the default.RECTANGLE_BY_WIDTH—The rectangle of the smallest width enclosing an input feature. CONVEX_HULL—The smallest convex polygon enclosing an input feature. CIRCLE—The smallest circle enclosing an input feature envelope. ENVELOPE—The envelope of an input feature. | String |
| group_option(Optional) | Specifies how the input features will be grouped; each group will be enclosed with one output polygon.NONE—Input features will not be grouped. This is the default. This option is not available for point input. ALL—All input features will be treated as one group. LIST—Input features will be grouped based on their common values in the specified field or fields in the group field parameter. | String |
| group_field[group_field,...](Optional) | The field or fields in the input features that will be used to group features, when LIST is specified as group_option. At least one group field is required for LIST option. All features that have the same value in the specified field or fields will be treated as a group. | Field |
| mbg_fields_option(Optional) | Specifies whether to add the geometric attributes in the output feature class or omit them in the output feature class.NO_MBG_FIELDS—Omits any input attributes in the output feature class. This is the default.MBG_FIELDS—Adds the geometric attributes in the output feature class. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.MinimumBoundingGeometry(in_features, out_feature_class, {geometry_type}, {group_option}, {group_field}, {mbg_fields_option})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.MinimumBoundingGeometry_management("parks.shp",
                                         "c:/output/output.gdb/parks_mbg",
                                         "RECTANGLE_BY_AREA", "NONE")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "C:/data"
arcpy.MinimumBoundingGeometry_management("parks.shp",
                                         "c:/output/output.gdb/parks_mbg",
                                         "RECTANGLE_BY_AREA", "NONE")
```

### Example 4

```python
# Name: MinimumBoundingGeometry.py
# Description: Use MinimumBoundingGeometry function to find an area 
#              for each multipoint input feature.

# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Create variables for the input and output feature classes
inFeatures = "treeclusters.shp"
outFeatureClass = "forests.shp"

# Use MinimumBoundingGeometry function to get a convex hull area
#         for each cluster of trees which are multipoint features
arcpy.MinimumBoundingGeometry_management(inFeatures, outFeatureClass, 
                                         "CONVEX_HULL", "NONE")
```

### Example 5

```python
# Name: MinimumBoundingGeometry.py
# Description: Use MinimumBoundingGeometry function to find an area 
#              for each multipoint input feature.

# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Create variables for the input and output feature classes
inFeatures = "treeclusters.shp"
outFeatureClass = "forests.shp"

# Use MinimumBoundingGeometry function to get a convex hull area
#         for each cluster of trees which are multipoint features
arcpy.MinimumBoundingGeometry_management(inFeatures, outFeatureClass, 
                                         "CONVEX_HULL", "NONE")
```

---

## Mirror (Data Management)

## Summary

Reorients the raster by turning it over, from left to right, along the vertical axis through the center of the raster.

## Usage

- This tool flips the raster from left to right along the vertical axis through the center of the region.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| in_raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| out_raster | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |

## Code Samples

### Example 1

```python
arcpy.management.Mirror(in_raster, out_raster)
```

### Example 2

```python
import arcpy
arcpy.Mirror_management("c:/data/image.tif", "c:/data/mirror.tif")
```

### Example 3

```python
import arcpy
arcpy.Mirror_management("c:/data/image.tif", "c:/data/mirror.tif")
```

### Example 4

```python
##====================================
##Mirror
##Usage: Mirror_management in_raster out_raster
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Mirror a TIFF format image
arcpy.Mirror_management("image.tif", "mirror.tif")
```

### Example 5

```python
##====================================
##Mirror
##Usage: Mirror_management in_raster out_raster
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Mirror a TIFF format image
arcpy.Mirror_management("image.tif", "mirror.tif")
```

---

## Mosaic Dataset To Mobile Mosaic Dataset (Data Management)

## Summary

Converts a mosaic dataset into a mobile mosaic dataset that's compatible with ArcGIS Maps SDKs for Native Apps. A mobile mosaic dataset resides in a mobile geodatabase.

## Usage

- When converting to a mobile mosaic dataset, you have the option to apply raster functions that are associated with the mosaic dataset by checking Convert Rasters. Leaving Convert Rasters unchecked will only convert the source data.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that will be converted to a mobile mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| Mobile Geodatabase | The geodatabase where the converted mosaic dataset will be created. | File |
| Mosaic Dataset Name | The name of the mobile mosaic dataset that will be created. | String |
| Query Definition (Optional) | An SQL expression that will be used to select specific items to add to the mobile mosaic dataset. | SQL Expression |
| Selection Feature(Optional) | The mosaic dataset items that will be included in the output based on the extent of another image or feature class. Items that lay along the defined extent will be included in the mosaic dataset. They will not be clipped. Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Output Data Folder(Optional) | The folder where a copy of the source data will be created. If the Convert Rasters parameter is checked, any raster functions associated with the mosaic dataset will be processed before creating the copy. | Folder |
| Convert Rasters(Optional) | Specifies whether the raster functions associated with the input mosaic dataset will be converted before creating the mobile mosaic dataset. If there are raster functions that are not supported by Native Maps SDKs, an error message will be returned.Unchecked—Raster items with functions that are not supported by Native Maps SDKs will not be converted. This is the default.Checked—The raster function chain will be applied and the output will be saved as converted raster items. | Boolean |
| Output Base Name(Optional) | Appends a prefix to each item, which is copied or converted into the output data folder. | String |
| Output Format(Optional) | Specifies the format that will be used for the rasters written to the output data folder.TIFF—The TIFF format will be used.PNG—The PNG format will be used.JPEG—The JPEG format will be used.JPEG2000—The JPEG2000 format will be used. | String |
| Compression Method(Optional) | Specifies the compression method that will be used for transmitting the mosaicked image from the computer to the display (or from the server to the client).None—No compression will be used.JPEG—Compression up to 8:1 will be used, which is suitable for backdrops.LZ77—Compression of approximately 2:1 will be used, which is suitable for analysis.RLE—Lossless compression will be used, which is suitable for categorical datasets. | String |
| Compression Quality(Optional) | The compression quality level, which is a value from 0 to 100. A higher number means better image quality but less compression. This parameter only applies when the Output Format parameter value is specified as JPEG or JP2000. | Long |
| in_mosaic_dataset | The mosaic dataset that will be converted to a mobile mosaic dataset. | Mosaic Dataset; Mosaic Layer |
| out_mobile_gdb | The geodatabase where the converted mosaic dataset will be created. | File |
| mosaic_dataset_name | The name of the mobile mosaic dataset that will be created. | String |
| where_clause(Optional) | An SQL expression that will be used to select specific items to add to the mobile mosaic dataset. | SQL Expression |
| selection_feature(Optional) | The mosaic dataset items that will be included in the output based on the extent of another image or feature class. Items that lay along the defined extent will be included in the mosaic dataset. They will not be clipped. MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| out_data_folder(Optional) | The folder where a copy of the source data will be created. If the convert_rasters parameter is set to ALWAYS, any raster functions associated with the mosaic dataset will be processed before creating the copy. | Folder |
| convert_rasters(Optional) | Specifies whether the raster functions associated with the input mosaic dataset will be converted before creating the mobile mosaic dataset. If you have raster functions that are not supported by Native Maps SDKs, the tool will return the appropriate error message.AS_REQUIRED—Raster items with functions that are not supported by Native Maps SDKs will not be converted. This is the default.ALWAYS—The raster function chain will be applied and the output will be saved as converted raster items. | Boolean |
| out_name_prefix(Optional) | Appends a prefix to each item, which is copied or converted into the output data folder. | String |
| format(Optional) | Specifies the format that will be used for the rasters written to the output data folder.TIFF—The TIFF format will be used.PNG—The PNG format will be used.JPEG—The JPEG format will be used.JP2—The JPEG2000 format will be used. | String |
| compression_method(Optional) | Specifies the compression method that will be used for transmitting the mosaicked image from the computer to the display (or from the server to the client).NONE—No compression will be used.JPEG—Compression up to 8:1 will be used, which is suitable for backdrops.LZW—Compression of approximately 2:1 will be used, which is suitable for analysis.RLE—Lossless compression will be used, which is suitable for categorical datasets. | String |
| compression_quality(Optional) | The compression quality level, which is a value from 0 to 100. A higher number means better image quality but less compression. This parameter only applies when the format parameter is specified as JPEG or JP2. | Long |

## Code Samples

### Example 1

```python
arcpy.management.MosaicDatasetToMobileMosaicDataset(in_mosaic_dataset, out_mobile_gdb, mosaic_dataset_name, {where_clause}, {selection_feature}, {out_data_folder}, {convert_rasters}, {out_name_prefix}, {format}, {compression_method}, {compression_quality})
```

### Example 2

```python
import arcpy

arcpy.MosaicDatasetToMobileMosaicDataset_management( 
	“c:/someproject/md/fgdb.gdb/somemd”,
	“c:/someproject/runtime/somesql.geodatabase”, 
	“somemd”, “ProductName == ‘Landsat8’”, 
	“c:/someproject/aoi/somefc.shp”,
	“c:/someproject/runtime/data”, 
	“ALWAYS”, “TIFF”, “JPEG”, “75”)
```

### Example 3

```python
import arcpy

arcpy.MosaicDatasetToMobileMosaicDataset_management( 
	“c:/someproject/md/fgdb.gdb/somemd”,
	“c:/someproject/runtime/somesql.geodatabase”, 
	“somemd”, “ProductName == ‘Landsat8’”, 
	“c:/someproject/aoi/somefc.shp”,
	“c:/someproject/runtime/data”, 
	“ALWAYS”, “TIFF”, “JPEG”, “75”)
```

---

## Mosaic To New Raster (Data Management)

## Summary

Merges multiple raster datasets into a new raster dataset.

## Usage

- The input raster datasets are all the raster datasets you would like to mosaic together. The inputs must have the same number of bands and same bit depth; otherwise, the tool will exit with an error message.
- The Mosaic tool has more parameters available when combining datasets into an existing raster, such as options to ignore background and NoData values.
- You must set the pixel type to match your existing input raster datasets. If you do not set the pixel type, the 8-bit default will be used and your output may be incorrect.
- You can save your output to BIL, BIP, BMP, BSQ, DAT, Esri Grid , GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, or a geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- The GIF format only supports single-band raster datasets.
- When mosaicking with raster datasets containing color maps, it is important to note differences across the color maps for each raster dataset you mosaic. In this situation, use the Mosaic tool for rasters with different color maps; however, you must choose the proper Mosaic Colormap Mode parameter value. If an improper colormap mode is chosen, your output may not be as expected.
- This tool does not honor the Output extent environment setting for enterprise geodatabases. If you want a specific extent for your output raster, consider using the Clip tool. You can either clip the input rasters prior to using this tool, or clip the output of this tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rasters | The raster datasets that you want to merge together. The inputs must have the same number of bands and same bit depth. | Mosaic Dataset; Raster Dataset; Raster Layer |
| Output Location | The folder or geodatabase to store the raster. | Workspace |
| Raster Dataset Name with Extension | The name of the dataset you are creating. When storing the raster dataset in a file format, specify the file extension as follows: .bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | String |
| Spatial Reference for Raster(Optional) | The coordinate system for the output raster dataset. | Coordinate System |
| Pixel Type(Optional) | The bit depth, or radiometric resolution of the mosaic dataset.If you do not set the pixel type, the 8-bit default will be used and your output may be incorrect.1 bit—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2 bit—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4 bit—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8 bit unsigned—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8 bit signed—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16 bit unsigned—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16 bit signed—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32 bit unsigned—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32 bit signed—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32 bit float—The pixel type will be a 32-bit data type supporting decimals.64 bit—The pixel type will be a 64-bit data type supporting decimals. | String |
| Cellsize(Optional) | The pixel size that will be used for the new raster dataset. | Double |
| Number of Bands | The number of bands that the output raster will have. | Long |
| Mosaic Operator(Optional) | The method used to mosaic overlapping areas.For more information about each mosaic operator, refer to the Mosaic Operator help topic.First—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.Last—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.Blend—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.Mean—The output cell value of the overlapping areas will be the average value of the overlapping cells.Minimum—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.Maximum—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.Sum—The output cell value of the overlapping areas will be the total sum of the overlapping cells. | String |
| Mosaic Colormap Mode(Optional) | Applies when the input raster datasets have a colormap.Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.For more information about each colormap mode, see the Mosaic colormap mode help topic.First—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.Last—The color map from the last raster dataset in the list will be applied to the output raster mosaic.Match—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.Reject—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |
| input_rasters[input_raster,...] | The raster datasets that you want to merge together. The inputs must have the same number of bands and same bit depth. | Mosaic Dataset; Raster Dataset; Raster Layer |
| output_location | The folder or geodatabase to store the raster. | Workspace |
| raster_dataset_name_with_extension | The name of the dataset you are creating. When storing the raster dataset in a file format, specify the file extension as follows: .bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | String |
| coordinate_system_for_the_raster(Optional) | The coordinate system for the output raster dataset. | Coordinate System |
| pixel_type(Optional) | The bit depth, or radiometric resolution of the mosaic dataset.If you do not set the pixel type, the 8-bit default will be used and your output may be incorrect. 1_BIT—The pixel type will be a 1-bit unsigned integer. The values can be 0 or 1.2_BIT—The pixel type will be a 2-bit unsigned integer. The values supported can range from 0 to 3.4_BIT—The pixel type will be a 4-bit unsigned integer. The values supported can range from 0 to 15.8_BIT_UNSIGNED—The pixel type will be an unsigned 8-bit data type. The values supported can range from 0 to 255.8_BIT_SIGNED—The pixel type will be a signed 8-bit data type. The values supported can range from -128 to 127.16_BIT_UNSIGNED—The pixel type will be a 16-bit unsigned data type. The values can range from 0 to 65,535.16_BIT_SIGNED—The pixel type will be a 16-bit signed data type. The values can range from -32,768 to 32,767.32_BIT_UNSIGNED—The pixel type will be a 32-bit unsigned data type. The values can range from 0 to 4,294,967,295.32_BIT_SIGNED—The pixel type will be a 32-bit signed data type. The values can range from -2,147,483,648 to 2,147,483,647.32_BIT_FLOAT—The pixel type will be a 32-bit data type supporting decimals.64_BIT—The pixel type will be a 64-bit data type supporting decimals. | String |
| cellsize(Optional) | The pixel size that will be used for the new raster dataset. | Double |
| number_of_bands | The number of bands that the output raster will have. | Long |
| mosaic_method(Optional) | The method used to mosaic overlapping areas.FIRST—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.LAST—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.BLEND—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.MEAN—The output cell value of the overlapping areas will be the average value of the overlapping cells.MINIMUM—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.MAXIMUM—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.SUM—The output cell value of the overlapping areas will be the total sum of the overlapping cells.For more information about each mosaic operator, refer to the Mosaic Operator help topic. | String |
| mosaic_colormap_mode(Optional) | Applies when the input raster datasets have a colormap.Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.FIRST—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.LAST—The color map from the last raster dataset in the list will be applied to the output raster mosaic.MATCH—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.REJECT—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |

## Code Samples

### Example 1

```python
arcpy.management.MosaicToNewRaster(input_rasters, output_location, raster_dataset_name_with_extension, {coordinate_system_for_the_raster}, {pixel_type}, {cellsize}, number_of_bands, {mosaic_method}, {mosaic_colormap_mode})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.MosaicToNewRaster_management("land1.tif;land2.tif", "Mosaic2New", \
                                   "landnew.tif", "World_Mercator.prj",\
                                   "8_BIT_UNSIGNED", "40", "1", "LAST","FIRST")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.MosaicToNewRaster_management("land1.tif;land2.tif", "Mosaic2New", \
                                   "landnew.tif", "World_Mercator.prj",\
                                   "8_BIT_UNSIGNED", "40", "1", "LAST","FIRST")
```

### Example 4

```python
##==================================
##Mosaic To New Raster
##Usage: MosaicToNewRaster_management inputs;inputs... output_location raster_dataset_name_with_extension 
##                                    {coordinate_system_for_the_raster} 8_BIT_UNSIGNED | 1_BIT | 2_BIT | 4_BIT 
##                                    | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 16_BIT_SIGNED | 32_BIT_FLOAT | 32_BIT_UNSIGNED 
##                                    | 32_BIT_SIGNED | | 64_BIT {cellsize} number_of_bands {LAST | FIRST | BLEND  | MEAN 
##                                    | MINIMUM | MAXIMUM} {FIRST | REJECT | LAST | MATCH}                               

import arcpy
arcpy.env.workspace = r"\\MyMachine\PrjWorkspace\RasGP"

##Mosaic several TIFF images to a new TIFF image
arcpy.MosaicToNewRaster_management("landsatb4a.tif;landsatb4b.tif","Mosaic2New", "landsat.tif", "World_Mercator.prj",\
                                   "8_BIT_UNSIGNED", "40", "1", "LAST","FIRST")
```

### Example 5

```python
##==================================
##Mosaic To New Raster
##Usage: MosaicToNewRaster_management inputs;inputs... output_location raster_dataset_name_with_extension 
##                                    {coordinate_system_for_the_raster} 8_BIT_UNSIGNED | 1_BIT | 2_BIT | 4_BIT 
##                                    | 8_BIT_SIGNED | 16_BIT_UNSIGNED | 16_BIT_SIGNED | 32_BIT_FLOAT | 32_BIT_UNSIGNED 
##                                    | 32_BIT_SIGNED | | 64_BIT {cellsize} number_of_bands {LAST | FIRST | BLEND  | MEAN 
##                                    | MINIMUM | MAXIMUM} {FIRST | REJECT | LAST | MATCH}                               

import arcpy
arcpy.env.workspace = r"\\MyMachine\PrjWorkspace\RasGP"

##Mosaic several TIFF images to a new TIFF image
arcpy.MosaicToNewRaster_management("landsatb4a.tif;landsatb4b.tif","Mosaic2New", "landsat.tif", "World_Mercator.prj",\
                                   "8_BIT_UNSIGNED", "40", "1", "LAST","FIRST")
```

---

## Mosaic (Data Management)

## Summary

Merges multiple existing raster datasets or mosaic datasets into an existing raster dataset.

## Usage

- The target raster must be an existing raster dataset, which can be an empty raster dataset or one that contains data.
- Mosaic is useful when two or more adjacent raster datasets need to be merged into one entity. Some mosaic techniques can help minimize the abrupt changes along the boundaries of the overlapping rasters. If the target raster is a CRF format raster, the extent will be updated.
- A multidimensional raster dataset can be created by adding one or more multidimensional rasters to an empty expandable CRF raster dataset. The multidimensional information from the first input multidimensional raster will be used to define the multidimensional information of the expandable raster dataset.
- The overlapping areas of the mosaic can be handled in several ways; for example, you can set the tool to keep only the first raster dataset's data, or you can blend the overlapping cell values. There are also several options for handling a color map, if the raster dataset uses one. For example, you can keep the color map of the most recent raster dataset used in the mosaic.
- The Target Raster parameter value is considered the first raster in the list of input rasters.
- For mosaicking of discrete data, the First, Minimum, or Maximum option in Mosaic Operator will provide the most meaningful results. The Blend and Mean options are best suited for continuous data.
- Whenever possible, use the Last option for the Mosaic Operator parameter to mosaic raster datasets to an existing raster dataset in a geodatabase; it is the most effective way to mosaic.
- For file-based rasters, Ignore Background Value must be set to the same value as NoData for the background value to be ignored. Geodatabase rasters will work without this extra step.
- When mosaicking with raster datasets containing color maps, it is important to note the differences across the color maps for each raster dataset you mosaic. You can use the Mosaic tool even if the raster datasets have different color maps; however, you must choose the proper color map mode. If an improper color map mode is chosen, the output may not turn out as you expect.
- The Color Matching Method parameter allows you to choose an algorithm to color match the datasets in your mosaic.
- For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended that you resample all the data using bilinear interpolation or cubic convolution before running Mosaic. Otherwise, Mosaic will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types.
- The Mosaic tool doesn't use the output extent environment setting because the tool tends to create very large raster datasets and the output extent setting may clip the data. If the output extent does need to be adjusted, use the Clip tool to clip the Target Raster parameter value after processing.
- When the target raster is a multidimensional CRF, only the matching slices at the same dimensions for the same variable will be updated.
- The Mosaic tool supports expandable CRF data. If the CRF dataset was created using the Create Raster Dataset tool, you can update the mosaic dataset with new data, and the extent will be adjusted automatically.
- The target raster and input rasters must have the same number of bands.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rasters | The raster datasets to be merged. | Mosaic Dataset; Raster Dataset; Raster Layer |
| Target Raster | The raster to which the input rasters will be added. This must be an existing raster dataset. By default, the target raster is considered the first raster in the list of input raster datasets. You can create an empty raster using the Create Raster Dataset tool. | Raster Dataset |
| Mosaic Operator(Optional) | Specifies the method that will be used to mosaic overlapping areas.For more information about each mosaic operator, see the Mosaic Operator help topic.First—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.Last—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.Blend—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.Mean—The output cell value of the overlapping areas will be the average value of the overlapping cells.Minimum—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.Maximum—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.Sum—The output cell value of the overlapping areas will be the total sum of the overlapping cells. | String |
| Mosaic Colormap Mode(Optional) | Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.For more information about each colormap mode, see the Mosaic colormap mode help topic.First—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.Last—The color map from the last raster dataset in the list will be applied to the output raster mosaic.Match—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.Reject—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |
| Ignore Background Value (Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset.For file-based rasters and geodatabase rasters, Ignore Background Value must be set to the same value as NoData for the background value to be ignored. Enterprise geodatabase rasters will work without this extra step. | Double |
| NoData Value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | Double |
| Convert 1 bit data to 8 bit (Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.Unchecked—No conversion will occur. This is the default.Checked—The input raster will be converted. | Boolean |
| Mosaicking Tolerance(Optional) | When mosaicking occurs, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, you need to decide whether to resample or shift the data. The mosaicking tolerance controls whether resampling of the pixels will occur or the pixels will be shifted.If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will occur. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not occur and a shift will be performed.The unit of tolerance is a pixel with a valid value range of 0 to 0.5. A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero guarantees resampling will occur if there is a misalignment in pixels.For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, resampling will occur since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, the pixels will be shifted. | Double |
| Color Matching Method (Optional) | Specifies the color matching method that will be applied to the rasters.None—No color matching method will be applied when mosaicking the raster datasets.Match statistics—Descriptive statistics from the overlapping areas will be matched; the transformation will then be applied to the entire target dataset.Match histogram—The histogram from the reference overlap area will be matched to the source overlap area; the transformation will then be applied to the entire target dataset.Linear correlation—Overlapping pixels will be matched and the rest of the source dataset will be interpolated; pixels without a one-to-one relationship will use a weighted average. | String |
| inputs[input,...] | The raster datasets to be merged. | Mosaic Dataset; Raster Dataset; Raster Layer |
| target | The raster to which the input rasters will be added. This must be an existing raster dataset. By default, the target raster is considered the first raster in the list of input raster datasets. You can create an empty raster using the Create Raster Dataset tool. | Raster Dataset |
| mosaic_type(Optional) | Specifies the method that will be used to mosaic overlapping areas.FIRST—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.LAST—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.BLEND—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.MEAN—The output cell value of the overlapping areas will be the average value of the overlapping cells.MINIMUM—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.MAXIMUM—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.SUM—The output cell value of the overlapping areas will be the total sum of the overlapping cells. | String |
| colormap(Optional) | Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.FIRST—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.LAST—The color map from the last raster dataset in the list will be applied to the output raster mosaic.MATCH—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.REJECT—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |
| background_value(Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset.For file-based rasters and geodatabase rasters, Ignore Background Value must be set to the same value as NoData for the background value to be ignored. Enterprise geodatabase rasters will work without this extra step. | Double |
| nodata_value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | Double |
| onebit_to_eightbit(Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.NONE—No conversion will occur. This is the default.OneBitTo8Bit—The input raster will be converted. | Boolean |
| mosaicking_tolerance(Optional) | When mosaicking occurs, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, you need to decide whether to resample or shift the data. The mosaicking tolerance controls whether resampling of the pixels will occur or the pixels will be shifted.If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will occur. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not occur and a shift will be performed.The unit of tolerance is a pixel with a valid value range of 0 to 0.5. A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero guarantees resampling will occur if there is a misalignment in pixels.For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, resampling will occur since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, the pixels will be shifted. | Double |
| MatchingMethod(Optional) | Specifies the color matching method that will be applied to the rasters.NONE—No color matching method will be applied when mosaicking the raster datasets.STATISTIC_MATCHING—Descriptive statistics from the overlapping areas will be matched; the transformation will then be applied to the entire target dataset.HISTOGRAM_MATCHING—The histogram from the reference overlap area will be matched to the source overlap area; the transformation will then be applied to the entire target dataset.LINEARCORRELATION_MATCHING—Overlapping pixels will be matched and the rest of the source dataset will be interpolated; pixels without a one-to-one relationship will use a weighted average. | String |

## Code Samples

### Example 1

```python
arcpy.management.Mosaic(inputs, target, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.Mosaic_management("land2.tif;land3.tif","land1.tif","LAST","FIRST",
                        "0", "9", "", "", "")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
arcpy.Mosaic_management("land2.tif;land3.tif","land1.tif","LAST","FIRST",
                        "0", "9", "", "", "")
```

### Example 4

```python
##==================================
##Mosaic
##Usage: Mosaic_management inputs;inputs... target {LAST | FIRST | BLEND | MEAN | MINIMUM | MAXIMUM} {FIRST | REJECT | LAST | MATCH} 
##                         {background_value} {nodata_value} {NONE | OneBitTo8Bit} {mosaicking_tolerance}  
##                         {NONE | STATISTIC_MATCHING | HISTOGRAM_MATCHING 
##                         | LINEARCORRELATION_MATCHING}

import arcpy
arcpy.env.workspace = r"\\workspace\PrjWorkspace\RasGP"

##Mosaic two TIFF images to a single TIFF image
##Background value: 0
##Nodata value: 9
arcpy.Mosaic_management("landsatb4a.tif;landsatb4b.tif","Mosaic\\landsat.tif","LAST","FIRST","0", "9", "", "", "")

##Mosaic several 3-band TIFF images to FGDB Raster Dataset with Color Correction
##Set Mosaic Tolerance to 0.3. Mismatch larget than 0.3 will be resampled
arcpy.Mosaic_management("rgb1.tif;rgb2.tif;rgb3.tif", "Mosaic.gdb\\rgb","LAST","FIRST","", "", "", "0.3", "HISTOGRAM_MATCHING")
```

### Example 5

```python
##==================================
##Mosaic
##Usage: Mosaic_management inputs;inputs... target {LAST | FIRST | BLEND | MEAN | MINIMUM | MAXIMUM} {FIRST | REJECT | LAST | MATCH} 
##                         {background_value} {nodata_value} {NONE | OneBitTo8Bit} {mosaicking_tolerance}  
##                         {NONE | STATISTIC_MATCHING | HISTOGRAM_MATCHING 
##                         | LINEARCORRELATION_MATCHING}

import arcpy
arcpy.env.workspace = r"\\workspace\PrjWorkspace\RasGP"

##Mosaic two TIFF images to a single TIFF image
##Background value: 0
##Nodata value: 9
arcpy.Mosaic_management("landsatb4a.tif;landsatb4b.tif","Mosaic\\landsat.tif","LAST","FIRST","0", "9", "", "", "")

##Mosaic several 3-band TIFF images to FGDB Raster Dataset with Color Correction
##Set Mosaic Tolerance to 0.3. Mismatch larget than 0.3 will be resampled
arcpy.Mosaic_management("rgb1.tif;rgb2.tif;rgb3.tif", "Mosaic.gdb\\rgb","LAST","FIRST","", "", "", "0.3", "HISTOGRAM_MATCHING")
```

---

## Multipart To Singlepart (Data Management)

## Summary

Creates a feature class of singlepart features by separating multipart input features.

## Usage

- The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output feature class and set to the input feature IDs.
- Each part of a multipart feature will be separated into individual features in the output feature class. Features that are already singlepart will not be affected.
- Most of the output feature types will be the same as input (input polygons remain polygons; input lines remain lines). If the input features are of type multipoint, the output feature class will be type point.
- To reconstruct multipart features from singlepart features based on a common field value, such as ORIG_FID, use the Dissolve tool.
- A multipatch feature will be separated into its constituent geometry parts. Each part can be defined by a collection of vertices containing x-, y-, and z-coordinates that are arranged as follows:Individual triangles that reference three vertices.Triangle strips that are defined by multiple triangles that successively share one common edge.Triangle fans that are defined by multiple triangles with a common point of origin.Rings that represent a coplanar region whose boundary is defined by four or more vertices.
- Individual triangles that reference three vertices.
- Triangle strips that are defined by multiple triangles that successively share one common edge.
- Triangle fans that are defined by multiple triangles with a common point of origin.
- Rings that represent a coplanar region whose boundary is defined by four or more vertices.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that can be any feature type. | Feature Layer |
| Output Feature Class | The output feature class containing features that vary by input feature type. | Feature Class |
| in_features | The input features that can be any feature type. | Feature Layer |
| out_feature_class | The output feature class containing features that vary by input feature type. | Feature Class |

## Code Samples

### Example 1

```python
arcpy.management.MultipartToSinglepart(in_features, out_feature_class)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.MultipartToSinglepart("landuse.shp",
                                       "c:/output/output.gdb/landuse_singlepart")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.MultipartToSinglepart("landuse.shp",
                                       "c:/output/output.gdb/landuse_singlepart")
```

### Example 4

```python
# Name: MultipartToSinglepart_Example2.py
# Description: Break all multipart features into singlepart features,
#              and report which features were separated.

# Import system modules
import arcpy
 
# Create variables for the input and output feature classes
inFeatureClass = "c:/data/gdb.gdb/vegetation"
outFeatureClass = "c:/data/gdb.gdb/vegetation_singlepart"

try:
    # Create list of all fields in inFeatureClass
    fieldNameList = [field.name for field in arcpy.ListFields(inFeatureClass)]

    # Add a field to the input that will be used as a unique identifier
    arcpy.management.AddField(inFeatureClass, "tmpUID", "double")
 
    # Determine what the name of the Object ID is 
    OIDFieldName = arcpy.Describe(inFeatureClass).OIDFieldName
   
    # Calculate the tmpUID to the OID
    arcpy.management.CalculateField(inFeatureClass, "tmpUID",
                                    f"!{OIDFieldName}!", "PYTHON3")
 
    # Run the tool to create a new fc with only singlepart features
    arcpy.management.MultipartToSinglepart(inFeatureClass, outFeatureClass)
 
    # Check if there is a different number of features in the output
    #   than there was in the input
    inCount = int(arcpy.management.GetCount(inFeatureClass)[0])
    outCount = int(arcpy.management.GetCount(outFeatureClass)[0])
    
    if inCount != outCount:
        # If there is a difference, print the FID of the input 
        #   features that were multipart
        arcpy.analysis.Frequency(outFeatureClass,
                                 outFeatureClass + "_freq", "tmpUID")
 
        # Use a search cursor to go through the table, and print the tmpUID 
        print("Multipart features from {0}".format(inFeatureClass))
        for row in arcpy.da.SearchCursor(outFeatureClass + "_freq",
                                         ["tmpUID"], "FREQUENCY > 1"):
            print(int(row[0]))
    else:
        print("No multipart features were found")

except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

### Example 5

```python
# Name: MultipartToSinglepart_Example2.py
# Description: Break all multipart features into singlepart features,
#              and report which features were separated.

# Import system modules
import arcpy
 
# Create variables for the input and output feature classes
inFeatureClass = "c:/data/gdb.gdb/vegetation"
outFeatureClass = "c:/data/gdb.gdb/vegetation_singlepart"

try:
    # Create list of all fields in inFeatureClass
    fieldNameList = [field.name for field in arcpy.ListFields(inFeatureClass)]

    # Add a field to the input that will be used as a unique identifier
    arcpy.management.AddField(inFeatureClass, "tmpUID", "double")
 
    # Determine what the name of the Object ID is 
    OIDFieldName = arcpy.Describe(inFeatureClass).OIDFieldName
   
    # Calculate the tmpUID to the OID
    arcpy.management.CalculateField(inFeatureClass, "tmpUID",
                                    f"!{OIDFieldName}!", "PYTHON3")
 
    # Run the tool to create a new fc with only singlepart features
    arcpy.management.MultipartToSinglepart(inFeatureClass, outFeatureClass)
 
    # Check if there is a different number of features in the output
    #   than there was in the input
    inCount = int(arcpy.management.GetCount(inFeatureClass)[0])
    outCount = int(arcpy.management.GetCount(outFeatureClass)[0])
    
    if inCount != outCount:
        # If there is a difference, print the FID of the input 
        #   features that were multipart
        arcpy.analysis.Frequency(outFeatureClass,
                                 outFeatureClass + "_freq", "tmpUID")
 
        # Use a search cursor to go through the table, and print the tmpUID 
        print("Multipart features from {0}".format(inFeatureClass))
        for row in arcpy.da.SearchCursor(outFeatureClass + "_freq",
                                         ["tmpUID"], "FREQUENCY > 1"):
            print(int(row[0]))
    else:
        print("No multipart features were found")

except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

---

## Package 3D Tiles (Data Management)

## Summary

Packages a 3D tiles layer or folder of 3D tiles content into a 3D tiles archive file.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input 3D Tiles | The input 3D tiles layer or folder. | 3D Tiles Layer; Folder; Layer File |
| Output File | The output 3D tiles archive file. | File |
| in_3dtiles | The input 3D tiles layer or folder. | 3D Tiles Layer; Folder; Layer File |
| out_file | The output 3D tiles archive file. | File |

## Code Samples

### Example 1

```python
arcpy.management.Package3DTiles(in_3dtiles, out_file)
```

### Example 2

```python
import arcpy
arcpy.management.Package3DTiles(r"C:temp\mesh_data", r"C:\temp\mesh.3tz")
```

### Example 3

```python
import arcpy
arcpy.management.Package3DTiles(r"C:temp\mesh_data", r"C:\temp\mesh.3tz")
```

---

## Package Layer (Data Management)

## Summary

Packages one or more layers and all referenced data sources to create a single compressed .lpkx file.

## Usage

- A warning is issued when this tool encounters an unsupported layer type. The unsupported layer will not be written to the output.
- All input layers must include a description for the tool to run. To add a description, right-click the layer, click Properties, and provide a description.
- When the Convert data to file geodatabase parameter is checked, the following occurs:Each unique data source will have a file geodatabase created in the consolidated folder or package.Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- Each unique data source will have a file geodatabase created in the consolidated folder or package.
- Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.
- Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- When the Convert data to file geodatabase parameter is not checked, the following occurs:The data source format of the input layers will be preserved when possible.ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- The data source format of the input layers will be preserved when possible.
- ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.
- In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.
- Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. By default, joined or related data sources will be consolidated in their entirety or, depending on the Select Related Rows parameter value, based on the extent specified for the Extent parameter.
- For feature layers, the Extent parameter is used to select the features that will be consolidated. For raster layers, the Extent parameter is used to clip the raster datasets.
- Some datasets reference other datasets. For example, a topology dataset may reference four feature classes. Other examples of datasets that reference other datasets include geometric networks, networks, and locators. When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.
- If the Schema only parameter is checked, only the schema of the input data sources will be consolidated or packaged. A schema is the structure or design of a feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on. Data and records will not be consolidated or packaged.
- Data sources that do not support schema only will not be consolidated or packaged. If the Schema only parameter is checked and the tool encounters a layer that is not supported for schema only, a warning message appears and that layer will be skipped. If the only layer specified is unsupported for schema only, the tool will fail.
- To unpack a layer package (.lpkx file), browse to the folder containing it in the Catalog window, right-click the package, and click Add to Current Map. Alternatively, you can use the Extract Package tool and specify an output folder. By default, Add to Current Map extracts the package into your user profile under C:\Users\<username>\Documents\ArcGIS\Packages. To change the extraction location, update the path under Other Packages and Files on the Share and Download Options dialog box.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The layers that will be packaged. | Layer; Table View |
| Output File | The location and name of the output package file (.lpkx) that will be created. | File |
| Convert data to file geodatabase (Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.Checked—All data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To include enterprise geodatabase data, check the Include Enterprise Geodatabase data instead of referencing the data parameter.Unchecked—Data formats will be preserved when possible. This is the default. | Boolean |
| Include Enterprise Geodatabase data instead of referencing the data (Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format.Checked—All enterprise geodatabase data sources will be converted to a file geodatabase. This is the default.Unchecked—All enterprise geodatabase data sources will be preserved and will be referenced in the resulting package. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The specified extent will be applied to all layers. This is the default.Checked—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| Schema only (Optional) | Specifies whether only the schema of the input layers will be consolidated or packaged.Unchecked—All features and records for input layers will be included in the consolidated folder or package. This is the default.Checked—Only the schema of the input layers will be consolidated or packaged. No features or records will be consolidated or packaged in the output folder. | Boolean |
| Package Version (Optional) | Specifies the ArcGIS Pro version the layer files will be compatible with and persisted to. Certain objects such as project, maps, and layers can be persisted to a specific version. Saving to an earlier version can be helpful if the project will be used with older software; however, it can also cause some objects and properties associated with certain functionality to be removed if they are not supported in the earlier version. All versions— The package will contain layer files compatible with all versions (ArcGIS Pro 1.2 and later). Current version— The package will contain a layer file compatible with the version of the current ArcGIS Pro release. 1.2—The package will contain a layer file compatible with ArcGIS Pro version 1.2 and later.2.x—The package will contain a layer file compatible with ArcGIS Pro version 2.0 and later.3.x—The package will contain a layer file compatible with ArcGIS Pro version 3.0 and later. | String |
| Additional Files(Optional) | Additional files that will be included in the package. | File |
| Summary(Optional) | The text that will be used as the output package's summary property. | String |
| Tags (Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Preserve mobile geodatabase (Optional) | Specifies whether input mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.This parameter overrides the Convert data to file geodatabase parameter when the input data is a mobile geodatabase. Unchecked—Mobile geodatabase data will be converted to file geodatabase format. This is the default.Checked—Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| Exclude Network Dataset (Optional) | For network analysis layers, specifies whether the network dataset will also be packaged.Unchecked—The network dataset will be included and packaged. This is the default.Checked—The network dataset will not be included. Only the selected network analysis layers will be packaged. | Boolean |
| in_layer[in_layer,...] | The layers that will be packaged. | Layer; Table View |
| output_file | The location and name of the output package file (.lpkx) that will be created. | File |
| convert_data(Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.CONVERT— Data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set the convert_arcsde_data parameter to CONVERT_ARCSDE.PRESERVE—Data formats will be preserved when possible. This is the default. | Boolean |
| convert_arcsde_data(Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format. CONVERT_ARCSDE— Enterprise geodatabase data will be converted to a file geodatabase and will be included in the consolidated folder or package. This is the default.PRESERVE_ARCSDE— Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ARCSDE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| schema_only(Optional) | Specifies whether only the schema of the input layers will be consolidated or packaged.ALL— All features and records will be consolidated or packaged. This is the default.SCHEMA_ONLY— Only the schema of the input layers will be consolidated or packaged. | Boolean |
| version[version,...](Optional) | Specifies the ArcGIS Pro version the layer files will be compatible with and persisted to. Certain objects such as project, maps, and layers can be persisted to a specific version. Saving to an earlier version can be helpful if the project will be used with older software; however, it can also cause some objects and properties associated with certain functionality to be removed if they are not supported in the earlier version. ALL— The package will contain layer files compatible with all versions (ArcGIS Pro 1.2 and later). CURRENT— The package will contain a layer file compatible with the version of the current ArcGIS Pro release. 1.2—The package will contain a layer file compatible with ArcGIS Pro version 1.2 and later.2.x—The package will contain a layer file compatible with ArcGIS Pro version 2.0 and later.3.x—The package will contain a layer file compatible with ArcGIS Pro version 3.0 and later. | String |
| additional_files[additional_files,...](Optional) | Additional files that will be included in the package. | File |
| summary(Optional) | The text that will be used as the output package's summary property. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| preserve_sqlite(Optional) | Specifies whether mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.This parameter overrides the convert_data parameter when the input data is a mobile geodatabase.CONVERT_SQLITE—Mobile geodatabase data will be converted to file geodatabase format. This is the default.PRESERVE_SQLITE— Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| exclude_network_dataset(Optional) | For network analysis layers, specifies whether the network dataset will also be packaged.INCLUDE_NETWORK_DATASET—The network dataset will be included and packaged. This is the default.EXCLUDE_NETWORK_DATASET— The network dataset will not be included. Only the network analysis layers will be packaged. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.PackageLayer(in_layer, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {version}, {additional_files}, {summary}, {tags}, {select_related_rows}, {preserve_sqlite}, {exclude_network_dataset})
```

### Example 2

```python
# In this code, it is assumed that the current project has a map open with a layer named "Streets".
import arcpy
outputFile = "c:/packages/streetsLayer.lpkx"
arcpy.management.PackageLayer("Streets", outputFile, "PRESERVE", 
                              "CONVERT_ARCSDE", "#", "ALL", "ALL", "CURRENT", 
                              "C:/readme.docx", "Summary of package", 
                              "parcel,montgomery")
```

### Example 3

```python
# In this code, it is assumed that the current project has a map open with a layer named "Streets".
import arcpy
outputFile = "c:/packages/streetsLayer.lpkx"
arcpy.management.PackageLayer("Streets", outputFile, "PRESERVE", 
                              "CONVERT_ARCSDE", "#", "ALL", "ALL", "CURRENT", 
                              "C:/readme.docx", "Summary of package", 
                              "parcel,montgomery")
```

### Example 4

```python
# Import system modules
import os
import arcpy

prj = arcpy.mp.ArcGISProject(r"\\fileServe\projects\Timbuktu\Timbuktu.aprx")
maps = prj.listMaps()[0]
lyrs = maps.listLayers()
for lyr in lyrs:
    if lyr.isFeatureLayer:
        arcpy.management.PackageLayer(lyr, os.path.join("c:/temp", lyr.name + ".lpkx"))
```

### Example 5

```python
# Import system modules
import os
import arcpy

prj = arcpy.mp.ArcGISProject(r"\\fileServe\projects\Timbuktu\Timbuktu.aprx")
maps = prj.listMaps()[0]
lyrs = maps.listLayers()
for lyr in lyrs:
    if lyr.isFeatureLayer:
        arcpy.management.PackageLayer(lyr, os.path.join("c:/temp", lyr.name + ".lpkx"))
```

---

## Package Locator (Data Management)

## Summary

Packages a locator or composite locator and creates a single compressed .gcpk file.

## Usage

- To create a package for a composite locator, ensure that the participating locators are stored in the file folder.
- A warning is issued when this tool encounters an invalid locator. The invalid locator will not be packaged.
- The locator package file (.gcpk) can be shared with other users.Learn more about sharing an address locator as a locator package.
- You can use the Extract Package tool and specify an output folder to unpack a locator package. You can also specify a folder in which to unpack packages in the Share and download options.
- Each locator will be copied to a unique folder created in the consolidated folder.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Locator | The locator or composite locator that will be packaged. | Address Locator |
| Output File | The name and location of the output locator package (.gcpk). | File |
| Composite locator only: copy participating locators in enterprise database instead of referencing them(Optional) |  | Boolean |
| Additional Files(Optional) | Additional files that will be included in the package. | File |
| Summary(Optional) | The text that will be used as the output package's summary property. | String |
| Tags (Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| in_locator | The locator or composite locator that will be packaged. | Address Locator |
| output_file | The name and location of the output locator package (.gcpk). | File |
| copy_arcsde_locator(Optional) | This parameter has no effect in ArcGIS Pro. It remains only to support backward compatibility. | Boolean |
| additional_files[additional_files,...](Optional) | Additional files that will be included in the package. | File |
| summary(Optional) | The text that will be used as the output package's summary property. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |

## Code Samples

### Example 1

```python
arcpy.management.PackageLocator(in_locator, output_file, {copy_arcsde_locator}, {additional_files}, {summary}, {tags})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/MyData/Locators"
arcpy.PackageLocator_management('Atlanta_composite', 'Altanta_composite.gcpk', 
                                "", "#", "Summary of package", 
                                "tag1; tag2; tag3")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/MyData/Locators"
arcpy.PackageLocator_management('Atlanta_composite', 'Altanta_composite.gcpk', 
                                "", "#", "Summary of package", 
                                "tag1; tag2; tag3")
```

### Example 4

```python
# Name: PackageLocatorEx.py
# Description:  Find all the locators that reside in a specified folder and 
#               create a locator package for each locator.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/MyData/Locators" 

# Loop through the workspace, find all the locators, and create a locator package 
# using the same name as the locator.
for loc in arcpy.ListFiles("*.loc"):
    print("Packaging " + loc)
    arcpy.PackageLocator_management(loc, os.path.splitext(loc)[0] + '.gcpk', "", 
                                    "#", "Summary of package","tag1; tag2; tag3")
```

### Example 5

```python
# Name: PackageLocatorEx.py
# Description:  Find all the locators that reside in a specified folder and 
#               create a locator package for each locator.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/MyData/Locators" 

# Loop through the workspace, find all the locators, and create a locator package 
# using the same name as the locator.
for loc in arcpy.ListFiles("*.loc"):
    print("Packaging " + loc)
    arcpy.PackageLocator_management(loc, os.path.splitext(loc)[0] + '.gcpk', "", 
                                    "#", "Summary of package","tag1; tag2; tag3")
```

---

## Package Map (Data Management)

## Summary

Packages a map and all referenced data sources to create a single compressed .mpkx file.

## Usage

- A warning is issued when this tool encounters an unsupported layer type. The unsupported layer will not be written to the output.
- The input layer must have a description for the tool to run. To add a description, right-click the layer, click Properties, and provide a description.
- When the Convert data to file geodatabase parameter is checked, the following occurs:Each unique data source will have a file geodatabase created in the consolidated folder or package.Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- Each unique data source will have a file geodatabase created in the consolidated folder or package.
- Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.
- Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- When the Convert data to file geodatabase parameter is not checked, the following occurs:The data source format of the input layers will be preserved when possible.ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- The data source format of the input layers will be preserved when possible.
- ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.
- In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.
- Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- When Support ArcGIS Runtime is checked, the Version parameter will be overridden and the package will contain geodatabases and a map compatible with the current release version only.
- For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. By default, joined or related data sources will be consolidated in their entirety or, depending on the Select Related Rows parameter value, based on the extent specified for the Extent parameter.
- For feature layers, the Extent parameter is used to select the features that will be consolidated. For raster layers, the Extent parameter is used to clip the raster datasets.
- Some datasets reference other datasets. For example, a topology dataset may reference four feature classes. Other examples of datasets that reference other datasets include geometric networks, networks, and locators. When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.
- If the Schema only parameter is checked, only the schema of the input data sources will be consolidated or packaged. A schema is the structure or design of a feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on. Data and records will not be consolidated or packaged.
- Data sources that do not support schema only will not be consolidated or packaged. If the Schema only parameter is checked and the tool encounters a layer that is not supported for schema only, a warning message appears and that layer will be skipped. If the only layer specified is unsupported for schema only, the tool will fail.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Map | The map that will be packaged. When running this tool in ArcGIS Pro, the input can be a map, scene, or basemap. | Map |
| Output File | The output map package (.mpkx file). | File |
| Convert data to file geodatabase (Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.Checked—All data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To include enterprise geodatabase data, check the Include Enterprise Geodatabase data instead of referencing the data parameter.Unchecked—Data formats will be preserved when possible. This is the default. | Boolean |
| Include Enterprise Geodatabase data instead of referencing the data (Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format.Checked—All enterprise geodatabase data sources will be converted to a file geodatabase. This is the default.Unchecked—All enterprise geodatabase data sources will be preserved and will be referenced in the resulting package. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The specified extent will be applied to all layers. This is the default.Checked—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| Support ArcGIS Maps SDKs(Optional) | Specifies whether the package will support ArcGIS Maps SDKs. To support ArcGIS Maps SDKs, all data sources will be converted to a file geodatabase, and an .msd file will be created in the output package.Unchecked—The output package will not support ArcGIS Maps SDKs.Checked—The output package will support ArcGIS Maps SDKs. | Boolean |
| Reference all data for Runtime(Optional) | Specifies whether a package that references the necessary data will be created rather than copying the data. This is helpful when trying to package large datasets that are available from a central location in an organization. Checked—A package that references the necessary data will be created rather than copying the data. Unchecked—A package that contains the necessary data will be created. This is the default. | Boolean |
| Package Version (Optional) | Specifies the version of the geodatabases that will be created in the resulting package. Specifying a version allows packages to be shared with earlier versions of ArcGIS and supports backward compatibility. Note:A package saved to an earlier version may lose properties that are only available in the later version. All versions—The package will contain maps compatible with all versions (ArcGIS Pro 1.2 and later). Current version— The package will contain a map compatible with the version of the current ArcGIS Pro release.1.2—The package will contain a map compatible with ArcGIS Pro version 1.2 and later.2.x—The package will contain a map compatible with ArcGIS Pro version 2.0 and later.3.x—The package will contain a map compatible with ArcGIS Pro version 3.0 and later. | String |
| Additional Files(Optional) | Additional files that will be included in the package. | File |
| Summary(Optional) | The text that will be used as the output package's summary property. | String |
| Tags (Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Preserve Mobile Geodatabase (Optional) | Specifies whether input mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.Unchecked—Mobile geodatabase data will be converted to file geodatabase format. This is the default.Checked—Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| Consolidate to a single file geodatabase(Optional) | Specifies whether map layers will be consolidated to a single file geodatabase or to multiple file geodatabases based on the number of unique data sources in the input map.Unchecked—The output will include a file geodatabase for each unique data source in the input map. Each unique data source in the input map will remain as a dedicated data source in the output. For example, if the input map has two layers, and each layer has a data source in a different enterprise geodatabase, the output layer data sources will be in separate geodatabases. This is the default.Checked—All map layers will be consolidated into a single file geodatabase. | Boolean |
| in_map[in_map,...] | The map that will be packaged. When running this tool in ArcGIS Pro, the input can be a map, scene, or basemap. | Map |
| output_file | The output map package (.mpkx file). | File |
| convert_data(Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.CONVERT— Data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set the convert_arcsde_data parameter to CONVERT_ARCSDE.PRESERVE—Data formats will be preserved when possible. This is the default. | Boolean |
| convert_arcsde_data(Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format. CONVERT_ARCSDE— Enterprise geodatabase data will be converted to a file geodatabase and will be included in the consolidated folder or package. This is the default.PRESERVE_ARCSDE— Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ARCSDE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| arcgisruntime(Optional) | Specifies whether the package will support ArcGIS Maps SDKs. To support ArcGIS Maps SDKs, all data sources will be converted to a file geodatabase, and an .msd file will be created in the output package.DESKTOP—The output package will not support ArcGIS Maps SDKs. Unless otherwise specified, data sources will not be converted to a file geodatabase, and an .msd file will not be created.RUNTIME— The output package will support ArcGIS Maps SDKs. All data sources will be converted to a file geodatabase, and an .msd file will be created in the output package. | Boolean |
| reference_all_data(Optional) | Specifies whether a package that references the necessary data will be created rather than copying the data. This is helpful when trying to package large datasets that are available from a central location in an organization. REFERENCED—A package that references the necessary data will be created rather than copying the data. NOT_REFERENCED— A package that contains the necessary data will be created. This is the default. | Boolean |
| version[version,...](Optional) | Specifies the version of the geodatabases that will be created in the resulting package. Specifying a version allows packages to be shared with earlier versions of ArcGIS and supports backward compatibility. Note:A package saved to an earlier version may lose properties that are only available in the later version. ALL—The package will contain maps compatible with all versions (ArcGIS Pro 1.2 and later). CURRENT— The package will contain a map compatible with the version of the current ArcGIS Pro release.1.2—The package will contain a map compatible with ArcGIS Pro version 1.2 and later.2.x—The package will contain a map compatible with ArcGIS Pro version 2.0 and later.3.x—The package will contain a map compatible with ArcGIS Pro version 3.0 and later. | String |
| additional_files[additional_files,...](Optional) | Additional files that will be included in the package. | File |
| summary(Optional) | The text that will be used as the output package's summary property. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| preserve_sqlite(Optional) | Specifies whether mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.CONVERT_SQLITE—Mobile geodatabase data will be converted to file geodatabase format. This is the default.PRESERVE_SQLITE— Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| consolidate_to_one_fgdb(Optional) | Specifies whether map layers will be consolidated to a single file geodatabase or to multiple file geodatabases based on the number of unique data sources in the input map.SINGLE_OUTPUT_WORKSPACE—All map layers will be consolidated into a single file geodatabase.MULTIPLE_OUTPUT_WORKSPACES—The output will include a file geodatabase for each unique data source in the input map. Each unique data source in the input map will remain as a dedicated data source in the output. For example, if the input map has two layers, and each layer has a data source in a different enterprise geodatabase, the output layer data sources will be in separate geodatabases. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.PackageMap(in_map, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {arcgisruntime}, {reference_all_data}, {version}, {additional_files}, {summary}, {tags}, {select_related_rows}, {preserve_sqlite}, {consolidate_to_one_fgdb})
```

### Example 2

```python
# In this code, it is assumed that a map named "World1" exists in the current project
import arcpy
outputFile = "c:/outputPackages/world_map.mpkx"
arcpy.management.PackageMap("World1", outputFile, "PRESERVE", "CONVERT_ARCSDE", "#", "ALL")
```

### Example 3

```python
# In this code, it is assumed that a map named "World1" exists in the current project
import arcpy
outputFile = "c:/outputPackages/world_map.mpkx"
arcpy.management.PackageMap("World1", outputFile, "PRESERVE", "CONVERT_ARCSDE", "#", "ALL")
```

---

## Package Project (Data Management)

## Summary

Consolidates and packages an ArcGIS Pro project (.aprx) and its contents (maps and data) to a packaged project file (.ppkx).

## Usage

- The data and elements in the project will be consolidated into the project package (.ppkx), including maps and 3D views, the data and layers in the maps, toolboxes, geoprocessing history items, styles, layouts, and folders or connections (server, workflow, and so on) when appropriate.
- The Share outside of organization parameter allows you to control whether a package will be created for internal (inside your organizations network) or external use. When Share outside of organization is checked, enterprise geodatabase layers, feature services, and data referenced across network shares (UNC path) will be copied (or converted) and included in the project package. Data stored on the local machine will also be consolidated and packaged.Note:Creating an internal package that contains referenced data will open with broken links if shared to colleagues who do not have access to networked resources (enterprise geodatabase, feature service, and data referenced through disk shares [UNC paths]).
- Templates can be created using the Package as template parameter. You can use a project template (.aptx file) to create a project by defining layers, maps, and data, as well as required connections. Learn more about creating a project template
- Use the Analyze Tools for Pro and Analyze Toolbox for Version tools to analyze toolboxes that are part of a project before the consolidation process is performed. Identified errors can stop the consolidation process. Fix the errors or remove the tool from the project. Toolboxes can be excluded from the output project package by unchecking the Include Toolboxes parameter.
- Similar to project toolboxes, geoprocessing history (the result information from running a geoprocessing tool) will be included in the package. Data that is required to process the history item will be included in the output package regardless of whether it is in a map or scene. Any history items that are invalid (failed during processing) or where data cannot be found will cause the packaging process to stop. You must remove or fix the history item in question. History items can be excluded from a project package by unchecking the Include History Items parameter.
- Connections—such as folder, server, database, and workflow—will only be included in an internal package. These items will be removed if the package is created with the Share outside of organization parameter checked.
- For feature layers, use the Extent parameter to select the features that will be consolidated. For raster layers, use the Extent parameter to clip the raster datasets.
- Additional files can be included in the package. Images, PDFs, Word documents, and .zip files can be included in the package using the Additional Files parameter.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Project | The project (.aprx file) that will be packaged. | File |
| Output File | The output project package (.ppkx file). | File |
| Share outside of organization(Optional) | Specifies whether the project and all data will be consolidated, converted, or copied into the package (for sharing outside your organization) or referenced as is without converting or including them in the package (for sharing within your organization).Note:Data and maps will be consolidated and packaged if the project references them from a local path, such as c:\gisdata\landrecords.gdb\, regardless of this parameter's setting.Unchecked—Data stored on networked resources such as enterprise geodatabases, feature services, and file shares (UNC path) will be referenced as is without converting them or including them in the package. This is the default. Checked—Data stored on networked resources will be consolidated, converted, or copied into the package. The data's formats will be preserved when possible. | Boolean |
| Package as template (Optional) | Specifies whether a project template or a project package will be created. Project templates can include maps, layouts, connections to databases and servers, and so on. A project template can be used to standardize a series of maps for different projects and to ensure that the correct layers are immediately available for everyone to use in their maps. Unchecked—A project package will be created. This is the default. Checked—A project template will be created. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply Extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The extent will be applied to all layers. This is the default.Checked—The extent will be applied to enterprise geodatabase layers only. | Boolean |
| Additional Files(Optional) | Additional files that will be included in the package. | File |
| Summary (Optional) | The summary information that will be added to the properties of the package. | String |
| Tags (Optional) | The tags that will be added to the properties of the package. Separate multiple tags with a comma or semicolon. | String |
| Package Version (Optional) | Specifies the ArcGIS Pro version that certain objects such as projects, maps, and layers will be compatible with and persisted to. Saving to an earlier version can be helpful if the project will be used with older software.Caution:A package saved to an earlier version may lose functionality or properties that are unsupported in the earlier version. All versions— The contents of the package will be compatible with all versions (ArcGIS Pro 2.1 and later). Current version— The contents of the package will be compatible with the current version of the ArcGIS Pro release. ArcGIS Pro 2.2— The contents of the package will be compatible with ArcGIS Pro version 2.2.ArcGIS Pro 2.3—The contents of the package will be compatible with ArcGIS Pro version 2.3.ArcGIS Pro 2.4—The contents of the package will be compatible with ArcGIS Pro version 2.4.ArcGIS Pro 2.5—The contents of the package will be compatible with ArcGIS Pro version 2.5.ArcGIS Pro 2.6—The contents of the package will be compatible with ArcGIS Pro version 2.6.ArcGIS Pro 2.7—The contents of the package will be compatible with ArcGIS Pro version 2.7.ArcGIS Pro 2.8—The contents of the package will be compatible with ArcGIS Pro version 2.8.ArcGIS Pro 2.9—The contents of the package will be compatible with ArcGIS Pro version 2.9.ArcGIS Pro 3.0—The contents of the package will be compatible with ArcGIS Pro version 3.0.ArcGIS Pro 3.1—The contents of the package will be compatible with ArcGIS Pro version 3.1.ArcGIS Pro 3.2—The contents of the package will be compatible with ArcGIS Pro version 3.2.ArcGIS Pro 3.3—The contents of the package will be compatible with ArcGIS Pro version 3.3.ArcGIS Pro 3.4—The contents of the package will be compatible with ArcGIS Pro version 3.4. | String |
| Include Toolboxes (Optional) | Specifies whether project toolboxes, and the data referenced by the tools in those toolboxes, will be consolidated and included in the output package. All projects require a default toolbox, which will be included in the output package regardless of this setting. A toolbox in a connected folder is not considered a project toolbox and is not impacted by this setting. Checked—Project toolboxes will be included in the output package. This is the default. Unchecked—Project toolboxes will not be included in the output package. | Boolean |
| Include History Items (Optional) | Specifies whether geoprocessing history items will be consolidated and included in the output package. Included history items will consolidate the data required to reprocess the history item. History items will be included—History items will be included in the output package. This is the default.History items will not be included—History items will not be included in the output package.Only valid history items will be included—Only valid history items will be included in the output package. History items are invalid if any of the original input layers or tools cannot be found. | String |
| Read Only Package (Optional) | Specifies whether the project will be read-only. Read-only projects cannot be modified or saved. Checked—The project will be read-only. Unchecked—The project will be writable. This is the default. | Boolean |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| Preserve Mobile Geodatabase (Optional) | Specifies whether input mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.Unchecked—Mobile geodatabase data will be converted to file geodatabase format. This is the default.Checked—Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |
| in_project | The project (.aprx file) that will be packaged. | File |
| output_file | The output project package (.ppkx file). | File |
| sharing_internal(Optional) | Specifies whether the project and all data will be consolidated, converted, or copied into the package (for sharing outside your organization) or referenced as is without converting or including them in the package (for sharing within your organization).INTERNAL— Data stored on networked resources such as enterprise geodatabases, feature services, and file shares (UNC path) will be referenced as is without converting them or including them in the package. This is the default. EXTERNAL—Data stored on networked resources will be consolidated, converted, or copied into the package. The data's formats will be preserved when possible. | Boolean |
| package_as_template(Optional) | Specifies whether a project template or a project package will be created. Project templates can include maps, layouts, connections to databases and servers, and so on. A project template can be used to standardize a series of maps for different projects and to ensure that the correct layers are immediately available for everyone to use in their maps. Learn more about creating a project templatePROJECT_PACKAGE— A project package will be created. This is the default. PROJECT_TEMPLATE—A project template will be created | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ENTERPRISE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| additional_files[additional_files,...](Optional) | Additional files that will be included in the package. | File |
| summary(Optional) | The summary information that will be added to the properties of the package. | String |
| tags(Optional) | The tags that will be added to the properties of the package. Separate multiple tags with a comma or semicolon. | String |
| version[version,...](Optional) | Specifies the ArcGIS Pro version that certain objects such as projects, maps, and layers will be compatible with and persisted to. Saving to an earlier version can be helpful if the project will be used with older software.Caution:A package saved to an earlier version may lose functionality or properties that are unsupported in the earlier version. ALL— The contents of the package will be compatible with all versions (ArcGIS Pro 2.1 and later). CURRENT— The contents of the package will be compatible with the current version of the ArcGIS Pro release. 2.2— The contents of the package will be compatible with ArcGIS Pro version 2.2.2.3—The contents of the package will be compatible with ArcGIS Pro version 2.3.2.4—The contents of the package will be compatible with ArcGIS Pro version 2.4.2.5—The contents of the package will be compatible with ArcGIS Pro version 2.5.2.6—The contents of the package will be compatible with ArcGIS Pro version 2.6.2.7—The contents of the package will be compatible with ArcGIS Pro version 2.7.2.8—The contents of the package will be compatible with ArcGIS Pro version 2.8.2.9—The contents of the package will be compatible with ArcGIS Pro version 2.9.3.0—The contents of the package will be compatible with ArcGIS Pro version 3.0.3.1—The contents of the package will be compatible with ArcGIS Pro version 3.1.3.2—The contents of the package will be compatible with ArcGIS Pro version 3.2.3.3—The contents of the package will be compatible with ArcGIS Pro version 3.3.3.4—The contents of the package will be compatible with ArcGIS Pro version 3.4. | String |
| include_toolboxes(Optional) | Specifies whether project toolboxes will be consolidated and included in the output package. All projects require a default toolbox, which will be included in the output package regardless of this setting.TOOLBOXES—Project toolboxes will be included in the output package. This is the default.NO_TOOLBOXES—Project toolboxes will not be included in the output package. | Boolean |
| include_history_items(Optional) | Specifies whether geoprocessing history items will be consolidated and included in the output package. Included history items will consolidate the data required to reprocess the history item. HISTORY_ITEMS—History items will be included in the output package. This is the default.NO_HISTORY_ITEMS—History items will not be included in the output package.VALID_HISTORY_ITEMS_ONLY—Only valid history items will be included in the output package. History items are invalid if any of the original input layers or tools cannot be found. | String |
| read_only(Optional) | Specifies whether the project will be read-only. Read-only projects cannot be modified or saved. READ_ONLY—The project will be read-only.READ_WRITE—The project will be writable. This is the default. | Boolean |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |
| preserve_sqlite(Optional) | Specifies whether mobile geodatabase data will be preserved in the output or written to file geodatabase format. If the input data is a mobile geodatabase network dataset, the output will be a mobile geodatabase.CONVERT_SQLITE—Mobile geodatabase data will be converted to file geodatabase format. This is the default.PRESERVE_SQLITE— Mobile geodatabase data will be preserved as mobile geodatabase data in the output. The geodatabase will be included in its entirety. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.PackageProject(in_project, output_file, {sharing_internal}, {package_as_template}, {extent}, {apply_extent_to_arcsde}, {additional_files}, {summary}, {tags}, {version}, {include_toolboxes}, {include_history_items}, {read_only}, {select_related_rows}, {preserve_sqlite})
```

### Example 2

```python
import arcpy
p = arcpy.mp.ArcGISProject("CURRENT")
arcpy.management.PackageProject(p.filePath, f'E:\gisShare\projects\Pipelines\SE_Pipeline.ppkx', 'INTERNAL')
```

### Example 3

```python
import arcpy
p = arcpy.mp.ArcGISProject("CURRENT")
arcpy.management.PackageProject(p.filePath, f'E:\gisShare\projects\Pipelines\SE_Pipeline.ppkx', 'INTERNAL')
```

### Example 4

```python
import os
import arcpy

enterpriseProjectDir = r"\\centralFileServer\gisData\ArcGISProProjects"
sharedProjectDir = r"c:\publicFiles\sharedProjects"

walk = arcpy.da.Walk(enterpriseProjectDir, datatype="Project")

for dirpath, dirnames, filenames in walk:
    for filename in filenames:
        if "oil" in filename.lower():
            project = os.path.join(dirpath, filename)
            out_ppkx = os.path.join(sharedProjectDir, os.path.splitext(os.path.basename(project))[0] + ".ppkx")
            print(f"Packaging: {project} to {out_ppkx}")
            arcpy.management.PackageProject(project, out_ppkx, "EXTERNAL")
```

### Example 5

```python
import os
import arcpy

enterpriseProjectDir = r"\\centralFileServer\gisData\ArcGISProProjects"
sharedProjectDir = r"c:\publicFiles\sharedProjects"

walk = arcpy.da.Walk(enterpriseProjectDir, datatype="Project")

for dirpath, dirnames, filenames in walk:
    for filename in filenames:
        if "oil" in filename.lower():
            project = os.path.join(dirpath, filename)
            out_ppkx = os.path.join(sharedProjectDir, os.path.splitext(os.path.basename(project))[0] + ".ppkx")
            print(f"Packaging: {project} to {out_ppkx}")
            arcpy.management.PackageProject(project, out_ppkx, "EXTERNAL")
```

---

## Package Result (Data Management)

## Summary

Packages one or more geoprocessing results, including all tools and input and output datasets, into a single compressed file (.gpkx).

## Usage

- When a tool is run, a geoprocessing history item is added to the Geoprocessing History section of the Catalog pane. Use this item as input to the Package Result tool. When a tool is run from Python, a Result object is returned. The Result object's resultID property can be used as input to this tool. The second code sample below demonstrates how to use a Result object's resultID property.
- When Support ArcGIS Maps SDKs is checked, the geoprocessing package created can be used in the ArcGIS Maps SDKs environment. To support the ArcGIS Maps SDKs environment, the following occur:All nongeodatabase data sources are converted to a file geodatabase.A copy of the tool being package is created in a new toolbox configured for publishing. Note:Starting at ArcGIS Pro 2.1, geoprocessing packages that support ArcGIS Maps SDKs can be created.
- All nongeodatabase data sources are converted to a file geodatabase.
- A copy of the tool being package is created in a new toolbox configured for publishing.
- When the Convert data to file geodatabase parameter is checked, the following occurs:Each unique data source will have a file geodatabase created in the consolidated folder or package.Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- Each unique data source will have a file geodatabase created in the consolidated folder or package.
- Compressed raster and vector formats will be converted to a file geodatabase, and compression will be lost.
- Enterprise geodatabase data will not be consolidated. To convert enterprise geodatabase data to a file geodatabase, check the Include Enterprise Geodatabase data instead of referencing the data parameter.
- When the Convert data to file geodatabase parameter is not checked, the following occurs:The data source format of the input layers will be preserved when possible.ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- The data source format of the input layers will be preserved when possible.
- ADRG, CADRG/ECRG, CIB, and RPF raster formats will convert to file geodatabase rasters. ArcGIS cannot natively write out these formats. They will be converted to file geodatabase rasters for efficiency.
- In the output folder structure, file geodatabases will be consolidated in a version-specific folder, and all other formats will be consolidated in the commonData folder.
- Compressed raster and vector formats will not be clipped even if an extent is specified for the Extent parameter.
- For layers that contain a join or participate in a relationship class, all joined or related data sources will be consolidated into the output folder. By default, joined or related data sources will be consolidated in their entirety or, depending on the Select Related Rows parameter value, based on the extent specified for the Extent parameter.
- For feature layers, the Extent parameter is used to select the features that will be consolidated. For raster layers, the Extent parameter is used to clip the raster datasets.
- Some datasets reference other datasets. For example, a topology dataset may reference four feature classes. Other examples of datasets that reference other datasets include geometric networks, networks, and locators. When consolidating or packaging a layer based on these types of datasets, the participating datasets will also be consolidated or packaged.
- If the Schema only parameter is checked, only the schema of the input and output data sources will be consolidated or packaged. A schema is the structure or design of a feature class or table that consists of field and table definitions, coordinate system properties, symbology, definition queries, and so on. Data and records will not be consolidated or packaged.
- Data sources that do not support schema only will not be consolidated or packaged. If the Schema only parameter is checked and the tool encounters a layer that is not supported for schema only, a warning message appears and that layer will be skipped. If the only layer specified is unsupported for schema only, the tool will fail.
- To unpack a geoprocessing package, use the Extract Package tool to extract the contents to a folder. You can then browse to the directory and explore the tool and data. Alternatively, right-click the package from a folder in the Catalog pane and extract it to the current map. The tool will be accessible from the Geoprocessing History section.By default, when extracting a package from the Catalog pane, the contents will be extracted into your user profile.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Result | The result that will be packaged. The input can be either a result from the history of the current project or a Result object's resultID property when the tool is being used in a Python script. | File; String |
| Output File | The name and location of the output package file (.gpkx). | File |
| Convert data to file geodatabase (Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.Checked—All data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To include enterprise geodatabase data, check the Include Enterprise Geodatabase data instead of referencing the data parameter.Unchecked—Data formats will be preserved when possible. This is the default. | Boolean |
| Include Enterprise Geodatabase data instead of referencing the data (Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format.Checked—All enterprise geodatabase data sources will be converted to a file geodatabase. This is the default.Unchecked—All enterprise geodatabase data sources will be preserved and will be referenced in the resulting package. | Boolean |
| Extent (Optional) | Specifies the extent that will be used to select or clip features.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Intersection of Inputs —The extent will be the intersecting extent of all inputs. Union of Inputs —The extent will be the combined extent of all inputs.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| Apply extent only to enterprise geodatabase layers (Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.Unchecked—The specified extent will be applied to all layers. This is the default.Checked—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| Schema only (Optional) | Specifies whether all features and records for input and output datasets or only the schema of input and output datasets will be consolidated or packaged.Unchecked—All features and records for input and output datasets will be included in the consolidated folder or package. This is the default.Checked—Only the schema of input and output datasets will be consolidated or packaged. No features or records will be consolidated or packaged in the output folder. | Boolean |
| Support ArcGIS Maps SDKs (Optional) | Specifies whether the package will support ArcGIS Maps SDKs. To support ArcGIS Maps SDKs, all data sources will be converted to a file geodatabase, and a server compatible tool will be created in the package. Unchecked—The output package will not support ArcGIS Maps SDKs. This is the default.Checked—The output package will support ArcGIS Maps SDKs. | Boolean |
| Additional Files(Optional) | Additional files that will be included in the package. | File |
| Summary(Optional) | The text that will be used as the output package's summary property. | String |
| Tags (Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| Package Version (Optional) | Specifies the ArcGIS Pro version that certain objects such as projects, maps, and layers will be compatible with and persisted to. Saving to an earlier version can be helpful if the project will be used with older software.Caution:A package saved to an earlier version may lose functionality or properties that are unsupported in the earlier version. All versions— The contents of the package will be compatible with all versions (ArcGIS Pro 2.1 and later). Current version— The contents of the package will be compatible with the current version of the ArcGIS Pro release. ArcGIS Pro 2.2— The contents of the package will be compatible with ArcGIS Pro version 2.2.ArcGIS Pro 2.3—The contents of the package will be compatible with ArcGIS Pro version 2.3.ArcGIS Pro 2.4—The contents of the package will be compatible with ArcGIS Pro version 2.4.ArcGIS Pro 2.5—The contents of the package will be compatible with ArcGIS Pro version 2.5.ArcGIS Pro 2.6—The contents of the package will be compatible with ArcGIS Pro version 2.6.ArcGIS Pro 2.7—The contents of the package will be compatible with ArcGIS Pro version 2.7.ArcGIS Pro 2.8—The contents of the package will be compatible with ArcGIS Pro version 2.8.ArcGIS Pro 2.9—The contents of the package will be compatible with ArcGIS Pro version 2.9.ArcGIS Pro 3.0—The contents of the package will be compatible with ArcGIS Pro version 3.0.ArcGIS Pro 3.1—The contents of the package will be compatible with ArcGIS Pro version 3.1.ArcGIS Pro 3.2—The contents of the package will be compatible with ArcGIS Pro version 3.2.ArcGIS Pro 3.3—The contents of the package will be compatible with ArcGIS Pro version 3.3.ArcGIS Pro 3.4—The contents of the package will be compatible with ArcGIS Pro version 3.4. | String |
| Keep only the rows which are related to features within the extent (Optional) | Specifies whether the specified extent will be applied to related data sources.Unchecked—Related data sources will be consolidated in their entirety. This is the default. Checked—Only related data corresponding to records within the specified extent will be consolidated. | Boolean |
| in_result[in_result,...] | The result that will be packaged. The input can be either a result from the history of the current project or a Result object's resultID property when the tool is being used in a Python script. | File; String |
| output_file | The name and location of the output package file (.gpkx). | File |
| convert_data(Optional) | Specifies whether input layers will be converted to a file geodatabase or preserved in their original format.CONVERT— Data will be converted to a file geodatabase. This option does not apply to enterprise geodatabase data sources. To convert enterprise geodatabase data, set the convert_arcsde_data parameter to CONVERT_ARCSDE.PRESERVE—Data formats will be preserved when possible. This is the default. | Boolean |
| convert_arcsde_data(Optional) | Specifies whether input enterprise geodatabase layers will be converted to a file geodatabase or preserved in their original format. CONVERT_ARCSDE— Enterprise geodatabase data will be converted to a file geodatabase and will be included in the consolidated folder or package. This is the default.PRESERVE_ARCSDE— Enterprise geodatabase data will be preserved and will be referenced in the consolidated folder or package. | Boolean |
| extent(Optional) | Specifies the extent that will be used to select or clip features.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| apply_extent_to_arcsde(Optional) | Specifies whether the specified extent will be applied to all layers or to enterprise geodatabase layers only.ALL— The specified extent will be applied to all layers. This is the default.ARCSDE_ONLY—The specified extent will be applied to enterprise geodatabase layers only. | Boolean |
| schema_only(Optional) | Specifies whether all records for input and output datasets or only the schema of input and output datasets will be consolidated or packaged.ALL— All records for input and output datasets will be consolidated or packaged. This is the default.SCHEMA_ONLY— Only the schema of input and output datasets will be consolidated or packaged. | Boolean |
| arcgisruntime(Optional) | Specifies whether the package will support ArcGIS Maps SDKs. To support ArcGIS Maps SDKs, all data sources will be converted to a file geodatabase.DESKTOP—The output package will not support ArcGIS Maps SDKs. This is the default.RUNTIME— The output package will support ArcGIS Maps SDKs. | Boolean |
| additional_files[additional_files,...](Optional) | Additional files that will be included in the package. | File |
| summary(Optional) | The text that will be used as the output package's summary property. | String |
| tags(Optional) | The tag information that will be added to the properties of the package. Multiple tags can be added or separated by a comma or semicolon. | String |
| version[version,...](Optional) | Specifies the ArcGIS Pro version that certain objects such as projects, maps, and layers will be compatible with and persisted to. Saving to an earlier version can be helpful if the project will be used with older software.Caution:A package saved to an earlier version may lose functionality or properties that are unsupported in the earlier version. ALL— The contents of the package will be compatible with all versions (ArcGIS Pro 2.1 and later). CURRENT— The contents of the package will be compatible with the current version of the ArcGIS Pro release. 2.2— The contents of the package will be compatible with ArcGIS Pro version 2.2.2.3—The contents of the package will be compatible with ArcGIS Pro version 2.3.2.4—The contents of the package will be compatible with ArcGIS Pro version 2.4.2.5—The contents of the package will be compatible with ArcGIS Pro version 2.5.2.6—The contents of the package will be compatible with ArcGIS Pro version 2.6.2.7—The contents of the package will be compatible with ArcGIS Pro version 2.7.2.8—The contents of the package will be compatible with ArcGIS Pro version 2.8.2.9—The contents of the package will be compatible with ArcGIS Pro version 2.9.3.0—The contents of the package will be compatible with ArcGIS Pro version 3.0.3.1—The contents of the package will be compatible with ArcGIS Pro version 3.1.3.2—The contents of the package will be compatible with ArcGIS Pro version 3.2.3.3—The contents of the package will be compatible with ArcGIS Pro version 3.3.3.4—The contents of the package will be compatible with ArcGIS Pro version 3.4. | String |
| select_related_rows(Optional) | Specifies whether the specified extent will be applied to related data sources.KEEP_ONLY_RELATED_ROWS—Only related data corresponding to records within the specified extent will be consolidated.KEEP_ALL_RELATED_ROWS—Related data sources will be consolidated in their entirety. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.PackageResult(in_result, output_file, {convert_data}, {convert_arcsde_data}, {extent}, {apply_extent_to_arcsde}, {schema_only}, {arcgisruntime}, {additional_files}, {summary}, {tags}, {version}, {select_related_rows})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/ResultFiles" 
arcpy.management.PackageResult('Parcel.rlt', 'Parcel.gpk', "PRESERVE", 
                               "CONVERT_ARCSDE", "#", "ALL", "ALL", 
                               "DESKTOP", r"C:\docs\readme.txt", 
                               "Summary text", "Tag1; tag2; tag3")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/ResultFiles" 
arcpy.management.PackageResult('Parcel.rlt', 'Parcel.gpk', "PRESERVE", 
                               "CONVERT_ARCSDE", "#", "ALL", "ALL", 
                               "DESKTOP", r"C:\docs\readme.txt", 
                               "Summary text", "Tag1; tag2; tag3")
```

### Example 4

```python
import arcpy

# Import toolbox with custom model inside
arcpy.ImportToolbox("c:/gisworkflows/ParcelTools.tbx")

# Run the tool and assign to a result variable
parcelUpdate = arcpy.ParcelUpdater_ParcelTools("c:/data/parcels.gdb/ward3", "UPDATE")

arcpy.management.PackageResult(parcelUpdate.resultID, "c:/gpks/parcelgpk.gpkx", 
                               "PRESERVE", "CONVERT_ARCSDE", "#", "ALL", 
                               "ALL", "DESKTOP", "#", "Summary text", "Tag1")
```

### Example 5

```python
import arcpy

# Import toolbox with custom model inside
arcpy.ImportToolbox("c:/gisworkflows/ParcelTools.tbx")

# Run the tool and assign to a result variable
parcelUpdate = arcpy.ParcelUpdater_ParcelTools("c:/data/parcels.gdb/ward3", "UPDATE")

arcpy.management.PackageResult(parcelUpdate.resultID, "c:/gpks/parcelgpk.gpkx", 
                               "PRESERVE", "CONVERT_ARCSDE", "#", "ALL", 
                               "ALL", "DESKTOP", "#", "Summary text", "Tag1")
```

---

## Pivot Table (Data Management)

## Summary

Creates a table from the input table by reducing redundancy in records and flattening one-to-many relationships.

## Usage

- This tool is typically used to reduce redundant records and flatten one-to-many relationships.
- The combination of the Input Fields, Pivot Field, and Value Field parameter values must be unique. Use the Frequency tool to determine if the combination is unique.
- If the Pivot Field value is a text field, its values must begin with a character (for example, a2) and not a number (for example, 2a). If the value of the first record begins with a number, all the output values will be 0.
- If the Pivot Field value is a numeric type, its value will be appended to its original field name in the output table.
- The number of fields in the output table will be determined by the number of input fields you specify, plus one field for each unique Pivot Field value. The number of records in the output table will be determined by the unique combination of values between the specified input fields and the pivot field.
- The tool will fail if the Pivot Field value contains Null values.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the records that will be pivoted. | Table View |
| Input Fields | The fields that define the records that will be included in the output table. | Field |
| Pivot Field | The field whose record values will be used to generate the field names in the output table. | Field |
| Value Field | The field whose values will populate the pivoted fields in the output table. | Field |
| Output Table | The table that will be created containing the pivoted records. | Table |
| in_table | The table containing the records that will be pivoted. | Table View |
| fields[fields,...] | The fields that define the records that will be included in the output table. | Field |
| pivot_field | The field whose record values will be used to generate the field names in the output table. | Field |
| value_field | The field whose values will populate the pivoted fields in the output table. | Field |
| out_table | The table that will be created containing the pivoted records. | Table |

## Code Samples

### Example 1

```python
arcpy.management.PivotTable(in_table, fields, pivot_field, value_field, out_table)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.PivotTable("attributes.dbf", "OwnerID", "AttrTagNam", 
                            "AttrValueS", "C:/output/attribPivoted.dbf")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.PivotTable("attributes.dbf", "OwnerID", "AttrTagNam", 
                            "AttrValueS", "C:/output/attribPivoted.dbf")
```

### Example 4

```python
# Name: PivotTable_Example2.py
# Description: Pivot the attributes table by the specified fields

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_table = "attributes.dbf"
fields = "OwnerID"
pivot_field = "AttrTagNam"
value_field = "AttrValueS"
out_table = "C:/output/attribPivot.dbf"

# Run PivotTable
arcpy.management.PivotTable(in_table, fields, pivot_field, value_field, out_table)
```

### Example 5

```python
# Name: PivotTable_Example2.py
# Description: Pivot the attributes table by the specified fields

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_table = "attributes.dbf"
fields = "OwnerID"
pivot_field = "AttrTagNam"
value_field = "AttrValueS"
out_table = "C:/output/attribPivot.dbf"

# Run PivotTable
arcpy.management.PivotTable(in_table, fields, pivot_field, value_field, out_table)
```

---

## Points To Line (Data Management)

## Summary

Creates line features from points.

## Usage

- Line features are only written to the output if the line will contain two or more vertices.
- If a field is specified as the Line Field parameter value, line features will be constructed with only points of the unique value in the field. The field will be included in the output feature class.
- If a field is specified as the Sort Field parameter value, points will be sorted in ascending order of the field.
- You can create polygons from input points by first checking the Close Line parameter to create closed line features. Then use the output line feature class as input to the Feature To Polygon tool to create polygon features.
- The Line Construction Method parameter specifies how line features will be constructed. Line features can be created by connecting points continuously or by connecting two consecutive points as they are sorted. For example, if the input contains three points that have a Line Field value of A, and four points have a Line Field value of B, the following behaviors are expected from the options:Construct continuous line—Two output lines, one with two segments and one with three segments, will be created. If the Close Line parameter is checked, an additional segment connecting the last point with the start point of each line feature will be added to form a closed line. Construct two-point line—Five output lines, each between consecutive points with the same Line Field value, will be created. If the Close Line parameter is checked, additional line features connecting the last point with the start point for each set of input points with the same Line Field value will be added to form a closed shape.
- Construct continuous line—Two output lines, one with two segments and one with three segments, will be created. If the Close Line parameter is checked, an additional segment connecting the last point with the start point of each line feature will be added to form a closed line.
- Construct two-point line—Five output lines, each between consecutive points with the same Line Field value, will be created. If the Close Line parameter is checked, additional line features connecting the last point with the start point for each set of input points with the same Line Field value will be added to form a closed shape.
- The Attribute Source parameter allows you to specify whether or how the attributes of the input points specified by the Transfer Fields parameter will be transferred to the output lines. The Attribute Source parameter supports the following options: None—No attributes will be transferred. From both start and end points—Attributes will be transferred from the start and end points of each line. The output field names and aliases will be prefixed by START_ and END_, for example, START_FIELD1 (START_ALIAS1), END_FIELD1 (END_ALIAS1), and so on.From start point—Attributes will be transferred from the start point of each line. From end point—Attributes will be transferred from the end point of each line.
- None—No attributes will be transferred.
- From both start and end points—Attributes will be transferred from the start and end points of each line. The output field names and aliases will be prefixed by START_ and END_, for example, START_FIELD1 (START_ALIAS1), END_FIELD1 (END_ALIAS1), and so on.
- From start point—Attributes will be transferred from the start point of each line.
- From end point—Attributes will be transferred from the end point of each line.
- Use the Transfer Fields parameter to identify the fields that will be transferred from the input. Field values will be transferred according to the Attribute Source parameter value. If a value of None is specified for that parameter, no attributes will be transferred.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The point features that will be used to construct lines. | Feature Layer |
| Output Feature Class | The line feature class that will be created from the input points. | Feature Class |
| Line Field (Optional) | The field that will be used to identify unique attribute values so line features can be constructed using points of the same values.If no field is specified, lines will be constructed without using unique attribute values. This is the default. | Field |
| Sort Field (Optional) | The field that will be used to sort the order of the points.If no field is specified, points used to create output line features will be sorted in the order they are found. This is the default. | Field |
| Close Line (Optional) | Specifies whether the output line features will be closed. Checked—For a continuous line, an extra segment connecting the last point with the first point will be included to form a closed line. For two-point lines, an extra line feature connecting the last point with the first point will be included to form a closed shape.Unchecked—No extra segment or line will be created to ensure a closed line or closed shape. This is the default. | Boolean |
| Line Construction Method(Optional) | Specifies the method that will be used to construct the line features.Construct continuous line—Line features will be created by connecting points continuously. This is the default.Construct two-point line—Line features will be created by connecting two consecutive points. | String |
| Attribute Source(Optional) | Specifies how the specified attributes will be transferred. None—No attributes will be transferred. This is the default.Both start and end points—The attributes from the start and end points of the line will be transferred.Start point—The attributes from the start point of the line will be transferred.End point—The attributes from the end point of the line will be transferred. | String |
| Transfer Fields(Optional) | The fields containing values that will be transferred from the source points to the output lines. If no fields are selected, no attributes will be transferred.If the Attribute Source parameter value is specified as None, this parameter will be inactive. | Field |
| Input_Features | The point features that will be used to construct lines. | Feature Layer |
| Output_Feature_Class | The line feature class that will be created from the input points. | Feature Class |
| Line_Field(Optional) | The field that will be used to identify unique attribute values so line features can be constructed using points of the same values.If no field is specified, lines will be constructed without using unique attribute values. This is the default. | Field |
| Sort_Field(Optional) | The field that will be used to sort the order of the points.If no field is specified, points used to create output line features will be sorted in the order they are found. This is the default. | Field |
| Close_Line(Optional) | Specifies whether the output line features will be closed.CLOSE—For a continuous line, an extra segment connecting the last point with the first point will be included to form a closed line. For two-point lines, an extra line feature connecting the last point with the first point will be included to form a closed shape.NO_CLOSE—No extra segment or line will be created to ensure a closed line or closed shape. This is the default. | Boolean |
| Line_Construction_Method(Optional) | Specifies the method that will be used to construct the line features. CONTINUOUS—Line features will be created by connecting points continuously. This is the default.TWO_POINT—Line features will be created by connecting two consecutive points. | String |
| Attribute_Source(Optional) | Specifies how the specified attributes will be transferred. NONE—No attributes will be transferred. This is the default.BOTH_ENDS—The attributes from the start and end points of the line will be transferred.START—The attributes from the start point of the line will be transferred.END—The attributes from the end point of the line will be transferred. | String |
| Transfer_Fields[Transfer_Fields,...](Optional) | The fields containing values that will be transferred from the source points to the output lines. If no fields are selected, no attributes will be transferred.If the Attribute_Source parameter value is specified as NONE, this parameter will be disabled. | Field |

## Code Samples

### Example 1

```python
arcpy.management.PointsToLine(Input_Features, Output_Feature_Class, {Line_Field}, {Sort_Field}, {Close_Line}, {Line_Construction_Method}, {Attribute_Source}, {Transfer_Fields})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.PointsToLine("calibration_points.shp",
                              "C:/output/output.gdb/out_lines",
                              "ROUTE1", "MEASURE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.PointsToLine("calibration_points.shp",
                              "C:/output/output.gdb/out_lines",
                              "ROUTE1", "MEASURE")
```

### Example 4

```python
# Description: Convert point features into line features

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
inFeatures = "calibration_points.shp"
outFeatures = "C:/output/output.gdb/out_lines"
lineField = "ROUTE1"
sortField = "MEASURE"

# Run PointsToLine 
arcpy.management.PointsToLine(inFeatures, outFeatures, lineField, sortField)
```

### Example 5

```python
# Description: Convert point features into line features

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
inFeatures = "calibration_points.shp"
outFeatures = "C:/output/output.gdb/out_lines"
lineField = "ROUTE1"
sortField = "MEASURE"

# Run PointsToLine 
arcpy.management.PointsToLine(inFeatures, outFeatures, lineField, sortField)
```

### Example 6

```python
# Description: Convert point features into line features

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/points.gdb"

# Set local variables
inFeatures = "in_points"
outFeatures = "out_lines"
lineField = "lineID"
sortField = "stopID"
transFields = ["OBJECTID", "stopID"]

# Run PointsToLine 
arcpy.management.PointsToLine(inFeatures, outFeatures, lineField, sortField,
                              "NO_CLOSE", "TWO_POINT", "BOTH_ENDS", transFields)
```

### Example 7

```python
# Description: Convert point features into line features

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/points.gdb"

# Set local variables
inFeatures = "in_points"
outFeatures = "out_lines"
lineField = "lineID"
sortField = "stopID"
transFields = ["OBJECTID", "stopID"]

# Run PointsToLine 
arcpy.management.PointsToLine(inFeatures, outFeatures, lineField, sortField,
                              "NO_CLOSE", "TWO_POINT", "BOTH_ENDS", transFields)
```

---

## Polygon To Line (Data Management)

## Summary

Creates a feature class containing lines that are converted from polygon boundaries with or without considering neighboring polygons.

## Usage

- If the Identify and store polygon neighboring information check box is checked (neighbor_option is set to IDENTIFY_NEIGHBORS in Python), the polygon neighboring relationship will be analyzed. As illustrated above, the boundaries are converted to lines, taking into account crossing or shared segments; two new fields, LEFT_FID and RIGHT_FID, will be added to the output feature class and set to the feature IDs of the input polygons to the left and right of each output line. The attributes of the input features will not be maintained in the output feature class. The following scenarios help you understand the process and output in more detail:In a polygon geometry, the outer boundary is always stored in a clockwise direction. If the polygon has a hole, the hole (or inner) boundary is always stored in a counterclockwise direction. Therefore, for a polygon with no neighbors to the left side (outside) of its outer boundary and the left side (inside) of the hole boundary, the resulting lines will have a value of -1 for LEFT_FID and the polygon feature ID as the RIGHT_FID.If a polygon contains another polygon, one output line in the clockwise direction will be generated representing the shared boundary, with its LEFT_FID set to the outer polygon feature ID and the RIGHT_FID set to the inner polygon feature ID.If two polygons share a portion of their boundaries, one output line will be generated representing the shared segment. The line direction will be arbitrary; the LEFT_FID and the RIGHT_FID will be set to the left or right polygon feature IDs accordingly.If a polygon overlaps another polygon, two output lines will be generated representing each crossing boundary twice: the first line will represent the outer boundary of one of the overlapping polygons, therefore, its LEFT_FID is the feature ID of the polygon it crosses, and its RIGHT_FID will be its own polygon feature ID; the second line will be in the opposite direction, splitting the other polygon, therefore, its LEFT_FID and RIGHT_FID will be the same as the other polygon feature ID.Multiparts in input polygons are not maintained; the output lines are all single part. For parametric (true) curve input features, output lines will remain true curves, even if they are split. This does not apply to shapefile data.This option uses a tiling process to handle very large datasets for better performance and scalability. For more details, see Tiled processing of large datasets.
- In a polygon geometry, the outer boundary is always stored in a clockwise direction. If the polygon has a hole, the hole (or inner) boundary is always stored in a counterclockwise direction. Therefore, for a polygon with no neighbors to the left side (outside) of its outer boundary and the left side (inside) of the hole boundary, the resulting lines will have a value of -1 for LEFT_FID and the polygon feature ID as the RIGHT_FID.
- If a polygon contains another polygon, one output line in the clockwise direction will be generated representing the shared boundary, with its LEFT_FID set to the outer polygon feature ID and the RIGHT_FID set to the inner polygon feature ID.
- If two polygons share a portion of their boundaries, one output line will be generated representing the shared segment. The line direction will be arbitrary; the LEFT_FID and the RIGHT_FID will be set to the left or right polygon feature IDs accordingly.
- If a polygon overlaps another polygon, two output lines will be generated representing each crossing boundary twice: the first line will represent the outer boundary of one of the overlapping polygons, therefore, its LEFT_FID is the feature ID of the polygon it crosses, and its RIGHT_FID will be its own polygon feature ID; the second line will be in the opposite direction, splitting the other polygon, therefore, its LEFT_FID and RIGHT_FID will be the same as the other polygon feature ID.
- Multiparts in input polygons are not maintained; the output lines are all single part.
- If the Identify and store polygon neighboring information check box is unchecked (neighbor_option is set to IGNORE_NEIGHBORS in Python), the polygon neighboring relationship will be ignored. Each input polygon boundary will be written out as an enclosed line feature. A multipart polygon will become a multipart line in the output. The attributes of the input features will be maintained in the output feature class. A new field, ORIG_FID, will be added to the output and set to the input feature IDs of each line.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input features that must be polygon. | Feature Layer |
| Output Feature Class | The output line feature class. | Feature Class |
| Identify and store polygon neighboring information (Optional) | Specifies whether or not to identify and store polygon neighboring information.Checked— Polygon neighboring relationship will be identified and stored in the output. If different segments of a polygon share boundary with different polygons, the boundary will be split such that each uniquely shared segment will become a line with its two neighboring polygon FIDs stored in the output, as shown in the illustration. This is the default.Unchecked— Polygon neighboring relationship will be ignored; every polygon boundary will become a line feature with its original polygon feature ID stored in the output. | Boolean |
| in_features | The input features that must be polygon. | Feature Layer |
| out_feature_class | The output line feature class. | Feature Class |
| neighbor_option(Optional) | Specifies whether or not to identify and store polygon neighboring information.IDENTIFY_NEIGHBORS—Polygon neighboring relationship will be identified and stored in the output. If different segments of a polygon share boundary with different polygons, the boundary will be split such that each uniquely shared segment will become a line with its two neighboring polygon FIDs stored in the output. This is the default.IGNORE_NEIGHBORS—Polygon neighboring relationship will be ignored; every polygon boundary will become a line feature with its original polygon feature ID stored in the output. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.PolygonToLine(in_features, out_feature_class, {neighbor_option})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.PolygonToLine_management("Habitat_Analysis.gdb/vegtype", 
                               "C:/output/Output.gdb/vegtype_lines",
                               "IGNORE_NEIGHBORS")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.PolygonToLine_management("Habitat_Analysis.gdb/vegtype", 
                               "C:/output/Output.gdb/vegtype_lines",
                               "IGNORE_NEIGHBORS")
```

### Example 4

```python
# Name: PolygonToLine_Example2.py
# Description: Use PolygonToLine function to convert polygons to lines,
#              and report how many shared or overlapping boundary lines
#              were found.

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/landcovers.gdb"
 
# Create variables for the input and output feature classes
inFeatureClass = "bldgs"
outFeatureClass = "bldgs_lines"
 
# Run PolygonToLine to convert polygons to lines using default neighbor_option
arcpy.PolygonToLine_management(inFeatureClass, outFeatureClass)

# Select lines that have LEFT_FID values greater than -1
arcpy.MakeFeatureLayer_management(outFeatureClass, "selection_lyr", 
                                  "\"LEFT_FID\" > -1")
result = arcpy.GetCount_management("selection_lyr")

if result[0] == "0":
    print("No overlapping or shared boundary lines were found.")
else:
    print("{} overlapping or shared boundary lines were found.".format(result[0]))
```

### Example 5

```python
# Name: PolygonToLine_Example2.py
# Description: Use PolygonToLine function to convert polygons to lines,
#              and report how many shared or overlapping boundary lines
#              were found.

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/landcovers.gdb"
 
# Create variables for the input and output feature classes
inFeatureClass = "bldgs"
outFeatureClass = "bldgs_lines"
 
# Run PolygonToLine to convert polygons to lines using default neighbor_option
arcpy.PolygonToLine_management(inFeatureClass, outFeatureClass)

# Select lines that have LEFT_FID values greater than -1
arcpy.MakeFeatureLayer_management(outFeatureClass, "selection_lyr", 
                                  "\"LEFT_FID\" > -1")
result = arcpy.GetCount_management("selection_lyr")

if result[0] == "0":
    print("No overlapping or shared boundary lines were found.")
else:
    print("{} overlapping or shared boundary lines were found.".format(result[0]))
```

---

## Post Version (Data Management)

## Summary

Posting is the process of applying the current edit session to the reconciled target version during version geodatabase editing. Before a version can be posted, it must be reconciled with a target version and all conflicts must be resolved.

## Usage

- Posting synchronizes the edit version with the reconciled version and saves the data.
- Posting can't be undone since you are applying changes to a version that you are not currently editing.
- If the reconciled version is modified between reconciling and posting, you will be notified to reconcile again before posting.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The ArcSDE geodatabase containing the edit version to be posted. | Workspace |
| Version Name | Name of the edit version to be posted to the target version. | String |
| in_workspace | The ArcSDE geodatabase containing the edit version to be posted. The default is to use the workspace defined in the environment. | Workspace |
| version_name | Name of the edit version to be posted to the target version. | String |

## Code Samples

### Example 1

```python
arcpy.management.PostVersion(in_workspace, version_name)
```

### Example 2

```python
import arcgisscripting
gp = arcgisscripting.create()
 
gp.postversion( "Database Connections\Connection to workspace.sde", "housing")
```

### Example 3

```python
import arcgisscripting
gp = arcgisscripting.create()
 
gp.postversion( "Database Connections\Connection to workspace.sde", "housing")
```

---

## Project Raster (Data Management)

## Summary

Transforms a raster dataset from one coordinate system to another.

## Usage

- The coordinate system defines how the raster data will be projected. You can use the same coordinate system for the data so it will all be in the same projection.
- A raster dataset will be projected into a new spatial reference using a bilinear interpolation approximation method that projects pixels on a coarse mesh grid and uses bilinear interpolation between the pixels.
- The error range for this tool is less than half a pixel.
- To apply the transformation without creating a file, use the Warp tool.
- You can use an existing spatial reference, import one from another dataset, or create one.
- This tool can only output a square pixel size.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- The Nearest option, which performs a nearest neighbor assignment, is the fastest of the four interpolation techniques. It is primarily used for categorical data, such as a land-use classification, because it will not change the pixel values. It should not be used for continuous data, such as elevation surfaces.
- The Bilinear option uses bilinear interpolation to determine the new value of a pixel based on a weighted distance average of the four nearest surrounding pixels. The Cubic option uses cubic convolution to determine the new pixel value by fitting a smooth curve through the surrounding points. These are the most appropriate choices for continuous data but may cause some smoothing. Cubic convolution may result in the output raster containing values outside the range of the input raster. Don't use either of these options with categorical data because different pixel values may be introduced, which may be undesirable.
- The cells of the raster dataset will be square and of equal area in map coordinate space, although the shape and area a cell represents on the surface of the earth will not be constant across a raster. This is because no map projection can preserve both shape and area simultaneously. The area represented by the cells will vary across the raster. Therefore, the cell size and the number of rows and columns in the output raster may change.
- Always specify an output cell size, unless you are projecting between spherical (latitude–longitude) coordinates and a planar coordinate system and don't know the appropriate cell size.
- The default cell size of the output raster is determined from the projected cell size at the center of the output raster. This is typically the intersection of the central meridian and latitude of true scale and is the area of least distortion. The boundary of the input raster is projected, and the minimum and maximum extents determine the size of the output raster. Each cell is projected back to the input coordinate system to determine the cell's value.
- The geographic transformation is an optional parameter when the input and output coordinate systems have the same datum. If the input and output datum are different, a geographic transformation must be specified.
- The registration point allows you to specify the origin point for anchoring the output cells. All output cells will be an interval of the cell size away from this point. This point does not need to be a corner coordinate or fall within the raster dataset. If a snap raster is set in the Environment settings, the registration point will be ignored.
- CLARKE 1866 is the default spheroid if it is not inherent to the projection (such as NEWZEALAND_GRID) or another is specified with the SPHEROID subcommand.
- The snap raster setting will take priority over the registration point if both are set.
- To perform a vertical transformation, check the optional Vertical parameter on the dialog box. By default, the Vertical parameter is only available when the input and output coordinate systems have a vertical coordinate system (VCS), and the input feature class coordinates have z-values. Also, additional data (coordinate systems data) setup must be installed on the system. When you provide the output coordinate system, you can specify both the geographic or projected coordinate system and a VCS. If the input and output VCS are different, an appropriate vertical and an optional geographic (datum) transformation are available. If a transformation should be applied in the opposite direction to its definition, choose the entry with the tilde (~) in front of the name.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset that will be transformed into a new projection. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The raster dataset with the new projection that will be created.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| Output Coordinate System | The coordinate system of the new raster dataset. | Coordinate System |
| Resampling Technique(Optional) | Specifies the resampling technique that will be used. The default is Nearest.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered.Nearest neighbor— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.Bilinear interpolation— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.Cubic convolution—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.Majority resampling—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |
| Output Cell Size(Optional) | The cell size of the new raster using an existing raster dataset or by specifying its width (x) and height (y). | Cell Size XY |
| Geographic Transformation(Optional) | The geographic transformation when projecting from one geographic system or datum to another. A transformation is required when the input and output coordinate systems have different datums. | String |
| Registration Point(Optional) | The lower left point for anchoring the output cells. This point does not need to be a corner coordinate or fall within the raster dataset. The Snap Raster environment setting will take priority over the Registration Point parameter. To set the registration point, ensure that the Snap Raster environment is not set. | Point |
| Input Coordinate System(Optional) | The coordinate system of the input raster dataset.This parameter is only active when the input has an unknown coordinate system. When this is the case, specify a current coordinate system for the raster layer. | Coordinate System |
| Vertical(Optional) | Specifies whether a vertical transformation will be applied.This option is active when the input and output coordinate systems have a vertical coordinate system and the input raster's coordinates have z-values.When this parameter is checked, the Geographic Transformation parameter can include ellipsoidal transformations and transformations between vertical datums. For example, ~NAD_1983_To_NAVD88_CONUS_GEOID12B_Height + NAD_1983_To_WGS_1984_1 transforms geometry vertices that are defined on NAD 1983 datum with NAVD 1988 heights into vertices on the WGS84 ellipsoid (with z-values representing ellipsoidal heights). The tilde (~) indicates reversed direction of transformation.Unchecked—No vertical transformation will be applied. The z-values of geometry coordinates will be ignored and the z-values will not be modified. This is the default.Checked—The transformation specified in the Geographic Transformation parameter will be applied. The tool transforms x-, y-, and z-values of geometry coordinates.Many vertical transformations require additional data files that must be installed using the ArcGIS Coordinate Systems Data installation package. | Boolean |
| in_raster | The raster dataset that will be transformed into a new projection. | Mosaic Layer; Raster Layer |
| out_raster | The raster dataset with the new projection that will be created.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| out_coor_system | The coordinate system of the new raster dataset.Valid values for this parameter are the following:An existing feature class, feature dataset, or raster dataset (basically anything with a coordinate system)An ArcPy SpatialReference object | Coordinate System |
| resampling_type(Optional) | Specifies the resampling technique that will be used. The default is Nearest.NEAREST— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.BILINEAR— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.CUBIC—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.MAJORITY—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered. | String |
| cell_size(Optional) | The cell size of the new raster using an existing raster dataset or by specifying its width (x) and height (y). | Cell Size XY |
| geographic_transform[geographic_transform,...](Optional) | The geographic transformation when projecting from one geographic system or datum to another. A transformation is required when the input and output coordinate systems have different datums. | String |
| Registration_Point(Optional) | The lower left point for anchoring the output cells. This point does not need to be a corner coordinate or fall within the raster dataset. The Snap Raster environment setting will take priority over the Registration Point parameter. To set the registration point, ensure that the Snap Raster environment is not set. | Point |
| in_coor_system(Optional) | The coordinate system of the input raster dataset.This parameter is only enabled when the input has an unknown coordinate system. When this is the case, specify a current coordinate system for the raster layer. | Coordinate System |
| vertical(Optional) | Specifies whether a vertical transformation will be applied.This parameter is enabled when the input and output coordinate systems have a vertical coordinate system and the input feature class coordinates have z-values. When the VERTICAL keyword is used, the geographic_transform parameter can include ellipsoidal transformations and transformations between vertical datums. For example, “~NAD_1983_To_NAVD88_CONUS_GEOID12B_Height + NAD_1983_To_WGS_1984_1” transforms geometry vertices that are defined on NAD 1983 datum with NAVD 1988 heights into vertices on the WGS84 ellipsoid (with z-values representing ellipsoidal heights). The tilde (~) indicates reversed direction of transformation.NO_VERTICAL—No vertical transformation will be applied. The z-values of geometry coordinates will be ignored and the z-values will not be modified. This is the default.VERTICAL—The transformation specified in the geographic_transform parameter will be applied. The tool transforms x-, y-, and z-values of geometry coordinates.Many vertical transformations require additional data files that must be installed using the ArcGIS Coordinate Systems Data installation package. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ProjectRaster(in_raster, out_raster, out_coor_system, {resampling_type}, {cell_size}, {geographic_transform}, {Registration_Point}, {in_coor_system}, {vertical})
```

### Example 2

```python
import arcpy
from arcpy import env
arcpy.ProjectRaster_management("c:/data/image.tif", "c:/output/reproject.tif",\
                               "World_Mercator.prj", "BILINEAR", "5",\
                               "NAD_1983_To_WGS_1984_5", "#", "#")
```

### Example 3

```python
import arcpy
from arcpy import env
arcpy.ProjectRaster_management("c:/data/image.tif", "c:/output/reproject.tif",\
                               "World_Mercator.prj", "BILINEAR", "5",\
                               "NAD_1983_To_WGS_1984_5", "#", "#")
```

### Example 4

```python
##====================================
##Project Raster
##Usage: ProjectRaster_management in_raster out_raster out_coor_system {NEAREST | BILINEAR 
##                                | CUBIC | MAJORITY} {cell_size} {geographic_transform;
##                                geographic_transform...} {Registration_Point} {in_coor_system}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Reproject a TIFF image with Datumn transfer
arcpy.ProjectRaster_management("image.tif", "reproject.tif", "World_Mercator.prj",\
                               "BILINEAR", "5", "NAD_1983_To_WGS_1984_5", "#", "#")

##Reproject a TIFF image that does not have a spatial reference
##Set snapping point to the top left of the original image
snapping_pnt = "1942602 304176"

arcpy.ProjectRaster_management("nosr.tif", "project.tif", "World_Mercator.prj", "BILINEAR",\
                               "5", "NAD_1983_To_WGS_1984_6", snapping_pnt,\
                               "NAD_1983_StatePlane_Washington_North.prj")
```

### Example 5

```python
##====================================
##Project Raster
##Usage: ProjectRaster_management in_raster out_raster out_coor_system {NEAREST | BILINEAR 
##                                | CUBIC | MAJORITY} {cell_size} {geographic_transform;
##                                geographic_transform...} {Registration_Point} {in_coor_system}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Reproject a TIFF image with Datumn transfer
arcpy.ProjectRaster_management("image.tif", "reproject.tif", "World_Mercator.prj",\
                               "BILINEAR", "5", "NAD_1983_To_WGS_1984_5", "#", "#")

##Reproject a TIFF image that does not have a spatial reference
##Set snapping point to the top left of the original image
snapping_pnt = "1942602 304176"

arcpy.ProjectRaster_management("nosr.tif", "project.tif", "World_Mercator.prj", "BILINEAR",\
                               "5", "NAD_1983_To_WGS_1984_6", snapping_pnt,\
                               "NAD_1983_StatePlane_Washington_North.prj")
```

---

## Project (Data Management)

## Summary

Projects spatial data from one coordinate system to another.

## Usage

- If the input feature class or dataset has an unknown or unspecified coordinate system, use the Input Coordinate System parameter to specify one. This parameter allows you to specify the data's coordinate system without having to modify the input data. Use the Define Projection tool to permanently assign a coordinate system to the dataset.
- Coverages, VPF coverages, raster datasets, and raster catalogs are not supported as input to this tool. Use the Project Raster tool to project raster datasets.
- The Geographic Transformation parameter is optional. When no geographic or datum transformation is required, no drop-down list will appear on the parameter, and it is left blank. When a transformation is required, a drop-down list will be generated based on the input and output datums, and a default transformation will be applied.For example, a geographic transformation is not required when projecting from GCS_North_American_1983 to NAD_1983_UTM_Zone_12N because both the input and output coordinate systems have the NAD_1983 datum. However, projecting from GCS_North_American_1983 to WGS_1984_UTM_Zone_12N requires a geographic transformation because the input coordinate system uses the NAD_1983 datum, while the output coordinate system uses the WGS_1984 datum.Tip:Transformations are bidirectional. For example, if you're converting data from WGS84 to NAD 1927, you can choose the NAD_1927_to_WGS_1984_3 transformation, and the tool will apply it correctly.All transformations in ArcGIS Pro can be found in the Geographic and vertical transformation tables.
- For example, a geographic transformation is not required when projecting from GCS_North_American_1983 to NAD_1983_UTM_Zone_12N because both the input and output coordinate systems have the NAD_1983 datum. However, projecting from GCS_North_American_1983 to WGS_1984_UTM_Zone_12N requires a geographic transformation because the input coordinate system uses the NAD_1983 datum, while the output coordinate system uses the WGS_1984 datum.Tip:Transformations are bidirectional. For example, if you're converting data from WGS84 to NAD 1927, you can choose the NAD_1927_to_WGS_1984_3 transformation, and the tool will apply it correctly.
- All transformations in ArcGIS Pro can be found in the Geographic and vertical transformation tables.
- When projecting the complex data types listed below, certain operations must be performed on the resulting data: A feature dataset containing a network dataset: the network dataset must be rebuiltA feature dataset containing a topology: the topology should be validated again
- A feature dataset containing a network dataset: the network dataset must be rebuilt
- A feature dataset containing a topology: the topology should be validated again
- If the input participates in relationship classes (as with feature-linked annotation), the relationship class will be transferred to the output. The exception is participating stand-alone tables.
- Depending on the input feature's coordinates and the horizon (valid extent) of the output coordinate system, multipoints, lines, and polygons may be clipped or split into more than one part when projecting them. Features that fall completely outside the horizon will be written to the output with a null shape. These can be deleted using the Repair Geometry tool.
- It is recommended that all inputs for a project be projected to the same coordinate system before performing the analysis. When all inputs are in the same coordinate system, you will avoid possible performance and functional issues caused by projecting data on the fly.
- In rare cases, projecting feature geometry may cause features to be transformed in a way that makes them invalid. Use the Check Geometry tool to detect invalid geometry and the Repair Geometry tool to repair any geometry issues.
- Feature classes participating in a geometric network cannot be projected independently; the entire feature dataset containing the network must be projected.
- Many geoprocessing tools honor the Output Coordinate System environment, and in many workflows, you can use this environment instead of the Project tool. For example, the Union tool honors the Output Coordinate System environment, which means you can union several feature classes together, all of which are in a different coordinate system, and write the output to a feature class in a different coordinate system.Learn more about geoprocessing environments
- Selection and definition queries on layers are supported by this tool. Only selected features in the layer will be projected when the selection or query is defined.
- When a feature class in a feature dataset is used as input, the output cannot be written to the same feature dataset. This is because feature classes in a feature dataset must all have the same coordinate system. In this case, the output feature class will be written to the geodatabase containing the feature dataset.
- When the Preserve Shape parameter is checked, the tool creates features that accurately represent their projected location. The tool adds extra vertices to the feature when projecting. The extra vertices preserve the projected shape of the feature. This parameter is useful when a line or polygon has few vertices. If the parameter is not checked, the existing vertices will be projected, and an output feature may not be accurately located in the new projection. The Maximum Offset Deviation parameter specifies how many vertices are added. The parameter value defines the maximum distance a projected feature can be offset from its exact projected location. The tool adds more vertices if the value is small. Use a value that suits your needs. For example, if the output is for general small-scale cartographic display, a large deviation may be acceptable. If the output is for large-scale, small-area analysis, a smaller deviation may be better.
- To perform a vertical transformation, check the Vertical parameter. By default, the Vertical parameter is inactive and becomes active when the input and output coordinate systems have a vertical coordinate system (VCS) and the input feature class coordinates have z-values. Also, additional data (coordinate systems data) setup must be installed on the system. When you select the output coordinate system, you can choose the geographic or projected coordinate system and a VCS. If the input and output VCS are different, an appropriate vertical and an optional geographic (datum) transformation are available. If a transformation should be applied in the opposite direction to its definition, choose the entry with the tilde (~) in front of the name.
- Parcel fabrics are projected by projecting the feature dataset containing the parcel fabric. Individual feature classes controlled by the parcel fabric cannot be projected separately. Linear units, area units, and point coordinates will be updated to match the units and coordinates of the target spatial reference.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset or Feature Class | The feature class, feature layer, feature dataset, scene layer, scene layer package, or OGC Geopackage to be projected. | Feature Layer; Feature Dataset; Scene Layer; Building Scene Layer; File |
| Output Dataset or Feature Class | The output dataset to which the results will be written. | Feature Class; Feature Dataset; File |
| Output Coordinate System | The coordinate system to which the input data will be projected. | Coordinate System |
| Geographic Transformation(Optional) | This method can be used to convert data between two geographic coordinate systems or datums. This optional parameter may be required if the input and output coordinate systems have different datums.Tip:The tool automatically applies a default transformation. You can choose a different transformation from the drop-down list. Transformations are bidirectional. For example, if you're converting data from WGS84 to NAD 1927, you can choose the NAD_1927_to_WGS_1984_3 transformation, and the tool will apply it correctly.The parameter provides a drop-down list of valid transformation methods. See the usage tips for additional information about how to choose one or more appropriate transformations. | String |
| Input Coordinate System(Optional) | The coordinate system of the input feature class or dataset. This parameter becomes active when the input has an unknown or unspecified coordinate system. This allows you to specify the data's coordinate system without having to modify the input data (which may not be possible if the input is in read-only format). | Coordinate System |
| Preserve Shape (Optional) | Specifies whether extra vertices will be added to the output lines or polygons so their projected shape is more accurate.Unchecked—Extra vertices will not be added to the output lines or polygons. This is the default.Checked—Extra vertices will be added to the output lines or polygons as needed, so their projected shape is more accurate. | Boolean |
| Maximum Offset Deviation (Optional) | The distance a projected line or polygon can deviate from its exact projected location when the Preserve Shape parameter is checked. The default is 100 times the x,y tolerance of the spatial reference of the output dataset. | Linear Unit |
| Vertical(Optional) | Specifies whether a vertical transformation will be applied.This parameter is only active when the input and output coordinate systems have a vertical coordinate system and the input feature class coordinates have z-values. Also, many vertical transformations require additional data files that must be installed using the ArcGIS Coordinate Systems Data installation package.When this parameter is checked, the Geographic Transformation parameter can include ellipsoidal transformations and transformations between vertical datums. For example, ~NAD_1983_To_NAVD88_CONUS_GEOID12B_Height + NAD_1983_To_WGS_1984_1 transforms geometry vertices that are defined on NAD 1983 datum with NAVD 1988 heights into vertices on the WGS84 ellipsoid (with z-values representing ellipsoidal heights). The tilde (~) indicates the reversed direction of transformation. This parameter is not compatible with the Preserve Shape parameter.Unchecked—A vertical transformation will not be applied. The z-values of geometry coordinates will be ignored and the z-values will not be modified. This is the default.Checked—The transformation specified in the Geographic Transformation parameter will be applied. The x-, y-, and z-values of geometry coordinates will be transformed. | Boolean |
| in_dataset | The feature class, feature layer, feature dataset, scene layer, scene layer package, or OGC Geopackage to be projected. | Feature Layer; Feature Dataset; Scene Layer; Building Scene Layer; File |
| out_dataset | The output dataset to which the results will be written. | Feature Class; Feature Dataset; File |
| out_coor_system | Valid values are a SpatialReference object, a file with a .prj extension, or a string representation of a coordinate system. | Coordinate System |
| transform_method[transform_method,...](Optional) | This method can be used to convert data between two geographic coordinate systems or datums. This optional parameter may be required if the input and output coordinate systems have different datums.To get a list of valid transformations, use the arcpy.ListTransformations method. The most appropriate transformation is usually the first one in the returned list. The list is sorted by amount of overlap of the data versus the areas of use of the transformations. If two or more transformations have the same amount of overlap with the data, the transformation accuracy values are used as a secondary sort parameter. Tip:Transformations are bidirectional. For example, if you're converting data from WGS84 to NAD 1927, you can choose the NAD_1927_to_WGS_1984_3 transformation, and the tool will apply it correctly. If no transformation is provided, a default transformation is used. This default transformation is suitable for general mapping applications but may not be suitable for applications that require precise locational accuracy. | String |
| in_coor_system(Optional) | The coordinate system of the input feature class or dataset. When the input has an unknown or unspecified coordinate system, you can specify the data's coordinate system without having to modify the input data (which may not be possible if the input is in read-only format). | Coordinate System |
| preserve_shape(Optional) | Specifies whether extra vertices will be added to the output lines or polygons so their projected shape is more accurate.NO_PRESERVE_SHAPE—Extra vertices will not be added to the output lines or polygons. This is the default.PRESERVE_SHAPE—Extra vertices will be added to the output lines or polygons as needed, so their projected shape is more accurate. | Boolean |
| max_deviation(Optional) | The distance a projected line or polygon can deviate from its exact projected location when the preserve_shape parameter is set to PRESERVE_SHAPE. The default is 100 times the x,y tolerance of the spatial reference of the output dataset. | Linear Unit |
| vertical(Optional) | Specifies whether a vertical transformation will be applied.This parameter is only enabled when the input and output coordinate systems have a vertical coordinate system and the input feature class coordinates have z-values. Also, many vertical transformations require additional data files that must be installed using the ArcGIS Coordinate Systems Data installation package. This parameter is not compatible with the preserve_shape parameter.NO_VERTICAL—A vertical transformation will not be applied. The z-values of geometry coordinates will be ignored and the z-values will not be modified. This is the default.VERTICAL—The transformation specified in the transform_method parameter will be applied. The x-, y-, and z-values of geometry coordinates will be transformed. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.Project(in_dataset, out_dataset, out_coor_system, {transform_method}, {in_coor_system}, {preserve_shape}, {max_deviation}, {vertical})
```

### Example 2

```python
import arcpy

# input data is in NAD 1983 UTM Zone 11N coordinate system
input_features = r"C:/data/Redlands.shp"

# output data
output_feature_class = r"C:/data/Redlands_Project.shp"

# create a spatial reference object for the output coordinate system
out_coordinate_system = arcpy.SpatialReference('NAD 1983 StatePlane California V FIPS 0405 (US Feet)')

# run the tool
arcpy.Project_management(input_features, output_feature_class, out_coordinate_system)
```

### Example 3

```python
import arcpy

# input data is in NAD 1983 UTM Zone 11N coordinate system
input_features = r"C:/data/Redlands.shp"

# output data
output_feature_class = r"C:/data/Redlands_Project.shp"

# create a spatial reference object for the output coordinate system
out_coordinate_system = arcpy.SpatialReference('NAD 1983 StatePlane California V FIPS 0405 (US Feet)')

# run the tool
arcpy.Project_management(input_features, output_feature_class, out_coordinate_system)
```

### Example 4

```python
# Name: Project_Example2.py

# Description: Project all feature classes in a geodatabase
# Requirements: os module

# Import system modules
import arcpy
import os

# Set environment settings
arcpy.env.workspace = "C:/data/Redlands.gdb"
arcpy.env.overwriteOutput = True

# Set local variables
outWorkspace = "C:/data/Redlands_utm11.gdb"

try:
    # Use ListFeatureClasses to generate a list of inputs 
    for infc in arcpy.ListFeatureClasses():
    
        # Determine if the input has a defined coordinate system, can't project it if it does not
        dsc = arcpy.Describe(infc)
    
        if dsc.spatialReference.Name == "Unknown":
            print('skipped this fc due to undefined coordinate system: ' + infc)
        else:
            # Determine the new output feature class path and name
            outfc = os.path.join(outWorkspace, infc)
            
            # Set output coordinate system
            outCS = arcpy.SpatialReference('NAD 1983 UTM Zone 11N')
            
            # run project tool
            arcpy.Project_management(infc, outfc, outCS)
            
            # check messages
            print(arcpy.GetMessages())
            
except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

### Example 5

```python
# Name: Project_Example2.py

# Description: Project all feature classes in a geodatabase
# Requirements: os module

# Import system modules
import arcpy
import os

# Set environment settings
arcpy.env.workspace = "C:/data/Redlands.gdb"
arcpy.env.overwriteOutput = True

# Set local variables
outWorkspace = "C:/data/Redlands_utm11.gdb"

try:
    # Use ListFeatureClasses to generate a list of inputs 
    for infc in arcpy.ListFeatureClasses():
    
        # Determine if the input has a defined coordinate system, can't project it if it does not
        dsc = arcpy.Describe(infc)
    
        if dsc.spatialReference.Name == "Unknown":
            print('skipped this fc due to undefined coordinate system: ' + infc)
        else:
            # Determine the new output feature class path and name
            outfc = os.path.join(outWorkspace, infc)
            
            # Set output coordinate system
            outCS = arcpy.SpatialReference('NAD 1983 UTM Zone 11N')
            
            # run project tool
            arcpy.Project_management(infc, outfc, outCS)
            
            # check messages
            print(arcpy.GetMessages())
            
except arcpy.ExecuteError:
    print(arcpy.GetMessages(2))
    
except Exception as ex:
    print(ex.args[0])
```

---

## Raster Compare (Data Management)

## Summary

Compares the properties of two raster datasets or two mosaic datasets.

## Usage

- The tool returns messages showing the comparison result.
- The parameter and attribute tolerances allow the comparisons to have a specified amount of deviation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Base Raster | The first raster or mosaic dataset to compare. | Raster Layer; Mosaic Layer |
| Input Test Raster | The second raster or mosaic dataset to compare with the first. | Raster Layer; Mosaic Layer |
| Compare Type(Optional) | Specifies the type of rasters that will be compared.Raster dataset—Two raster datasets will be compared. Geodatabase raster dataset—Two raster datasets in a geodatabase will be compared. Mosaic dataset—Two mosaic datasets will be compared. | String |
| Ignore Options(Optional) | Specifies the properties that will be ignored in the comparison.Band count—The number of bands will be ignored.Extent—The extent will be ignored.Columns and rows—The number of columns and rows will be ignored.Pixel type—The pixel type will be ignored.NoData—The NoData value will be ignored.Spatial reference—The spatial reference system will be ignored.Pixel value—The pixel values will be ignored.Colormap—Existing color maps will be ignored.Raster attribute table—Existing attribute tables will be ignored.Statistics—Statistics will be ignored.Metadata—Metadata will be ignored.Pyramids exist—Existing pyramids will be ignored.Compression type—The compression type will be ignored.Data source type—The data source type will be ignored. | String |
| Continue Comparison(Optional) | Specifies whether the comparison will stop if a mismatch is encountered.Unchecked—The comparison will stop if a mismatch is encountered. This is the default.Checked—The comparison will continue if a mismatch is encountered. | Boolean |
| Output Compare File(Optional) | A text file containing the comparison results. | File |
| Parameter Tolerance(Optional) | The tolerances that determine the range in which values are considered equal. The same tolerance can be applied to all parameters, or different tolerances can be applied to individual parameters. The tolerance type can be either a value or a fraction. If the tolerance type is a fraction, the tolerance for each pixel will be different since each pixel has a different value. For example, if a tolerance fraction is set to 0.5, the tolerance will be calculated as follows: If a pixel has a value of 0.2, the tolerance will be 0.1, since 0.5 * 0.2 = 0.1.If a pixel has a value of 3, the tolerance will be 1.5, since 0.5 * 3 = 1.5. | Value Table |
| Attribute Tolerance(Optional) | The fields that will be compared to determine if they are within a tolerance. The tolerance value is a value in the units of the attribute. | Value Table |
| Omit Fields(Optional) | The field or fields that will be omitted during comparison. | String |
| in_base_raster | The first raster or mosaic dataset to compare. | Raster Layer; Mosaic Layer |
| in_test_raster | The second raster or mosaic dataset to compare with the first. | Raster Layer; Mosaic Layer |
| compare_type(Optional) | Specifies the type of rasters that will be compared.RASTER_DATASET—Two raster datasets will be compared. GDB_RASTER_DATASET—Two raster datasets in a geodatabase will be compared. MOSAIC_DATASET—Two mosaic datasets will be compared. | String |
| ignore_option[ignore_option,...](Optional) | Specifies the properties that will be ignored in the comparison.BandCount—The number of bands will be ignored.Extent—The extent will be ignored.Columns And Rows—The number of columns and rows will be ignored.Pixel Type—The pixel type will be ignored.NoData—The NoData value will be ignored.Spatial Reference—The spatial reference system will be ignored.Pixel Value—The pixel values will be ignored.Colormap—Existing color maps will be ignored.Raster Attribute Table—Existing attribute tables will be ignored.Statistics—Statistics will be ignored.Metadata—Metadata will be ignored.Pyramids Exist—Existing pyramids will be ignored.Compression Type—The compression type will be ignored.Data Source Type—The data source type will be ignored. | String |
| continue_compare(Optional) | Specifies whether the comparison will stop if a mismatch is encountered.NO_CONTINUE_COMPARE—The comparison will stop if a mismatch is encountered. This is the default.CONTINUE_COMPARE—The comparison will continue if a mismatch is encountered. | Boolean |
| out_compare_file(Optional) | A text file containing the comparison results. | File |
| parameter_tolerances[[Parameter, Tolerance, Type],...](Optional) | The tolerances that determine the range in which values are considered equal. The same tolerance can be applied to all parameters, or different tolerances can be applied to individual parameters. The tolerance type can be either a value or a fraction. If the tolerance type is a fraction, the tolerance for each pixel will be different since each pixel has a different value. For example, if a tolerance fraction is set to 0.5, the tolerance will be calculated as follows: If a pixel has a value of 0.2, the tolerance will be 0.1, since 0.5 * 0.2 = 0.1.If a pixel has a value of 3, the tolerance will be 1.5, since 0.5 * 3 = 1.5. | Value Table |
| attribute_tolerances[[Field, Tolerance],...](Optional) | The fields that will be compared to determine if they are within a tolerance. The tolerance value is a value in the units of the attribute. | Value Table |
| omit_field[omit_field,...](Optional) | The field or fields that will be omitted during comparison. | String |

## Code Samples

### Example 1

```python
arcpy.management.RasterCompare(in_base_raster, in_test_raster, {compare_type}, {ignore_option}, {continue_compare}, {out_compare_file}, {parameter_tolerances}, {attribute_tolerances}, {omit_field})
```

### Example 2

```python
import arcpy
RasterCompare_management("C:/workspace/image1.tif","C:/workspace/image2.tif",\
                         "RASTER_DATASET","'Pyramids Exist'",\
                         "CONTINUE_COMPARE","C:/workspace/compare01.txt",\
                         "Pixel_Value 1 Value","Count 5","OID")
```

### Example 3

```python
import arcpy
RasterCompare_management("C:/workspace/image1.tif","C:/workspace/image2.tif",\
                         "RASTER_DATASET","'Pyramids Exist'",\
                         "CONTINUE_COMPARE","C:/workspace/compare01.txt",\
                         "Pixel_Value 1 Value","Count 5","OID")
```

### Example 4

```python
##====================================
##Raster Compare
##Usage: RasterCompare_management in_base_raster in_test_raster {RASTER_DATASET |
##                                GDB_RASTER_DATASET | GDB_RASTER_CATALOG |
##                                MOSAIC_DATASET} {ignore_option;ignore_option...}
##                                {NO_CONTINUE_COMPARE | CONTINUE_COMPARE} 
##                                {out_compare_file} {Parameter {Tolerance} {Type};
##                                Parameter {Tolerance} {Type}...} {Field {Tolerance};
##                                Field {Tolerance}...} {omit_field;omit_field...} 
    
    
try:
    import arcpy
    
    arcpy.env.workspace = "c:/workspace"
    
    ##Compare two Raster dataset
    arcpy.RasterCompare_management("raster_base.tif","raster_test.tif","RASTER_DATASET",\
                                   "","CONTINUE_COMPARE","compareresult.txt","","","")
    
    ##Compare two Raster Catalog with ignore options
    arcpy.RasterCompare_management("fgdb.gdb/rc_base","fgdb.gdb/rc_test","RASTER_CATALOG",\
                                   "IsManaged;Extent","CONTINUE_COMPARE","compareresult2.txt",\
                                   "","","DATE")
    
    ##Compare two Mosaic Dataset with torelance
    arcpy.RasterCompare_management("fgdb.gdb/md_base","fgdb.gdb/md_test","MOSAIC_DATASET",\
                                   "IsEmbedded;Seamline","CONTINUE_COMPARE","compareresult3.txt",\
                                   "All 0.00001 Fraction","HighPS 0.0001;LowPS 0.0001",\
                                   "ItemTS;UriHash")
    
except:
    print "Raster Compare exsample failed."
    print arcpy.GetMessages()
```

### Example 5

```python
##====================================
##Raster Compare
##Usage: RasterCompare_management in_base_raster in_test_raster {RASTER_DATASET |
##                                GDB_RASTER_DATASET | GDB_RASTER_CATALOG |
##                                MOSAIC_DATASET} {ignore_option;ignore_option...}
##                                {NO_CONTINUE_COMPARE | CONTINUE_COMPARE} 
##                                {out_compare_file} {Parameter {Tolerance} {Type};
##                                Parameter {Tolerance} {Type}...} {Field {Tolerance};
##                                Field {Tolerance}...} {omit_field;omit_field...} 
    
    
try:
    import arcpy
    
    arcpy.env.workspace = "c:/workspace"
    
    ##Compare two Raster dataset
    arcpy.RasterCompare_management("raster_base.tif","raster_test.tif","RASTER_DATASET",\
                                   "","CONTINUE_COMPARE","compareresult.txt","","","")
    
    ##Compare two Raster Catalog with ignore options
    arcpy.RasterCompare_management("fgdb.gdb/rc_base","fgdb.gdb/rc_test","RASTER_CATALOG",\
                                   "IsManaged;Extent","CONTINUE_COMPARE","compareresult2.txt",\
                                   "","","DATE")
    
    ##Compare two Mosaic Dataset with torelance
    arcpy.RasterCompare_management("fgdb.gdb/md_base","fgdb.gdb/md_test","MOSAIC_DATASET",\
                                   "IsEmbedded;Seamline","CONTINUE_COMPARE","compareresult3.txt",\
                                   "All 0.00001 Fraction","HighPS 0.0001;LowPS 0.0001",\
                                   "ItemTS;UriHash")
    
except:
    print "Raster Compare exsample failed."
    print arcpy.GetMessages()
```

---

## Raster To DTED (Data Management)

## Summary

Splits a raster dataset into separate files based on the DTED tiling structure.

## Usage

- There are three levels of the DTED tiling scheme available: DTED level 0, DTED level 1, and DTED level 2.
- The input can only be a single band raster dataset.
- The output spatial reference will be GCS_WGS84. Each tile's extent is one degree in each direction, plus a half pixel on each edge so adjacent tiles have one column and row of overlap. The output pixel size is dictated by the DTED level, and the data is converted and stored as signed, 16-bit integers.
- The DTED format is intended to be used with one band data that represents elevation, so this tool cannot be used for multiband images.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | Select a single band raster dataset that represents elevation. | Raster Layer |
| Output Folder | Select a destination where the folder structure and DTED files will be created. | Folder |
| DTED Level | Select an appropriate level based on the resolution of your elevation data.Level 0— 900 mLevel 1— 90 mLevel 2—30 m | String |
| Resampling Technique(Optional) | Choose an appropriate technique based on the type of data you have.Nearest—The fastest resampling method, and it minimizes changes to pixel values. Suitable for discrete data, such as land cover.Bilinear—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.Cubic—Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |
| in_raster | Select a single band raster dataset that represents elevation. | Raster Layer |
| out_folder | Select a destination where the folder structure and DTED files will be created. | Folder |
| dted_level | Select an appropriate level based on the resolution of your elevation data.DTED_0— 900 mDTED_1— 90 mDTED_2—30 m | String |
| resampling_type(Optional) | Choose an appropriate technique based on the type of data you have.NEAREST—The fastest resampling method, and it minimizes changes to pixel values. Suitable for discrete data, such as land cover.BILINEAR—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.CUBIC—Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |

## Code Samples

### Example 1

```python
arcpy.management.RasterToDTED(in_raster, out_folder, dted_level, {resampling_type})
```

### Example 2

```python
import arcpy
RasterToDTED_management("C:/workspace/image1.img","C:/workspace/outputDTED",
                        "DTED_0","BILINEAR")
```

### Example 3

```python
import arcpy
RasterToDTED_management("C:/workspace/image1.img","C:/workspace/outputDTED",
                        "DTED_0","BILINEAR")
```

### Example 4

```python
import arcpy
RasterToDTED_management("C:/workspace/image1.img","C:/workspace/outputDTED",
                        "DTED_0","BILINEAR")
```

### Example 5

```python
import arcpy
RasterToDTED_management("C:/workspace/image1.img","C:/workspace/outputDTED",
                        "DTED_0","BILINEAR")
```

---

## Re-export Unacknowledged Messages (Data Management)

## Summary

Creates an output delta file containing unacknowledged replica updates from a one-way or two-way replica geodatabase.

## Usage

- Use this tool when synchronizing a replica while disconnected. First run the Export Data Change Message tool, which creates a delta file with changes to synchronize. Then copy and import the delta file to the relative replica using the Import Message tool. If the delta file gets lost and you want to resend, use the Re-Export Unacknowledged Messages tool to regenerate the delta file. After the changes are imported, you can export an acknowledgment file from the relative replica using the Export Acknowledgement Message tool. Copy and import the acknowledgment file using the Import Message tool. If the acknowledgment is not received, the next time changes are sent, they will include the new changes and the previously sent changes.
- The output delta file can be a delta file geodatabase (.gdb) or a delta XML file (.xml). When specifying the output delta file, you must include the appropriate suffix (.gdb or .xml).
- This tool cannot be used for checkout replicas.
- To synchronize replicas in a connected mode, see the Synchronize Changes tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Export from Replica Geodatabase | The replica geodatabase from which the unacknowledged messages will be reexported. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| Output Delta File | The delta file to which data changes will be reexported. | File |
| Replica | The replica from which the unacknowledged messages will be reexported. | String |
| Export options | Specifies the changes that will be reexported.All unacknowledged—All changes with unacknowledged messages will be reexported.Most recent—Only those changes made since the last set of exported changes was sent will be reexported. | String |
| in_geodatabase | The replica geodatabase from which the unacknowledged messages will be reexported. The geodatabase can be a local geodatabase or a geodata service. | Workspace; GeoDataServer |
| output_delta_file | The delta file to which data changes will be reexported. | File |
| in_replica | The replica from which the unacknowledged messages will be reexported. | String |
| in_export_option | Specifies the changes that will be reexported.ALL_UNACKNOWLEDGED—All changes with unacknowledged messages will be reexported.MOST_RECENT—Only those changes made since the last set of exported changes was sent will be reexported. | String |

## Code Samples

### Example 1

```python
arcpy.management.ReExportUnacknowledgedMessages(in_geodatabase, output_delta_file, in_replica, in_export_option)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ReExportUnacknowledgedMessages("MySDEdata.sde", "dataChanges2.gdb", 
                                                "MyReplica1", "ALL_UNACKNOWLEDGED")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.management.ReExportUnacknowledgedMessages("MySDEdata.sde", "dataChanges2.gdb", 
                                                "MyReplica1", "ALL_UNACKNOWLEDGED")
```

### Example 4

```python
# Name: ReExportUnacknowledgedMessages_Example2.py
# Description: Reexport all unacknowledged messages from an SDE replica workspace.
# Changes are exported to a delta geodatabase

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_gdb = "MySDEdata.sde"
output_file = "dataChanges2.gdb"
replica_name = "MyReplica1"
export_option = "ALL_UNACKNOWLEDGED"

# Run ReExportUnacknowledgedMessages
arcpy.management.ReExportUnacknowledgedMessages(replica_gdb, output_file, 
                                                replica_name, export_option)
```

### Example 5

```python
# Name: ReExportUnacknowledgedMessages_Example2.py
# Description: Reexport all unacknowledged messages from an SDE replica workspace.
# Changes are exported to a delta geodatabase

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_gdb = "MySDEdata.sde"
output_file = "dataChanges2.gdb"
replica_name = "MyReplica1"
export_option = "ALL_UNACKNOWLEDGED"

# Run ReExportUnacknowledgedMessages
arcpy.management.ReExportUnacknowledgedMessages(replica_gdb, output_file, 
                                                replica_name, export_option)
```

---

## Rebuild Indexes (Data Management)

## Summary

Rebuild existing attribute or spatial indexes in enterprise geodatabases. Indexes can also be rebuilt on states and state_lineage geodatabase system tables and the delta tables of datasets that are registered to participate in traditional versioning. Out-of-date indexes can lead to poor query performance.

## Usage

- Data must be from a database or an enterprise geodatabase. This tool does not work with file geodatabases.
- After data loading, deleting, updating, and compressing operations, it is important to rebuild indexes.
- This tool rebuilds the attribute and spatial indexes of base tables, delta tables, and archive tables in versioned and archive-enabled geodatabases.
- The Include System Tables parameter determines if indexes are rebuilt on the states and state lineages tables in the specified geodatabase. When the option is unselected, the indexes on these tables are not rebuilt. If the Include System Tables parameter is unchecked, at least one dataset needs to be selected in the Datasets to Rebuild Indexes For to run the tool.
- The Datasets to Rebuild Indexes For parameter's Add Value button is used only in ModelBuilder. In ModelBuilder, where the preceding tool has not been run or its derived data does not exist, the Datasets to Rebuild Indexes For parameter may not be populated with values. The Add Value button allows you to add expected values so you can complete the dialog box and continue to build your model.
- This tool is not supported with SAP HANA.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The connection (.sde file) to the database or geodatabase that contains the data for which you want to rebuild indexes. | Workspace |
| Include System Tables | Indicates whether indexes will be rebuilt on the states and state lineages tables.You must connect as the geodatabase administrator for this option to be activated.This option only applies to geodatabases. If you connect to a database, this option is disabled.Unchecked—Indexes will not be rebuilt on the states and state lineages table. This is the default.Checked—Indexes will be rebuilt on the states and state lineages tables. | Boolean |
| Datasets to Rebuild Indexes For (Optional) | Names of the datasets that will have their indexes rebuilt. Only datasets that are owned by the connected user are displayed. | String |
| Rebuild Delta Tables Only (Optional) | Indicates whether indexes will be rebuilt only on the delta tables of the selected datasets. This option will be unavailable if there are no datasets selected in the list of input datasets.This option only applies to geodatabases. If you connect to a database, this option is disabled.Checked—Indexes will only be rebuilt for the delta tables of the selected datasets. This option can be used for cases where the business tables for the selected datasets are not updated often and there are a high volume of edits in the delta tables. This is the default.Unchecked—Indexes will be rebuilt on all indexes for the selected datasets. This includes spatial indexes as well as user-created attribute indexes and any geodatabase-maintained indexes for the dataset. | Boolean |
| input_database | The connection (.sde file) to the database or geodatabase that contains the data for which you want to rebuild indexes. | Workspace |
| include_system | Indicates whether indexes will be rebuilt on the states and state lineages tables.Note:You must connect as the geodatabase administrator in the connection file you specified for the input_database for this option to be executed successfully.This option only applies to geodatabases. This option is ignored if you connect to a database.NO_SYSTEM— Indexes will not be rebuilt on the states and state lineages table. This is the default.SYSTEM— Indexes will be rebuilt on the states and state lineages tables. | Boolean |
| in_datasets[in_datasets,...](Optional) | Names of the datasets that will have their indexes rebuilt. Dataset names use paths relative to the input_database; full paths are not accepted as input. | String |
| delta_only(Optional) | Indicates how the indexes will be rebuilt on the selected datasets. This option has no effect if in_datasets is empty.This option only applies to geodatabases. If the input workspace is a database, this option will be ignored.ALL—Indexes will be rebuilt on all indexes for the selected datasets. This includes spatial indexes as well as user-created attribute indexes and any geodatabase-maintained indexes for the dataset.ONLY_DELTAS—Indexes will only be rebuilt for the delta tables of the selected datasets. This option can be used for cases where the business tables for the selected datasets are not updated often and there is a high volume of edits in the delta tables. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RebuildIndexes(input_database, include_system, {in_datasets}, {delta_only})
```

### Example 2

```python
# Import system modules
import arcpy

arcpy.RebuildIndexes_management("c:/Connections/GDB@DC@server.sde", "NO_SYSTEM","db1.GDB.Roads;db1.GDB.Parcels", "ALL")
```

### Example 3

```python
# Import system modules
import arcpy

arcpy.RebuildIndexes_management("c:/Connections/GDB@DC@server.sde", "NO_SYSTEM","db1.GDB.Roads;db1.GDB.Parcels", "ALL")
```

### Example 4

```python
# Name: RebuildIndexes.py
# Description: rebuilds indexes on delta tables for all datasets in an
# enterprise geodatabase that belong to the connected user

# Import system modules
import arcpy, os

# set workspace
workspace = "C:/MyProject/mySDEConnection.sde"

# set the workspace environment
arcpy.env.workspace = workspace

# NOTE: Rebuild indexes can accept a Python list of datasets.

# Get a list of all the datasets the user has access to.
# First, get all the stand alone tables, feature classes and rasters.
dataList = arcpy.ListTables() + arcpy.ListFeatureClasses() + arcpy.ListRasters()

# Next, for feature datasets get all of the datasets and featureclasses
# from the list and add them to the master list.
for dataset in arcpy.ListDatasets("", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses() + arcpy.ListDatasets()

# reset the workspace
arcpy.env.workspace = workspace

# Get the user name for the workspace
userName = arcpy.Describe(workspace).connectionProperties.user.lower()

# remove any datasets that are not owned by the connected user.
userDataList = [ds for ds in dataList if ds.lower().find(f".{userName}.") > -1]

# Execute rebuild indexes
# Note: to use the "SYSTEM" option the workspace user must be an administrator.
arcpy.RebuildIndexes_management(workspace, "NO_SYSTEM", userDataList, "ALL")
print('Rebuild Complete')
```

### Example 5

```python
# Name: RebuildIndexes.py
# Description: rebuilds indexes on delta tables for all datasets in an
# enterprise geodatabase that belong to the connected user

# Import system modules
import arcpy, os

# set workspace
workspace = "C:/MyProject/mySDEConnection.sde"

# set the workspace environment
arcpy.env.workspace = workspace

# NOTE: Rebuild indexes can accept a Python list of datasets.

# Get a list of all the datasets the user has access to.
# First, get all the stand alone tables, feature classes and rasters.
dataList = arcpy.ListTables() + arcpy.ListFeatureClasses() + arcpy.ListRasters()

# Next, for feature datasets get all of the datasets and featureclasses
# from the list and add them to the master list.
for dataset in arcpy.ListDatasets("", "Feature"):
    arcpy.env.workspace = os.path.join(workspace,dataset)
    dataList += arcpy.ListFeatureClasses() + arcpy.ListDatasets()

# reset the workspace
arcpy.env.workspace = workspace

# Get the user name for the workspace
userName = arcpy.Describe(workspace).connectionProperties.user.lower()

# remove any datasets that are not owned by the connected user.
userDataList = [ds for ds in dataList if ds.lower().find(f".{userName}.") > -1]

# Execute rebuild indexes
# Note: to use the "SYSTEM" option the workspace user must be an administrator.
arcpy.RebuildIndexes_management(workspace, "NO_SYSTEM", userDataList, "ALL")
print('Rebuild Complete')
```

---

## Recalculate Feature Class Extent (Data Management)

## Summary

Recalculates the xy, z, and m extent properties of a feature class based on the features in the feature class.

## Usage

- This tool updates geodatabase feature classes or shapefiles (point, multipoint, line, or polygon).
- In an enterprise geodatabase, extent is a property of the feature class schema and requires an exclusive schema lock to run.
- When using this tool with an enterprise geodatabase feature class as input, the extent is calculated based on the features that exist in that feature class in all versions. The extent will not reduce or expand unless this tool is run after the database has been compressed.
- The Store Extent parameter is supported for unregistered spatial tables in a database or enterprise geodatabase. If the parameter is checked, the new extent will be stored on the shape column metadata of the underlying table. If the parameter is unchecked, the extent will be recalculated but the value will not be stored, which can be useful when the underlying data will be updated frequently.
- This tool will fail if you do not have permissions to edit the feature class.
- Recalculating the feature class extent cannot be undone.
- The values of the extent are returned by the arcpy.Describe function's extent property.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Class | The shapefile or geodatabase feature class that will be updated. | Feature Layer |
| Store Extent (Optional) | Specifies whether the extent will be stored for feature classes that are not registered. This parameter is only active when the input feature class is an unregistered spatial table in a database or enterprise geodatabase.If the input feature class is updated frequently, you may choose not to store the recalculated extent value. If you choose to store the extent, the extent will not be recalculated each time the feature class is added to a map.Checked—The extent will be stored for the input feature class.Unchecked—The extent will not be stored for the input feature class. This is the default. | Boolean |
| in_features | The shapefile or geodatabase feature class that will be updated. | Feature Layer |
| store_extent(Optional) | Specifies whether the extent will be stored for feature classes that are not registered. This parameter is only supported when the input feature class is an unregistered spatial table in a database or enterprise geodatabase.If the input feature class is updated frequently, you may choose not to store the recalculated extent value. If you choose to store the extent, the extent will not be recalculated each time the feature class is added to the map.STORE_EXTENT—The extent will be stored for the input feature class.DO_NOT_STORE_EXTENT—The extent will not be stored for the input feature class. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RecalculateFeatureClassExtent(in_features, {store_extent})
```

### Example 2

```python
import arcpy
feature_class = r"C:\Data\europe.gdb\norway_cities"
arcpy.management.RecalculateFeatureClassExtent(feature_class)
```

### Example 3

```python
import arcpy
feature_class = r"C:\Data\europe.gdb\norway_cities"
arcpy.management.RecalculateFeatureClassExtent(feature_class)
```

---

## Reclassify Field (Data Management)

## Summary

Reclassifies values in a numerical or text field into classes based on bounds defined manually or using a reclassification method.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class containing the field that will be reclassified. | Table View; Raster Layer; Mosaic Layer |
| Field to Reclassify | The field that will be reclassified. The field must be numeric or text. | Field |
| Reclassification Method (Optional) | Specifies the reclassification method that will be used for field values in the Field to Reclassify parameter value.Defined interval— Classes will be created with the same class range over the span of the values of the field to reclassify.Equal interval—Classes will be created with equal class ranges divided into a specified number of classes. This is the default.Geometric interval—Classes will be created with geometrically increasing or decreasing class ranges into a specified number of classes.Manual interval— Class breaks and reclassed values will be manually specified. Natural breaks (Jenks)— Classes will be created of natural groupings in the data using the Jenks natural breaks algorithm. Quantile— Classes will be created in which each class includes an equal number of values.Standard deviation— Classes will be created by adding and subtracting a fraction of the standard deviation above and below the average value.Unique values—Classes will be created in which each unique value of the field becomes a class. | String |
| Number of Classes (Optional) | The target number of classes in the reclassified field. The maximum number of classes is 256. | Long |
| Interval Size (Optional) | The class interval size for the reclassified field. The provided value must result in at least 3 classes and not more than 1,000 classes. | Double |
| Number of Standard Deviations (Optional) | Specifies the number of standard deviations that will be used for the reclassified field. Class breaks and categories will be created with equal interval ranges that are a proportion of the standard deviation from the mean. One standard deviation—Intervals will be created using one standard deviation. This is the default.One half of a standard deviation—Intervals will be created using half of one standard deviation. One third of a standard deviation—Intervals will be created using a third of one standard deviation. One quarter of a standard deviation—Intervals will be created using a quarter of one standard deviation. | String |
| Reclassification Table (Optional) | The upper bound and reclassed value for the manual reclassification method. | Value Table |
| Reverse Values (Descending) (Optional) | Specifies how the reclassified values will be ordered.Checked—Classes will be assigned values in descending order; the class with the highest values is assigned 1, the next highest class is assigned 2, and so on.Unchecked—Classes will be assigned values in ascending order; the class with the lowest values is assigned 1, the next lowest class is assigned 2, and so on. This is the default. | Boolean |
| Output Field Name (Optional) | The name or prefix of the output field. If the field to reclassify is a numerical field, two fields will be created, and this name will prefix the field names. If the field to reclassify is a text field, one new field will be created with this name. | String |
| in_table | The input table or feature class containing the field that will be reclassified. | Table View; Raster Layer; Mosaic Layer |
| field | The field that will be reclassified. The field must be numeric or text. | Field |
| method(Optional) | Specifies the reclassification method that will be used for the field values in the field parameter value. DEFINED_INTERVAL— Classes will be created with the same class range over the span of the values of the field to reclassify.EQUAL_INTERVAL—Classes will be created with equal class ranges divided into a specified number of classes. This is the default.GEOMETRIC_INTERVAL—Classes will be created with geometrically increasing or decreasing class ranges into a specified number of classes.MANUAL— Class breaks and reclassed values will be manually specified. NATURAL_BREAKS— Classes will be created of natural groupings in the data using the Jenks natural breaks algorithm. QUANTILE— Classes will be created in which each class includes an equal number of values.STANDARD_DEVIATION— Classes will be created by adding and subtracting a fraction of the standard deviation above and below the average value.UNIQUE_VALUES—Classes will be created in which each unique value of the field becomes a class. | String |
| classes(Optional) | The target number of classes in the reclassified field. The maximum number of classes is 256. | Long |
| interval(Optional) | The class interval size for the reclassified field. The provided value must result in at least 3 classes and not more than 1,000 classes. | Double |
| standard_deviations(Optional) | Specifies the number of standard deviations that will be used for the reclassified field. Class breaks and categories will be created with equal interval ranges that are a proportion of the standard deviation from the mean. ONE—Intervals will be created using one standard deviation. This is the default.HALF—Intervals will be created using half of one standard deviation. THIRD—Intervals will be created using a third of one standard deviation. QUARTER—Intervals will be created using a quarter of one standard deviation. | String |
| reclass_table[reclass_table,...](Optional) | The upper bound and reclassed value for the manual reclassification method. | Value Table |
| reverse_values(Optional) | Specifies the order of the reclassified values.DESC—Classes will be assigned values in descending order; the class with the highest values is assigned 1, the next highest class is assigned 2, and so on.ASC—Classes will be assigned values in ascending order; the class with the lowest values is assigned 1, the next lowest class is assigned 2, and so on. This is the default. | Boolean |
| output_field_name(Optional) | The name or prefix of the output field. If the field to reclassify is a numerical field, two fields will be created, and this name will prefix the field names. If the field to reclassify is a text field, one new field will be created with this name. | String |

## Code Samples

### Example 1

```python
arcpy.management.ReclassifyField(in_table, field, {method}, {classes}, {interval}, {standard_deviations}, {reclass_table}, {reverse_values}, {output_field_name})
```

### Example 2

```python
arcpy.management.ReclassifyField("Demographics", "Population", 
      "EQUAL_INTERVAL", 10, None, "", None, None, "Population_EQUAL_INTERVAL")
```

### Example 3

```python
arcpy.management.ReclassifyField("Demographics", "Population", 
      "EQUAL_INTERVAL", 10, None, "", None, None, "Population_EQUAL_INTERVAL")
```

### Example 4

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Reclassify\\MyData.gdb"
    in_table = "Demographics"

    # Set the input field that will be reclassified
    field = "Population"

    # Set the reclassification method
    method = "MANUAL"

    # Set the reclassification table
    reclass_table = "10000 Village;100000 Town;1000000 City"

    # Set the output field name
    output_field_name = "SettlementType"

    # Run the Reclassify Field tool
    arcpy.management.ReclassifyField(in_table, field, method, "", 
          None, "", reclass_Table, None, output_field_name)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 5

```python
# Import system modules.
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Reclassify\\MyData.gdb"
    in_table = "Demographics"

    # Set the input field that will be reclassified
    field = "Population"

    # Set the reclassification method
    method = "MANUAL"

    # Set the reclassification table
    reclass_table = "10000 Village;100000 Town;1000000 City"

    # Set the output field name
    output_field_name = "SettlementType"

    # Run the Reclassify Field tool
    arcpy.management.ReclassifyField(in_table, field, method, "", 
          None, "", reclass_Table, None, output_field_name)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

---

## Reconcile Version (Data Management)

## Summary

Reconciles a version against another version in its lineage.

## Usage

- The reconcile process requires that you are the only user currently editing the version and the only user able to edit the version throughout the reconcile process until you save or post.
- The reconcile process requires that you have full permissions to all the feature classes that have been modified in the version being edited.
- Versioning tools only work with ArcGIS enterprise data. File and geodatabases don't support versioning.
- The geodatabase is designed to efficiently manage and support long transactions using versions.
- The reconcile process detects differences between the edit version and the target version and flags these differences as conflicts. If conflicts exist, they should be resolved.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The enterprise geodatabase containing the reconcilable version. | Workspace |
| Version Name | Name of the Edit Version to be reconciled with the Target Version. | String |
| Target Version | Name of any version in the direct ancestry of the Edit version, such as the parent version or the default version. | String |
| Conflict Definition(Optional) | Describes the conditions required for a conflict to occur:Checked—Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. Unchecked—Only changes to the same attribute of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. | String |
| Conflict Resolution(Optional) | Describes the behavior if a conflict is detected:Checked—For all conflicts, resolves in favor of the target version. This is the default. Unchecked—For all conflicts, resolves in favor of the edit version. | String |
| Acquire locks during reconcile(Optional) | Determines whether feature locks will be acquired.Checked—Acquires locks when there is no intention of posting the edit session. This is the default.Unchecked—No locks are acquired and the edit session will be posted to the target version. | Boolean |
| Abort if conflicts(Optional) | Determines if the reconcile process should be aborted if conflicts are found between the target version and the edit version.Checked—Aborts the reconcile if conflicts are found.Unchecked—Does not abort the reconcile if conflicts are found. This is the default. | Boolean |
| Post version after reconcile(Optional) | Posts the current edit session to the reconciled target version.Checked—Current edits will be posted to the target version after the reconcile.Unchecked—Current edits will not be posted to the target version after the reconcile. This is the default. | Boolean |
| in_workspace | The enterprise geodatabase containing the reconcilable version. The default is to use the workspace defined in the environment. | Workspace |
| version_name | Name of the Edit Version to be reconciled with the Target Version. | String |
| target_name | Name of any version in the direct ancestry of the Edit version, such as the parent version or the default version. | String |
| conflict_definition(Optional) | Describes the conditions required for a conflict to occur:BY_OBJECT—Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. BY_ATTRIBUTE—Only changes to the same attribute of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. | String |
| conflict_resolution(Optional) | Describes the behavior if a conflict is detected:FAVOR_TARGET_VERSION—For all conflicts, resolves in favor of the target version. This is the default. FAVOR_EDIT_VERSION—For all conflicts, resolves in favor of the edit version. | String |
| aquired_locks(Optional) | Determines whether feature locks will be acquired.LOCK_ACQUIRED—Acquires locks when there is no intention of posting the edit session. This is the default. NO_LOCK_ACQUIRED—No locks are acquired and the edit session will be posted to the target version. | Boolean |
| abort_if_conflicts(Optional) | Determines if the reconcile process should be aborted if conflicts are found between the target version and the edit version.NO_ABORT—Does not abort the reconcile if conflicts are found. This is the default. ABORT_CONFLICTS—Aborts the reconcile if conflicts are found. | Boolean |
| post(Optional) | Posts the current edit session to the reconciled target version.NO_POST—Current edits will not be posted to the target version after the reconcile. This is the default.POST—Current edits will be posted to the target version after the reconcile. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ReconcileVersion(in_workspace, version_name, target_name, {conflict_definition}, {conflict_resolution}, {aquired_locks}, {abort_if_conflicts}, {post})
```

### Example 2

```python
# Description: Reconciles a version with a version in the version lineage and 
               then posts that version.

# Import system modules
import arcpy

# Set local variables
inWorkspace = "Database Connections/ninefour@gdb.sde"
versionName = "myVersion"
targetVersion = "dbo.DEFAULT"

# Run ReconcileVersion
arcpy.management.ReconcileVersion(inWorkspace, versionName, targetVersion, "BY_OBJECT", 
                                  "FAVOR_TARGET_VERSION", "LOCK_acquireD", "NO_ABORT", 
                                  "POST")
```

### Example 3

```python
# Description: Reconciles a version with a version in the version lineage and 
               then posts that version.

# Import system modules
import arcpy

# Set local variables
inWorkspace = "Database Connections/ninefour@gdb.sde"
versionName = "myVersion"
targetVersion = "dbo.DEFAULT"

# Run ReconcileVersion
arcpy.management.ReconcileVersion(inWorkspace, versionName, targetVersion, "BY_OBJECT", 
                                  "FAVOR_TARGET_VERSION", "LOCK_acquireD", "NO_ABORT", 
                                  "POST")
```

---

## Reconcile Versions (Data Management)

## Summary

Reconciles a version or multiple versions with a target version.

## Usage

- The reconcile process requires that you are the only user currently editing the version and the only user who will edit the version throughout the reconcile process until you save or post.
- The reconcile process requires that you have full permissions to all the feature classes that have been modified in the version being edited.
- Versioning tools work with enterprise geodatabases. File geodatabases do not support versioning.
- This tool supports reconcile and post operations for data published from a branch workspace with version management capabilities enabled. Reconcile and post operations for branch versioning using a geodatabase connection file are not supported.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The enterprise geodatabase that contains the versions to be reconciled. For branch versioning, this will be the feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer) or the feature layer portal item. | Workspace |
| Reconcile Mode | Specifies the versions that will be reconciled when the tool is run. If the input is a branch workspace, the only valid option for this parameter is to reconcile all versions.Reconcile all versions—Edit versions will be reconciled with the target version. This is the default.Reconcile blocking versions only—Versions that are blocking the target version from compressing will be reconciled. This option uses the recommended reconcile order. | String |
| Target Version (Optional) | The name of any version in the direct ancestry of the edit version, such as the parent version or the default version. It typically contains edits from other versions that you want included in the edit version.If the input is a branch workspace, the only valid option for this parameter is to reconcile with the default version. | String |
| Edit Versions (Optional) | The name of the edit version or versions to be reconciled with the selected target version. Only versions that are in the direct ancestry of the selected target version will be displayed. | String |
| Acquire Locks(Optional) | Specifies whether feature locks will be acquired.If the input is a branch workspace, locks are not acquired during the reconcile process.Checked—Locks will be acquired during the reconcile process. Use this option when the intention is to post edits. It ensures that the target version is not modified in the time between the reconcile and post operations. This is the default.Unchecked—Locks will not be acquired during the reconcile process. This allows multiple users to reconcile in parallel. Use this option when the edit version will not be posted to the target version because the target version may be modified in the time between the reconcile and post operations. | Boolean |
| Abort if Conflicts Detected(Optional) | Specifies whether the reconcile process will end if conflicts are found between the target version and the edit version during the reconcile process.Checked—The reconcile will end if conflicts are found.Unchecked—The reconcile will not end if conflicts are found. This is the default. | Boolean |
| Conflict Definition(Optional) | Specifies whether the conditions required for a conflict to occur will be defined by object (row) or by attribute (column).Conflicts defined by object (by row)—Conflicts will be defined by object. Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. Conflicts defined by attribute (by column)—Conflicts will be defined by attribute. Only changes to the same attribute (column) of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to other attributes will not be considered a conflict during reconcile. | String |
| Conflict Resolution(Optional) | Specifies the resolution that will be used if a conflict is detected.If the input is a branch workspace, the default is to favor the edit version.Resolve conflicts in favor of the target version—All conflicts will be resolved in favor of the target version. This is the default for traditional versioning. Resolve conflicts in favor of the edit version—All conflicts will be resolved in favor of the edit version. This is the default for branch versioning. | String |
| Post Versions After Reconcile(Optional) | Specifies whether the current edit session will be posted to the reconciled target version.Checked—The current edit version will be posted to the target version after the reconcile.Unchecked—The current edit version will not be posted to the target version after the reconcile. This is the default. | Boolean |
| Delete Versions After Post (Optional) | Specifies whether the reconciled edit version will be deleted after posting. This parameter only applies when the Post Versions After Reconcile parameter is checked.Checked—The current edit version that was reconciled will be deleted after being posted to the target version. Unchecked—The current edit version that was reconciled will not be deleted. This is the default. | Boolean |
| Reconcile Versions Log (Optional) | The name and location where the log file will be written. The log file is an ASCII file containing the contents of the geoprocessing messages. | File |
| Proceed if unreviewed conflicts are detected (Optional) | Specifies whether the reconcile will proceed if existing unreviewed conflicts are detected before the reconcile process starts. If you proceed, existing conflicts from previous sessions will be lost when the tool runs. This parameter is only applicable to branch versioning.Checked—The reconcile process will proceed if existing unreviewed conflicts are detected. This is the default. Unchecked—The reconcile process will not proceed if existing unreviewed conflicts are detected. | Boolean |
| Reconcile checkout replica versions (Optional) | Specifies whether the reconcile process will include checkout replica versions. If you are creating a checkout replica as part of a geodatabase replication workflow, an associated version is created in the geodatabase. This option allows you to include or remove these types of versions from the list of versions to be reconciled. This parameter is not applicable to branch versioning.Checked—The reconcile process will include checkout replica versions. This is the default.Unchecked—The reconcile process will not include checkout replica versions. | Boolean |
| input_database | The enterprise geodatabase that contains the versions to be reconciled. The default is to use the geoprocessing Current Workspace environment.For branch versioning, this will be the feature service URL (that is, https://mysite.mydomain/server/rest/services/ElectricNetwork/FeatureServer). | Workspace |
| reconcile_mode | Specifies the versions that will be reconciled when the tool is run. If the input is a branch workspace, the only valid option for this parameter is to reconcile all versions.ALL_VERSIONS—Edit versions will be reconciled with the target version. This is the default.BLOCKING_VERSIONS—Versions that are blocking the target version from compressing will be reconciled. This option uses the recommended reconcile order. | String |
| target_version(Optional) | The name of any version in the direct ancestry of the edit version, such as the parent version or the default version. It typically contains edits from other versions that you want included in the edit version.If the input is a branch workspace, the only valid option for this parameter is to reconcile with the default version. | String |
| edit_versions[edit_versions,...](Optional) | The name of the edit version or versions to be reconciled with the selected target version. This can be an individual version name or a list of version names. | String |
| acquire_locks(Optional) | Specifies whether feature locks will be acquired.If the input is a branch workspace, locks are not acquired during the reconcile process.LOCK_ACQUIRED—Locks will be acquired during the reconcile process. Use this option when the intention is to post edits. It ensures that the target version is not modified in the time between the reconcile and post operations. This is the default.NO_LOCK_ACQUIRED—Locks will not be acquired during the reconcile process. This allows multiple users to reconcile in parallel. Use this option when the edit version will not be posted to the target version because the target version may be modified in the time between the reconcile and post operations. | Boolean |
| abort_if_conflicts(Optional) | Specifies whether the reconcile process will end if conflicts are found between the target version and the edit version during the reconcile process.NO_ABORT—The reconcile will not end if conflicts are found. This is the default. ABORT_CONFLICTS—The reconcile will end if conflicts are found. | Boolean |
| conflict_definition(Optional) | Specifies whether the conditions required for a conflict to occur will be defined by object (row) or by attribute (column).BY_OBJECT—Conflicts will be defined by object. Any changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default. BY_ATTRIBUTE—Conflicts will be defined by attribute. Only changes to the same attribute (column) of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to other attributes will not be considered a conflict during reconcile. | String |
| conflict_resolution(Optional) | Specifies the resolution that will be used if a conflict is detected.If the input is a branch workspace, the default is to favor the edit version.FAVOR_TARGET_VERSION—All conflicts will be resolved in favor of the target version. This is the default for traditional versioning. FAVOR_EDIT_VERSION—All conflicts will be resolved in favor of the edit version. This is the default for branch versioning. | String |
| with_post(Optional) | Specifies whether the current edit session will be posted to the reconciled target version.NO_POST—The current edit version will not be posted to the target version after the reconcile. This is the default.POST—The current edit version will be posted to the target version after the reconcile. | Boolean |
| with_delete(Optional) | Specifies whether the reconciled edit version will be deleted after posting. This parameter only applies when the with_post parameter is set to POST.DELETE_VERSION—The current edit version that was reconciled will be deleted after being posted to the target version.KEEP_VERSION—The current edit version that was reconciled will not be deleted. This is the default. | Boolean |
| out_log(Optional) | The name and location where the log file will be written. The log file is an ASCII file containing the contents of the geoprocessing messages. | File |
| proceed_if_conflicts_not_reviewed(Optional) | Specifies whether the reconcile will proceed if existing unreviewed conflicts are detected before the reconcile process starts. If you proceed, existing conflicts from previous sessions will be lost when the tool runs. This parameter is only applicable to branch versioning.PROCEED—The reconcile process will proceed if existing unreviewed conflicts are detected. This is the default.NOT_PROCEED—The reconcile process will not proceed if existing unreviewed conflicts are detected. | Boolean |
| reconcile_checkout_versions(Optional) | Specifies whether the reconcile process will include checkout replica versions. If you are creating a checkout replica as part of a geodatabase replication workflow, an associated version is created in the geodatabase. This option allows you to include or remove these types of versions from the list of versions to be reconciled. This parameter is not applicable to branch versioning.RECONCILE—The reconcile process will include checkout replica versions. This is the default.DO_NOT_RECONCILE—The reconcile process will not include checkout replica versions. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ReconcileVersions(input_database, reconcile_mode, {target_version}, {edit_versions}, {acquire_locks}, {abort_if_conflicts}, {conflict_definition}, {conflict_resolution}, {with_post}, {with_delete}, {out_log}, {proceed_if_conflicts_not_reviewed}, {reconcile_checkout_versions})
```

### Example 2

```python
# Name: ReconcileVersions.py
# Description: Reconciles all versions owned by a user with SDE.Default

# Import system modules
import arcpy, os

# Set workspace
workspace = 'C:/Data/connections/bender@production.sde'

# Set the workspace environment
arcpy.env.workspace = workspace

# Use a list comprehension to get a list of version names where the owner
# is the current user and make sure sde.default is not selected.
verList = [ver.name for ver in arcpy.da.ListVersions() if ver.isOwner
           == True and ver.name.lower() != 'sde.default']

arcpy.ReconcileVersions_management(workspace,
                                   "ALL_VERSIONS",
                                   "SDE.Default",
                                   verList,
                                   "LOCK_ACQUIRED",
                                   "NO_ABORT",
                                   "BY_OBJECT",
                                   "FAVOR_TARGET_VERSION",
                                   "NO_POST",
                                   "KEEP_VERSION",
                                   "c:\RecLog.txt")
print('Reconciling Complete')
```

### Example 3

```python
# Name: ReconcileVersions.py
# Description: Reconciles all versions owned by a user with SDE.Default

# Import system modules
import arcpy, os

# Set workspace
workspace = 'C:/Data/connections/bender@production.sde'

# Set the workspace environment
arcpy.env.workspace = workspace

# Use a list comprehension to get a list of version names where the owner
# is the current user and make sure sde.default is not selected.
verList = [ver.name for ver in arcpy.da.ListVersions() if ver.isOwner
           == True and ver.name.lower() != 'sde.default']

arcpy.ReconcileVersions_management(workspace,
                                   "ALL_VERSIONS",
                                   "SDE.Default",
                                   verList,
                                   "LOCK_ACQUIRED",
                                   "NO_ABORT",
                                   "BY_OBJECT",
                                   "FAVOR_TARGET_VERSION",
                                   "NO_POST",
                                   "KEEP_VERSION",
                                   "c:\RecLog.txt")
print('Reconciling Complete')
```

---

## Recover File Geodatabase (Data Management)

## Summary

Recovers data from a file geodatabase that has become corrupt.

## Usage

- The Recover File Geodatabase tool can only recover simple feature classes and tables. Complex data and relationships will not be recovered.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input File Geodatabase | Input corrupt file geodatabase. | Workspace |
| Output Location | Output folder location for the recovered file geodatabase. | Folder |
| File Geodatabase Name | Name for the output file geodatabase. | String |
| input_file_gdb | Input corrupt file geodatabase. | Workspace |
| output_location | Output folder location for the recovered file geodatabase. | Folder |
| out_name | Name for the output file geodatabase. | String |

## Code Samples

### Example 1

```python
arcpy.management.RecoverFileGDB(input_file_gdb, output_location, out_name)
```

### Example 2

```python
arcpy.RecoverFileGDB_management('C:/fgdb/Whistler.gdb', 'C:/recoveredData', 'recoveredWhistler.gdb')
```

### Example 3

```python
arcpy.RecoverFileGDB_management('C:/fgdb/Whistler.gdb', 'C:/recoveredData', 'recoveredWhistler.gdb')
```

### Example 4

```python
# Name: RecoverFileGeodatabase.py
# Description: Use the RecoverFileGeodatabase tool to recover the data
#              contained in a damaged file geodatabase.

# Import system modules
import arcpy

# Set local variables
geodatabase = "C:/fgdb/Whistler.gdb"
output_location = "C:/recoveredData"
recovered_name = "recoveredWhistler.gdb"

# Process: Recover the data
arcpy.RecoverFileGDB_management(geodatabase, output_location, recovered_name)
```

### Example 5

```python
# Name: RecoverFileGeodatabase.py
# Description: Use the RecoverFileGeodatabase tool to recover the data
#              contained in a damaged file geodatabase.

# Import system modules
import arcpy

# Set local variables
geodatabase = "C:/fgdb/Whistler.gdb"
output_location = "C:/recoveredData"
recovered_name = "recoveredWhistler.gdb"

# Process: Recover the data
arcpy.RecoverFileGDB_management(geodatabase, output_location, recovered_name)
```

---

## Refresh Excel (Data Management)

## Summary

Refreshes a Microsoft Excel file in ArcGIS Pro.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Excel File | The Excel file that will be refreshed. | File |
| in_excel_file | The Excel file that will be refreshed. | File |

## Code Samples

### Example 1

```python
arcpy.management.RefreshExcel(in_excel_file)
```

### Example 2

```python
import arcpy
arcpy.management.RefreshExcel("C:\\MyProject\\MyExcelData.xls")
```

### Example 3

```python
import arcpy
arcpy.management.RefreshExcel("C:\\MyProject\\MyExcelData.xls")
```

---

## Register As Versioned (Data Management)

## Summary

Registers an enterprise geodatabase dataset as versioned.

## Usage

- Versioning tools only work with datasets in an enterprise geodatabase. File geodatabases don't support versioning.
- The type of versioning used is determined by the database connection for the input dataset; Versioning Type is a property set in the Geodatabase Connection Properties dialog of a database connection. Learn how to set the versioning type using the geodatabase connection properties.
- Registering a feature dataset as versioned registers all feature classes in the feature dataset as versioned.
- The input dataset must be from a database connection established as the data owner.
- Archive-enabled datasets cannot be registered as versioned. If you've already enabled archiving on your data but you also want to register your data as versioned, you must first disable archiving, register the data as versioned, and re-enable archiving.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The dataset to be registered as versioned. | Table View; Feature Dataset |
| Register the selected objects with the option to move edits to base(Optional) | Specifies whether edits made to the default version will be moved to the base tables. This parameter is not applicable for branch versioning Checked—The dataset will be versioned with the option of moving edits to base.Unchecked—The dataset will be versioned without the option of moving edits to base. This is the default. | Boolean |
| in_dataset | The dataset to be registered as versioned. | Table View; Feature Dataset |
| edit_to_base(Optional) | Specifies whether edits made to the default version will be moved to the base tables. This parameter is not applicable for branch versioning.NO_EDITS_TO_BASE—The dataset will not be versioned with the option of moving edits to base. This is the default. EDITS_TO_BASE—The dataset will be versioned with the option of moving edits to base. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RegisterAsVersioned(in_dataset, {edit_to_base})
```

### Example 2

```python
# Name: RegisterAsVersioned_Example.py
# Description: Registers dataset as versioned

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/Connections/ninefour@gdb.sde/ninefour.GDB.ctgFuseFeature"

# Run RegisterAsVersioned
arcpy.management.RegisterAsVersioned(datasetName, "NO_EDITS_TO_BASE")
```

### Example 3

```python
# Name: RegisterAsVersioned_Example.py
# Description: Registers dataset as versioned

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/Connections/ninefour@gdb.sde/ninefour.GDB.ctgFuseFeature"

# Run RegisterAsVersioned
arcpy.management.RegisterAsVersioned(datasetName, "NO_EDITS_TO_BASE")
```

---

## Register Raster (Data Management)

## Summary

Automatically aligns a raster to a reference image or uses a control point file for georegistration. If the input dataset is a mosaic dataset, the tool will operate on each mosaic dataset item. To automatically register the image, the input raster and the reference raster must be in a relatively close geographic area. The tool will run faster if the raster datasets are in close alignment. You may need to create a link file, also known as a control point file, with a few links to get your input raster into the same map space.

## Usage

- The input raster will have its georeferencing information updated.
- A control point table can be created using the Georeference tab.
- The Reset keyword (register_mode = "RESET" in Python) allows you to remove any geographic transformation that has been applied using this tool.
- This tool allows you to register all the items within a mosaic dataset, a subset of items, or a single item within a mosaic dataset. If you only want to register a specific item, you will need to enter the full path of the mosaic dataset and a unique query for the item you want to update.The following are examples of how to choose an item within a mosaic dataset as your input raster:\\Myserver\MyFolder\MyMosaicDataset\OBJECTID=1\\Myserver\MyFolder\MyMosaicDataset\NAME='tileName01'If you want to choose a subset of the items, you can make a selection on the mosaic dataset layer.
- \\Myserver\MyFolder\MyMosaicDataset\OBJECTID=1
- \\Myserver\MyFolder\MyMosaicDataset\NAME='tileName01'
- When the input raster is a mosaic dataset with pan-sharpened items, this tool can register the multispectral raster to the panchromatic raster when you choose the Register multispectral keyword within the Register Mode parameter (register_mode = "REGISTER_MS" in Python). Make sure you leave the Reference Raster and the Input Link File parameters empty.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster that you want to realign. Registering a mosaic dataset item will update that particular item within the mosaic dataset.A mosaic dataset item will have the path to the mosaic dataset followed by the Object ID of the item. For example, the first item in the mosaic dataset would have the following path: .\mosaicDataset\objectid=1. | Mosaic Layer; Raster Dataset; Raster Layer |
| Register Mode | Specifies the registration mode. You can either register the raster with a transformation or reset the transformation.Register—Apply a geometric transformation to the input raster.Register multispectral—Register the multispectral input to the panchromatic input. This is only used for mosaic datasets that have a misalignment between the two.Reset— Remove the geometric transformation previously added by this tool.Create links—Create a link file with automatically generated links. | String |
| Reference Raster(Optional) | The raster dataset that will align the input raster dataset. Leave this parameter empty if you want to register your multispectral mosaic dataset items to their associated panchromatic raster datasets. | Image Service; Internet Tiled Layer; Map Server Layer; Map Server; Mosaic Layer; Raster Dataset; Raster Layer; WMS Map |
| Input Link File (Optional) | The file that has the coordinates to link the input raster dataset with the reference. The input link table works with one mosaic item in the mosaic layer. The input must specify which item is being processed, either selecting the item or specifying the ObjectID in the input. Leave this parameter empty to register multispectral mosaic dataset items with the associated panchromatic raster datasets. | Feature Class; Text File |
| Transformation Type (Optional) | Specifies the method for shifting the raster dataset.Shift only— This method uses a zero-order polynomial to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift.Affine transformation—A first-order polynomial (affine) fits a flat plane to the input points.Second-order polynomial transformation—A second-order polynomial fits a somewhat more complicated surface to the input points.Third-order polynomial transformation—A third-order polynomial fits a more complicated surface to the input points.Adjust transformation— This method combines a polynomial transformation and uses a triangulated irregular network (TIN) interpolation technique to optimize for both global and local accuracy.Spline transformation— This method transforms the source control points precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points are not.Projective transformation— This method warps lines so they remain straight. In doing so, lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.Similarity transformation— This is a first-order transformation that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit.Frame transformation—This method uses an image resection algorithm on aerial images. The image resection algorithm refines the exterior orientation (perspective, omega, phi, and kappa) of the image from known ground control points, using a least-square fitting method. Each image must have at least three noncollinear points. When the input is a mosaic dataset, it will register the selected images one at a time. | String |
| Output Link File (Optional) | If specified, a text file will be written containing the links created by this tool. This file can be used in the Warp From File tool. The output link table works with one mosaic dataset item in the mosaic layer. The input must specify which item is being processed, either selecting the item or specifying the ObjectID in the input. | Text File |
| Maximum RMS (Optional) | The amount of modeled error (in pixels) that you want in the output. The default is 0.5, and values below 0.3 are not recommended as this leads to overfitting. | Double |
| in_raster | The raster that you want to realign. Registering a mosaic dataset item will update that particular item within the mosaic dataset.A mosaic dataset item will have the path to the mosaic dataset followed by the Object ID of the item. For example, the first item in the mosaic dataset would have the following path: .\mosaicDataset\objectid=1. | Mosaic Layer; Raster Dataset; Raster Layer |
| register_mode | Specifies the registration mode. You can either register the raster with a transformation or reset the transformation.REGISTER—Apply a geometric transformation to the input raster.REGISTER_MS—Register the multispectral input to the panchromatic input. This is only used for mosaic datasets that have a misalignment between the two.RESET— Remove the geometric transformation previously added by this tool.CREATE_LINKS—Create a link file with automatically generated links. | String |
| reference_raster(Optional) | The raster dataset that will align the input raster dataset. Leave this parameter empty if you want to register your multispectral mosaic dataset items to their associated panchromatic raster datasets. | Image Service; Internet Tiled Layer; Map Server Layer; Map Server; Mosaic Layer; Raster Dataset; Raster Layer; WMS Map |
| input_link_file(Optional) | The file that has the coordinates to link the input raster dataset with the reference. The input link table works with one mosaic item in the mosaic layer. The input must specify which item is being processed, either selecting the item or specifying the ObjectID in the input. Leave this parameter empty to register multispectral mosaic dataset items with the associated panchromatic raster datasets. | Feature Class; Text File |
| transformation_type(Optional) | Specifies the method for shifting the raster dataset.POLYORDER0— This method uses a zero-order polynomial to shift your data. This is commonly used when your data is already georeferenced, but a small shift will better line up your data. Only one link is required to perform a zero-order polynomial shift.POLYSIMILARITY— This is a first-order transformation that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit.POLYORDER1—A first-order polynomial (affine) fits a flat plane to the input points.POLYORDER2—A second-order polynomial fits a somewhat more complicated surface to the input points.POLYORDER3—A third-order polynomial fits a more complicated surface to the input points.ADJUST— This method combines a polynomial transformation and uses a triangulated irregular network (TIN) interpolation technique to optimize for both global and local accuracy.SPLINE— This method transforms the source control points precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points are not.PROJECTIVE— This method warps lines so they remain straight. In doing so, lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.FRAME—This method uses an image resection algorithm on aerial images. The image resection algorithm refines the exterior orientation (perspective, omega, phi, and kappa) of the image from known ground control points, using a least-square fitting method. Each image must have at least three noncollinear points. When the input is a mosaic dataset, it will register the selected images one at a time. | String |
| output_cpt_link_file(Optional) | If specified, a text file will be written containing the links created by this tool. This file can be used in the Warp From File tool. The output link table works with one mosaic dataset item in the mosaic layer. The input must specify which item is being processed, either selecting the item or specifying the ObjectID in the input. | Text File |
| maximum_rms_value(Optional) | The amount of modeled error (in pixels) that you want in the output. The default is 0.5, and values below 0.3 are not recommended as this leads to overfitting. | Double |

## Code Samples

### Example 1

```python
arcpy.management.RegisterRaster(in_raster, register_mode, {reference_raster}, {input_link_file}, {transformation_type}, {output_cpt_link_file}, {maximum_rms_value})
```

### Example 2

```python
import arcpy
arcpy.RegisterRaster_management(
     "\\cpu\data\nonref.tif", "REGISTER", "\\cpu\data\yesref.tif",
     "\\cpu\data\links.txt", "POLYORDER1", "#")
```

### Example 3

```python
import arcpy
arcpy.RegisterRaster_management(
     "\\cpu\data\nonref.tif", "REGISTER", "\\cpu\data\yesref.tif",
     "\\cpu\data\links.txt", "POLYORDER1", "#")
```

### Example 4

```python
# Register raster using only control points

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
rdname = "irs_ps.img"
mode = "REGISTER"
refrd = ""
linkfile = "C:/Workspace/irs_controls_13.txt"
order = "POLYORDER2"
    
arcpy.RegisterRaster_management(
     rdname, mode, refrd, linkfile, order)
```

### Example 5

```python
# Register raster using only control points

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
rdname = "irs_ps.img"
mode = "REGISTER"
refrd = ""
linkfile = "C:/Workspace/irs_controls_13.txt"
order = "POLYORDER2"
    
arcpy.RegisterRaster_management(
     rdname, mode, refrd, linkfile, order)
```

---

## Register With Geodatabase (Data Management)

## Summary

Registers feature classes, tables, views, and raster layers with the geodatabase. Registering is used for data created in the database with third-party tools using SQL or in ArcGIS Pro with tools that do not register with the geodatabase (Create Unregistered Feature Class, Create Unregistered Table, and Create Database View tools).

## Usage

- Views you create in file and enterprise geodatabases using the Create Database View tool can be registered with the geodatabase.
- To register with an enterprise geodatabase, you must be connected as the owner of the input dataset.
- Views that are registered with the geodatabase cannot participate in some geodatabase behavior and are read-only through ArcGIS clients. Views are not supported in feature services even if you register them with the geodatabase.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Datasets | The feature class, table, view, or raster created using third-party tools or SQL, or the view created using the Create Database View tool that will be registered with the geodatabase. The dataset must exist in the same database as the geodatabase. The workspaces supported include file, mobile, and enterprise geodatabases. | Table View; Raster Layer |
| Object ID Field (Optional) | The field that will be used as the ObjectID field. When using an existing field from the input datasets, an integer data type is required. If an existing field is not used, an ObjectID field will be created and populated.Note:When registering a view, an existing integer field is required. | Field |
| Shape Field (Optional) | The field that identifies the shape of the features. If the input dataset contains a spatial data type column, include this field during the registration process. | Field |
| Geometry Type (Optional) | Specifies the geometry type. If the Shape Field parameter value is provided, you must specify a geometry type. Supported geometry types are point, multipoint, polygon, polyline, and multipatch. If the dataset being registered contains existing features, the geometry type specified must match the entity type of these features.Point—The geometry type will be point.Multipoint—The geometry type will be multipoint.Polygon—The geometry type will be polygon.Polyline—The geometry type will be polyline.Multipatch—The geometry type will be multipatch. | String |
| Coordinate System (Optional) | If the Shape Field parameter value is provided and the table is empty, specify the coordinate system to be used for features. If the dataset being registered contains existing features, the coordinate system specified must match the coordinate system of the existing features. | Spatial Reference |
| Specify Extent (Optional) | If the Shape Field parameter value is provided, specify the allowable coordinate range for x,y coordinates. If the dataset being registered contains existing features, the extent of the existing features will be used. | Envelope |
| in_dataset | The feature class, table, view, or raster created using third-party tools or SQL, or the view created using the Create Database View tool that will be registered with the geodatabase. The dataset must exist in the same database as the geodatabase. The workspaces supported include file, mobile, and enterprise geodatabases. | Table View; Raster Layer |
| in_object_id_field(Optional) | The field that will be used as the ObjectID field. When using an existing field from the input datasets, an integer data type is required. If an existing field is not used, an ObjectID field will be created and populated.Note:When registering a view, an existing integer field is required. | Field |
| in_shape_field(Optional) | The field that identifies the shape of the features. If the input dataset contains a spatial data type column, include this field during the registration process. | Field |
| in_geometry_type(Optional) | Specifies the geometry type. If the in_shape_field parameter value is provided, you must specify a geometry type. If the dataset being registered contains existing features, the geometry type specified must match the entity type of these features.POINT—The geometry type will be point.MULTIPOINT—The geometry type will be multipoint.POLYGON—The geometry type will be polygon.POLYLINE—The geometry type will be polyline.MULTIPATCH—The geometry type will be multipatch. | String |
| in_spatial_reference(Optional) | If the in_shape_field parameter value is provided and the table is empty, specify the coordinate system to be used for features. If the dataset being registered contains existing features, the coordinate system specified must match the coordinate system of the existing features. Valid values are a Spatial Reference object, a file with a .prj extension, or a string representation of a coordinate system. | Spatial Reference |
| in_extent(Optional) | If the in_shape_field parameter value is provided, specify the allowable coordinate range for x,y coordinates in the following order: "XMin YMin XMax YMax". If the dataset being registered contains existing features, the extent of the existing features will be used. | Envelope |

## Code Samples

### Example 1

```python
arcpy.management.RegisterWithGeodatabase(in_dataset, {in_object_id_field}, {in_shape_field}, {in_geometry_type}, {in_spatial_reference}, {in_extent})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"Database Connections/connection.sde"
arcpy.management.RegisterWithGeodatabase("database.owner.COUNTIES", "OID", 
                                         "Shape", "POINT")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"Database Connections/connection.sde"
arcpy.management.RegisterWithGeodatabase("database.owner.COUNTIES", "OID", 
                                         "Shape", "POINT")
```

### Example 4

```python
# RegisterWithGeodatabase.py
# Description: Simple example showing use of RegisterWithGeodatabase tool
 
# Import system modules
import arcpy

# Set variables
inTable = r"c:\connectionFiles\Connection to esriServer.sde\database.dbo.cities"
oid_field = "OID"
shape_field = "Shape"
geometry_type = "POINT"
sr = arcpy.SpatialReference(4326)
in_extent = "11 10 14 34"

# Process: Register With Geodatabase
arcpy.management.RegisterWithGeodatabase(inTable, oid_field, shape_field, 
                                         geometry_type, sr, in_extent)
```

### Example 5

```python
# RegisterWithGeodatabase.py
# Description: Simple example showing use of RegisterWithGeodatabase tool
 
# Import system modules
import arcpy

# Set variables
inTable = r"c:\connectionFiles\Connection to esriServer.sde\database.dbo.cities"
oid_field = "OID"
shape_field = "Shape"
geometry_type = "POINT"
sr = arcpy.SpatialReference(4326)
in_extent = "11 10 14 34"

# Process: Register With Geodatabase
arcpy.management.RegisterWithGeodatabase(inTable, oid_field, shape_field, 
                                         geometry_type, sr, in_extent)
```

### Example 6

```python
# RegisterWithGeodatabase.py
# Description: Example showing use of RegisterWithGeodatabase tool with a file 
#              gdb view.

# Import system modules
import arcpy

# Create a view in the geodatabase
arcpy.management.CreateDatabaseView("C:\\testdata\\mytest.gdb",
                                    "trees",
                                    "select objectid, owner, parcel from inventory where type = trees")

# Set variables
inTable = r"C:\\testdata\\mytest.gdb\\trees"

# Process: Register With Geodatabase
arcpy.management.RegisterWithGeodatabase(inTable, "objectid")
```

### Example 7

```python
# RegisterWithGeodatabase.py
# Description: Example showing use of RegisterWithGeodatabase tool with a file 
#              gdb view.

# Import system modules
import arcpy

# Create a view in the geodatabase
arcpy.management.CreateDatabaseView("C:\\testdata\\mytest.gdb",
                                    "trees",
                                    "select objectid, owner, parcel from inventory where type = trees")

# Set variables
inTable = r"C:\\testdata\\mytest.gdb\\trees"

# Process: Register With Geodatabase
arcpy.management.RegisterWithGeodatabase(inTable, "objectid")
```

---

## Remove 3D Formats From Multipatch (Data Management)

## Summary

Removes the 3D formats referenced by a 3D object feature layer.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The multipatch feature class that was converted to a 3D object feature class. | Feature Layer |
| Use multipatch materials (Optional) | Note:This parameter is no longer supported. The option to control whether multipatch materials were used was removed to improve this tool's usability. Materials will always be used when they are available through the 3D object feature layer, and they will be automatically removed when the 3D object feature layer capabilities are removed from the multipatch. Additional parameters are available using the dedicated API for multipatches. | Boolean |
| 3D Formats to Remove (Optional) | Specifies the 3D model formats referenced by the 3D object feature layer that will be removed. Only the formats that have been linked to the input features can be specified.Collada (.dae)—The COLLADA format will be removed.Autodesk Drawing (.dwg)—The DWG format will be removed.Autodesk (.fbx)—The Autodesk FilmBox format will be removed.Khronos Group glTF binary (.glb)—The binary Graphics Library Transmission format will be removed.Khronos Group glTF json (.gltf)—The JSON Graphics Library Transmission format will be removed.Industry Foundation Classes (.ifc)—The Industry Foundation Classes format will be removed.Wavefront (.obj)—The Wavefront format will be removed.Universal Scene Description (.usdc)— The Universal Scene Description format will be removed. Compressed Universal Scene Description (.usdz)— The compressed version of the Universal Scene Description format will be removed. | String |
| in_features | The multipatch feature class that was converted to a 3D object feature class. | Feature Layer |
| multipatch_materials(Optional) | Note:This parameter is no longer supported. The option to control whether multipatch materials were used was removed to improve this tool's usability. Materials will always be used when they are available through the 3D object feature layer, and they will be automatically removed when the 3D object feature layer capabilities are removed from the multipatch. Additional parameters are available using the dedicated API for multipatches. | Boolean |
| formats[[formats],...](Optional) | Specifies the 3D model formats referenced by the 3D object feature layer that will be removed. Only the formats that have been linked to the input features can be specified.FMT3D_DAE—The COLLADA format will be removed.FMT3D_DWG—The DWG format will be removed.FMT3D_FBX—The Autodesk FilmBox format will be removed.FMT3D_GLB—The binary Graphics Library Transmission format will be removed.FMT3D_GLTF—The JSON Graphics Library Transmission format will be removed.FMT3D_IFC—The Industry Foundation Classes format will be removed.FMT3D_OBJ—The Wavefront format will be removed.FMT3D_USDC— The Universal Scene Description format will be removed. FMT3D_USDZ— The compressed version of the Universal Scene Description format will be removed. | String |

## Code Samples

### Example 1

```python
arcpy.management.Remove3DFormats(in_features, {multipatch_materials}, {formats})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/city_models.gdb"
arcpy.management.Remove3DFormats('Downtown_Buildings', 
                                 'MULTIPATCH_WITHOUT_MATERIALS', 
                                 ['FMT3D_DAE', 'FMT3D_OBJ'])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/city_models.gdb"
arcpy.management.Remove3DFormats('Downtown_Buildings', 
                                 'MULTIPATCH_WITHOUT_MATERIALS', 
                                 ['FMT3D_DAE', 'FMT3D_OBJ'])
```

---

## Remove Attachments (Data Management)

## Summary

Removes attachments from geodatabase feature class or table records.

## Usage

- This tool does not honor selections.
- An alternative to using this tool is to delete selected records from the InputDataset__ATTACH table in the same geodatabase as the Input Dataset value, which stores attachments and maintains linkage to the Input Dataset value.
- This tool supports an ArcGIS Enterprise hosted feature layer as input.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | A geodatabase table or feature class from which attachments will be removed. Attachments are not removed directly from this table; they are removed from the related attachment table that stores the attachments. The dataset must have attachments enabled. | Table View |
| Input Join Field | A field from the Input Dataset parameter value that contains values that match the values in the Match Join Field parameter value. Records that have join field values that match the Input Dataset parameter value and the Match Table parameter value will have attachments removed. This field can be an Object ID field or any other identifying attribute. | Field |
| Match Table | A table that identifies which input records will have attachments removed. | Table View |
| Match Join Field | A field from the match table that indicates which records in the Input Dataset parameter value will have specified attachments removed. This field can have values that match the Input Dataset Object ID field or some other identifying attribute. | Field |
| Match Name Field (Optional) | A field from the match table that contains the names of the attachments that will be removed from the Input Dataset parameter value's records. If no name field is specified, all attachments will be removed from each record specified in the Match Join Field parameter value. If a name field is specified but a record has a null or empty value in the name field, all attachments will be removed from that record. This field's values should be the short names of the attachments to remove, not the full paths to the files used to make the original attachments. | Field |
| in_dataset | A geodatabase table or feature class from which attachments will be removed. Attachments are not removed directly from this table; they are removed from the related attachment table that stores the attachments. The dataset must have attachments enabled. | Table View |
| in_join_field | A field from the in_dataset parameter value that contains values that match the values in the in_match_join_field parameter value. Records that have join field values that match the in_dataset parameter value and the in_match_table parameter value will have attachments removed. This field can be an Object ID field or any other identifying attribute. | Field |
| in_match_table | A table that identifies which input records will have attachments removed. | Table View |
| in_match_join_field | A field from the match table that indicates which records in the in_dataset parameter value will have specified attachments removed. This field can have values that match the in_dataset Object ID field or some other identifying attribute. | Field |
| in_match_name_field(Optional) | A field from the match table that contains the names of the attachments that will be removed from the in_dataset parameter value's records. If no name field is specified, all attachments will be removed from each record specified in the in_match_join_field parameter value. If a name field is specified but a record has a null or empty value in the name field, all attachments will be removed from that record. This field's values should be the short names of the attachments to remove, not the full paths to the files used to make the original attachments. | Field |

## Code Samples

### Example 1

```python
arcpy.management.RemoveAttachments(in_dataset, in_join_field, in_match_table, in_match_join_field, {in_match_name_field})
```

### Example 2

```python
import arcpy
arcpy.management.RemoveAttachments(r"C:\Data\City.gdb\Parcels", "ParcelID", 
                                   r"C:\Data\matchtable.csv", "ParcelID", 
                                   "Picture")
```

### Example 3

```python
import arcpy
arcpy.management.RemoveAttachments(r"C:\Data\City.gdb\Parcels", "ParcelID", 
                                   r"C:\Data\matchtable.csv", "ParcelID", 
                                   "Picture")
```

### Example 4

```python
# Delete unnecessary attachments from a feature class using a match table
import arcpy

input = r"C:\Data\City.gdb\Parcels"
input_field = "OBJECTID"
match_field = "MatchID"
name_field = "Filename"
workspace = arcpy.Describe(input).path

# Create a new geodatabase Match Table for the RemoveAttachments tool
# to delete the designated attachments for the associated ObjectID and file name.
match_table = arcpy.management.CreateTable(workspace, "remove_matchtable")
arcpy.management.AddFields(match_table, [['MatchID', 'LONG'], ['Filename', 'TEXT']])

# Create a list to remove attachments pic1a.jpg and pic1b.jpg from feature 1,
# pic3.jpg from feature 3, and pic4.jpg from feature 4.
delete_list = [[1, "pic1a.jpg"], [1, "pic1b.jpg"], [3, "pic3.jpg"], [4, "pic4.jpg"]]

# Iterate through the delete list and write it to the Match Table.
with arcpy.da.InsertCursor(match_table, ['MatchID', 'FileName']) as match_cursor:
    for row in delete_list:
        new_row = row[0], row[1]
        match_cursor.insertRow(new_row)

del match_cursor

# Use the match table with the Remove Attachments tool.
arcpy.management.RemoveAttachments(input, input_field, match_table, match_field, name_field)
```

### Example 5

```python
# Delete unnecessary attachments from a feature class using a match table
import arcpy

input = r"C:\Data\City.gdb\Parcels"
input_field = "OBJECTID"
match_field = "MatchID"
name_field = "Filename"
workspace = arcpy.Describe(input).path

# Create a new geodatabase Match Table for the RemoveAttachments tool
# to delete the designated attachments for the associated ObjectID and file name.
match_table = arcpy.management.CreateTable(workspace, "remove_matchtable")
arcpy.management.AddFields(match_table, [['MatchID', 'LONG'], ['Filename', 'TEXT']])

# Create a list to remove attachments pic1a.jpg and pic1b.jpg from feature 1,
# pic3.jpg from feature 3, and pic4.jpg from feature 4.
delete_list = [[1, "pic1a.jpg"], [1, "pic1b.jpg"], [3, "pic3.jpg"], [4, "pic4.jpg"]]

# Iterate through the delete list and write it to the Match Table.
with arcpy.da.InsertCursor(match_table, ['MatchID', 'FileName']) as match_cursor:
    for row in delete_list:
        new_row = row[0], row[1]
        match_cursor.insertRow(new_row)

del match_cursor

# Use the match table with the Remove Attachments tool.
arcpy.management.RemoveAttachments(input, input_field, match_table, match_field, name_field)
```

---

## Remove Attribute Index (Data Management)

## Summary

Deletes an attribute index from an existing table, feature class, shapefile, or attributed relationship class.

## Usage

- This tool accepts shapefiles, geodatabase feature classes, and attribute relationship classes as input.
- If the Index Name parameter is empty when the Input Table parameter value has been provided, there are no attribute indexes in the dataset.
- Once an index has been added, it can be deleted and re-added at any point in the lifetime of the feature class or table.
- For enterprise geodatabase data that is not registered as versioned, you can delete both unique and nonunique indexes on GlobalID fields. However, if you remove the index on a GlobalID field, it is recommended that you re-add an index on the GlobalID field to improve performance for attribute queries on the feature class or table.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input containing the index or indexes that will be deleted. The input can be a table, a feature class, or an attributed relationship class. | Table View; Raster Layer; Mosaic Layer |
| Index Name or Indexed Item | The name of the index or indexes that will be deleted. | String |
| in_table | The input containing the index or indexes that will be deleted. The input can be a table, a feature class, or an attributed relationship class. | Table View; Raster Layer; Mosaic Layer |
| index_name[index_name,...] | The name of the index or indexes that will be deleted. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveIndex(in_table, index_name)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/input/indices.gdb"
arcpy.management.RemoveIndex("lakes", ["IndexA", "IndexB"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/input/indices.gdb"
arcpy.management.RemoveIndex("lakes", ["IndexA", "IndexB"])
```

### Example 4

```python
import arcpy
 
# Set a default workspace
arcpy.env.workspace = "c:/data"
 
# Remove two indexes from the feature class
arcpy.management.RemoveIndex("/county.gdb/lots", ["indexa", "indexb"])
```

### Example 5

```python
import arcpy
 
# Set a default workspace
arcpy.env.workspace = "c:/data"
 
# Remove two indexes from the feature class
arcpy.management.RemoveIndex("/county.gdb/lots", ["indexa", "indexb"])
```

---

## Remove Contingent Value (Data Management)

## Summary

Removes a contingent value from a field group.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Target Table | The input geodatabase feature class or table containing the contingent value that will be removed. | Table View |
| Contingent Value ID | The unique contingent value ID. To view the contingent value ID in the Contingent Values view, click the Toggle Value IDs button on the ribbon. In Python, this value can be accessed using the arcpy.da.ListContingentValues function. | String |
| target_table | The input geodatabase feature class or table containing the contingent value that will be removed. | Table View |
| id | The unique contingent value ID. To view the contingent value ID in the Contingent Values view, click the Toggle Value IDs button on the ribbon. In Python, this value can be accessed using the arcpy.da.ListContingentValues function. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveContingentValue(target_table, id)
```

### Example 2

```python
import arcpy
arcpy.RemoveContingentValue_management(
    "C:\\MyProject\\myCOnn.sde\\mygdb.USER1.myFC", 10)
```

### Example 3

```python
import arcpy
arcpy.RemoveContingentValue_management(
    "C:\\MyProject\\myCOnn.sde\\mygdb.USER1.myFC", 10)
```

---

## Remove Depth Map (Data Management)

## Summary

Removes the depth map from a mosaic dataset. Other than the depth map removal, the tool will not update the mosaic dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The mosaic dataset that will have the depth map removed. | Mosaic Dataset; Mosaic Layer |
| Query Definition (Optional) | An SQL expression that will be used to select items in the mosaic dataset depth map to be removed. If not specified, all depth map content in the source raster’s folder will be removed. | SQL Expression |
| in_mosaic_dataset | The mosaic dataset that will have the depth map removed. | Mosaic Dataset; Mosaic Layer |
| where_clause(Optional) | An SQL expression that will be used to select items in the mosaic dataset depth map to be removed. If not specified, all depth map content in the source raster’s folder will be removed. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.RemoveDepthMap(in_mosaic_dataset, {where_clause})
```

### Example 2

```python
# Tool in Ortho Mapping toolset 

# Import system modules 

import arcpy

# Execute 

arcpy.management.RemoveDepthMap(in_mosaic_dataset= r"C:\CDM.gdb\YVWD", where_clause="OBJECTID >2")
```

### Example 3

```python
# Tool in Ortho Mapping toolset 

# Import system modules 

import arcpy

# Execute 

arcpy.management.RemoveDepthMap(in_mosaic_dataset= r"C:\CDM.gdb\YVWD", where_clause="OBJECTID >2")
```

### Example 4

```python
# Tool in Ortho Mapping toolset 

# Import system modules 

import arcpy 

# Define input parameters. All depth map content will be removed if not specified in where_clause. 

in_mosaic_dataset= r"C:\CDM_RM.gdb\YVWD" 

# Execute 

arcpy.management.RemoveDepthMap(in_mosaic_dataset)
```

### Example 5

```python
# Tool in Ortho Mapping toolset 

# Import system modules 

import arcpy 

# Define input parameters. All depth map content will be removed if not specified in where_clause. 

in_mosaic_dataset= r"C:\CDM_RM.gdb\YVWD" 

# Execute 

arcpy.management.RemoveDepthMap(in_mosaic_dataset)
```

---

## Remove Domain From Field (Data Management)

## Summary

Removes an attribute domain association from a feature class or table field.

## Usage

- This tool is the opposite operation from the Assign Domain To Field function. Removing a domain from a field removes the association between a field and an attribute domain.
- When a domain is removed from a field, the attribute validation rule for that field is removed from the database.
- The same attribute domain can be associated with multiple fields of the same table, feature class, or subtype as well as with multiple tables and feature classes. Removing a domain from a field will not affect other domain associations.
- Current map layers may be used to define the Input table.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table containing the attribute domain that will be removed. | Table View |
| Field Name | The field that will no longer be associated with an attribute domain. | Field |
| Subtype(Optional) | The subtype code(s) that will no longer be associated with an attribute domain. | String |
| in_table | The input table containing the attribute domain that will be removed. | Table View |
| field_name | The field that will no longer be associated with an attribute domain. | Field |
| subtype_code[subtype_code,...](Optional) | The subtype code(s) that will no longer be associated with an attribute domain. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveDomainFromField(in_table, field_name, {subtype_code})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.RemoveDomainFromField_management("montgomery.gdb/water/distribmains", "DIAMETER")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.RemoveDomainFromField_management("montgomery.gdb/water/distribmains", "DIAMETER")
```

### Example 4

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"

# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)
 
# Process: Create a table from the domain to edit it with ArcMap editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)

# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

### Example 5

```python
# Name: RemoveDomainsExample.py
# Purpose: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
import os
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "data"
 
# set local parameters
inFeatures = "Montgomery.gdb/Water/DistribMains"
inField = "MATERIAL"
dWorkspace = "Montgomery.gdb"
domName = "Material"
codedValue =  "ACP: Asbestos concrete"
codeField = "TYPE"
descField = "DESRIPT"

# Process: Remove the constraint from the material field
arcpy.RemoveDomainFromField_management(inFeatures, inField)
 
# Edit the domain values
# Process: Remove a coded value from the domain
arcpy.DeleteCodedValueFromDomain_management(dWorkspace, domName, codedValue)
 
# Process: Create a table from the domain to edit it with ArcMap editing tools
arcpy.DomainToTable_management(dWorkspace, domname, 
                               os.path.join(dWorkspace, domname), codeField, 
                               descField)

# Process: Delete the domain
arcpy.DeleteDomain_management(dWorkspace, domName)
 
# Edit the domain table outside of geoprocessing
# and then bring the domain back in with the TableToDomain process
```

---

## Remove Feature Class From Topology (Data Management)

## Summary

Removes a feature class from a topology.

## Usage

- Removing a feature class from a topology also removes all the topology rules associated with that feature class.
- Removing a feature class from a topology will require the entire topology to be validated.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology from which to remove the feature class. | Topology Layer |
| Feature Class to Remove | The feature class to remove from the topology. | String |
| in_topology | The topology from which to remove the feature class. | Topology Layer |
| in_featureclass | The feature class to remove from the topology. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveFeatureClassFromTopology(in_topology, in_featureclass)
```

### Example 2

```python
import arcpy
arcpy.RemoveFeatureClassFromTopology_management("C:/Datasets/TestGPTopology.gdb/LegalFabric/topology", "Parcel_line")
```

### Example 3

```python
import arcpy
arcpy.RemoveFeatureClassFromTopology_management("C:/Datasets/TestGPTopology.gdb/LegalFabric/topology", "Parcel_line")
```

### Example 4

```python
# Name: RemoveClassFromTopology_Example.py
# Description: Removes a feature class from participating in a topology

# Import system modules
import arcpy

topo = "C:/Datasets/TestGPTopology.gdb/LegalFabric/topology"
fc = "Parcel_line"
arcpy.RemoveFeatureClassFromTopology_management(topo, fc)
```

### Example 5

```python
# Name: RemoveClassFromTopology_Example.py
# Description: Removes a feature class from participating in a topology

# Import system modules
import arcpy

topo = "C:/Datasets/TestGPTopology.gdb/LegalFabric/topology"
fc = "Parcel_line"
arcpy.RemoveFeatureClassFromTopology_management(topo, fc)
```

---

## Remove Field Conflict Filter (Data Management)

## Summary

Removes a field conflict filter for a given field in a geodatabase table or feature class.

## Usage

- When running from the tool dialog, only fields that already have filters applied will be displayed.
- The ArcPy function arcpy.da.ListFieldConflictFilters can be used to identify fields that have filters applied.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | Table or feature class containing the field or fields to be removed as conflict filters. | Table View |
| Field Name | Field or list of fields to be removed as conflict filters. | Field |
| table | Table or feature class containing the field or fields to be removed as conflict filters. | Table View |
| fields[fields,...] | Field or list of fields to be removed as conflict filters. | Field |

## Code Samples

### Example 1

```python
arcpy.management.RemoveFieldConflictFilter(table, fields)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/Connections/airport.sde"
arcpy.RemoveFieldConflictFilter_management("Primary_UG", "phase")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/Connections/airport.sde"
arcpy.RemoveFieldConflictFilter_management("Primary_UG", "phase")
```

### Example 4

```python
import arcpy
arcpy.env.workspace = "d:/Connections/airport.sde"
arcpy.RemoveFieldConflictFilter_management("Primary_UG", ["phase", "material"])
```

### Example 5

```python
import arcpy
arcpy.env.workspace = "d:/Connections/airport.sde"
arcpy.RemoveFieldConflictFilter_management("Primary_UG", ["phase", "material"])
```

---

## Remove Files From LAS Dataset (Data Management)

## Summary

Removes one or more LAS files and surface constraint features from a LAS dataset.

## Usage

- File paths must use the folder separator associated with the operating system.
- Only a folder that directly contains LAS files can be provided as an input. When a folder is specified, all LAS files that reside in that folder will be removed from the LAS dataset.
- Surface constraint features need only be cited by their name, not their extension or path. For example, boundary.shp and sample.gdb/boundary are referred to as boundary.
- The presence of a LAS dataset pyramid changes the LAS dataset's schema, rendering the LAS dataset unusable in ArcGIS Desktop or in ArcGIS Pro releases prior to 2.6. This tool can be used to delete the pyramid to allow those applications to use the LAS dataset. However, creating a new LAS dataset for those applications is preferable since the processing time for generating the LAS dataset display pyramid is significantly greater than the time for creating a LAS dataset. Only consider deleting the display pyramid if a different pyramid point selection method is desired.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input LAS Dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| LAS Files or Folders(Optional) | The name of the LAS files or folders containing LAS files whose reference will be removed from the LAS dataset. | String |
| Surface Constraints(Optional) | The name of the surface constraint features that will be removed from the LAS dataset. | String |
| Delete Pyramid (Optional) | Specifies whether the LAS dataset's display pyramid will be deleted. Checked—The LAS dataset's display pyramid will be deleted.Unchecked—The LAS dataset's display pyramid will not be deleted. This is the default. | Boolean |
| in_las_dataset | The LAS dataset that will be processed. | LAS Dataset Layer |
| in_files[in_files,...](Optional) | The name of the LAS files or folders containing LAS files whose reference will be removed from the LAS dataset. | String |
| in_surface_constraints[in_surface_constraints,...](Optional) | The name of the surface constraint features that will be removed from the LAS dataset. | String |
| delete_pyramid(Optional) | Specifies whether the LAS dataset's display pyramid will be deleted.DELETE_PYRAMID—The LAS dataset's display pyramid will be deleted.NO_DELETE_PYRAMID—The LAS dataset's display pyramid will not be deleted. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RemoveFilesFromLasDataset(in_las_dataset, {in_files}, {in_surface_constraints}, {delete_pyramid})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.RemoveFilesFromLasDataset_management("test.lasd", 
                                           "LA_N; LA_S/LA_5S4E.las",
                                           "boundary.shp; streams.shp")
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data"
arcpy.RemoveFilesFromLasDataset_management("test.lasd", 
                                           "LA_N; LA_S/LA_5S4E.las",
                                           "boundary.shp; streams.shp")
```

### Example 4

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

### Example 5

```python
'''*********************************************************************
Name: Modify Files in LAS Dataset& Calculate Stats for LASD
Description: Adds files & surface constraints to a LAS dataset, then
             calculates statistics and generates report.
*********************************************************************'''
# Import system modules
import arcpy

try:
    # Script variables
    arcpy.env.workspace = 'C:/data'
    lasd = 'sample.lasd'
    oldLas = ['2006', '2007/file2.las']
    newLas = ['2007_updates_1', '2007_updates_2']
    oldSurfaceConstraints = ['boundary.shp', 'streams.shp']
    newSurfaceConstraints = [['sample.gdb/boundary', '<None>',
                              'Soft_Clip']
                             ['sample.gdb/streams', 'Shape.Z',
                              'Hard_Line']]
    arcpy.management.RemoveFilesFromLasDataset(lasd, oldLas,
                                               oldSurfaceConstraints)
    arcpy.management.AddFilesToLasDataset(lasd, newLas, 'RECURSION',
                                          newSurfaceConstraints)
    arcpy.management.LasDatasetStatistics(lasd, "UPDATED_FILES",
                                          "lasd_stats.txt",
                                          "LAS_FILE", "DECIMAL_POINT",
                                          "SPACE", "LAS_summary.txt")
except arcpy.ExecuteError:
    print(arcpy.GetMessages())
except Exception as err:
    print(err.args[0])
```

---

## Remove Join (Data Management)

## Summary

Removes a join from a feature layer or table view.

## Usage

- The Join parameter value is the name of the table that was joined to the input layer or table view.If the join table was a dBASE file named MyTable.dbf, the join name would be MyTable; so to remove it, specify MyTable.If the join table was a geodatabase or INFO table named MyTable2, the Join Name would be MyTable2; so to remove it, specify MyTable2.The join name will not reflect the name of the table view, but rather the source of the table view. For example, if a table view is named TableView1 and points to mytable.dbf, the name of the join will be mytable .
- If the join table was a dBASE file named MyTable.dbf, the join name would be MyTable; so to remove it, specify MyTable.
- If the join table was a geodatabase or INFO table named MyTable2, the Join Name would be MyTable2; so to remove it, specify MyTable2.
- The join name will not reflect the name of the table view, but rather the source of the table view. For example, if a table view is named TableView1 and points to mytable.dbf, the name of the join will be mytable .
- When a layer is joined to two tables and the first join is removed, both joins will be removed. For example, Layer1 is joined to TableA. Then Layer1 is joined to TableB. If the join to TableA is removed, the join to TableB is also removed.
- In ModelBuilder, you can use the Make Feature Layer tool to create a layer from a feature class, and the Make Table View tool to create a table view from an input table or feature class. These layers or table views can then be used as the input to the Add Join and Remove Join tools.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Layer Name or Table View | The layer or table view from which the join will be removed. | Mosaic Layer; Raster Layer; Table View |
| Join(Optional) | The name of the join to be removed.If no name is provided, the tool will remove all joins from the input. | String |
| in_layer_or_view | The layer or table view from which the join will be removed. | Mosaic Layer; Raster Layer; Table View |
| join_name(Optional) | The name of the join to be removed.If no name is provided, the tool will remove all joins from the input. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveJoin(in_layer_or_view, {join_name})
```

### Example 2

```python
import arcpy
arcpy.management.RemoveJoin("veglayer", "vegtable")
```

### Example 3

```python
import arcpy
arcpy.management.RemoveJoin("veglayer", "vegtable")
```

### Example 4

```python
# AddFieldFromJoin.py
# Description: Add a field to a table, and calculate its values based
#              on the values in a field from a joined table

# Import system modules
import arcpy

# set the environments
arcpy.env.workspace = "C:/data"
arcpy.env.qualifiedFieldNames = "UNQUALIFIED"
    
# Define script parameters    
inFeatures = "Habitat_Analysis.gdb/vegtype"
layerName = "veg_layer"
newField = "description"
joinTable = "vegtable.dbf"
joinField = "HOLLAND95"
calcExpression = "!vegtable.VEG_TYPE!"
outFeature = "Habitat_Analysis.gdb/vegjoin335"
    
# Add the new field
arcpy.management.AddField(inFeatures, newField, "TEXT")
    
# Create a feature layer from the vegtype feature class
arcpy.management.MakeFeatureLayer(inFeatures,  layerName)
    
# Join the feature layer to a table
arcpy.management.AddJoin(layerName, joinField, joinTable, joinField)
    
# Populate the newly created field with values from the joined table
arcpy.management.CalculateField(layerName, newField, calcExpression, "PYTHON")
    
# Remove the join
arcpy.management.RemoveJoin(layerName, "vegtable")
    
# Copy the layer to a new permanent feature class
arcpy.management.CopyFeatures(layerName, outFeature)
```

### Example 5

```python
# AddFieldFromJoin.py
# Description: Add a field to a table, and calculate its values based
#              on the values in a field from a joined table

# Import system modules
import arcpy

# set the environments
arcpy.env.workspace = "C:/data"
arcpy.env.qualifiedFieldNames = "UNQUALIFIED"
    
# Define script parameters    
inFeatures = "Habitat_Analysis.gdb/vegtype"
layerName = "veg_layer"
newField = "description"
joinTable = "vegtable.dbf"
joinField = "HOLLAND95"
calcExpression = "!vegtable.VEG_TYPE!"
outFeature = "Habitat_Analysis.gdb/vegjoin335"
    
# Add the new field
arcpy.management.AddField(inFeatures, newField, "TEXT")
    
# Create a feature layer from the vegtype feature class
arcpy.management.MakeFeatureLayer(inFeatures,  layerName)
    
# Join the feature layer to a table
arcpy.management.AddJoin(layerName, joinField, joinTable, joinField)
    
# Populate the newly created field with values from the joined table
arcpy.management.CalculateField(layerName, newField, calcExpression, "PYTHON")
    
# Remove the join
arcpy.management.RemoveJoin(layerName, "vegtable")
    
# Copy the layer to a new permanent feature class
arcpy.management.CopyFeatures(layerName, outFeature)
```

---

## Remove Rasters From Mosaic Dataset (Data Management)

## Summary

Removes selected rasters from a mosaic dataset.

## Usage

- You must specify a selection or a query ; otherwise, the tool will not run. To delete all the records from the mosaic dataset, specify a query that selects all the rasters, such as "OBJECTID>=0".
- If the overviews are generated in the mosaic dataset, they will be deleted when they are removed, because they are managed by the mosaic dataset. If you created the overviews in a folder or a location other than the default location, they are not fully managed by the mosaic dataset and you can remove them without deleting them from disk. You may want to do this if you are using the overviews elsewhere.
- If you identify the affected overviews but do not delete them, you can use the Build Overviews tool to regenerate the affected overviews.
- This tool will also delete the cache created for each item in the mosaic dataset. Both raster cache and LAS cache can be removed. The properties for the cache for these datasets is defined in their functions.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset containing the rasters that will be removed. | Mosaic Layer |
| Query Definition(Optional) | An SQL expression to select the raster datasets that will be removed from the mosaic dataset.You must specify a selection or a query; otherwise, the tool will not run. To delete all the records from the mosaic dataset, specify a query that selects all the rasters, such as "OBJECTID>=0". | SQL Expression |
| Update Boundary (Optional) | Specifies whether the boundary polygon of the mosaic dataset will be updated. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels.Checked—The boundary polygon of the mosaic dataset will be updated. This is the default.Unchecked—The boundary polygon of the mosaic dataset will not be updated. | Boolean |
| Mark Affected Overviews (Optional) | Specifies whether affected overviews will be identified.When the rasters in a mosaic dataset have been removed, overviews created using those rasters may no longer be accurate. Use this parameter to identify affected overviews so they can be updated or removed if they are no longer needed.Checked—The affected overviews will be identified. This is the default.Unchecked—The affected overviews will not be identified. | Boolean |
| Delete Overview Images (Optional) | Specifies whether the overviews associated with the selected rasters will be deleted.Checked—The overviews associated with the selected rasters will be deleted. This is the default.Unchecked—The overviews associated with the selected rasters will not be deleted. | Boolean |
| Delete Item Cache (Optional) | Specifies whether the cache that is based on any source raster dataset that will be removed from the mosaic dataset will also be removed.Checked—The cache that is based on any source raster dataset that will be removed from the mosaic dataset will also be removed. This is the default.Unchecked—The cache will not be removed and will remain a part of the mosaic dataset. | Boolean |
| Remove Mosaic Dataset Items (Optional) | Specifies whether mosaic dataset items will be removed.Checked—Mosaic dataset items will be removed. This is the default.Unchecked—Mosaic dataset items will not be removed. | Boolean |
| Update Cell Size Ranges (Optional) | Specifies whether the cell size ranges for the mosaic dataset will be updated.Checked—The cell size ranges for the mosaic dataset will be updated. This is the default.Unchecked—The cell size ranges for the mosaic dataset will not be updated | Boolean |
| in_mosaic_dataset | The mosaic dataset containing the rasters that will be removed. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select the raster datasets that will be removed from the mosaic dataset.You must specify a selection or a query; otherwise, the tool will not run. To delete all the records from the mosaic dataset, specify a query that selects all the rasters, such as "OBJECTID>=0". | SQL Expression |
| update_boundary(Optional) | Specifies whether the boundary polygon of the mosaic dataset will be updated. By default, the boundary merges all the footprint polygons to create a single boundary representing the extent of the valid pixels.UPDATE_BOUNDARY—The boundary polygon of the mosaic dataset will be updated. This is the default.NO_BOUNDARY— The boundary polygon of the mosaic dataset will not be updated. | Boolean |
| mark_overviews_items(Optional) | Specifies whether affected overviews will be identified.When the rasters in a mosaic dataset have been removed, overviews created using those rasters may no longer be accurate. Use this parameter to identify affected overviews so they can be updated or removed if they are no longer needed.MARK_OVERVIEW_ITEMS—The affected overviews will be identified. This is the default.NO_MARK_OVERVIEW_ITEMS—The affected overviews will not be identified. | Boolean |
| delete_overview_images(Optional) | Specifies whether the overviews associated with the selected rasters will be removed.DELETE_OVERVIEW_IMAGES—The overviews associated with the selected rasters will be deleted. This is the default.NO_DELETE_OVERVIEW_IMAGES—The overviews associated with the selected rasters will not be deleted. | Boolean |
| delete_item_cache(Optional) | Specifies whether the cache that is based on any source raster dataset that will be removed from the mosaic dataset will also be removed.DELETE_ITEM_CACHE—The cache that is based on any source raster dataset that will be removed from the mosaic dataset will also be removed. This is the default.NO_DELETE_ITEM_CACHE—The cache will not be removed and will remain a part of the mosaic dataset. | Boolean |
| remove_items(Optional) | Specifies whether mosaic dataset items will be removed.REMOVE_MOSAICDATASET_ITEMS—Mosaic dataset items will be removed. This is the default.NO_REMOVE_MOSAICDATASET_ITEMS—Mosaic dataset items will not be removed. | Boolean |
| update_cellsize_ranges(Optional) | Specifies whether the cell size ranges for the mosaic dataset will be updated.UPDATE_CELL_SIZES—The cell size ranges for the mosaic dataset will be updated. Use this if you are removing all of the imagery at a specific cell size. This is the default.NO_CELL_SIZES—The cell size ranges for the mosaic dataset will not be updated. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RemoveRastersFromMosaicDataset(in_mosaic_dataset, {where_clause}, {update_boundary}, {mark_overviews_items}, {delete_overview_images}, {delete_item_cache}, {remove_items}, {update_cellsize_ranges})
```

### Example 2

```python
import arcpy
arcpy.RemoveRastersFromMosaicDataset_management(
     "C:/Workspace/remove.gdb/md", "YEAR<1999", "UPDATE_BOUNDARY", 
     "MARK_OVERVIEW_ITEMS",  "#", "#", "#", "#")
```

### Example 3

```python
import arcpy
arcpy.RemoveRastersFromMosaicDataset_management(
     "C:/Workspace/remove.gdb/md", "YEAR<1999", "UPDATE_BOUNDARY", 
     "MARK_OVERVIEW_ITEMS",  "#", "#", "#", "#")
```

### Example 4

```python
#Delete Overviews with Query

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "remove.gdb/md2"
query = "#"
updatebnd = "#"
markovr = "#"
delovr = "DELETE_OVERVIEW_IMAGES"
delitemcache = "#"
removeitem = "NO_REMOVE_MOSAICDATASET_ITEMS"
updatecs = "UPDATE_CELL_SIZES"

arcpy.RemoveRastersFromMosaicDataset_management(
     mdname, query, updatebnd, markovr, delovr, delitemcache, 
     removeitem, updatecs)
```

### Example 5

```python
#Delete Overviews with Query

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "remove.gdb/md2"
query = "#"
updatebnd = "#"
markovr = "#"
delovr = "DELETE_OVERVIEW_IMAGES"
delitemcache = "#"
removeitem = "NO_REMOVE_MOSAICDATASET_ITEMS"
updatecs = "UPDATE_CELL_SIZES"

arcpy.RemoveRastersFromMosaicDataset_management(
     mdname, query, updatebnd, markovr, delovr, delitemcache, 
     removeitem, updatecs)
```

---

## Remove Relate (Data Management)

## Summary

Removes a relate from a feature layer or a table view.

## Usage

- The Make Feature Layer tool makes a layer from a feature class, and the Make Table View tool creates a table view from an input table or feature class. The layer or table view can then be used as the input to the Add Relate and Remove Relate tools.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Layer Name or Table View | The layer or table view from which to remove the relate. | Mosaic Layer; Raster Layer; Table View |
| Relate Name | The name of the relate to remove. | String |
| in_layer_or_view | The layer or table view from which to remove the relate. | Mosaic Layer; Raster Layer; Table View |
| relate_name | The name of the relate to remove. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveRelate(in_layer_or_view, relate_name)
```

### Example 2

```python
import arcpy
arcpy.RemoveRelate_management( "Parcel_layer", "Owner2Parcel")
```

### Example 3

```python
import arcpy
arcpy.RemoveRelate_management( "Parcel_layer", "Owner2Parcel")
```

---

## Remove Rule From Relationship Class (Data Management)

## Summary

Removes a rule from a relationship class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Relationship Class | The relationship class with the rule to remove. | Relationship Class |
| Origin Subtype (Optional) | If the origin class has subtypes, the subtype that is associated with the relationship class rule to be deleted. | String |
| Destination Subtype (Optional) | If the destination class has subtypes, the subtype that is associated with the relationship class rule to be deleted. | String |
| Remove All (Optional) | Specifies the relationship rules to be removed from the relationship class. Checked—All relationship rules will be removed from the input relationship class.Unchecked—Only rules from the origin and destination subtypes specified will be removed. This is the default. | Boolean |
| in_rel_class | The relationship class with the rule to remove. | Relationship Class |
| origin_subtype(Optional) | If the origin class has subtypes, the subtype that is associated with the relationship class rule to be deleted. | String |
| destination_subtype(Optional) | If the destination class has subtypes, the subtype that is associated with the relationship class rule to be deleted. | String |
| remove_all(Optional) | Specifies the relationship rules to be removed from the relationship class. REMOVE—All relationship rules will be removed from the input relationship class.NOT_ALL—Only rules from the origin and destination subtypes specified will be removed. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.RemoveRuleFromRelationshipClass(in_rel_class, {origin_subtype}, {destination_subtype}, {remove_all})
```

### Example 2

```python
import arcpy
arcpy.management.RemoveRuleFromRelationshipClasss(
    "C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", "Residential", 
    "House")
```

### Example 3

```python
import arcpy
arcpy.management.RemoveRuleFromRelationshipClasss(
    "C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", "Residential", 
    "House")
```

---

## Remove Rule From Topology (Data Management)

## Summary

Removes a rule from a topology.

## Usage

- When running this tool using scripting, the feature class ObjectClassID involved in the topology rule to be removed must be specified in parentheses after the rule name. For exampleMust Not Overlap (2) where "2" is the ObjectClassID of the feature class that participates in the Must Not Overlap rule that is to be removed from the topology. Must Be Properly Inside (78-79) where "78" and "79" are the ObjectClassID of the feature classes that participate in the Must Be Properly Inside rule that is to be removed from the topology See the tool code samples for an example. If you are using the tool from the tool dialog box the list of rules is presented in a drop-down list.Tip:You can find the ObjectClassID value for a feature class by right-clicking the layer in the Contents pane, select Properties, and click the Source tab. On the Feature Class row, click the Object Class ID button (it looks like a spyglass), and the ObjectClassID is displayed.
- Must Not Overlap (2) where "2" is the ObjectClassID of the feature class that participates in the Must Not Overlap rule that is to be removed from the topology.
- Must Be Properly Inside (78-79) where "78" and "79" are the ObjectClassID of the feature classes that participate in the Must Be Properly Inside rule that is to be removed from the topology
- Removing a rule will require the entire extent of the topology to be validated.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology from which to remove a rule. | Topology Layer |
| Rule | The topology rule to remove from the topology. | String |
| in_topology | The topology from which to remove a rule. | Topology Layer |
| in_rule | The topology rule to remove from the topology. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveRuleFromTopology(in_topology, in_rule)
```

### Example 2

```python
import arcpy
arcpy.RemoveRuleFromTopology_management("C:/CityData.gdb/LegalFabric/topology", "Must Not Have Dangles (21)")
```

### Example 3

```python
import arcpy
arcpy.RemoveRuleFromTopology_management("C:/CityData.gdb/LegalFabric/topology", "Must Not Have Dangles (21)")
```

### Example 4

```python
# Name: RemoveRuleFromTopology_Example.py
# Description: Removes a rule from a topology

# Import system modules
import arcpy

topo = "C:/CityData.mdb/LegalFabric/topology"
rule = "Must Not Have Dangles (21)"
arcpy.RemoveRuleFromTopology_management(topo, rule)
```

### Example 5

```python
# Name: RemoveRuleFromTopology_Example.py
# Description: Removes a rule from a topology

# Import system modules
import arcpy

topo = "C:/CityData.mdb/LegalFabric/topology"
rule = "Must Not Have Dangles (21)"
arcpy.RemoveRuleFromTopology_management(topo, rule)
```

---

## Remove Spatial Index (Data Management)

## Summary

Deletes the spatial index from a shapefile or file geodatabase, mobile geodatabase, or an enterprise geodatabase feature class.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The shapefile or file geodatabase, mobile geodatabase, or an enterprise geodatabase feature class from which a spatial index will be removed. | Feature Layer; Mosaic Layer |
| in_features | The shapefile or file geodatabase, mobile geodatabase, or an enterprise geodatabase feature class from which a spatial index will be removed. | Feature Layer; Mosaic Layer |

## Code Samples

### Example 1

```python
arcpy.management.RemoveSpatialIndex(in_features)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"
arcpy.RemoveSpatialIndex_management("LPI.Land/LPI.PLSSFirstDivision")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"
arcpy.RemoveSpatialIndex_management("LPI.Land/LPI.PLSSFirstDivision")
```

### Example 4

```python
# Name: RemoveSpatialIndex_Example2.py
# Description: Removes a spatial index from a SDE feature class.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"

# Set local variables
in_features = "LPI.Land/LPI.PLSSFirstDivision"

# Execute RemoveSpatialIndex
arcpy.RemoveSpatialIndex_management(in_features)
```

### Example 5

```python
# Name: RemoveSpatialIndex_Example2.py
# Description: Removes a spatial index from a SDE feature class.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "c:/Connections/Connection to esoracle.sde"

# Set local variables
in_features = "LPI.Land/LPI.PLSSFirstDivision"

# Execute RemoveSpatialIndex
arcpy.RemoveSpatialIndex_management(in_features)
```

---

## Remove Subtype (Data Management)

## Summary

Removes a subtype from the input table using its code.

## Usage

- Subtypes are removed using their integer code.
- You can also view and manage subtypes in Subtypes view which can be opened by clicking the Subtypes button found in the Design section of the Data ribbon, or the by clicking the Subtypes button on the Fields view ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The feature class or table containing the subtype definition. | Table View |
| Subtype Code | The subtype code to remove a subtype from the input table or feature class. | String |
| in_table | The feature class or table containing the subtype definition. | Table View |
| subtype_code[subtype_code,...] | The subtype code to remove a subtype from the input table or feature class. | String |

## Code Samples

### Example 1

```python
arcpy.management.RemoveSubtype(in_table, subtype_code)
```

### Example 2

```python
import arcpy
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
arcpy.RemoveSubtype_management ("water/fittings", ["4","7"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
arcpy.RemoveSubtype_management ("water/fittings", ["4","7"])
```

### Example 4

```python
#Name: RemoveSubtype.py
# Purpose: Remove subtypes from a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
  
# Set local parameters
inFeatures = "water/fittings"
stypeList = ["5", "6", "7"]
 
# Process: Remove Subtype Codes...
arcpy.RemoveSubtype_management(inFeatures, stypeList)
```

### Example 5

```python
#Name: RemoveSubtype.py
# Purpose: Remove subtypes from a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
  
# Set local parameters
inFeatures = "water/fittings"
stypeList = ["5", "6", "7"]
 
# Process: Remove Subtype Codes...
arcpy.RemoveSubtype_management(inFeatures, stypeList)
```

---

## Rename (Data Management)

## Summary

Changes the name of a dataset. This includes a variety of data types, including feature dataset, raster, table, and shapefile.

## Usage

- The output name must be unique. If it is not, an error message is reported, even if the geoprocessing Allow geoprocessing tools to overwrite existing datasets setting is enabled.
- The tool also renames the alias of the data.
- The tool does not rename layers, since a layer is a reference to a dataset.
- The tool does not rename fields in the dataset. For example, if you have a field named ROADS_ID in a feature class named Roads, renaming the Roads feature class to Streets does not rename the ROADS_ID field to STREETS_ID.
- The tool does not work with data stored in a DB2 database due to database constraints.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Data Element | The input data to be renamed. | Data Element |
| Output Data Element | The name of the output data. | Data Element |
| Data Type | The type of data to be renamed. This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |
| in_data | The input data to be renamed. | Data Element |
| out_data | The name of the output data. | Data Element |
| data_type | The type of data to be renamed. This parameter is only necessary in the event of a name conflict between two different data types. For example, a geodatabase can contain a relationship class with an identical name to a feature class. If that is the case, specify the relevant keyword.FeatureClass—In the event of duplicate names, the feature class will be used.FeatureDataset—In the event of duplicate names, the feature dataset will be used.MosaicDataset—In the event of duplicate names, the mosaic dataset will be used.ParcelFabric—In the event of duplicate names, the parcel fabric will be used.RelationshipClass—In the event of duplicate names, the relationship class will be used.Topology—In the event of duplicate names, the topology will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.Rename(in_data, out_data, data_type)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Rename("customers.dbf", "customers_2010.dbf")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.Rename("customers.dbf", "customers_2010.dbf")
```

### Example 4

```python
# Description: Rename a file geodatabase feature class

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/workspace/test.gdb"

# Set local variables
in_data =  "test"
out_data = "testFC"
data_type = "FeatureClass"

# Run Rename
arcpy.management.Rename(in_data, out_data, data_type)
```

### Example 5

```python
# Description: Rename a file geodatabase feature class

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/workspace/test.gdb"

# Set local variables
in_data =  "test"
out_data = "testFC"
data_type = "FeatureClass"

# Run Rename
arcpy.management.Rename(in_data, out_data, data_type)
```

---

## Reorder Attribute Rule (Data Management)

## Summary

Reorders the evaluation order of an attribute rule.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table that contains the attribute rule with the evaluation order that will be updated. | Table View |
| Calculation Rule Name | The name of the calculation rule that will have its evaluation order updated. | String |
| Evaluation Order | The new evaluation order for the rule. For example, if there are five rules and a particular rule is in position 5 (the fifth order position, to be evaluated last) but you want it to be evaluated in position 2 (to be evaluated second), enter 2 for the value. The evaluation order for the rules after position 2 will be reassigned (that is, position 2 becomes position 3, position 3 becomes position 4, and position 4 becomes position 5). | Long |
| in_table | The table that contains the attribute rule with the evaluation order that will be updated. | Table View |
| name | The name of the calculation rule that will have its evaluation order updated. | String |
| evaluation_order | The new evaluation order for the rule. For example, if there are five rules and a particular rule is in position 5 (the fifth order position, to be evaluated last) but you want it to be evaluated in position 2 (to be evaluated second), enter 2 for the value. The evaluation order for the rules after position 2 will be reassigned (that is, position 2 becomes position 3, position 3 becomes position 4, and position 4 becomes position 5). | Long |

## Code Samples

### Example 1

```python
arcpy.management.ReorderAttributeRule(in_table, name, evaluation_order)
```

### Example 2

```python
import arcpy
arcpy.management.ReorderAttributeRule("C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes", 
                                      "calculateRuleLabel", 1)
```

### Example 3

```python
import arcpy
arcpy.management.ReorderAttributeRule("C:\\MyProject\\sdeConn.sde\\progdb.user1.GasPipes", 
                                      "calculateRuleLabel", 1)
```

---

## Repair Geometry (Data Management)

## Summary

Inspects features for geometry problems and repairs them. If a problem is found, a repair will be performed, and a one-line description will identify the feature, as well as the geometry problem that was repaired.

## Usage

- Valid input formats are shapefiles, and feature classes stored in a file geodatabase, enterprise database, enterprise geodatabase, GeoPackage, or SpatiaLite database. For feature classes stored in an enterprise database or enterprise geodatabase, the following spatial types are supported:Microsoft SQL Server—Geometry and GeographyPostgreSQL—PostGIS, Geometry, and GeographyOracle—SDO_GeometrySAP HANA—ST_Geometry License:A Desktop Basic license allows you to apply this tool to only shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database. A Desktop Standard or Desktop Advanced license allows you to also apply this tool to feature classes stored in an enterprise database or enterprise geodatabase.
- Microsoft SQL Server—Geometry and Geography
- PostgreSQL—PostGIS, Geometry, and Geography
- Oracle—SDO_Geometry
- SAP HANA—ST_Geometry
- Feature classes that are stored in an enterprise geodatabase and registered as versioned are not supported.
- The Enable Undo toggle button is not valid for inputs from an enterprise geodatabase.
- Below is the list of geometry problems and the corresponding repair that will be performed by the tool:Null geometry—The record will be deleted from the feature class. To keep records with null geometry, uncheck the Delete Features with Null Geometry parameter.Short segment—The geometry's short segment will be deleted.Incorrect ring ordering—The geometry will be updated with the correct ring ordering.Incorrect segment orientation—The geometry will be updated with the correct segment orientation.Self intersections—The areas of overlap in a polygon will be dissolved.Unclosed rings—The unclosed rings will be closed by connecting the end points of the rings.Empty parts—The parts that are null or empty will be deleted.Duplicate vertex—One of the vertices will be deleted.Mismatched attributes—The z- or m-coordinate will be updated to match.Discontinuous parts—Multiple parts will be created from the existing discontinuous part.Empty Z values—The z-value will be set to 0.Bad envelope—The feature's envelope will be updated to be correct.Bad dataset extent—The feature class extent will be updated to be correct.Below is the list of geometry problems that may occur from data stored in an enterprise geodatabase and the corresponding repair (if one exists) that will be performed by the tool:NEEDS_REORDERING—The shape will be reordered and duplicate points will be removed. SE_INVALID_ENTITY_TYPE—The entity type will not be repaired (the feature must be deleted). SE_SHAPE_INTEGRITY_ERROR—The shape may not be repaired. SE_INVALID_SHAPE_OBJECT—The shape object may not be repaired. SE_COORD_OUT_OF_BOUNDS—The coordinate will not be repaired. SE_POLY_SHELLS_OVERLAP—Overlapping shells will be merged. SE_TOO_FEW_POINTS—The points will not be repaired. SE_INVALID_PART_SEPARATOR—The part separator may not be repaired. SE_INVALID_POLYGON_CLOSURE—Unclosed shells will be discarded (the resulting polygon may be left empty). SE_INVALID_OUTER_SHELL—An attempt to repair the feature's outer shells will be made; shells may be discarded in the process. SE_ZERO_AREA_POLYGON—The polygon will be converted to an empty shape. SE_POLYGON_HAS_VERTICAL_LINE—The vertical line will be removed (the shape may be converted to 2D). SE_OUTER_SHELLS_OVERLAP—Overlapping parts will be merged. SE_SELF_INTERSECTING—Intersection points will be added as needed.Note: Some problems associated with data stored in an enterprise database may not be repairable with ArcGIS tools.
- Null geometry—The record will be deleted from the feature class. To keep records with null geometry, uncheck the Delete Features with Null Geometry parameter.
- Short segment—The geometry's short segment will be deleted.
- Incorrect ring ordering—The geometry will be updated with the correct ring ordering.
- Incorrect segment orientation—The geometry will be updated with the correct segment orientation.
- Self intersections—The areas of overlap in a polygon will be dissolved.
- Unclosed rings—The unclosed rings will be closed by connecting the end points of the rings.
- Empty parts—The parts that are null or empty will be deleted.
- Duplicate vertex—One of the vertices will be deleted.
- Mismatched attributes—The z- or m-coordinate will be updated to match.
- Discontinuous parts—Multiple parts will be created from the existing discontinuous part.
- Empty Z values—The z-value will be set to 0.
- Bad envelope—The feature's envelope will be updated to be correct.
- Bad dataset extent—The feature class extent will be updated to be correct.
- NEEDS_REORDERING—The shape will be reordered and duplicate points will be removed.
- SE_INVALID_ENTITY_TYPE—The entity type will not be repaired (the feature must be deleted).
- SE_SHAPE_INTEGRITY_ERROR—The shape may not be repaired.
- SE_INVALID_SHAPE_OBJECT—The shape object may not be repaired.
- SE_COORD_OUT_OF_BOUNDS—The coordinate will not be repaired.
- SE_POLY_SHELLS_OVERLAP—Overlapping shells will be merged.
- SE_TOO_FEW_POINTS—The points will not be repaired.
- SE_INVALID_PART_SEPARATOR—The part separator may not be repaired.
- SE_INVALID_POLYGON_CLOSURE—Unclosed shells will be discarded (the resulting polygon may be left empty).
- SE_INVALID_OUTER_SHELL—An attempt to repair the feature's outer shells will be made; shells may be discarded in the process.
- SE_ZERO_AREA_POLYGON—The polygon will be converted to an empty shape.
- SE_POLYGON_HAS_VERTICAL_LINE—The vertical line will be removed (the shape may be converted to 2D).
- SE_OUTER_SHELLS_OVERLAP—Overlapping parts will be merged.
- SE_SELF_INTERSECTING—Intersection points will be added as needed.
- After performing a repair, the tool will reevaluate the resulting geometry and if another problem is found, the relevant repair will be performed for that problem. For example, the result of repairing a geometry with the Incorrect ring ordering problem may be a geometry with the Null geometry problem.
- The Esri validation option ensures that geometry is topologically correct using the Esri Simplify method. Only the Esri validation is available for data stored in an enterprise geodatabase.
- The Open Geospatial Consortium (OGC) validation method ensures that geometry complies with the OGC specification as defined in OpenGIS Implementation Standard for Geographic information – simple feature access – Part 1: common architecture.
- After a feature's geometry is repaired using the OGC option, any subsequent edit or modification may cause the geometry to no longer comply with the OGC specification. After feature modification, run the Check Geometry tool to check for new geometry issues. If necessary, rerun the Repair Geometry tool.
- Geometry that is validated or repaired using the OGC option will be valid for the Esri option. To learn more about both methods, see What is a simple polygon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The feature class or layer to be processed.License:A Desktop Basic license only allows shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database as valid input feature formats. A Desktop Standard or Desktop Advanced license also allows feature classes stored in an enterprise database or enterprise geodatabase to be used as valid input feature formats. | Feature Layer |
| Delete Features with Null Geometry(Optional) | Specifies whether features with null geometries will be deleted.Checked—Features with null geometry will be deleted from the input. This is the default. Unchecked—Features with null geometry will not be deleted from the input.Note:Delete null is not available for data stored in an enterprise database, enterprise geodatabase, GeoPackage, or SpatiaLite database. | Boolean |
| Validation Method(Optional) | Specifies the geometry validation method that will be used to identify geometry problems.Esri—The Esri geometry validation method will be used. This is the default.OGC—The OGC geometry validation method will be used. | String |
| in_features | The feature class or layer to be processed.License:A Desktop Basic license only allows shapefiles and feature classes stored in a file geodatabase, GeoPackage, or SpatiaLite database as valid input feature formats. A Desktop Standard or Desktop Advanced license also allows feature classes stored in an enterprise database or enterprise geodatabase to be used as valid input feature formats. | Feature Layer |
| delete_null(Optional) | Specifies whether features with null geometries will be deleted.DELETE_NULL— Features with null geometry will be deleted from the input. This is the default.KEEP_NULL— Features with null geometry will not be deleted from the input. Note:Only KEEP_NULL is valid for inputs from an enterprise database, enterprise geodatabase, GeoPackage, or SpatiaLite database. | Boolean |
| validation_method(Optional) | Specifies the geometry validation method that will be used to identify geometry problems.ESRI—The Esri geometry validation method will be used. This is the default.OGC—The OGC geometry validation method will be used. | String |

## Code Samples

### Example 1

```python
arcpy.management.RepairGeometry(in_features, {delete_null}, {validation_method})
```

### Example 2

```python
import arcpy
arcpy.management.RepairGeometry("c:/data/sketchy.shp")
```

### Example 3

```python
import arcpy
arcpy.management.RepairGeometry("c:/data/sketchy.shp")
```

### Example 4

```python
# Description: 
#   Goes through the table generated by the Check Geometry tool and does 
#   the following
#   1) backs-up all features which will be 'fixed' to a "_bad_geom" feature class
#   2) runs repairGeometry on all feature classes listed in the table 

import arcpy
import os
 
# Table that was produced by Check Geometry tool
table = r"c:\temp\data.gdb\cg_sample1"
 
# Create local variables
fcs = []
 
# Loop through the table and get the list of fcs
for row in arcpy.da.SearchCursor(table, ("CLASS")):
    # Get the class (feature class) from the cursor
    if not row[0] in fcs:
        fcs.append(row[0])
 
# Now loop through the fcs list, backup the bad geometries into fc + "_bad_geom"
# then repair the fc
print("> Processing {0} feature classes".format(len(fcs)))
for fc in fcs:
    print("Processing " + fc)
    lyr = 'temporary_layer'
    if arcpy.Exists(lyr):
        arcpy.Delete_management(lyr)
    
    tv = "cg_table_view"
    if arcpy.Exists(tv):
        arcpy.Delete_management(tv)

    arcpy.MakeTableView_management(table, tv, ("\"CLASS\" = '%s'" % fc))
    arcpy.MakeFeatureLayer_management(fc, lyr)
    arcpy.AddJoin_management(lyr, arcpy.Describe(lyr).OIDFieldName, tv, "FEATURE_ID")
    arcpy.CopyFeatures_management(lyr, fc + "_bad_geom")
    arcpy.RemoveJoin_management(lyr, os.path.basename(table))
    arcpy.RepairGeometry_management(lyr)
```

### Example 5

```python
# Description: 
#   Goes through the table generated by the Check Geometry tool and does 
#   the following
#   1) backs-up all features which will be 'fixed' to a "_bad_geom" feature class
#   2) runs repairGeometry on all feature classes listed in the table 

import arcpy
import os
 
# Table that was produced by Check Geometry tool
table = r"c:\temp\data.gdb\cg_sample1"
 
# Create local variables
fcs = []
 
# Loop through the table and get the list of fcs
for row in arcpy.da.SearchCursor(table, ("CLASS")):
    # Get the class (feature class) from the cursor
    if not row[0] in fcs:
        fcs.append(row[0])
 
# Now loop through the fcs list, backup the bad geometries into fc + "_bad_geom"
# then repair the fc
print("> Processing {0} feature classes".format(len(fcs)))
for fc in fcs:
    print("Processing " + fc)
    lyr = 'temporary_layer'
    if arcpy.Exists(lyr):
        arcpy.Delete_management(lyr)
    
    tv = "cg_table_view"
    if arcpy.Exists(tv):
        arcpy.Delete_management(tv)

    arcpy.MakeTableView_management(table, tv, ("\"CLASS\" = '%s'" % fc))
    arcpy.MakeFeatureLayer_management(fc, lyr)
    arcpy.AddJoin_management(lyr, arcpy.Describe(lyr).OIDFieldName, tv, "FEATURE_ID")
    arcpy.CopyFeatures_management(lyr, fc + "_bad_geom")
    arcpy.RemoveJoin_management(lyr, os.path.basename(table))
    arcpy.RepairGeometry_management(lyr)
```

---

## Repair Mosaic Dataset Paths (Data Management)

## Summary

Resets paths to source imagery if you have moved or copied a mosaic dataset.

## Usage

- You need to know the file path location in order to change it. You can use the Export Mosaic Dataset Paths tool to retrieve the original path names.
- You can type an asterisk (*) as the original path if you wish to change all your paths.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset with the broken paths. | Mosaic Layer |
| Paths List | A list of the paths to remap. Include the current path stored in the mosaic dataset and the path to which it will be changed. You can enter an asterisk (*) as the original path if you wish to change all your paths. | Value Table |
| Query Definition (Optional) | An SQL expression to limit the repairs to selected rasters within the mosaic dataset. | SQL Expression |
| in_mosaic_dataset | The mosaic dataset with the broken paths. | Mosaic Layer |
| paths_list[[original_path, {new_path}],...] | A list of the paths to remap. Include the current path stored in the mosaic dataset and the path to which it will be changed. You can enter an asterisk (*) as the original path if you wish to change all your paths. | Value Table |
| where_clause(Optional) | An SQL expression to limit the repairs to selected rasters within the mosaic dataset. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.RepairMosaicDatasetPaths(in_mosaic_dataset, paths_list, {where_clause})
```

### Example 2

```python
import arcpy
arcpy.RepairMosaicDatasetPaths_management(
     "C:/Workspace/repairmd.gdb/md", 
     "\\\\server1\\md\\fgdb.gdb\\md c:\\storage\\md\\mdgdb.gdb\\md", 
     "#")
```

### Example 3

```python
import arcpy
arcpy.RepairMosaicDatasetPaths_management(
     "C:/Workspace/repairmd.gdb/md", 
     "\\\\server1\\md\\fgdb.gdb\\md c:\\storage\\md\\mdgdb.gdb\\md", 
     "#")
```

### Example 4

```python
#Repair mosaic dataset paths

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "repairmd.gdb/md"
paths = "e:/temp/data c:/storage/mddata/e;d:/temp/data c:/storage/mddata/d"
query = "#"

arcpy.RepairMosaicDatasetPaths_management(mdname, paths, query)
```

### Example 5

```python
#Repair mosaic dataset paths

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "repairmd.gdb/md"
paths = "e:/temp/data c:/storage/mddata/e;d:/temp/data c:/storage/mddata/d"
query = "#"

arcpy.RepairMosaicDatasetPaths_management(mdname, paths, query)
```

---

## Repair Trajectory Dataset Paths (Data Management)

## Summary

Repairs paths to source data for a trajectory dataset.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Trajectory Dataset | The input trajectory dataset. | Trajectory Layer |
| Paths List | A list of paths to remap. | Value Table |
| Query Definition (Optional) | An SQL expression that will limit the repairs to selected items in the trajectory dataset. | SQL Expression |
| in_trajectory_dataset | The input trajectory dataset. | Trajectory Layer |
| paths_list[paths_list,...] | A list of paths to remap. | Value Table |
| where_clause(Optional) | An SQL expression that will limit the repairs to selected items in the trajectory dataset. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.RepairTrajectoryDatasetPaths(in_trajectory_dataset, paths_list, {where_clause})
```

### Example 2

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"
paths_list = r"C:\Data\Altimetry C:\Data\Cryosat"
where_clause = ""

# Execute
repair_output = arcpy.management.RepairTrajectoryDatasetPaths(in_trajectory_dataset, paths_list , where_clause)
```

### Example 3

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"
paths_list = r"C:\Data\Altimetry C:\Data\Cryosat"
where_clause = ""

# Execute
repair_output = arcpy.management.RepairTrajectoryDatasetPaths(in_trajectory_dataset, paths_list , where_clause)
```

### Example 4

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"
paths_list = "* C:\data\CryoSat"
where_clause = "OBJECTID<2"


# Execute
repair_output = arcpy.management.RepairTrajectoryDatasetPaths(in_trajectory_dataset, paths_list, where_clause)
```

### Example 5

```python
# Import system modules
import arcpy
from arcpy.ia import *

# Set local variables
in_trajectory_dataset = r"C:\temp\trajectory_data.gdb\trajectory_dataset"
paths_list = "* C:\data\CryoSat"
where_clause = "OBJECTID<2"


# Execute
repair_output = arcpy.management.RepairTrajectoryDatasetPaths(in_trajectory_dataset, paths_list, where_clause)
```

---

## Repair Version Metadata (Data Management)

## Summary

Repairs inconsistencies in the versioning system tables of a geodatabase that contains traditional versions.

## Usage

- Run the Diagnose Version Metadata tool to determine if there are inconsistencies in your geodatabase versioning tables before you run the Repair Version Metadata tool.
- This tool can only be run on an enterprise geodatabase.
- This tool does not support geodatabases in SAP HANA because they don't support traditional versioning.
- Only the geodatabase administrator can run the Repair Version Metadata tool.
- Always create a database backup before running the Repair Version Metadata tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection (.sde file) to the enterprise geodatabase in which versioning system table inconsistencies exist. The connection must be made as the geodatabase administrator. | Workspace |
| Repair Version Metadata Log | The output log file. The log file is an ASCII file containing the results of the repair operation. | File |
| input_database | The database connection (.sde file) to the enterprise geodatabase in which versioning system table inconsistencies exist. The connection must be made as the geodatabase administrator. | Workspace |
| out_log | The output log file. The log file is an ASCII file containing the results of the repair operation. | File |

## Code Samples

### Example 1

```python
arcpy.management.RepairVersionMetadata(input_database, out_log)
```

### Example 2

```python
import arcpy

input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
arcpy.RepairVersionMetadata_management(input_database, out_log)
```

### Example 3

```python
import arcpy

input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
arcpy.RepairVersionMetadata_management(input_database, out_log)
```

### Example 4

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"

# Process: Repair Version Metadata
arcpy.RepairVersionMetadata_management(input_database, out_log)
```

### Example 5

```python
# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\myconnections\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"

# Process: Repair Version Metadata
arcpy.RepairVersionMetadata_management(input_database, out_log)
```

---

## Repair Version Tables (Data Management)

## Summary

Repairs inconsistencies in the delta (A and D) tables of datasets that are registered for traditional versioning.

## Usage

- Run the Diagnose Version Tables tool to determine if there are inconsistencies in your geodatabase delta tables before you run the Repair Version Tables tool.
- This tool can only be run on an enterprise geodatabase.
- This tool does not support geodatabases in SAP HANA because they don't support traditional versioning.
- Only the geodatabase administrator can run the Repair Version Tables tool.
- Always create a database backup before running the Repair Version Tables tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | The database connection (.sde file) to the enterprise geodatabase in which delta table inconsistencies exist. The connection must be made as the geodatabase administrator. | Workspace |
| Repair Version Tables Log | The location where the log file will be written and the name of the log file. The log file is an ASCII file containing the results of the repair operation. | File |
| Target Version (Optional) | The geodatabase version to be repaired. The drop-down list is populated with the existing versions in the geodatabase specified for the Input Database Connection parameter. If no version is selected, all versions are processed. | String |
| Input Tables (Optional) | A single table or a text file containing a list of versioned tables with the associated delta tables to be repaired. Use fully-qualified table names in the text file, and place one table name per line. If no table or file is specified, all tables are processed. | String |
| input_database | The database connection (.sde file) to the enterprise geodatabase in which delta table inconsistencies exist. The connection must be made as the geodatabase administrator. | Workspace |
| out_log | The location where the log file will be written and the name of the log file. The log file is an ASCII file containing the results of the repair operation. | File |
| target_version(Optional) | The geodatabase version to be repaired. If no version is specified, all versions are processed. | String |
| input_tables[input_tables,...](Optional) | A single table or a text file containing a list of versioned tables with the associated delta tables to be repaired. Use fully-qualified table names in the text file, and place one table name per line. If no table or file is specified, all tables are processed. | String |

## Code Samples

### Example 1

```python
arcpy.management.RepairVersionTables(input_database, out_log, {target_version}, {input_tables})
```

### Example 2

```python
import arcpy

input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

arcpy.RepairVersionMetadata_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 3

```python
import arcpy

input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

arcpy.RepairVersionMetadata_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 4

```python
# Description: repair version metadata

# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

# Process: Repair Version Metadata
arcpy.RepairVersionMetadata_management(input_database, out_log, target_version, 
                                       input_tables)
```

### Example 5

```python
# Description: repair version metadata

# Set the necessary product code
import arceditor
 
# Import arcpy module
import arcpy

# Local variables:
input_database = "c:\\temp\\productiongdb.sde"
out_log = "c:\\temp\\gdb_repair.log"
target_version = "SDE.Default"
input_tables = "GIS.Parcels"

# Process: Repair Version Metadata
arcpy.RepairVersionMetadata_management(input_database, out_log, target_version, 
                                       input_tables)
```

---

## Resample (Data Management)

## Summary

Changes the spatial resolution of a raster dataset and sets rules for aggregating or interpolating values across the new pixel sizes.

## Usage

- The cell size can be changed, but the extent of the raster dataset will remain the same.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- The Output Cell Size parameter can resample the output to the same cell size as an existing raster layer, or it can output a specific X and Y cell size.
- There are four options for the Resampling Technique parameter:Nearest—Performs a nearest neighbor assignment and is the fastest of the interpolation methods. It is used primarily for discrete data, such as a land-use classification, since it will not change the values of the cells. The maximum spatial error will be one-half the cell size.Majority—Performs a majority algorithm and determines the new value of the cell based on the most popular values in the filter window. It is mainly used with discrete data just as the nearest neighbor method; the Majority option tends to give a smoother result than Nearest. The majority resampling method will find corresponding 4 by 4 cells in the input space that are closest to the center of the output cell and use the majority of the 4 by 4 neighbors. Bilinear—Performs a bilinear interpolation and determines the new value of a cell based on a weighted distance average of the four nearest input cell centers. It is useful for continuous data and will cause some smoothing of the data.Cubic—Performs a cubic convolution and determines the new value of a cell based on fitting a smooth curve through the 16 nearest input cell centers. It is appropriate for continuous data, although it may result in the output raster containing values outside the range of the input raster. If this is unacceptable, use Bilinear instead. The output from cubic convolution is geometrically less distorted than the raster achieved by running the nearest neighbor resampling algorithm. The disadvantage of the Cubic option is that it requires more processing time. The Bilinear and Cubic options should not be used with categorical data, since the cell values may be altered.
- Nearest—Performs a nearest neighbor assignment and is the fastest of the interpolation methods. It is used primarily for discrete data, such as a land-use classification, since it will not change the values of the cells. The maximum spatial error will be one-half the cell size.
- Majority—Performs a majority algorithm and determines the new value of the cell based on the most popular values in the filter window. It is mainly used with discrete data just as the nearest neighbor method; the Majority option tends to give a smoother result than Nearest. The majority resampling method will find corresponding 4 by 4 cells in the input space that are closest to the center of the output cell and use the majority of the 4 by 4 neighbors.
- Bilinear—Performs a bilinear interpolation and determines the new value of a cell based on a weighted distance average of the four nearest input cell centers. It is useful for continuous data and will cause some smoothing of the data.
- Cubic—Performs a cubic convolution and determines the new value of a cell based on fitting a smooth curve through the 16 nearest input cell centers. It is appropriate for continuous data, although it may result in the output raster containing values outside the range of the input raster. If this is unacceptable, use Bilinear instead. The output from cubic convolution is geometrically less distorted than the raster achieved by running the nearest neighbor resampling algorithm. The disadvantage of the Cubic option is that it requires more processing time.
- If the center of the pixel in output space falls exactly the same as one of the pixels in the input cells, that particular cell value receives all the weights, causing the output pixel to be the same as the cell center. This will affect the result of bilinear interpolation and cubic convolution.
- The lower left corner of the output raster dataset will be the same map space coordinate location as the lower left corner of the input raster dataset.
- The number of rows and columns in the output raster are determined as follows:columns = (xmax - xmin) / cell size rows = (ymax - ymin) / cell size
- If there is any remainder from the above equations, rounding of the number of columns and rows is performed.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset with the spatial resolution to be changed. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| Output Raster Dataset | The name, location, and format of the dataset being created..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to JPEG, JPEG 2000, or TIFF format, or in a geodatabase, you can specify a compression type and compression quality. | Raster Dataset |
| Output Cell Size(Optional) | The cell size of the new raster using an existing raster dataset or by specifying its width (x) and height (y). | Cell Size XY |
| Resampling Technique(Optional) | Specifies the resampling technique to be used.Nearest— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.Bilinear— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.Cubic—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.Majority—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |
| in_raster | The raster dataset with the spatial resolution to be changed. | Mosaic Dataset; Mosaic Layer; Raster Dataset; Raster Layer |
| out_raster | The name, location, and format of the dataset being created..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing a raster dataset to JPEG, JPEG 2000, or TIFF format, or in a geodatabase, you can specify a compression type and compression quality. | Raster Dataset |
| cell_size(Optional) | The cell size of the new raster using an existing raster dataset or by specifying its width (x) and height (y). You can specify the cell size in the following ways: Use a single number specifying a square cell size.Use two numbers that specify the x and y cell size, which is space delimited.Use the path of a raster dataset from which the square cell size will be imported. | Cell Size XY |
| resampling_type(Optional) | Specifies the resampling technique to be used.NEAREST— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.BILINEAR— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.CUBIC—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.MAJORITY—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |

## Code Samples

### Example 1

```python
columns = (xmax - xmin) / cell size
 rows = (ymax - ymin) / cell size
```

### Example 2

```python
columns = (xmax - xmin) / cell size
 rows = (ymax - ymin) / cell size
```

### Example 3

```python
arcpy.management.Resample(in_raster, out_raster, {cell_size}, {resampling_type})
```

### Example 4

```python
import arcpy
arcpy.Resample_management("c:/data/image.tif", "resample.tif", "10 20", "NEAREST")
```

### Example 5

```python
import arcpy
arcpy.Resample_management("c:/data/image.tif", "resample.tif", "10 20", "NEAREST")
```

### Example 6

```python
# Resample TIFF image to a higher resolution

import arcpy
arcpy.env.workspace = r"C:/Workspace"
    
arcpy.Resample_management("image.tif", "resample.tif", "10", "CUBIC")
```

### Example 7

```python
# Resample TIFF image to a higher resolution

import arcpy
arcpy.env.workspace = r"C:/Workspace"
    
arcpy.Resample_management("image.tif", "resample.tif", "10", "CUBIC")
```

---

## Rescale (Data Management)

## Summary

Resizes a raster by the specified x and y scale factors.

## Usage

- The output size is multiplied by the scale factor for both the x and y directions. The number of columns and rows stays the same in this process, but the cell size is multiplied by the scale factor.
- The scale factor must be positive.
- A scale factor greater than one means the image will be rescaled to a larger dimension, resulting in a larger extent because of a larger cell size.
- A scale factor less than one means the image will be rescaled to a smaller dimension, resulting in a smaller extent because of a smaller cell size.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input raster. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| X Scale Factor | The factor by which to scale the cell size in the x direction.The factor must be greater than zero. | Double |
| Y Scale Factor | The factor by which to scale the cell size in the y direction.The factor must be greater than zero. | Double |
| in_raster | The input raster. | Mosaic Layer; Raster Layer |
| out_raster | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| x_scale | The factor by which to scale the cell size in the x direction.The factor must be greater than zero. | Double |
| y_scale | The factor by which to scale the cell size in the y direction.The factor must be greater than zero. | Double |

## Code Samples

### Example 1

```python
arcpy.management.Rescale(in_raster, out_raster, x_scale, y_scale)
```

### Example 2

```python
import arcpy
arcpy.Rescale_management("c:/data/image.tif", "c:/output/rescale.tif", "4", "4")
```

### Example 3

```python
import arcpy
arcpy.Rescale_management("c:/data/image.tif", "c:/output/rescale.tif", "4", "4")
```

### Example 4

```python
##====================================
##Rescale
##Usage: Usage: Rescale_management in_raster out_raster x_scale y_scale
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Rescale a TIFF image by a factor of 4 in both directions
arcpy.Rescale_management("image.tif", "rescale.tif", "4", "4")
```

### Example 5

```python
##====================================
##Rescale
##Usage: Usage: Rescale_management in_raster out_raster x_scale y_scale
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Rescale a TIFF image by a factor of 4 in both directions
arcpy.Rescale_management("image.tif", "rescale.tif", "4", "4")
```

---

## Rotate (Data Management)

## Summary

Turns a raster dataset around a specified pivot point.

## Usage

- By default, the rotation is around the center point of the raster. The rotation point can be changed using the optional Pivot Point parameter.
- Resampling is only done if the angle is not a multiple of 90.
- Specify a rotation angle between 0 and 360 to rotate the raster clockwise. To rotate the raster in the counterclockwise direction, specify the angle as a negative value.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster dataset to rotate. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| Angle | Specify a value between 0 and 360 degrees the raster will be rotated in the clockwise direction. To rotate the raster in the counterclockwise direction, specify the angle as a negative value. The angle can be specified as an integer or a floating-point value. | Double |
| Pivot Point(Optional) | The point the raster will rotate around. If left blank, the lower left corner of the input raster dataset will serve as the pivot. | Point |
| Resampling Technique(Optional) | Specifies the resampling technique that will be used. The default is Nearest.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered.Nearest neighbor— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.Bilinear interpolation— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.Cubic convolution—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.Majority resampling—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |
| Clipping Extent(Optional) | The processing extent of the raster dataset. The source data will be clipped to the specified extent before rotation.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| in_raster | The raster dataset to rotate. | Mosaic Layer; Raster Layer |
| out_raster | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| angle | Specify a value between 0 and 360 degrees the raster will be rotated in the clockwise direction. To rotate the raster in the counterclockwise direction, specify the angle as a negative value. The angle can be specified as an integer or a floating-point value. | Double |
| pivot_point(Optional) | The point the raster will rotate around. If left blank, the lower left corner of the input raster dataset will serve as the pivot. | Point |
| resampling_type(Optional) | Specifies the resampling technique that will be used. The default is Nearest.NEAREST— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.BILINEAR— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.CUBIC—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.MAJORITY—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered. | String |
| clipping_extent(Optional) | The processing extent of the raster dataset. The source data will be clipped to the specified extent before rotation.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |

## Code Samples

### Example 1

```python
arcpy.management.Rotate(in_raster, out_raster, angle, {pivot_point}, {resampling_type}, {clipping_extent})
```

### Example 2

```python
import arcpy
arcpy.Rotate_management("c:/data/image.tif", "c:/output/rotate.tif", "30",\
                        "1940000 304000", "BILINEAR")
```

### Example 3

```python
import arcpy
arcpy.Rotate_management("c:/data/image.tif", "c:/output/rotate.tif", "30",\
                        "1940000 304000", "BILINEAR")
```

### Example 4

```python
##====================================
##Rotate
##Usage: Rotate_management in_raster out_raster angle {pivot_point} {NEAREST | BILINEAR | CUBIC | MAJORITY}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"
pivot_point = "1942602 304176"

##Rescale a TIFF image by a factor of 4 in both directions
arcpy.Rotate_management("image.tif", "rotate.tif", "30", pivot_point, "BILINEAR")
```

### Example 5

```python
##====================================
##Rotate
##Usage: Rotate_management in_raster out_raster angle {pivot_point} {NEAREST | BILINEAR | CUBIC | MAJORITY}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"
pivot_point = "1942602 304176"

##Rescale a TIFF image by a factor of 4 in both directions
arcpy.Rotate_management("image.tif", "rotate.tif", "30", pivot_point, "BILINEAR")
```

---

## Save To Layer File (Data Management)

## Summary

Creates an output layer file (.lyrx) from a map layer. The layer file stores many properties of the input layer such as symbology, labeling, and custom pop-ups.

## Usage

- In addition to map layers, this tool also accepts layers created by tools such as Make Feature Layer or Make XY Event Layer.
- If the input layer has a selection applied to it, the output layer file will maintain this selection.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer | The map layer that will be saved to disk as a layer file. | Layer; Table View |
| Output Layer | The output layer file (.lyrx) that will be created. | Layer File |
| Store Relative Path (Optional) | Specifies whether the output layer file will store a relative path to the source data stored on disk or an absolute path. Unchecked—The output layer file will store an absolute path to the source data stored on disk. This is the default.Checked—The output layer file will store a relative path to the source data stored on disk. If the output layer file is moved, its source path will update to where the source data should be in relation to the new path. | Boolean |
| Layer Version (Optional) | Specifies the version of the output layer file. Current—The current version. This is the default.Legacy: This parameter is no longer supported. It remains only for the backward compatibility of scripts and models.Layer files created in a particular release are supported in all minor releases of the same series. For example, a layer file saved in ArcGIS Pro 3.1 can be used in all ArcGIS Pro 3.x releases.To save a layer and its properties to a different major release of ArcGIS Pro, use the Package Layer tool. | String |
| in_layer | The map layer that will be saved to disk as a layer file. | Layer; Table View |
| out_layer | The output layer file (.lyrx) that will be created. | Layer File |
| is_relative_path(Optional) | Specifies whether the output layer file will store a relative path to the source data stored on disk or an absolute path. ABSOLUTE—The output layer file will store an absolute path to the source data stored on disk. This is the default.RELATIVE—The output layer file will store a relative path to the source data stored on disk. If the output layer file is moved, its source path will update to where the source data should be in relation to the new path. | Boolean |
| version(Optional) | Specifies the version of the output layer file. Legacy: This parameter is no longer supported. It remains only for the backward compatibility of scripts and models.Layer files created in a particular release are supported in all minor releases of the same series. For example, a layer file saved in ArcGIS Pro 3.1 can be used in all ArcGIS Pro 3.x releases.To save a layer and its properties to a different major release of ArcGIS Pro, use the Package Layer tool.CURRENT—The current version. This is the default. | String |

## Code Samples

### Example 1

```python
arcpy.management.SaveToLayerFile(in_layer, out_layer, {is_relative_path}, {version})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SaveToLayerFile("studyquads.shp", "C:/output/studyquadsLyr.lyrx", "ABSOLUTE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SaveToLayerFile("studyquads.shp", "C:/output/studyquadsLyr.lyrx", "ABSOLUTE")
```

### Example 4

```python
# Description: Save a layer to a file on disk

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_features = "study_quads.shp"
where_clause = '"NAME" = \'LA MESA\''
in_layer = "studyquadsLyr"
out_layer_file = "studyquadsLyr.lyrx"

# Run MakeFeatureLayer
arcpy.management.MakeFeatureLayer(in_features, "study_quads_lyr", where_clause)

# Run SaveToLayerFile
arcpy.management.SaveToLayerFile("study_quads_lyr", out_layer_file, "ABSOLUTE")
```

### Example 5

```python
# Description: Save a layer to a file on disk

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/data"

# Set local variables
in_features = "study_quads.shp"
where_clause = '"NAME" = \'LA MESA\''
in_layer = "studyquadsLyr"
out_layer_file = "studyquadsLyr.lyrx"

# Run MakeFeatureLayer
arcpy.management.MakeFeatureLayer(in_features, "study_quads_lyr", where_clause)

# Run SaveToLayerFile
arcpy.management.SaveToLayerFile("study_quads_lyr", out_layer_file, "ABSOLUTE")
```

---

## Save Toolbox To Version (Data Management)

## Summary

Analyzes and saves a toolbox for use with a specific earlier version of ArcGIS software.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Toolbox | The input toolbox (.tbx or .atbx file) that will be analyzed and saved. The file will not be modified.The Python toolbox format (.pyt file) is not supported as an input. | Toolbox |
| Target Version | Specifies the software version that will be used for toolbox compatibility issue analysis. ArcGIS Desktop 10.6.0—ArcGIS Desktop 10.6.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Desktop 10.7.0—ArcGIS Desktop 10.7.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Desktop 10.8.0—ArcGIS Desktop 10.8.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Desktop 10.8.2—ArcGIS Desktop 10.8.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.2—ArcGIS Pro 2.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.3—ArcGIS Pro 2.3 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.4—ArcGIS Pro 2.4 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.5—ArcGIS Pro 2.5 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.6—ArcGIS Pro 2.6 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.7—ArcGIS Pro 2.7 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.8—ArcGIS Pro 2.8 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 2.9—ArcGIS Pro 2.9 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 3.0—ArcGIS Pro 3.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 3.1—ArcGIS Pro 3.1 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 3.2—ArcGIS Pro 3.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.ArcGIS Pro 3.3—ArcGIS Pro 3.3 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version. | String |
| Output Toolbox | The toolbox that will be created for use with ArcGIS software of the specified Target Version parameter value. | Toolbox |
| Error on missing tool (Optional) | Specifies whether an error will be produced if a tool is encountered that is not present at the target version.Checked—An error will be produced and the output toolbox will not be created. This is the default.Unchecked—A warning message will be produced and the output toolbox will be created. For model tools, the problematic tool will be removed from the model, which will require manual editing. | Boolean |
| Error on missing required parameter(Optional) | Specifies whether an error will be produced if a parameter is encountered that is not present at the target version and that parameter has a value that is not its default value.Checked—An error will be produced and the output toolbox will not be created. This is the default.Unchecked—A warning message will be produced, the parameter will be removed from the model, and the output toolbox will be created. | Boolean |
| Error on invalid parameter value(Optional) | Specifies whether an error will be produced if a parameter value is encountered that is not present in its parameter filter at the target version.Checked—An error will be produced and the output toolbox will not be created. This is the default.Unchecked—A warning message will be produced and the output toolbox will be created. The output toolbox will produce an error if it has a value that is not in domain or is invalid. | Boolean |
| in_toolbox | The input toolbox (.tbx or .atbx file) that will be analyzed and saved. The file will not be modified.The Python toolbox format (.pyt file) is not supported as an input. | Toolbox |
| version | Specifies the software version that will be used for toolbox compatibility issue analysis. 10.6.0—ArcGIS Desktop 10.6.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.10.7.0—ArcGIS Desktop 10.7.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.10.8.0—ArcGIS Desktop 10.8.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.10.8.2—ArcGIS Desktop 10.8.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.2—ArcGIS Pro 2.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.3—ArcGIS Pro 2.3 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.4—ArcGIS Pro 2.4 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.5—ArcGIS Pro 2.5 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.6—ArcGIS Pro 2.6 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.7—ArcGIS Pro 2.7 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.8—ArcGIS Pro 2.8 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.2.9—ArcGIS Pro 2.9 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.3.0—ArcGIS Pro 3.0 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.3.1—ArcGIS Pro 3.1 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.3.2—ArcGIS Pro 3.2 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version.3.3—ArcGIS Pro 3.3 will be used for toolbox compatibility issue analysis. The output toolbox will be saved to this version. | String |
| out_toolbox | The toolbox that will be created for use with ArcGIS software of the specified version parameter value. | Toolbox |
| missing_tool(Optional) | Specifies whether an error will be produced if a tool is encountered that is not present at the target version.ERROR_ON_MISSING_TOOL—An error will be produced and the output toolbox will not be created. This is the default.WARN_ON_MISSING_TOOL—A warning message will be produced and the output toolbox will be created. For model tools, the problematic tool will be removed from the model, which will require manual editing. | Boolean |
| missing_param(Optional) | Specifies whether an error will be produced if a parameter is encountered that is not present at the target version and that parameter has a value that is not its default value.ERROR_ON_MISSING_REQUIRED_PARAM—An error will be produced and the output toolbox will not be created. This is the default.WARN_ON_MISSING_REQUIRED_PARAM—A warning message will be produced, the parameter will be removed from the model, and the output toolbox will be created. | Boolean |
| invalid_param_value(Optional) | Specifies whether an error will be produced if a parameter value is encountered that is not present in its parameter filter at the target version.ERROR_ON_INVALID_PARAM_VALUE—An error will be produced and the output toolbox will not be created. This is the default.WARN_ON_INVALID_PARAM_VALUE—A warning message will be produced and the output toolbox will be created. The output toolbox will produce an error with a value that is not in domain or is invalid. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SaveToolboxToVersion(in_toolbox, version, out_toolbox, {missing_tool}, {missing_param}, {invalid_param_value})
```

### Example 2

```python
import arcpy
arcpy.management.SaveToolboxToVersion(r"C:\toolboxes\MyTools.atbx", "2.7", "C:\toolboxes\MyTools_27.tbx")
```

### Example 3

```python
import arcpy
arcpy.management.SaveToolboxToVersion(r"C:\toolboxes\MyTools.atbx", "2.7", "C:\toolboxes\MyTools_27.tbx")
```

---

## Select Layer By Attribute (Data Management)

## Summary

Adds, updates, or removes a selection based on an attribute query.

## Usage

- If the input is a feature class or dataset path, this tool will create and return a new layer with the result of the tool applied.
- If a definition query is present on the input, only the features or rows matching the definition query will be used in the selection.
- The number of selected records will be listed in the geoprocessing history. Click Parameters > Count to access them. Additionally, the Get Count tool can be used to count the number of selected records. From Python, the number of selected records can also be accessed from the tool's Result object.
- When applying selections, a selection of zero records is possible. For example, selecting a field value that is not present, results in a selection of zero records. A geoprocessing tool using the layer as input will use no records from the input.
- The field delimiters used in an SQL expression differ depending on the format of the queried data. For instance, file geodatabases and shapefiles use double quotes, and enterprise geodatabases don't use field delimiters. You can use the AddFieldDelimiters function to help ensure that the field delimiters used with the SQL expression are correct.
- If the input's data source is a feature service, it is recommended that the underlying ArcGIS Server use standardized SQL queries.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Rows | The data to which the selection will be applied. | Table View; Raster Layer; Mosaic Layer |
| Selection Type(Optional) | Specifies how the selection will be applied and what to do if a selection already exists.New selection—The resulting selection will replace the current selection. This is the default. Add to the current selection—The resulting selection will be added to the current selection if one exists. If no selection exists, this is the same as the new selection option. Remove from the current selection—The resulting selection will be removed from the current selection. If no selection exists, this option has no effect. Select subset from the current selection—The resulting selection will be combined with the current selection. Only records that are common to both remain selected. Switch the current selection—The selection will be switched. All records that were selected will be removed from the current selection, and all records that were not selected will be added to the current selection. The Expression parameter is ignored when this option is specified. Clear the current selection—The selection will be cleared or removed. The Expression parameter is ignored when this option is specified. | String |
| Expression(Optional) | An SQL expression used to select a subset of records. | SQL Expression |
| Invert Where Clause (Optional) | Specifies whether the expression will be used as is, or the opposite of the expression will be used.Unchecked—The query will be used as is. This is the default. Checked—The opposite of the query will be used. If the Selection Type parameter is used, the reversal of the selection occurs before it is combined with existing selections. | Boolean |
| in_layer_or_view | The data to which the selection will be applied. | Table View; Raster Layer; Mosaic Layer |
| selection_type(Optional) | Specifies how the selection will be applied and what to do if a selection already exists.NEW_SELECTION—The resulting selection will replace the current selection. This is the default. ADD_TO_SELECTION—The resulting selection will be added to the current selection if one exists. If no selection exists, this is the same as the new selection option. REMOVE_FROM_SELECTION—The resulting selection will be removed from the current selection. If no selection exists, this option has no effect. SUBSET_SELECTION—The resulting selection will be combined with the current selection. Only records that are common to both remain selected. SWITCH_SELECTION—The selection will be switched. All records that were selected will be removed from the current selection, and all records that were not selected will be added to the current selection. The where_clause parameter is ignored when this option is specified. CLEAR_SELECTION—The selection will be cleared or removed. The where_clause parameter is ignored when this option is specified. | String |
| where_clause(Optional) | An SQL expression used to select a subset of records. | SQL Expression |
| invert_where_clause(Optional) | Specifies whether the expression will be used as is, or the opposite of the expression will be used.NON_INVERT—The query will be used as is. This is the default.INVERT—The opposite of the query will be used. If the selection_type parameter is used, the reversal of the selection occurs before it is combined with existing selections. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SelectLayerByAttribute(in_layer_or_view, {selection_type}, {where_clause}, {invert_where_clause})
```

### Example 2

```python
import arcpy
arcpy.management.SelectLayerByAttribute("states", "NEW_SELECTION", 
                                        "[NAME] = 'California'")
```

### Example 3

```python
import arcpy
arcpy.management.SelectLayerByAttribute("states", "NEW_SELECTION", 
                                        "[NAME] = 'California'")
```

### Example 4

```python
# Name: ExtractFeaturesByLocationAndAttribute.py
# Description: Extract features to a new feature class based on a spatial 
# relationship to another layer and an attribute query

# Import system modules
import arcpy

# Set the workspace
arcpy.env.workspace = 'c:/data/mexico.gdb'

# Select all cities that overlap the chihuahua polygon
chihuahua_cities = arcpy.management.SelectLayerByLocation('cities', 'INTERSECT', 
                                                          'chihuahua', 0, 
                                                          'NEW_SELECTION')

# Within selected features, further select only those cities with a 
# population > 10,000   
arcpy.management.SelectLayerByAttribute(chihuahua_cities, 'SUBSET_SELECTION', 
                                        '"population" > 10000')

# Write the selected features to a new feature class
arcpy.management.CopyFeatures(chihuahua_cities, 'chihuahua_10000plus')
```

### Example 5

```python
# Name: ExtractFeaturesByLocationAndAttribute.py
# Description: Extract features to a new feature class based on a spatial 
# relationship to another layer and an attribute query

# Import system modules
import arcpy

# Set the workspace
arcpy.env.workspace = 'c:/data/mexico.gdb'

# Select all cities that overlap the chihuahua polygon
chihuahua_cities = arcpy.management.SelectLayerByLocation('cities', 'INTERSECT', 
                                                          'chihuahua', 0, 
                                                          'NEW_SELECTION')

# Within selected features, further select only those cities with a 
# population > 10,000   
arcpy.management.SelectLayerByAttribute(chihuahua_cities, 'SUBSET_SELECTION', 
                                        '"population" > 10000')

# Write the selected features to a new feature class
arcpy.management.CopyFeatures(chihuahua_cities, 'chihuahua_10000plus')
```

---

## Select Layer By Location (Data Management)

## Summary

Selects features based on a spatial relationship to features in another dataset or the same dataset.

## Usage

- If the input is a feature class or dataset path, this tool will create and return a new layer with the result of the tool applied.
- The coordinate system in which the spatial relationship is evaluated can affect the result. Features that intersect in one coordinate system may not intersect in another. This tool evaluates a spatial relationship in the coordinate system of the Input Features parameter value. Set the Output Coordinate System environment to Current Map [Map] to use the same coordinate system as the current display.
- This tool evaluates a spatial relationship in the coordinate system of the Input Features parameter value. Set the Output Coordinate System environment to Current Map [Map] to use the same coordinate system as the current display.
- To select features based on their spatial relationships to other features in the same layer, see the examples in Select based on spatial relationship within the layer.
- The number of selected records will be listed in the geoprocessing history. Click Parameters > Count to access them. Additionally, the Get Count tool can be used to count the number of selected records. From Python, the number of selected records can also be accessed from the tool's Result object.
- When using this tool, it is a possible and a valid result for none of the features to meet the criteria for selection. If this occurs, a selection will be applied to the layer with zero features selected. Geoprocessing tools that use this layer as input will get no features when they are run. For example, the Get Count tool will return a result of 0. To remove this, or any other, selection use the Select Layer By Attribute tool's Selection Type parameter's Clear the current selection option.
- If the input layer has a definition query, only the features matching the definition query will be used in the operation and be candidates for selection.
- For more information about using the 3D spatial relationship options Intersect 3D and Within a distance 3D, see Select By Location: 3D relationships.
- For the Relationship parameter, the Intersect (DBMS) option may provide better performance than the Intersect option when using enterprise geodatabase data; however, this option is only supported under specific conditions. If all conditions are met, the spatial operation will be performed in the enterprise geodatabase database management system (DBMS) rather than on the client. Consider the following when using this spatial relationship option:The following requirements are necessary for the operation to run in the DBMS:The Input Features and Selecting Features parameter values must be from the same enterprise geodatabase workspace and have the same spatial reference and geometry storage type.The user connecting to the geodatabase must have privileges to create a view in the database where the feature classes are stored.Supported geometry storage types for this option are ST_Geometry (IBM Db2, Oracle, PostgreSQL, and SAP HANA), PostGIS (PostgreSQL), SDO_GEOMETRY (Oracle), and Geometry and Geography (Microsoft SQL Server). See Geodatabase management for information about installing and configuring your DBMS as well as information about configuring the geometry storage type of your choice so it will be available for use. See the vendor documentation specific to your DBMS to determine what to expect for each geometry storage type. There may be storage limitations that will impact performance and scalability when running spatial operations.If the feature classes in Oracle use ST_Geometry to store spatial data, you must configure the Oracle extproc to access ST_Geometry. See Configure the extproc to access ST_Geometry in Oracle for more information.The Search Distance parameter is not set.The Selection Type parameter value is New selection.Existing selections that were made before running the tool were made using a layer definition query, not a selection set. The spatial operation is performed without applying an x,y tolerance during processing. Using an x,y tolerance is not supported in the DBMS. This may result in slightly different selections being returned compared to when the analysis is performed on the client with an x,y tolerance applied. See Feature class basics for more information about how an x,y tolerance is applied during client-side operations.
- The following requirements are necessary for the operation to run in the DBMS:The Input Features and Selecting Features parameter values must be from the same enterprise geodatabase workspace and have the same spatial reference and geometry storage type.The user connecting to the geodatabase must have privileges to create a view in the database where the feature classes are stored.Supported geometry storage types for this option are ST_Geometry (IBM Db2, Oracle, PostgreSQL, and SAP HANA), PostGIS (PostgreSQL), SDO_GEOMETRY (Oracle), and Geometry and Geography (Microsoft SQL Server). See Geodatabase management for information about installing and configuring your DBMS as well as information about configuring the geometry storage type of your choice so it will be available for use. See the vendor documentation specific to your DBMS to determine what to expect for each geometry storage type. There may be storage limitations that will impact performance and scalability when running spatial operations.If the feature classes in Oracle use ST_Geometry to store spatial data, you must configure the Oracle extproc to access ST_Geometry. See Configure the extproc to access ST_Geometry in Oracle for more information.The Search Distance parameter is not set.The Selection Type parameter value is New selection.Existing selections that were made before running the tool were made using a layer definition query, not a selection set.
- The Input Features and Selecting Features parameter values must be from the same enterprise geodatabase workspace and have the same spatial reference and geometry storage type.
- The user connecting to the geodatabase must have privileges to create a view in the database where the feature classes are stored.
- Supported geometry storage types for this option are ST_Geometry (IBM Db2, Oracle, PostgreSQL, and SAP HANA), PostGIS (PostgreSQL), SDO_GEOMETRY (Oracle), and Geometry and Geography (Microsoft SQL Server). See Geodatabase management for information about installing and configuring your DBMS as well as information about configuring the geometry storage type of your choice so it will be available for use. See the vendor documentation specific to your DBMS to determine what to expect for each geometry storage type. There may be storage limitations that will impact performance and scalability when running spatial operations.
- If the feature classes in Oracle use ST_Geometry to store spatial data, you must configure the Oracle extproc to access ST_Geometry. See Configure the extproc to access ST_Geometry in Oracle for more information.
- The Search Distance parameter is not set.
- The Selection Type parameter value is New selection.
- Existing selections that were made before running the tool were made using a layer definition query, not a selection set.
- The spatial operation is performed without applying an x,y tolerance during processing. Using an x,y tolerance is not supported in the DBMS. This may result in slightly different selections being returned compared to when the analysis is performed on the client with an x,y tolerance applied. See Feature class basics for more information about how an x,y tolerance is applied during client-side operations.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The features that will be evaluated using the Selecting Features parameter values. The selection will be applied to these features. | Feature Layer; Raster Layer; Mosaic Layer |
| Relationship(Optional) | Specifies the spatial relationship that will be evaluated.Intersect—The features in the input layer will be selected if they intersect a selecting feature. This is the default.Intersect 3D—The features in the input layer will be selected if they intersect a selecting feature in three-dimensional space (x, y, and z).Intersect (DBMS)—The features in the input layer will be selected if they intersect a selecting feature.This option applies to enterprise geodatabases only. The selection will be processed in the enterprise geodatabase DBMS rather than on the client when all requirements are met (see usage notes). This option may provide better performance than performing the selection on the client.Within a distance—The features in the input layer will be selected if they are within the specified distance (using Euclidean distance) of a selecting feature. Use the Search Distance parameter to specify the distance.Within a distance 3D—The features in the input layer will be selected if they are within a specified distance of a selecting feature in three-dimensional space. Use the Search Distance parameter to specify the distance.Within a distance geodesic—This spatial relationship is the same as the Within a distance option except that geodesic distance is used rather than planar distance. Distance between features will be calculated using a geodesic formula that takes into account the curvature of the spheroid and correctly handles data near and across the dateline and poles. Choose this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations. Use the Search Distance parameter to specify the distance. Contains—The features in the input layer will be selected if they contain a selecting feature.Completely contains—The features in the input layer will be selected if they completely contain a selecting feature.Contains Clementini—This spatial relationship produces the same results as the Contains option except that if the selecting feature is entirely on the boundary of the input feature (no part is properly inside or outside), the feature will not be selected.Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.Within—The features in the input layer will be selected if they are within a selecting feature.Completely within—The features in the input layer will be selected if they are completely within or contained by a selecting feature.Within Clementini—The result will be identical to the Within option result except that if the entirety of the feature in the input layer is on the boundary of the feature in the selecting layer, the feature will not be selected.Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.Are identical to—The features in the input layer will be selected if they are identical (in geometry) to a selecting feature.Boundary touches—The features in the input layer will be selected if they have a boundary that touches a selecting feature. When the input features are lines or polygons, the boundary of the input feature can only touch the boundary of the selecting feature, and no part of the input feature can cross the boundary of the selecting feature.Share a line segment with—The features in the input layer will be selected if they share a line segment with a selecting feature. The input and selecting features must be line or polygon.Crossed by the outline of—The features in the input layer will be selected if they are crossed by the outline of a selecting feature. The input and selecting features must be lines or polygons. If polygons are used for the input or selecting layer, the polygon's boundary (line) will be used. Lines that cross at a point will be selected; lines that share a line segment will not be selected.Have their center in—The features in the input layer will be selected if their center falls within a selecting feature. The center of the feature is calculated as follows: for polygon and multipoint, the geometry's centroid is used; for line input, the geometry's midpoint is used. | String |
| Selecting Features(Optional) | The features in the Input Features parameter will be selected based on their relationship to the features from this layer or feature class. | Feature Layer |
| Search Distance(Optional) | The distance that will be searched. This parameter is only valid if the Relationship parameter is set to Intersect, Intersect 3D, Within a distance, Within a distance 3D, Within a distance geodesic, Contains, or Have their center in.If the Within a distance geodesic option is specified, use a linear unit such as kilometers or miles. | Linear Unit |
| Selection Type(Optional) | Specifies how the selection will be applied to the input and how it will be combined with an existing selection. This tool does not include an option to clear an existing selection; use the Select Layer By Attribute tool with the Selection Type parameter set to Clear the current selection to do that.New selection—The resulting selection will replace any existing selection. This is the default.Add to the current selection—The resulting selection will be added to an existing selection. If no selection exists, this is the same as the New selection option.Remove from the current selection—The resulting selection will be removed from an existing selection. If no selection exists, the operation will have no effect.Select subset from the current selection—The resulting selection will be combined with the existing selection. Only records that are common to both will remain selected.Switch the current selection—The selection will be switched. All records that were selected will be removed from the selection, and all records that were not selected will be added to the selection.The Selecting Features and Relationship parameters are ignored when this option is specified. | String |
| Invert Spatial Relationship (Optional) | Specifies whether the spatial relationship evaluation result or the opposite result will be used. For example, this parameter can be used to get a list of features that do not intersect or are not within a given distance of features in another dataset.Unchecked—The evaluation result will be used. This is the default.Checked—The opposite of the evaluation result will be used. If the Selection Type parameter is set, the reversal of the selection will occur before it is combined with existing selections. | Boolean |
| in_layer[in_layer,...] | The features that will be evaluated using the select_features parameter values. The selection will be applied to these features. | Feature Layer; Raster Layer; Mosaic Layer |
| overlap_type(Optional) | Specifies the spatial relationship that will be evaluated.INTERSECT—The features in the input layer will be selected if they intersect a selecting feature. This is the default. INTERSECT_3D—The features in the input layer will be selected if they intersect a selecting feature in three-dimensional space (x, y, and z).INTERSECT_DBMS—The features in the input layer will be selected if they intersect a selecting feature.This option applies to enterprise geodatabases only. The selection will be processed in the enterprise geodatabase DBMS rather than on the client when all requirements are met (see usage notes).This option may provide better performance than performing the selection on the client.WITHIN_A_DISTANCE—The features in the input layer will be selected if they are within the specified distance (using Euclidean distance) of a selecting feature. Use the search_distance parameter to specify the distance.WITHIN_A_DISTANCE_3D—The features in the input layer will be selected if they are within a specified distance of a selecting feature in three-dimensional space. Use the search_distance parameter to specify the distance.WITHIN_A_DISTANCE_GEODESIC—This spatial relationship is the same as the WITHIN_A_DISTANCE option except that geodesic distance is used rather than planar distance. Distance between features will be calculated using a geodesic formula that takes into account the curvature of the spheroid and correctly handles data near and across the dateline and poles. Choose this option if the data covers a large geographic extent or the coordinate system of the inputs is unsuitable for distance calculations. Use the search_distance parameter to specify the distance.CONTAINS—The features in the input layer will be selected if they contain a selecting feature.COMPLETELY_CONTAINS—The features in the input layer will be selected if they completely contain a selecting feature.CONTAINS_CLEMENTINI—This spatial relationship produces the same results as the CONTAINS option except that if the selecting feature is entirely on the boundary of the input feature (no part is properly inside or outside), the feature will not be selected.Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.WITHIN—The features in the input layer will be selected if they are within a selecting feature.COMPLETELY_WITHIN—The features in the input layer will be selected if they are completely within or contained by a selecting feature.WITHIN_CLEMENTINI—The result will be identical to the WITHIN option result except that if the entirety of the feature in the input layer is on the boundary of the feature in the selecting layer, the feature will not be selected.Clementini defines the boundary polygon as the line separating inside and outside, the boundary of a line is defined as its end points, and the boundary of a point is always empty.ARE_IDENTICAL_TO—The features in the input layer will be selected if they are identical (in geometry) to a selecting feature.BOUNDARY_TOUCHES—The features in the input layer will be selected if they have a boundary that touches a selecting feature. When the input features are lines or polygons, the boundary of the input feature can only touch the boundary of the selecting feature, and no part of the input feature can cross the boundary of the selecting feature.SHARE_A_LINE_SEGMENT_WITH—The features in the input layer will be selected if they share a line segment with a selecting feature. The input and selecting features must be line or polygon.CROSSED_BY_THE_OUTLINE_OF—The features in the input layer will be selected if they are crossed by the outline of a selecting feature. The input and selecting features must be lines or polygons. If polygons are used for the input or selecting layer, the polygon's boundary (line) will be used. Lines that cross at a point will be selected; lines that share a line segment will not be selected.HAVE_THEIR_CENTER_IN—The features in the input layer will be selected if their center falls within a selecting feature. The center of the feature is calculated as follows: for polygon and multipoint, the geometry's centroid is used; for line input, the geometry's midpoint is used. | String |
| select_features(Optional) | The features in the Input Features parameter will be selected based on their relationship to the features from this layer or feature class. | Feature Layer |
| search_distance(Optional) | The distance that will be searched. This parameter is only valid if the overlap_type parameter is set to INTERSECT, INTERSECT_3D, WITHIN_A_DISTANCE, WITHIN_A_DISTANCE_3D, WITHIN_A_DISTANCE_GEODESIC, CONTAINS, or HAVE_THEIR_CENTER_IN.If the WITHIN_A_DISTANCE_GEODESIC option is specified, use a linear unit such as kilometers or miles. | Linear Unit |
| selection_type(Optional) | Specifies how the selection will be applied to the input and how it will be combined with an existing selection. This tool does not include an option to clear an existing selection; use the Select Layer By Attribute tool with the selection_type parameter set to CLEAR_SELECTION to do that.NEW_SELECTION—The resulting selection will replace any existing selection. This is the default.ADD_TO_SELECTION—The resulting selection will be added to an existing selection. If no selection exists, this is the same as the NEW_SELECTION option.REMOVE_FROM_SELECTION—The resulting selection will be removed from an existing selection. If no selection exists, the operation will have no effect.SUBSET_SELECTION—The resulting selection will be combined with the existing selection. Only records that are common to both will remain selected.SWITCH_SELECTION—The selection will be switched. All records that were selected will be removed from the selection, and all records that were not selected will be added to the selection.The select_features and overlap_type parameters are ignored when this option is specified. | String |
| invert_spatial_relationship(Optional) | Specifies whether the spatial relationship evaluation result or the opposite result will be used. For example, this parameter can be used to get a list of features that do not intersect or are not within a given distance of features in another dataset.NOT_INVERT—The evaluation result will be used. This is the default.INVERT—The opposite of the evaluation result will be used. If the selection_type parameter is set, the reversal of the selection will occur before it is combined with existing selections. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SelectLayerByLocation(in_layer, {overlap_type}, {select_features}, {search_distance}, {selection_type}, {invert_spatial_relationship})
```

### Example 2

```python
import arcpy
arcpy.management.SelectLayerByLocation("parcel_lyr", "have_their_center_in", 
                                       "c:/kamsack.gdb/city_limits")
```

### Example 3

```python
import arcpy
arcpy.management.SelectLayerByLocation("parcel_lyr", "have_their_center_in", 
                                       "c:/kamsack.gdb/city_limits")
```

### Example 4

```python
# Description: Extract features to a new feature class based on a 
#              location and an attribute query

# Import arcpy and set path to data
import arcpy
arcpy.env.workspace = "c:/data/mexico.gdb"

# Make a layer and select cities that overlap the chihuahua polygon
chihuahua_cities = arcpy.management.SelectLayerByLocation('cities', 'INTERSECT', 
                                                          'chihuahua')

# From the previous selection, select a subset of cities that have 
# population > 10,000
arcpy.management.SelectLayerByAttribute(chihuahua_cities, 
                                        'SUBSET_SELECTION', 
                                        '"population" > 10000')

# If features matched criteria, write them to a new feature class
matchcount = int(arcpy.management.GetCount(chihuahua_cities)[0]) 

if matchcount == 0:
    print('no features matched spatial and attribute criteria')
else:
    arcpy.management.CopyFeatures(chihuahua_cities, 'chihuahua_10000plus')
    print('{0} cities that matched criteria written to {0}'.format(
        matchcount, chihuahua_10000plus))
```

### Example 5

```python
# Description: Extract features to a new feature class based on a 
#              location and an attribute query

# Import arcpy and set path to data
import arcpy
arcpy.env.workspace = "c:/data/mexico.gdb"

# Make a layer and select cities that overlap the chihuahua polygon
chihuahua_cities = arcpy.management.SelectLayerByLocation('cities', 'INTERSECT', 
                                                          'chihuahua')

# From the previous selection, select a subset of cities that have 
# population > 10,000
arcpy.management.SelectLayerByAttribute(chihuahua_cities, 
                                        'SUBSET_SELECTION', 
                                        '"population" > 10000')

# If features matched criteria, write them to a new feature class
matchcount = int(arcpy.management.GetCount(chihuahua_cities)[0]) 

if matchcount == 0:
    print('no features matched spatial and attribute criteria')
else:
    arcpy.management.CopyFeatures(chihuahua_cities, 'chihuahua_10000plus')
    print('{0} cities that matched criteria written to {0}'.format(
        matchcount, chihuahua_10000plus))
```

### Example 6

```python
# Description: Select features within a distance

# Import arcpy and set path to data
import arcpy

arcpy.env.workspace = r"c:\data\mexico.gdb"

arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE', 
                                       'chihuahua', '1.5 Miles')
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE_GEODESIC', 
                                       'chihuahua', '200 Kilometers')

# When using WITHIN_A_DISTANCE, if distance units are not specified, the 
# distance value is assumed to be in the units of the input dataset's coordinate 
# system
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE', 
                                       'chihuahua', '200')

# When using WITHIN_A_DISTANCE_GEODESIC, if distance units are not specified, 
# the distance value is assumed to be in meters
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE_GEODESIC', 
                                       'chihuahua', '200')
```

### Example 7

```python
# Description: Select features within a distance

# Import arcpy and set path to data
import arcpy

arcpy.env.workspace = r"c:\data\mexico.gdb"

arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE', 
                                       'chihuahua', '1.5 Miles')
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE_GEODESIC', 
                                       'chihuahua', '200 Kilometers')

# When using WITHIN_A_DISTANCE, if distance units are not specified, the 
# distance value is assumed to be in the units of the input dataset's coordinate 
# system
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE', 
                                       'chihuahua', '200')

# When using WITHIN_A_DISTANCE_GEODESIC, if distance units are not specified, 
# the distance value is assumed to be in meters
arcpy.management.SelectLayerByLocation('cities', 'WITHIN_A_DISTANCE_GEODESIC', 
                                       'chihuahua', '200')
```

---

## Set Cluster Tolerance (Data Management)

## Summary

Sets the cluster tolerance of a topology.

## Usage

- You cannot alter the cluster tolerance for a topology if the topology has been registered as versioned.
- Changing the cluster tolerance will require the entire topology be validated.
- For more information about cluster tolerance, see Topology in ArcGIS

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The topology for which you want to change the cluster tolerance. | Topology Layer |
| Cluster Tolerance | The value to be set as the cluster tolerance property of the selected topology. If you enter a value of zero, the default or minimum cluster tolerance will be applied to the topology. | Double |
| in_topology | The topology for which you want to change the cluster tolerance. | Topology Layer |
| cluster_tolerance | The value to be set as the cluster tolerance property of the selected topology. If you enter a value of zero, the default or minimum cluster tolerance will be applied to the topology. | Double |

## Code Samples

### Example 1

```python
arcpy.management.SetClusterTolerance(in_topology, cluster_tolerance)
```

### Example 2

```python
# Name: SetClusterTolerance_Example.py
# Description: Updates the cluster tolerance property on a topology

# Import system modules
import arcpy

arcpy.SetClusterTolerance_management("D:/Calgary/Trans.mdb/Streets/Street_Topo", 0.00015)
```

### Example 3

```python
# Name: SetClusterTolerance_Example.py
# Description: Updates the cluster tolerance property on a topology

# Import system modules
import arcpy

arcpy.SetClusterTolerance_management("D:/Calgary/Trans.mdb/Streets/Street_Topo", 0.00015)
```

---

## Set Default Subtype (Data Management)

## Summary

Sets the default value or code for the input table's subtype.

## Usage

- The input table must contain subtype codes before setting a default code. Use the Add Subtype and Set Subtype Field tools to create subtype codes.
- You can also view and manage subtypes in Subtypes view which can be opened by clicking the Subtypes button found in the Design section of the Data ribbon, or the by clicking the Subtypes button on the Fields view ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class whose subtype default value will be set. | Table View |
| Subtype Code | The unique default value for a subtype. | Long |
| in_table | The input table or feature class whose subtype default value will be set. | Table View |
| subtype_code | The unique default value for a subtype. | Long |

## Code Samples

### Example 1

```python
arcpy.management.SetDefaultSubtype(in_table, subtype_code)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetDefaultSubtype_management("water/fittings", 5)
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetDefaultSubtype_management("water/fittings", 5)
```

### Example 4

```python
#Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
   
# Set local parameters
inFeatures = "water/fittings"
 
# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")
     
# Process: Add Subtypes...
# Store all the suptype values in a dictionary with the subtype code as the "key" and the 
# subtype description as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", "7": "Plug", 
             "8": "Reducer", "9": "Saddle", "10": "Sleeve", "11": "Tap", "12": "Tee", 
             "13": "Weld", "14": "Riser"} 

# Use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     
			
# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4")
```

### Example 5

```python
#Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace =  "C:/data/Montgomery.gdb"
   
# Set local parameters
inFeatures = "water/fittings"
 
# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")
     
# Process: Add Subtypes...
# Store all the suptype values in a dictionary with the subtype code as the "key" and the 
# subtype description as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", "7": "Plug", 
             "8": "Reducer", "9": "Saddle", "10": "Sleeve", "11": "Tap", "12": "Tee", 
             "13": "Weld", "14": "Riser"} 

# Use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     
			
# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4")
```

---

## Set Feature Class Split Model (Data Management)

## Summary

Defines the behavior of a split operation on a feature class.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Class | The feature class on which the split model will be set. | Feature Layer |
| Split Model (Optional) | Specifies the split model that will be applied to the input feature class. Delete/Insert/Insert—The original feature will be deleted, and both parts of the split feature will be inserted as new features with two new rows in the table.Update/Insert—The original feature will be updated, becoming the largest feature, and the smaller feature will be inserted as a new row in the table. This is the default. | String |
| in_feature_class | The feature class on which the split model will be set. | Feature Layer |
| split_model(Optional) | Specifies the split model that will be applied to the input feature class. DELETE_INSERT_INSERT—The original feature will be deleted, and both parts of the split feature will be inserted as new features with two new rows in the table.UPDATE_INSERT—The original feature will be updated, becoming the largest feature, and the smaller feature will be inserted as a new row in the table. This is the default. | String |

## Code Samples

### Example 1

```python
arcpy.management.SetFeatureClassSplitModel(in_feature_class, {split_model})
```

### Example 2

```python
import arcpy
arcpy.management.SetFeatureClassSplitModel("C:\\MyProject\\sdeConn.sde\\progdb.user1.Parcels", 
                                           "DELETE_INSERT_INSERT")
```

### Example 3

```python
import arcpy
arcpy.management.SetFeatureClassSplitModel("C:\\MyProject\\sdeConn.sde\\progdb.user1.Parcels", 
                                           "DELETE_INSERT_INSERT")
```

---

## Set Mosaic Dataset Properties (Data Management)

## Summary

Defines the defaults for displaying a mosaic dataset and serving it as an image service.

## Usage

- The current mosaic dataset properties are on the Defaults tab of the Mosaic Dataset Properties dialog box.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset with the properties that will be set. | Mosaic Layer |
| Rows of Maximum Image Size of Requests(Optional) | The maximum number of rows for the mosaicked image, generated by the mosaic dataset for each request. This can help control how much work the server has to do when clients view the imagery. A higher number will create a larger image but will increase the amount of time to process the mosaic dataset. If the value is too low, the image may not display. | Long |
| Columns of Maximum Image Size of Requests(Optional) | The maximum number of columns for the mosaicked image, generated by the mosaic dataset for each request. This can help control how much work the server has to do when clients view the imagery. A higher number will create a larger image but will increase the amount of time to process the mosaic dataset. If the value is too low, the image may not display. | Long |
| Allowed Transmission Compression(Optional) | Specifies the compression methods that will be used to transmit the mosaicked image from the computer to the display (or from the server to the client).None—No compression will be used.JPEG— Compression up to 8:1 will be used, which is suitable for backdrops.LZ77— Compression of approximately 2:1 will be used, which is suitable for analysis.LERC—Compression between 10:1 and 20:1 will be used, which is fast and suitable for serving raw imagery with high bit depth (12 bit to 32 bit). | String |
| Default Compression Type(Optional) | Specifies the default compression type. The default compression must be in the list of values used for the Allowed Transmission Compression parameter or must be set in the mosaic dataset's Allowed Compression Methods property.None—No compression will be used.JPEG— Compression up to 8:1 will be used, which is suitable for backdrops.LZ77— Compression of approximately 2:1 will be used, which is suitable for analysis.LERC—Compression between 10:1 and 20:1 will be used, which is fast and suitable for serving raw imagery with high bit depth (12 bit to 32 bit).None—No compression will be used.JPEG— Compression up to 8:1 will be used, which is suitable for backdrops.LZ77— Compression of approximately 2:1 will be used, which is suitable for analysis.LERC—Compression between 10:1 and 20:1 will be used, which is fast and suitable for serving raw imagery with high bit depth (12 bit to 32 bit). | String |
| JPEG Quality(Optional) | The compression quality when using JPEG. Compression quality ranges from 1 to 100. A higher number means better image quality but less compression. | Long |
| LERC Tolerance(Optional) | The maximum per pixel error when using LERC compression. This value is specified in the units of the mosaic dataset. For example, if the error is 10 centimeters and the mosaic dataset is in meters, enter 0.1. | Double |
| Resampling Technique(Optional) | Specifies how pixel values will be calculated when the dataset is displayed at small scales. Choose an appropriate technique based on the type of data.Nearest—The value of each pixel will be from the nearest corresponding pixel. This technique is suitable for discrete data, such as land cover.This is the fastest resampling technique. It minimizes the changes to pixel values since it uses the value from the nearest pixel. Bilinear—The value of each pixel will be calculated by averaging the values of the surrounding four pixels (based on distance). This technique is suitable for continuous data.Cubic—The value of each pixel will be calculated by fitting a smooth curve based on the surrounding 16 pixels. This technique produces the smoothest image but can create values outside of the range in the source data. It is suitable for continuous data.Majority—The value of each pixel will be based on the most popular value in a 3 by 3 window. This technique is suitable for discrete data. | String |
| Clip To Footprints(Optional) | Specifies whether rasters will be clipped to the footprint. Often the raster dataset and its footprint have the same extent. If they differ, the raster dataset can be clipped to the footprint. Unchecked—The rasters will not be clipped to the footprint. This is the default.Checked—The rasters will be clipped to the footprint. | Boolean |
| Footprints May Contain NoData(Optional) | Specifies whether pixels with NoData values will be shown.Checked—Pixels with NoData values will be shown.Unchecked—Pixels with NoData values will not be shown. You may notice an improvement in performance; however, if the imagery does include NoData values, they will appear as holes in the mosaic dataset. | Boolean |
| Clip To Boundary(Optional) | Specifies whether the mosaicked image will be clipped to the boundary. Often the mosaic dataset and its boundary have the same extent. If they differ, the mosaic dataset can be clipped to the boundary.Checked—The mosaicked image will be clipped to the boundary. This is the default. Unchecked—The mosaicked image will not be clipped to the boundary. | Boolean |
| Color Correction(Optional) | Specifies whether color correction will be used on the mosaic dataset.Unchecked—Color correction will not be used. This is the default.Checked—The color correction that has been set up for the mosaic dataset will be used. | Boolean |
| Allowed Mensuration Capabilities(Optional) | Specifies the measurements that will be performed on the mosaic dataset. The ability to perform vertical measurements is dependent on the imagery and may require a DEM.None—No mensuration capabilities will be performed.Basic—Ground measurements such as distance, point, centroid, and area calculations will be performed.Feature base to top of feature—Measurements from the base to the top of features will be performed. Rational polynomial coefficients must be imbedded in the imagery.Feature base to top of shadow—Measurements from the base of a feature to the top of its shadow will be performed. Sun azimuth and sun elevation information is required.Top of feature to top of shadow—Measurements from the top of a feature to the top of its shadow will be performed. Sun azimuth, sun elevation, and rational polynomial coefficients are required.Measure in 3D—Measurements in 3D will be performed if a DEM is available. | String |
| Default Mensuration(Optional) | Specifies the default mensuration capability for the mosaic dataset. The default mensuration value must be set in the list of values used for the Allowed Mensuration Capabilities parameter or be set in the mosaic dataset's Mensuration Capabilities property.None—No mensuration capabilities will be performed.Basic— Ground measurements such as distance, point, centroid, and area calculations will be performed.Feature base to top of feature—Measurements from the base to the top of features will be performed. Rational polynomial coefficients must be imbedded in the imagery.Feature base to top of shadow—Measurements from the base of a feature to the top of its shadow will be performed. Sun azimuth and sun elevation information is required.Top of feature to top of shadow—Measurements from the top of a feature to the top of its shadow will be performed. Sun azimuth, sun elevation, and rational polynomial coefficients are required.Measure in 3D—Measurements in 3D will be performed if a DEM is available.None—No mensuration capabilities will be performed.Basic— Ground measurements such as distance, point, centroid, and area calculations will be performed.Feature base to top of feature—Measurements from the base to the top of features will be performed. Rational polynomial coefficients must be imbedded in the imagery.Feature base to top of shadow—Measurements from the base of a feature to the top of its shadow will be performed. Sun azimuth and sun elevation information is required.Top of feature to top of shadow—Measurements from the top of a feature to the top of its shadow will be performed. Sun azimuth, sun elevation, and rational polynomial coefficients are required.Measure in 3D—Measurements in 3D will be performed if a DEM is available. | String |
| Allowed Mosaic Methods(Optional) | Specifies the rules for displaying overlapping imagery.None—Rasters will be ordered based on the ObjectID field in the mosaic dataset attribute table.Center—Imagery that is closest to the center of the screen will be displayed.Northwest—Imagery that is closest to the northwest corner of the mosaic dataset boundary will be displayed.Lock raster—Selected raster datasets will be displayed.By attribute—Imagery will be displayed and prioritized based on a field in the attribute table.Nadir—Rasters with viewing angles closest to zero will be displayed.Viewpoint—Imagery that is closest to a selected viewing angle will be displayed. Seamline—Transitions between images will be smoothed using seamlines. | String |
| Default Mosaic Methods(Optional) | Specifies the mosaic method that will be used for views. The default mosaic method must be set in the list of values used for the Allowed Mosaic Methods parameter or be set in the mosaic dataset's Allowed Mosaic Methods property.None—Rasters will be ordered based on the ObjectID field in the mosaic dataset attribute table.Center—Imagery that is closest to the center of the screen will be displayed.Northwest—Imagery that is closest to the northwest corner of the mosaic dataset boundary will be displayed.Lock raster—Selected raster datasets will be displayed.By attribute—Imagery will be displayed and prioritized based on a field in the attribute table.Nadir—Rasters with viewing angles closest to zero will be displayed.Viewpoint—Imagery that is closest to a selected viewing angle will be displayed. Seamline—Transitions between images will be smoothed using seamlines.None—Rasters will be ordered based on the ObjectID field in the mosaic dataset attribute table.Center—Imagery that is closest to the center of the screen will be displayed.Northwest—Imagery that is closest to the northwest corner of the mosaic dataset boundary will be displayed.Lock raster—Selected raster datasets will be displayed.By attribute—Imagery will be displayed and prioritized based on a field in the attribute table.Nadir—Rasters with viewing angles closest to zero will be displayed.Viewpoint—Imagery that is closest to a selected viewing angle will be displayed. Seamline—Transitions between images will be smoothed using seamlines. | String |
| Order Field(Optional) | The field that will be used when ordering rasters using the By attribute value of the Default Mosaic Methods parameter. The list of available fields are defined as those in the attribute table that are of type metadata and are integer. This list can include, but is not limited to, the following:MinPSMaxPSLowPSHighPSCenterXCenterYZOrderShape_LengthShape_AreaIf the field is a numeric or date field, the Order Base parameter must be set.This parameter is not needed if the By attribute value is not part of the Allowed Mosaic Methods list. | String |
| Order Base(Optional) | Sorts the rasters based on their difference from this value in the field selected in the Order Field parameter.If a Date attribute is used, it must be in one of the following formats:YYYY/MM/DD HH:mm:ss.sYYYY/MM/DD HH:mm:ssYYYY/MM/DD HH:mmYYYY/MM/DD HHYYYY/MM/DDYYYY/MMYYYY This parameter is required only if the By attribute value is specified for the Allowed Mosaic Methods parameter. | String |
| Sorting Order Ascending(Optional) | Specifies whether the rasters will be sorted in an ascending or a descending order.Checked—Rasters will be sorted in an ascending order. This is the default.Unchecked—Rasters will be sorted in a descending order. This parameter is required only if the By attribute value is specified for the Allowed Mosaic Methods parameter. | Boolean |
| Mosaic Operator(Optional) | Specifies the rule that will be used for resolving overlapping pixels.First—The first image in the attribute table will be displayed.Last—The last image in the attribute table will be displayed.Minimum—The lowest pixel values will be displayed.Maximum—The highest pixel values will be displayed.Mean—The arithmetic mean will be used to average overlapping pixels.Blend—A distance weighted algorithm will be used to average overlapping pixels.Sum—All of the overlapping pixel values will be added together. | String |
| Blend Width(Optional) | The number of pixels to which the Blend value of the Mosaic Operator parameter will be applied. | Long |
| View Point Spacing X(Optional) | A numeric value that will be used to horizontally shift the center of the image. The units are the same as the spatial reference system.This parameter is only applicable if the Allowed Mosaic Methods parameter is set to Viewpoint. | Double |
| View Point Spacing Y(Optional) | A numeric value that will be used to vertically shift the center of the image. The units are the same as the spatial reference system.This parameter is only applicable if the Allowed Mosaic Methods parameter is set to Viewpoint. | Double |
| Max Number Per Mosaic(Optional) | The maximum number of raster datasets that will be displayed at a given time in a mosaic dataset. | Long |
| Cell Size Tolerance Factor(Optional) | The maximum pixel size difference that is allowed before images are considered to have a different cell pixel.This allows images of similar spatial resolutions to be considered as having the same nominal resolution. For example, with a factor of 0.1, all images with cell sizes within 10 percent of each other will be grouped for tools and operations that use cell sizes. | Double |
| Output Cell Size(Optional) | The cell size of the mosaic dataset using an existing raster dataset or its specified width (x) and height (y). | Cell Size XY |
| Metadata Level(Optional) | Specifies the level of metadata that will be exposed from the server to a client when publishing the mosaic dataset.Full metadata—Metadata regarding the processing applied at the mosaic dataset level as well as metadata related to the individual raster datasets will be exposed.No metadata—No metadata will be exposed to the client.Basic metadata—Metadata related to individual raster datasets, such as the number of columns and rows, cell size, and spatial reference information, will be exposed. | String |
| Allowed Transmission Field(Optional) | The fields in the attribute table that clients can view. By default, the list includes the following:NameMinPSMaxPSLowPSHighPSTagGroupNameProductNameCenterXCenterYZOrderShape_LengthShape_Area | String |
| Use Time(Optional) | Specifies whether the mosaic dataset will be time aware. If time is activated, the start and end fields and the time format must be specified.Unchecked—The mosaic dataset will not be time aware. This is the default. Checked—The mosaic dataset will be time aware. This allows the client to use the Time Slider. | Boolean |
| Start Time Field(Optional) | The field in the attribute table that shows the start time. | String |
| End Time Field(Optional) | The field in the attribute table that shows the end time. | String |
| Time Format(Optional) | Specifies the time format that will be used for the mosaic dataset for parameters such as Start Time Field and End Time Field.YYYY (Year)—The time format will be year.YYYYMM (Year and month)—The time format will be year and month.YYYY/MM (Year and month)—The time format will be year and month.YYYY-MM (Year and month)—The time format will be year and month.YYYYMMDD (Year, month, and day)—The time format will be year, month, and day.YYYY/MM/DD (Year, month, and day)—The time format will be year, month, and day.YYYY-MM-DD (Year, month, and day)—The time format will be year, month, and day.YYYYMMDDhhmmss (Year, month, day, hour, minute, and seconds)—The time format will be year, month, day, hour, minute, and seconds.YYYY/MM/DD hh:mm:ss (Year, month, day, hour, minute, and seconds)—The time format will be year, month, day, hour, minute, and seconds.YYYY-MM-DD hh:mm:ss (Year, month, day, hour, minute, and seconds)—The time format will be year, month, day, hour, minute, and seconds.YYYYMMDDhhmmss.s (Year, month, day, hour, minute, seconds, and fraction of seconds)—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds.YYYY/MM/DD hh:mm:ss.s (Year, month, day, hour, minute, seconds, and fraction of seconds)—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds.YYYY-MM-DD hh:mm:ss.s (Year, month, day, hour, minute, seconds, and fraction of seconds)—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds. | String |
| Geographic Transformation(Optional) | The geographic transformations that will be associated with the mosaic dataset. | String |
| Max Number of Download Items(Optional) | The maximum number of raster datasets that will be downloaded per request. | Long |
| Max Number of Records Returned(Optional) | The maximum number of records that will be downloaded per request. | Long |
| Data Source Type(Optional) | Specifies the type of imagery in the mosaic dataset.Generic—The mosaic dataset contains no specified data type.Thematic—The mosaic dataset contains thematic data with discrete values, such as land cover. Processed—The mosaic dataset has been color adjusted.Elevation—The mosaic dataset contains elevation data.Scientific—The mosaic dataset contains scientific data.Two variable vector—The mosaic dataset has two variables.Magnitude and direction—The mosaic dataset has magnitude and direction. | String |
| Minimum Pixel Contribution(Optional) | The minimum number of pixels required for a mosaic dataset item to be considered significant enough to be used in the mosaic dataset. Because of overlapping imagery, an item may display only a small sliver of its overall image. Skipping these mosaic dataset items will improve performance of the mosaic dataset. | Long |
| Processing Templates(Optional) | The function chains that will be used to process a mosaic dataset or the mosaic dataset items on the fly. You can add, remove, or reorder the function chains. All the template names that are added must be unique.For information about working with function chains, see Raster function template. | File; String |
| Default Processing Template(Optional) | The default function chain. The default function chain will be applied when the mosaic dataset is accessed. | String |
| Time Interval(Optional) | The duration of each time step interval. The time step interval defines the granularity of the temporal data. The unit of time is specified in the Time Interval Units parameter. | Double |
| Time Interval Units(Optional) | Specifies the measurement unit that will be used for the time interval. None—No time unit exists or it is unknown.Milliseconds—The time unit will be milliseconds.Seconds—The time unit will be seconds.Minutes—The time unit will be minutes.Hours—The time unit will be hours.Days—The time unit will be days.Weeks—The time unit will be weeks.Months—The time unit will be months.Years—The time unit will be years.Decades—The time unit will be decades.Centuries—The time unit will be centuries. | String |
| Product Definition(Optional) | Specifies a template that is either specific to the type of imagery you are working with or generic. The generic options include the standard supported raster sensor types as follows:None—No band ordering is specified for the mosaic dataset. This is the default.Natural color—A 3-band mosaic dataset, with red, green, and blue wavelength ranges will be created. This is designed for natural color imagery.Natural color and infrared—A 4-band mosaic dataset, with red, green, blue, and near infrared wavelength ranges will be created.U and V—A mosaic dataset displaying two variables will be created.Magnitude and Direction—A mosaic dataset displaying magnitude and direction will be created.Color infrared—A 3-band mosaic dataset, with near infrared, red, and green wavelength ranges will be created.BlackSky—A 3-band mosaic dataset using the BlackSky wavelength ranges will be createdDMCii—A 3-band mosaic dataset using the DMCii wavelength ranges will be created.Deimos-2—A 4-band mosaic dataset using the Deimos-2 wavelength ranges will be created.DubaiSat-2—A 4-band mosaic dataset using the DubaiSat-2 wavelength ranges will be created.FORMOSAT-2—A 4-band mosaic dataset using the FORMOSAT-2 wavelength ranges will be created.GeoEye-1—A 4-band mosaic dataset using the GeoEye-1 wavelength ranges will be created.GF-1 Panchromatic/Multispectral (PMS)—A 4-band mosaic dataset using the Gaofen-1 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-1 Wide Field of View (WFV)—A 4-band mosaic dataset using the Gaofen-1 Wide Field of View Sensor wavelength ranges will be created.GF-2 Panchromatic/Multispectral (PMS)—A 4-band mosaic dataset using the Gaofen-2 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-4 Panchromatic/Multispectral Imagery (PMI)—A 4-band mosaic dataset using the Gaofen-4 panchromatic and multispectral wavelength ranges will be created.HJ 1A/1B Multispectral/Hyperspectral—A 4-band mosaic dataset using the Huan Jing-1 CCD Multispectral or Hyperspectral Sensor wavelength ranges will be created.IKONOS—A 4-band mosaic dataset using the IKONOS wavelength ranges will be created.Jilin-1—A 3-band mosaic dataset using the Jilin-1 wavelength ranges will be created.KOMPSAT-2—A 4-band mosaic dataset using the KOMPSAT-2 wavelength ranges will be created.KOMPSAT-3—A 4-band mosaic dataset using the KOMPSAT-3 wavelength ranges will be created.Landsat TM and ETM+—A 6-band mosaic dataset using the Landsat 5 and 7 wavelength ranges from the TM and ETM+ sensors will be created.Landsat OLI—An 8-band mosaic dataset using the Landsat 8 wavelength ranges will be created.Landsat 9—An 8-band mosaic dataset using the Landsat 9 wavelength ranges will be created.Landsat MSS—A 4-band mosaic dataset using the Landsat wavelength ranges from the MSS sensor will be created.PlanetScope—A 5-band mosaic dataset using the PlanetScope wavelength ranges will be created.Pleiades 1—A 4-band mosaic dataset using the Pleiades 1 wavelength ranges will be created.Pleiades Neo—A 6-band mosaic dataset using the Pleiades Neo wavelength ranges will be created.QuickBird—A 4-band mosaic dataset using the QuickBird wavelength ranges will be created.RapidEye—A 5-band mosaic dataset using the RapidEye wavelength ranges will be created.Sentinel 2 MSI—A 13-band mosaic dataset using the Sentinel 2 MSI wavelength ranges will be created.SkySat-C—A 4-band mosaic dataset using the SkySat-C MSI wavelength ranges will be created.SPOT-5—A 4-band mosaic dataset using the SPOT-5 wavelength ranges will be created.SPOT-6—A 4-band mosaic dataset using the SPOT-6 wavelength ranges will be created.SPOT-7—A 4-band mosaic dataset using the SPOT-7 wavelength ranges will be created.SuperView-1—A 4-band mosaic dataset using the SuperView-1 wavelength ranges will be created.TH-01—A 4-band mosaic dataset using the Tian Hui-1 wavelength ranges will be created.Vision-1—A 4-band mosaic dataset using the Vision-1 wavelength ranges will be created.WorldView-2—An 8-band mosaic dataset using the WorldView-2 wavelength ranges will be created.WorldView-3—An 8-band mosaic dataset using the WorldView-3 wavelength ranges will be created.WorldView-4—A 4-band mosaic dataset using the WorldView-4 wavelength ranges will be created.ZY-1 Panchromatic/Multispectral—A 3-band mosaic dataset using the ZiYuan-1 panchromatic/multispectral wavelength ranges will be created.ZY-3 CRESDA—A 4-band mosaic dataset using the ZiYuan-3 CRESDA wavelength ranges will be created.ZY3 SASMAC—A 4-band mosaic dataset using the ZiYuan-3 SASMAC wavelength ranges will be created.Custom—The number of bands and the average wavelength for each band are defined using the Product Band Definitions parameter (product_band_definitions in Python). | String |
| Product Band Definitions (Optional) | The wavelength ranges, number of bands, and band order definitions. To edit the number of bands, use the Add another and Remove controls. To rearrange the band order, right-click a band definition and move the band up or down in the list. | Value Table |
| in_mosaic_dataset | The mosaic dataset with the properties that will be set. | Mosaic Layer |
| rows_maximum_imagesize(Optional) | The maximum number of rows for the mosaicked image, generated by the mosaic dataset for each request. This can help control how much work the server has to do when clients view the imagery. A higher number will create a larger image but will increase the amount of time to process the mosaic dataset. If the value is too low, the image may not display. | Long |
| columns_maximum_imagesize(Optional) | The maximum number of columns for the mosaicked image, generated by the mosaic dataset for each request. This can help control how much work the server has to do when clients view the imagery. A higher number will create a larger image but will increase the amount of time to process the mosaic dataset. If the value is too low, the image may not display. | Long |
| allowed_compressions[allowed_compressions,...](Optional) | Specifies the compression methods that will be used to transmit the mosaicked image from the computer to the display (or from the server to the client).None—No compression will be used.JPEG— Compression up to 8:1 will be used, which is suitable for backdrops.LZ77— Compression of approximately 2:1 will be used, which is suitable for analysis.LERC—Compression between 10:1 and 20:1 will be used, which is fast and suitable for serving raw imagery with high bit depth (12 bit to 32 bit). | String |
| default_compression_type(Optional) | Specifies the default compression type. The default compression must be in the list of values used for the allowed_compressions parameter or must be set in the mosaic dataset's Allowed Compression Methods property.None—No compression will be used.JPEG— Compression up to 8:1 will be used, which is suitable for backdrops.LZ77— Compression of approximately 2:1 will be used, which is suitable for analysis.LERC—Compression between 10:1 and 20:1 will be used, which is fast and suitable for serving raw imagery with high bit depth (12 bit to 32 bit). | String |
| JPEG_quality(Optional) | The compression quality when using JPEG. Compression quality ranges from 1 to 100. A higher number means better image quality but less compression. | Long |
| LERC_Tolerance(Optional) | The maximum per pixel error when using LERC compression. This value is specified in the units of the mosaic dataset. For example, if the error is 10 centimeters and the mosaic dataset is in meters, enter 0.1. | Double |
| resampling_type(Optional) | Specifies how pixel values will be calculated when the dataset is displayed at small scales. Choose an appropriate technique based on the type of data.NEAREST—The value of each pixel will be from the nearest corresponding pixel. This technique is suitable for discrete data, such as land cover.This is the fastest resampling technique. It minimizes the changes to pixel values since it uses the value from the nearest pixel. BILINEAR—The value of each pixel will be calculated by averaging the values of the surrounding four pixels (based on distance). This technique is suitable for continuous data.CUBIC—The value of each pixel will be calculated by fitting a smooth curve based on the surrounding 16 pixels. This technique produces the smoothest image but can create values outside of the range in the source data. It is suitable for continuous data.MAJORITY—The value of each pixel will be based on the most popular value in a 3 by 3 window. This technique is suitable for discrete data. | String |
| clip_to_footprints(Optional) | Specifies whether rasters will be clipped to the footprint. Often the raster dataset and its footprint have the same extent. If they differ, the raster dataset can be clipped to the footprint.NOT_CLIP—The rasters will not be clipped to the footprint. This is the default.CLIP—The rasters will be clipped to the footprint. | Boolean |
| footprints_may_contain_nodata(Optional) | Specifies whether pixels with NoData values will be shown.FOOTPRINTS_MAY_CONTAIN_NODATA—Pixels with NoData values will be shown. This is the default.FOOTPRINTS_DO_NOT_CONTAIN_NODATA—Pixels with NoData values will not be shown. You may notice an improvement in performance; however, if the imagery does include NoData values, they will appear as holes in the mosaic dataset. | Boolean |
| clip_to_boundary(Optional) | Specifies whether the mosaicked image will be clipped to the boundary. Often the mosaic dataset and its boundary have the same extent. If they differ, the mosaic dataset can be clipped to the boundary.CLIP— The mosaicked image will be clipped to the boundary. This is the default.NOT_CLIP—The mosaicked image will not be clipped to the boundary. | Boolean |
| color_correction(Optional) | Specifies whether color correction will be used on the mosaic dataset.NOT_APPLY—Color correction will not be used. This is the default.APPLY—The color correction that has been set up for the mosaic dataset will be used. | Boolean |
| allowed_mensuration_capabilities[allowed_mensuration_capabilities,...](Optional) | Specifies the measurements that will be performed on the mosaic dataset. The ability to perform vertical measurements is dependent on the imagery and may require a DEM.None—No mensuration capabilities will be performed.Basic—Ground measurements such as distance, point, centroid, and area calculations will be performed.Base-Top Height—Measurements from the base to the top of features will be performed. Rational polynomial coefficients must be imbedded in the imagery.Base-Top Shadow Height—Measurements from the base of a feature to the top of its shadow will be performed. Sun azimuth and sun elevation information is required.Top-Top Shadow Height—Measurements from the top of a feature to the top of its shadow will be performed. Sun azimuth, sun elevation, and rational polynomial coefficients are required.3D—Measurements in 3D will be performed if a DEM is available. | String |
| default_mensuration_capabilities(Optional) | Specifies the default mensuration capability for the mosaic dataset. The default mensuration value must be set in the list of values used for the allowed_mensuration_capabilities parameter or be set in the mosaic dataset's Mensuration Capabilities property.None—No mensuration capabilities will be performed.Basic— Ground measurements such as distance, point, centroid, and area calculations will be performed.Base-Top Height—Measurements from the base to the top of features will be performed. Rational polynomial coefficients must be imbedded in the imagery.Base-Top Shadow Height—Measurements from the base of a feature to the top of its shadow will be performed. Sun azimuth and sun elevation information is required.Top-Top Shadow Height—Measurements from the top of a feature to the top of its shadow will be performed. Sun azimuth, sun elevation, and rational polynomial coefficients are required.3D—Measurements in 3D will be performed if a DEM is available. | String |
| allowed_mosaic_methods[allowed_mosaic_methods,...](Optional) | Specifies the rules for displaying overlapping imagery.None—Rasters will be ordered based on the ObjectID field in the mosaic dataset attribute table.Center—Imagery that is closest to the center of the screen will be displayed.NorthWest—Imagery that is closest to the northwest corner of the mosaic dataset boundary will be displayed.LockRaster—Selected raster datasets will be displayed.ByAttribute—Imagery will be displayed and prioritized based on a field in the attribute table.Nadir—Rasters with viewing angles closest to zero will be displayed.Viewpoint—Imagery that is closest to a selected viewing angle will be displayed. Seamline—Transitions between images will be smoothed using seamlines. | String |
| default_mosaic_method(Optional) | Specifies the default mosaic method that will be used for the mosaic dataset. The default mosaic method must be set in the list of values used for the allowed_mosaic_methods parameter or be set in the mosaic dataset's Allowed Mosaic Methods property.None—Rasters will be ordered based on the ObjectID field in the mosaic dataset attribute table.Center—Imagery that is closest to the center of the screen will be displayed.NorthWest—Imagery that is closest to the northwest corner of the mosaic dataset boundary will be displayed.LockRaster—Selected raster datasets will be displayed.ByAttribute—Imagery will be displayed and prioritized based on a field in the attribute table.Nadir—Rasters with viewing angles closest to zero will be displayed.Viewpoint—Imagery that is closest to a selected viewing angle will be displayed. Seamline—Transitions between images will be smoothed using seamlines. | String |
| order_field(Optional) | The field that will be used when ordering rasters using the ByAttribute value of the default_mosaic_method parameter. The list of fields is defined as those in the attribute table that are of type metadata and are integer. This list can include, but is not limited to, the following:MinPSMaxPSLowPSHighPSCenterXCenterYZOrderShape_LengthShape_AreaIf the field is a numeric or date field, the order_base parameter must be set.This parameter is not needed if the ByAttribute value is not in the allowed_mosaic_methods list. | String |
| order_base(Optional) | Sorts the rasters based on their difference from this value in the field selected in the order_field parameterIf a Date attribute is used, it must be in one of the following formats:YYYY/MM/DD HH:mm:ss.sYYYY/MM/DD HH:mm:ssYYYY/MM/DD HH:mmYYYY/MM/DD HHYYYY/MM/DDYYYY/MMYYYY This parameter is required only if the ByAttribute value is specified for the allowed_mosaic_methods parameter. | String |
| sorting_order(Optional) | Specifies whether the rasters will be sorted in an ascending or a descending order.ASCENDING—Rasters will be sorted in an ascending order. This is the default.DESCENDING—Rasters will be sorted in a descending order. This parameter is required only if the ByAttribute value is specified for the allowed_mosaic_methods parameter. | Boolean |
| mosaic_operator(Optional) | Specifies the rule that will be used for resolving overlapping pixels.FIRST—The first image in the attribute table will be displayed.LAST—The last image in the attribute table will be displayed.MIN—The lowest pixel values will be displayed.MAX—The highest pixel values will be displayed.MEAN—The arithmetic mean will be used to average overlapping pixels.BLEND—A distance weighted algorithm will be used to average overlapping pixels.SUM—All of the overlapping pixel values will be added together. | String |
| blend_width(Optional) | The number of pixels to which the BLEND value of the mosaic_operator parameter will be applied. | Long |
| view_point_x(Optional) | A numeric value that will be used to horizontally shift the center of the image. The units are the same as the spatial reference system.This parameter is only applicable if the allowed_mosaic_methods parameter is set to Viewpoint. | Double |
| view_point_y(Optional) | A numeric value that will be used to vertically shift the center of the image. The units are the same as the spatial reference system.This parameter is only applicable if the allowed_mosaic_methods parameter is set to Viewpoint. | Double |
| max_num_per_mosaic(Optional) | The maximum number of raster datasets that will be displayed at a given time in a mosaic dataset. | Long |
| cell_size_tolerance(Optional) | The maximum pixel size difference that is allowed before images are considered to have a different cell pixel.This allows images of similar spatial resolutions to be considered as having the same nominal resolution. For example, with a factor of 0.1, all images with cell sizes within 10 percent of each other will be grouped for tools and operations that use cell sizes. | Double |
| cell_size(Optional) | The cell size of the mosaic dataset using an existing raster dataset or its specified width (x) and height (y). If you specify the cell size, you can use a single value for a square cell size, or x and y values for a rectangular cell size. | Cell Size XY |
| metadata_level(Optional) | Specifies the level of metadata that will be exposed from the server to a client when publishing the mosaic dataset.FULL—Metadata regarding the processing applied at the mosaic dataset level as well as metadata related to the individual raster datasets will be exposed.NONE—No metadata will be exposed to the client.BASIC—Metadata related to individual raster datasets, such as the number of columns and rows, cell size, and spatial reference information, will be exposed. | String |
| transmission_fields[transmission_fields,...](Optional) | The fields in the attribute table that clients can view. By default, the list includes the following:NameMinPSMaxPSLowPSHighPSTagGroupNameProductNameCenterXCenterYZOrderShape_LengthShape_Area | String |
| use_time(Optional) | Specifies whether the mosaic dataset will be time aware. If time is activated, the start and end fields and the time format must be specified.DISABLED—The mosaic dataset will not be time aware. This is the default.ENABLED—The mosaic dataset will be time aware. This allows the client to use the Time Slider. | Boolean |
| start_time_field(Optional) | The field in the attribute table that shows the start time. | String |
| end_time_field(Optional) | The field in the attribute table that shows the end time. | String |
| time_format(Optional) | Specifies the time format that will be used for the mosaic dataset for parameters such as start_time_field and end_time_field.YYYY—The time format will be year.YYYYMM—The time format will be year and month.YYYY/MM—The time format will be year and month.YYYY-MM—The time format will be year and month.YYYYMMDD—The time format will be year, month, and day.YYYY/MM/DD—The time format will be year, month, and day.YYYY-MM-DD—The time format will be year, month, and day.YYYYMMDDhhmmss—The time format will be year, month, day, hour, minute, and seconds.YYYY/MM/DD hh:mm:ss—The time format will be year, month, day, hour, minute, and seconds.YYYY-MM-DD hh:mm:ss—The time format will be year, month, day, hour, minute, and seconds.YYYYMMDDhhmmss.s—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds.YYYY/MM/DD hh:mm:ss.s—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds.YYYY-MM-DD hh:mm:ss.s—The time format will be year, month, day, hour, minute, seconds, and fraction of seconds. | String |
| geographic_transform[geographic_transform,...](Optional) | The geographic transformations that will be associated with the mosaic dataset. | String |
| max_num_of_download_items(Optional) | The maximum number of raster datasets that will be downloaded per request. | Long |
| max_num_of_records_returned(Optional) | The maximum number of records that will be downloaded per request. | Long |
| data_source_type(Optional) | Specifies the type of imagery in the mosaic dataset.GENERIC—The mosaic dataset contains no specified data type.THEMATIC—The mosaic dataset contains thematic data with discrete values, such as land cover. PROCESSED—The mosaic dataset has been color adjusted.ELEVATION—The mosaic dataset contains elevation data.SCIENTIFIC—The mosaic dataset contains scientific data.VECTOR_UV—The mosaic dataset has two variables.VECTOR_MAGDIR—The mosaic dataset has magnitude and direction. | String |
| minimum_pixel_contribution(Optional) | The minimum number of pixels required for a mosaic dataset item to be considered significant enough to be used in the mosaic dataset. Because of overlapping imagery, an item may display only a small sliver of its overall image. Skipping these mosaic dataset items will improve performance of the mosaic dataset. | Long |
| processing_templates[processing_templates,...](Optional) | The function chains that will be used to process a mosaic dataset or the mosaic dataset items on the fly. You can add, remove, or reorder the function chains. All the template names that are added must be unique.For information about working with function chains, see Raster function template. | File; String |
| default_processing_template(Optional) | The default function chain. The default function chain will be applied when the mosaic dataset is accessed. | String |
| time_interval(Optional) | The duration of each time step interval. The time step interval defines the granularity of the temporal data. The unit of time is specified in the time_interval_units parameter. | Double |
| time_interval_units(Optional) | Specifies the measurement unit that will be used for the time interval. None—No time unit exists or it is unknown.Milliseconds—The time unit will be milliseconds.Seconds—The time unit will be seconds.Minutes—The time unit will be minutes.Hours—The time unit will be hours.Days—The time unit will be days.Weeks—The time unit will be weeks.Months—The time unit will be months.Years—The time unit will be years.Decades—The time unit will be decades.Centuries—The time unit will be centuries. | String |
| product_definition(Optional) | Specifies a template that is either specific to the type of imagery you are working with or generic. The generic options include the standard supported raster sensor types as follows:NONE—No band ordering is specified for the mosaic dataset. This is the default.NATURAL_COLOR_RGB—A 3-band mosaic dataset, with red, green, and blue wavelength ranges will be created. This is designed for natural color imagery.NATURAL_COLOR_RGBI—A 4-band mosaic dataset, with red, green, blue, and near infrared wavelength ranges will be created.VECTOR_FIELD_UV—A mosaic dataset displaying two variables will be created.VECTOR_FIELD_MAGNITUDE_DIRECTION—A mosaic dataset displaying magnitude and direction will be created.FALSE_COLOR_IRG—A 3-band mosaic dataset, with near infrared, red, and green wavelength ranges will be created.BLACKSKY—A 3-band mosaic dataset using the BlackSky wavelength ranges will be createdDMCII_3BANDS—A 3-band mosaic dataset using the DMCii wavelength ranges will be created.DEIMOS2_4BANDS—A 4-band mosaic dataset using the Deimos-2 wavelength ranges will be created.DUBAISAT-2_4BANDS—A 4-band mosaic dataset using the DubaiSat-2 wavelength ranges will be created.FORMOSAT-2_4BANDS—A 4-band mosaic dataset using the FORMOSAT-2 wavelength ranges will be created.GEOEYE-1_4BANDS—A 4-band mosaic dataset using the GeoEye-1 wavelength ranges will be created.GF-1 PMS_4BANDS—A 4-band mosaic dataset using the Gaofen-1 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-1 WFV_4BANDS—A 4-band mosaic dataset using the Gaofen-1 Wide Field of View Sensor wavelength ranges will be created.GF-2 PMS_4BANDS—A 4-band mosaic dataset using the Gaofen-2 Panchromatic Multispectral Sensor wavelength ranges will be created.GF-4 PMI_4BANDS—A 4-band mosaic dataset using the Gaofen-4 panchromatic and multispectral wavelength ranges will be created.HJ 1A/1B CCD_4BANDS—A 4-band mosaic dataset using the Huan Jing-1 CCD Multispectral or Hyperspectral Sensor wavelength ranges will be created.IKONOS_4BANDS—A 4-band mosaic dataset using the IKONOS wavelength ranges will be created.JILIN-1_3BANDS—A 3-band mosaic dataset using the Jilin-1 wavelength ranges will be created.KOMPSAT-2_4BANDS—A 4-band mosaic dataset using the KOMPSAT-2 wavelength ranges will be created.KOMPSAT-3_4BANDS—A 4-band mosaic dataset using the KOMPSAT-3 wavelength ranges will be created.LANDSAT_6BANDS—A 6-band mosaic dataset using the Landsat 5 and 7 wavelength ranges from the TM and ETM+ sensors will be created.LANDSAT_8BANDS—An 8-band mosaic dataset using the Landsat 8 wavelength ranges will be created.LANDSAT_9BANDS—An 8-band mosaic dataset using the Landsat 9 wavelength ranges will be created.LANDSAT_MSS_4BANDS—A 4-band mosaic dataset using the Landsat wavelength ranges from the MSS sensor will be created.PLANETSCOPE—A 5-band mosaic dataset using the PlanetScope wavelength ranges will be created.PLEIADES-1_4BANDS—A 4-band mosaic dataset using the Pleiades 1 wavelength ranges will be created.PLEIADES_NEO_6BANDS—A 6-band mosaic dataset using the Pleiades Neo wavelength ranges will be created.QUICKBIRD_4BANDS—A 4-band mosaic dataset using the QuickBird wavelength ranges will be created.RAPIDEYE_5BANDS—A 5-band mosaic dataset using the RapidEye wavelength ranges will be created.SENTINEL2_13BANDS—A 13-band mosaic dataset using the Sentinel 2 MSI wavelength ranges will be created.SKYSAT_4BANDS—A 4-band mosaic dataset using the SkySat-C MSI wavelength ranges will be created.SPOT-5_4BANDS—A 4-band mosaic dataset using the SPOT-5 wavelength ranges will be created.SPOT-6_4BANDS—A 4-band mosaic dataset using the SPOT-6 wavelength ranges will be created.SPOT-7_4BANDS—A 4-band mosaic dataset using the SPOT-7 wavelength ranges will be created.SUPERVIEW-1_4BANDS—A 4-band mosaic dataset using the SuperView-1 wavelength ranges will be created.TH-01_4BANDS—A 4-band mosaic dataset using the Tian Hui-1 wavelength ranges will be created.WORLDVIEW-2_8BANDS—An 8-band mosaic dataset using the WorldView-2 wavelength ranges will be created.WORLDVIEW-3_8BANDS—An 8-band mosaic dataset using the WorldView-3 wavelength ranges will be created.WORLDVIEW-4_4BANDS—A 4-band mosaic dataset using the WorldView-4 wavelength ranges will be created.VISION-1_4BANDS—A 4-band mosaic dataset using the Vision-1 wavelength ranges will be created.ZY1-02C PMS_3BANDS—A 3-band mosaic dataset using the ZiYuan-1 panchromatic/multispectral wavelength ranges will be created.ZY3-CRESDA_4BANDS—A 4-band mosaic dataset using the ZiYuan-3 CRESDA wavelength ranges will be created.ZY3-SASMAC_4BANDS—A 4-band mosaic dataset using the ZiYuan-3 SASMAC wavelength ranges will be created.CUSTOM—The number of bands and the average wavelength for each band are defined using the Product Band Definitions parameter (product_band_definitions in Python). | String |
| product_band_definitions[Band Name {Wavelength Minimum} {Wavelength Maximum},...](Optional) | The wavelength ranges, number of bands, and band order definitions. You can edit the product_definition values and add new bands using the CUSTOM product definition. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.SetMosaicDatasetProperties(in_mosaic_dataset, {rows_maximum_imagesize}, {columns_maximum_imagesize}, {allowed_compressions}, {default_compression_type}, {JPEG_quality}, {LERC_Tolerance}, {resampling_type}, {clip_to_footprints}, {footprints_may_contain_nodata}, {clip_to_boundary}, {color_correction}, {allowed_mensuration_capabilities}, {default_mensuration_capabilities}, {allowed_mosaic_methods}, {default_mosaic_method}, {order_field}, {order_base}, {sorting_order}, {mosaic_operator}, {blend_width}, {view_point_x}, {view_point_y}, {max_num_per_mosaic}, {cell_size_tolerance}, {cell_size}, {metadata_level}, {transmission_fields}, {use_time}, {start_time_field}, {end_time_field}, {time_format}, {geographic_transform}, {max_num_of_download_items}, {max_num_of_records_returned}, {data_source_type}, {minimum_pixel_contribution}, {processing_templates}, {default_processing_template}, {time_interval}, {time_interval_units}, {product_definition}, {product_band_definitions})
```

### Example 2

```python
import arcpy
arcpy.SetMosaicDatasetProperties_management(
    "c:/workspace/mdproperties.gdb/md", cell_size="0.2", metadata_level="BASIC", 
    transmission_fields="NAME;MINPS;MAXPS;GROUPNAME;PRODUCTNAME;ZORDER;YEARS;YEARE", 
    use_time="ENABLED", start_time_field="YEARS", end_time_field="YEARE", 
    time_format="YYYYMM", geographic_transform=
    "NAD_1983_HARN_To_WGS_1984_2", max_num_of_download_items="10", 
    max_num_of_records_returned="500", source_type="GENERIC", 
    minimum_pixel_contribution="50", processing_templates="None;C:/Test/NewRFTs/Aspect.rft.xml", 
    default_processing_template="None")
```

### Example 3

```python
import arcpy
arcpy.SetMosaicDatasetProperties_management(
    "c:/workspace/mdproperties.gdb/md", cell_size="0.2", metadata_level="BASIC", 
    transmission_fields="NAME;MINPS;MAXPS;GROUPNAME;PRODUCTNAME;ZORDER;YEARS;YEARE", 
    use_time="ENABLED", start_time_field="YEARS", end_time_field="YEARE", 
    time_format="YYYYMM", geographic_transform=
    "NAD_1983_HARN_To_WGS_1984_2", max_num_of_download_items="10", 
    max_num_of_records_returned="500", source_type="GENERIC", 
    minimum_pixel_contribution="50", processing_templates="None;C:/Test/NewRFTs/Aspect.rft.xml", 
    default_processing_template="None")
```

### Example 4

```python
#Set mosaic dataset imagery properties group

import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.SetMosaicDatasetProperties_management(
    "mdproperties.gdb/md", "525340", "3909809", "None;JPEG", "JPEG",
    "85", "0.5", "CUBIC", "CLIP", "FOOTPRINTS_MAY_CONTAIN_NODATA",
    "NOT_CLIP", "APPLY", "Base-Top Height;Top-Top Shadow Height",
    "Base-Top Height")
```

### Example 5

```python
#Set mosaic dataset imagery properties group

import arcpy
arcpy.env.workspace = "C:/Workspace"

arcpy.SetMosaicDatasetProperties_management(
    "mdproperties.gdb/md", "525340", "3909809", "None;JPEG", "JPEG",
    "85", "0.5", "CUBIC", "CLIP", "FOOTPRINTS_MAY_CONTAIN_NODATA",
    "NOT_CLIP", "APPLY", "Base-Top Height;Top-Top Shadow Height",
    "Base-Top Height")
```

---

## Set Raster Properties (Data Management)

## Summary

Sets the data type, statistics, and NoData values on a raster or mosaic dataset.

## Usage

- You can define the statistics for a raster or mosaic dataset using this tool. Typically, you use this tool if you do not want statistics calculated. You can set the minimum, maximum, standard deviation, and mean values for each band. These statistics can be read from an .xml file.
- The properties that can be set with this tool determine the default rendering settings in ArcGIS Pro, as well as statistics that are used by other tools.You can set the following properties:Data Source Type—Define whether the cell values represent elevation or categorical data, or whether the values have been processed by other methods and do not require stretching when displaying the data.Statistics Per Band—For each band, define the minimum, maximum, mean, and standard deviation values.Bands for NoData Value—For each band, define a NoData value.
- Data Source Type—Define whether the cell values represent elevation or categorical data, or whether the values have been processed by other methods and do not require stretching when displaying the data.
- Statistics Per Band—For each band, define the minimum, maximum, mean, and standard deviation values.
- Bands for NoData Value—For each band, define a NoData value.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster or mosaic dataset with the properties to be set. | Mosaic Layer ; Raster Layer |
| Data Source Type (Optional) | Specifies the type of imagery in the mosaic dataset.Generic— The mosaic dataset does not have a specified data type.Elevation— The mosaic dataset contains elevation data.Thematic—The mosaic dataset contains thematic data, which has discrete values, such as land cover.Processed—The mosaic dataset has been color balanced.Scientific—The data has scientific information and will be displayed with the blue to red color ramp by default.Vector UV—The data is a two-band raster that contains a U and a V component of vector field data.Magnitude and Direction—The data is a two-band raster that contains the magnitude and direction of vector field data.Date—The data has date information and will be displayed in date format. | String |
| Statistics Per Band(Optional) | The bands and values for the minimum, maximum, mean, and standard deviation. | Value Table |
| Import Statistics From File (Optional) | An .xml file that contains the statistics. | File |
| Bands for NoData Value (Optional) | The NoData value for each band. Each band can have a unique NoData value defined, or the same value can be specified for all bands.Click the NoData drop-down arrow, choose a band from the list, and click the Add button to add the band to the table. Then enter a value or multiple values. If you choose multiple NoData values, separate each value with a space. | Value Table |
| Key Properties (Optional) | The natively supported properties. The data used may have additional properties not included in the following list. The properties are not case sensitive.AcquisitionDateBandNameBlockNameCloudCoverDatasetTagFlowDirectionFootprintHighCellSizeLowCellSizeMinCellSizeMaxCellSizeOffNadirParentRasterTypeParentTemplatePerspectiveXPerspectiveYPerspectiveZProductNameRadianceBiasRadianceGainReflectanceBiasReflectanceGainSegmentedSensorAzimuthSensorElevationSensorNameSolarIrradianceSourceBandIndexSunAzimuthSunElevationThermalConstant_K1ThermalConstant_K2VerticalAccuracyWavelengthMinWavelengthMax | Value Table |
| Multidimensional information (Optional) | The dimensional information for the raster dataset. Setting dimensional information will convert the dimensionless raster into a multidimensional raster.If the dimension is time, the dimension name must be StdTime. The format for time is either year-month-day (2021-10-01) or year-month-dayThh:mm:ss (2021-10-01T01:00:00).To define a variable with both time and elevation, add the variable with time first; then add the same variable with the z-dimension. | Value Table |
| in_raster | The raster or mosaic dataset with the properties to be set. | Mosaic Layer ; Raster Layer |
| data_type(Optional) | Specifies the type of imagery in the mosaic dataset.GENERIC— The mosaic dataset does not have a specified data type.ELEVATION— The mosaic dataset contains elevation data.THEMATIC—The mosaic dataset contains thematic data, which has discrete values, such as land cover.PROCESSED—The mosaic dataset has been color balanced.SCIENTIFIC—The data has scientific information and will be displayed with the blue to red color ramp by default.VECTOR_UV—The data is a two-band raster that contains a U and a V component of vector field data.VECTOR_MAGDIR—The data is a two-band raster that contains the magnitude and direction of vector field data.DATE—The data has date information and will be displayed in date format. | String |
| statistics[[band_index, min, max, mean, std_dev],...](Optional) | The bands and values for the minimum, maximum, mean, and standard deviation. | Value Table |
| stats_file(Optional) | An .xml file that contains the statistics. | File |
| nodata[[band index, nodata_value],...](Optional) | The NoData value for each band. Each band can have a unique NoData value defined, or the same value can be specified for all bands. To define multiple NoData values for each band selection, use a space delimiter between each NoData value. | Value Table |
| key_properties[key_properties,...](Optional) | The natively supported properties. The data used may have additional properties not included in the following list. The properties are not case sensitive.AcquisitionDateBandNameBlockNameCloudCoverDatasetTagFlowDirectionFootprintHighCellSizeLowCellSizeMinCellSizeMaxCellSizeOffNadirParentRasterTypeParentTemplatePerspectiveXPerspectiveYPerspectiveZProductNameRadianceBiasRadianceGainReflectanceBiasReflectanceGainSegmentedSensorAzimuthSensorElevationSensorNameSolarIrradianceSourceBandIndexSunAzimuthSunElevationThermalConstant_K1ThermalConstant_K2VerticalAccuracyWavelengthMinWavelengthMax | Value Table |
| multidimensional_info[multidimensional_info,...](Optional) | The dimensional information for the raster dataset. Setting dimensional information will convert the dimensionless raster into a multidimensional raster.If the dimension is time, the dimension name must be StdTime. The format for time is either year-month-day (2021-10-01) or year-month-dayThh:mm:ss (2021-10-01T01:00:00).To define a variable with both time and elevation, add the variable with time first; then add the same variable with the z-dimension. | Value Table |

## Code Samples

### Example 1

```python
arcpy.management.SetRasterProperties(in_raster, {data_type}, {statistics}, {stats_file}, {nodata}, {key_properties}, {multidimensional_info})
```

### Example 2

```python
import arcpy
arcpy.SetRasterProperties_management("\\cpu\data\srtm.tif", "ELEVATION", 
                                     "1 50 400 5 28" , "#" , "#")
```

### Example 3

```python
import arcpy
arcpy.SetRasterProperties_management("\\cpu\data\srtm.tif", "ELEVATION", 
                                     "1 50 400 5 28" , "#" , "#")
```

### Example 4

```python
#Set raster dataset type and statistics

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
arcpy.SetRasterProperties_management("srtmraster.tif", "ELEVATION", 
                                         "1 50 400 5 28", "#", "#")
```

### Example 5

```python
#Set raster dataset type and statistics

import arcpy
arcpy.env.workspace = "C:/Workspace"
    
arcpy.SetRasterProperties_management("srtmraster.tif", "ELEVATION", 
                                         "1 50 400 5 28", "#", "#")
```

---

## Set Relationship Class Split Policy (Data Management)

## Summary

Defines the split policy for related features.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Relationship Class | The relationship class on which the split policy will be set. The origin feature class must be a polyline or polygon feature class and the destination must be a nonspatial table. | Relationship Class |
| Split Policy | Specifies the split policy to apply to the relationship class. Default (composite)— If the feature class split model is Delete/Insert/Insert, the relationships and the part objects will be deleted. If the feature class split model is Update/Insert, the relationships on the largest resulting feature will be preserved. This is the default split policy for composite relationship classes.Default (simple)— The relationships on the largest resulting feature will be preserved. This is the default split policy for simple relationship classes.Duplicate related objects—Copies of the related objects will be generated and assigned to both resulting parts. The relationship class must be Global ID based to use this split policy. | String |
| in_rel_class | The relationship class on which the split policy will be set. The origin feature class must be a polyline or polygon feature class and the destination must be a nonspatial table. | Relationship Class |
| split_policy | Specifies the split policy to apply to the relationship class. DEFAULT_COMPOSITE— If the feature class split model is Delete/Insert/Insert, the relationships and the part objects will be deleted. If the feature class split model is Update/Insert, the relationships on the largest resulting feature will be preserved. This is the default split policy for composite relationship classes.DEFAULT_SIMPLE— The relationships on the largest resulting feature will be preserved. This is the default split policy for simple relationship classes.DUPLICATE_RELATED_OBJECTS—Copies of the related objects will be generated and assigned to both resulting parts. The relationship class must be Global ID based to use this split policy. | String |

## Code Samples

### Example 1

```python
arcpy.management.SetRelationshipClassSplitPolicy(in_rel_class, split_policy)
```

### Example 2

```python
import arcpy
arcpy.management.SetRelationshipClassSplitPolicy("C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", 
                                                 "DUPLICATE_RELATED_OBJECTS")
```

### Example 3

```python
import arcpy
arcpy.management.SetRelationshipClassSplitPolicy("C:\\MyProject\\sdeConn.sde\\progdb.user1.ParcelsToBuildings", 
                                                 "DUPLICATE_RELATED_OBJECTS")
```

---

## Set Subtype Field (Data Management)

## Summary

Defines the field in the input table or feature class that stores the subtype codes.

## Usage

- A feature class or table can have only one subtype field.
- After a subtype field is set, subtype codes can be added to the feature class or table using the Add Subtype tool.
- You can also view and manage subtypes in Subtypes view which can be opened by clicking the Subtypes button found in the Design section of the Data ribbon, or the by clicking the Subtypes button on the Fields view ribbon.
- This tool can also be used to clear the subtype field if a subtype field is no longer needed.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class that contains the field to set as a subtype field. | Table View |
| Field Name(Optional) | The integer field that will store the subtype codes. | Field |
| Clear Value(Optional) | Specifies whether to clear the subtype field. Checked—The subtype field will be cleared (set to null).Unchecked—The subtype field will not be cleared. This is the default. | Boolean |
| in_table | The input table or feature class that contains the field to set as a subtype field. | Table View |
| field(Optional) | The integer field that will store the subtype codes. | Field |
| clear_value(Optional) | Specifies whether to clear the subtype field.CLEAR_SUBTYPE_FIELD—The subtype field will be cleared (set to null).DO_NOT_CLEAR—The subtype field will not be cleared. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SetSubtypeField(in_table, {field}, {clear_value})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "TYPECODE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "TYPECODE")
```

### Example 4

```python
# Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every 
# time)
arcpy.env.workspace = "C:/data/Montgomery.gdb"

# Set local parameters
inFeatures = "water/fittings"

# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")

# Process: Add Subtypes...
# Store all the subtype values in a dictionary with the subtype code as the 
# "key" and the subtype description as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", "7":"Plug", 
             "8": "Reducer", "9": "Saddle", "10": "Sleeve", "11": "Tap", 
             "12": "Tee", "13": "Weld", "14": "Riser"} 
    
# Use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     

# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4", "")
```

### Example 5

```python
# Name: ManageSubtypes.py
# Purpose: Create a subtype definition

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every 
# time)
arcpy.env.workspace = "C:/data/Montgomery.gdb"

# Set local parameters
inFeatures = "water/fittings"

# Process: Set Subtype Field...
arcpy.SetSubtypeField_management(inFeatures, "TYPECODE")

# Process: Add Subtypes...
# Store all the subtype values in a dictionary with the subtype code as the 
# "key" and the subtype description as the "value" (stypeDict[code])
stypeDict = {"0": "Unknown", "1": "Bend", "2": "Cap", "3": "Cross", 
             "4": "Coupling", "5": "Expansion joint", "6": "Offset", "7":"Plug", 
             "8": "Reducer", "9": "Saddle", "10": "Sleeve", "11": "Tap", 
             "12": "Tee", "13": "Weld", "14": "Riser"} 
    
# Use a for loop to cycle through the dictionary
for code in stypeDict:
    arcpy.AddSubtype_management(inFeatures, code, stypeDict[code])     

# Process: Set Default Subtype...
arcpy.SetDefaultSubtype_management(inFeatures, "4", "")
```

### Example 6

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "", "TRUE")
```

### Example 7

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.SetSubtypeField_management("water/fittings", "", "TRUE")
```

---

## Set Value For Range Domain (Data Management)

## Summary

Sets the minimum and maximum values for an existing Range domain.

## Usage

- A range domain specifies a valid range of values for a numeric attribute. For example, a valid range of water main pressure values might be between 50 and 75 psi.
- Domain management involves the following steps:Create the domain using the Create Domain tool.Add values to or set the range of values for the domain using the Add Coded Value To Domain tool or this tool.Associate the domain with a feature class using the Assign Domain To Field tool.
- Create the domain using the Create Domain tool.
- Add values to or set the range of values for the domain using the Add Coded Value To Domain tool or this tool.
- Associate the domain with a feature class using the Assign Domain To Field tool.
- You can also manage domains in Domains view which can be opened by clicking the Domains button found in the Design group on the Data ribbon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase containing the domain to be updated. | Workspace |
| Domain Name | The name of the range domain to be updated. | String |
| Minimum Value | The minimum value of the range domain. | String |
| Maximum Value | The maximum value of the range domain. | String |
| in_workspace | The geodatabase containing the domain to be updated. | Workspace |
| domain_name | The name of the range domain to be updated. | String |
| min_value | The minimum value of the range domain. | String |
| max_value | The maximum value of the range domain. | String |

## Code Samples

### Example 1

```python
arcpy.management.SetValueForRangeDomain(in_workspace, domain_name, min_value, max_value)
```

### Example 2

```python
import arcpy
arcpy.env.workspace =  "C:/data"
arcpy.SetValueForRangeDomain_management("montgomery.gdb", "RotAngle", 0, 359)
```

### Example 3

```python
import arcpy
arcpy.env.workspace =  "C:/data"
arcpy.SetValueForRangeDomain_management("montgomery.gdb", "RotAngle", 0, 359)
```

### Example 4

```python
# Name: CreateRangeDomain.py
# Purpose: Create an attribute domain to constrain valid rotation angle

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"

# Set local parameters
dWorkspace = "montgomery.gdb"
domName = "RotAngle2"
domDesc = "Valid rotation angle"
minRange = 0
maxRange = 359
inFeatures = "Montgomery.gdb/Water/fittings"
inField = "ANGLE"

# Process: Create the range domain
arcpy.CreateDomain_management(dWorkspace, domName, domDesc, "LONG", "RANGE")

# Process: Set the minimum and maximum values for the range domain
arcpy.SetValueForRangeDomain_management(dWorkspace, domname, minRange, maxRange)

# Process: Constrain the fitting rotation angle
arcpy.AssignDomainToField_management( inFeatures, inField, domName)
```

### Example 5

```python
# Name: CreateRangeDomain.py
# Purpose: Create an attribute domain to constrain valid rotation angle

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"

# Set local parameters
dWorkspace = "montgomery.gdb"
domName = "RotAngle2"
domDesc = "Valid rotation angle"
minRange = 0
maxRange = 359
inFeatures = "Montgomery.gdb/Water/fittings"
inField = "ANGLE"

# Process: Create the range domain
arcpy.CreateDomain_management(dWorkspace, domName, domDesc, "LONG", "RANGE")

# Process: Set the minimum and maximum values for the range domain
arcpy.SetValueForRangeDomain_management(dWorkspace, domname, minRange, maxRange)

# Process: Constrain the fitting rotation angle
arcpy.AssignDomainToField_management( inFeatures, inField, domName)
```

---

## Shape manipulation

## Code Samples

### Example 1

```python
polygon_to_point(!Shape!)
```

### Example 2

```python
polygon_to_point(!Shape!)
```

### Example 3

```python
polygon_to_multipoint(!Shape!)
```

### Example 4

```python
polygon_to_multipoint(!Shape!)
```

### Example 5

```python
polygon_to_diameter_polyline(!Shape!)
```

### Example 6

```python
polygon_to_diameter_polyline(!Shape!)
```

### Example 7

```python
polygon_to_polyline(!Shape!)
```

### Example 8

```python
polygon_to_polyline(!Shape!)
```

### Example 9

```python
polyline_to_polygon(!Shape!, 20)
```

### Example 10

```python
polyline_to_polygon(!Shape!, 20)
```

### Example 11

```python
polyline_to_point(!Shape!, 0.5)
```

### Example 12

```python
polyline_to_point(!Shape!, 0.5)
```

### Example 13

```python
polyline_to_multipoint(!Shape!)
```

### Example 14

```python
polyline_to_multipoint(!Shape!)
```

### Example 15

```python
point_to_polygon(!Shape!, 50)
```

### Example 16

```python
point_to_polygon(!Shape!, 50)
```

### Example 17

```python
point_to_polyline(!Shape!, 30, 100)
```

### Example 18

```python
point_to_polyline(!Shape!, 30, 100)
```

### Example 19

```python
point_to_multipoint(!Shape!, 30, 200)
```

### Example 20

```python
point_to_multipoint(!Shape!, 30, 200)
```

### Example 21

```python
multipoint_to_polygon(!Shape!)
```

### Example 22

```python
multipoint_to_polygon(!Shape!)
```

### Example 23

```python
multipoint_to_polyline(!Shape!)
```

### Example 24

```python
multipoint_to_polyline(!Shape!)
```

### Example 25

```python
multipoint_to_point(!Shape!)
```

### Example 26

```python
multipoint_to_point(!Shape!)
```

### Example 27

```python
create_point(!x!, !y!, !z!, !m!, 8745)
```

### Example 28

```python
create_point(!x!, !y!, !z!, !m!, 8745)
```

### Example 29

```python
move(!Shape!, 50, 50, 50)
```

### Example 30

```python
move(!Shape!, 50, 50, 50)
```

---

## Share Package (Data Management)

## Summary

Shares a package by uploading it to ArcGIS Online or ArcGIS Enterprise.

## Usage

- The following are supported package types: Geoprocessing packages (.gpk and .gpkx)Layer packages (.lpk and .lpkx)Locator packages (.gcpk)Map packages (.mpk and .mpkx)Tile packages (.tpk and .tpkx)Mobile map packages (.mmpk)Mobile scene packages (.mspk)Scene layer packages (.slpk)Vector tile packages (.vtpk)Project packages and project templates (.ppkx and .aptx)
- Geoprocessing packages (.gpk and .gpkx)
- Layer packages (.lpk and .lpkx)
- Locator packages (.gcpk)
- Map packages (.mpk and .mpkx)
- Tile packages (.tpk and .tpkx)
- Mobile map packages (.mmpk)
- Mobile scene packages (.mspk)
- Scene layer packages (.slpk)
- Vector tile packages (.vtpk)
- Project packages and project templates (.ppkx and .aptx)
- The package file size limit is 500 GB. For more information, see Considerations and limitations.Tip:If you have a scene layer package that is larger than 500 GB and want to publish it as a web scene layer to ArcGIS Online, add it to a 3D scene and share it as a web scene instead. The scene layer package is published as a web scene layer with the web scene. If you have a scene layer package in a scene, you can share it as a web scene layer directly. Right-click the layer, point to the Sharing menu, and click Share As Web Layer .
- You cannot enter your ArcGIS Online or ArcGIS Enterprise username and password when using this tool in ArcGIS Pro. The tool will obtain your credentials from ArcGIS Pro. You must be signed in and connected to ArcGIS Online or ArcGIS Enterprise before you can share a package using this tool.
- A summary and one or more tags are required when sharing a package to an ArcGIS Enterprise 10.9 or earlier portal. The summary and tags, along with an optional package description and credits, will be used when searching for packages online.
- If a package of the same name exists in your ArcGIS Online or ArcGIS Enterprise organization, it will be overwritten.
- If you share a tile package, scene layer package, or vector tile package, you can publish the package automatically as a web layer of the corresponding type. (A tile package is published as a web tile layer, a scene layer package as a web scene layer, and a vector tile package as a vector tile layer.) To publish a web layer successfully, you must have the necessary privileges in your active portal account.
- To share a package to ArcGIS Online with a public account, your Esri Global Account must be registered as a member of ArcGIS Online. To create and register an Esri Global Account, go to arcgis.com/home/signup.html.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Package | The input layer (.lpk or .lpkx), scene layer (.slpk), map (.mpk or .mpkx), geoprocessing (.gpk or .gpkx), tile (.tpk or .tpkx), mobile map (.mmpk), vector tile (.vtpk), address locator (.gcpk), or project (.ppkx or .aptx) package file. | File |
| Username | The ArcGIS Online or Portal for ArcGIS username. This parameter is not available from the Geoprocessing pane. You must sign in to the active portal from the sign in option at the top of the application. | String |
| Password | The ArcGIS Online or ArcGIS Enterprise password. This parameter is not available from the Geoprocessing pane. You must sign in to the active portal from the sign in option at the top of the application. | Encrypted String |
| Summary (Optional) | The summary of the package. The summary is displayed in the item information of the package on ArcGIS Online or ArcGIS Enterprise. | String |
| Tags(Optional) | The tags used to describe and identify the package. Separate multiple tags with a comma. | String |
| Credits(Optional) | The credits for the package. This is generally the name of the organization that is given credit for authoring and providing the content for the package. | String |
| Share with everyone (public)(Optional) | Specifies whether the input package will be shared with and available to the public.Checked—The input package will be shared with the public. Anyone can access and see it.Unchecked—The input package will be shared with the package owner and any selected groups. This is the default. | Boolean |
| Groups (Optional) | The groups the package will be shared with. | String |
| Share within organization only (Optional) | Specifies whether the input package will be available within your organization only or shared publicly with everyone. Everybody— The package will be shared with the public. Anyone can access and see it. This is the default. Within my organization— The package will be shared within your organization only. | Boolean |
| Publish web layer (Optional) | Specifies whether the package will be published as a web layer to your portal. Only tile packages, vector tile packages, and scene layer packages are supported. Unchecked—The package will be uploaded without publishing. This is the default.Checked—The package will be uploaded and published as a web layer with the same name. | Boolean |
| Folder (Optional) | An existing folder or the name of a new folder on the portal for the package. If a web layer is published, it is stored in this folder. | String |
| in_package | The input layer (.lpk or .lpkx), scene layer (.slpk), map (.mpk or .mpkx), geoprocessing (.gpk or .gpkx), tile (.tpk or .tpkx), mobile map (.mmpk), vector tile (.vtpk), address locator (.gcpk), or project (.ppkx or .aptx) package file. | File |
| username | The ArcGIS Online or ArcGIS Enterprise username. This parameter has been deprecated and should consist of an empty string. Before running the Python script, you must sign in to the active portal from the application. Alternatively, you can sign in using the SignInToPortal function. | String |
| password | The ArcGIS Online or ArcGIS Enterprise password. This parameter has been deprecated and should consist of an empty string. Before running the Python script, you must sign in to the active portal from the application. Alternatively, you can sign in using the SignInToPortal function. | Encrypted String |
| summary(Optional) | The summary of the package. The summary is displayed in the item information of the package on ArcGIS Online or ArcGIS Enterprise. | String |
| tags(Optional) | The tags used to describe and identify the package. Separate multiple tags with a comma. | String |
| credits(Optional) | The credits for the package. This is generally the name of the organization that is given credit for authoring and providing the content for the package. | String |
| public(Optional) | Specifies whether the input package will be shared with and available to the public. EVERYBODY— The input package will be shared with the public. Anyone can access and see it. MYGROUPS— The input package will be shared with the package owner and any selected groups. This is the default. | Boolean |
| groups[group_name,...](Optional) | The groups the package will be shared with. | String |
| organization(Optional) | Specifies whether the input package will be available within your organization only or shared publicly with everyone. EVERYBODY— The package will be shared with the public. Anyone can access and see it. This is the default. MYORGANIZATION— The package will be shared within your organization only. | Boolean |
| publish_web_layer(Optional) | Specifies whether the package will be published as a web layer to your portal. Only tile packages, vector tile packages, and scene layer packages are supported.FALSE—The package will be uploaded without publishing. This is the default.TRUE—The package will be uploaded and published as a web layer with the same name. | Boolean |
| portal_folder(Optional) | An existing folder or the name of a new folder on the portal for the package. If a web layer is published, it is stored in this folder. | String |

## Code Samples

### Example 1

```python
arcpy.management.SharePackage(in_package, username, password, {summary}, {tags}, {credits}, {public}, {groups}, {organization}, {publish_web_layer}, {portal_folder})
```

### Example 2

```python
import arcpy
arcpy.management.SharePackage(r"C:\states.lpkx", "", "", 
                              "My Summary", "tag1, tag2", "My Credits", 
                              "MYGROUPS", "My Group")
```

### Example 3

```python
import arcpy
arcpy.management.SharePackage(r"C:\states.lpkx", "", "", 
                              "My Summary", "tag1, tag2", "My Credits", 
                              "MYGROUPS", "My Group")
```

### Example 4

```python
import arcpy
arcpy.management.SharePackage(r"C:\states.tpk", "", "", 
                              "My Summary", "tag1, tag2", "My Credits", 
                              "MYGROUPS", "My Group", "MYORGANIZATION", "TRUE", 
                              "My Folder")
```

### Example 5

```python
import arcpy
arcpy.management.SharePackage(r"C:\states.tpk", "", "", 
                              "My Summary", "tag1, tag2", "My Credits", 
                              "MYGROUPS", "My Group", "MYORGANIZATION", "TRUE", 
                              "My Folder")
```

### Example 6

```python
# Name: SharePackageExample.py
# Description:  Find all map packages that reside in a specified folder 
#               and upload them to the active portal.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/data/my_packages" 

# Loop through the workspace to find all map packages 
for mpkx in arcpy.ListFiles("*.mpkx"):
    print("Uploading " + mpkx)
    arcpy.management.SharePackage(mpkx, "", "", 
                                  "My Summary", "tag1, tag2", 
                                  "My Credits", "MYGROUPS", "My Group")
```

### Example 7

```python
# Name: SharePackageExample.py
# Description:  Find all map packages that reside in a specified folder 
#               and upload them to the active portal.

# import system modules
import os
import arcpy

# Set environment settings
arcpy.env.overwriteOutput = True
arcpy.env.workspace = "C:/data/my_packages" 

# Loop through the workspace to find all map packages 
for mpkx in arcpy.ListFiles("*.mpkx"):
    print("Uploading " + mpkx)
    arcpy.management.SharePackage(mpkx, "", "", 
                                  "My Summary", "tag1, tag2", 
                                  "My Credits", "MYGROUPS", "My Group")
```

---

## Shift (Data Management)

## Summary

Moves (slides) the raster to a new geographic location based on x and y shift values. This tool is helpful if your raster dataset needs to be shifted to align with another data file.

## Usage

- The cell size of the output raster will be the same as that of the input raster.
- The number of rows and columns in the output raster will be the same as those of the input raster, no matter what parameters are specified.
- The coordinates of the lower left corner of the output raster will be offset from the input raster by the x and y shift coordinate values specified.
- Using a negative shift x-coordinate value will shift the output to the left. A positive shift x-coordinate value will shift the output to the right. Using a negative shift y-coordinate value will shift the output down. A positive shift y-coordinate value will shift the output to the top.
- The output raster dataset is nudged according to the location of the input snap raster, so the new shifted raster dataset can be aligned perfectly with another raster dataset.
- This tool does not perform any resampling or warping.
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| Shift X Coordinates by | The value used to shift the x-coordinates. | Double |
| Shift Y Coordinates by | The value used to shift the y-coordinates. | Double |
| Input Snap Raster(Optional) | The raster dataset used to align the cells of the output raster dataset. | Raster Layer |
| in_raster | The input raster dataset. | Mosaic Layer; Raster Layer |
| out_raster | The output raster dataset.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri GridWhen storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset.When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, a TIFF format file, or a geodatabase, you can specify Compression Type and Compression Quality values in the geoprocessing environments. | Raster Dataset |
| x_value | The value used to shift the x-coordinates. | Double |
| y_value | The value used to shift the y-coordinates. | Double |
| in_snap_raster(Optional) | The raster dataset used to align the cells of the output raster dataset. | Raster Layer |

## Code Samples

### Example 1

```python
arcpy.management.Shift(in_raster, out_raster, x_value, y_value, {in_snap_raster})
```

### Example 2

```python
import arcpy
arcpy.Shift_management("c:/data/image.tif", "c:/output/shift.tif", "100",\
                       "150", "snap.tif")
```

### Example 3

```python
import arcpy
arcpy.Shift_management("c:/data/image.tif", "c:/output/shift.tif", "100",\
                       "150", "snap.tif")
```

### Example 4

```python
##====================================
##Shift
##Usage: Shift_management in_raster out_raster x_value y_value {in_snap_raster}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Shift a TIFF image by 4.5 in X direction and 6 in Y direction
##Snap the output to a existing raster dataset
arcpy.Shift_management("image.tif", "shift.tif", "4.5", "6", "snap.tif")
```

### Example 5

```python
##====================================
##Shift
##Usage: Shift_management in_raster out_raster x_value y_value {in_snap_raster}
    
import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Shift a TIFF image by 4.5 in X direction and 6 in Y direction
##Snap the output to a existing raster dataset
arcpy.Shift_management("image.tif", "shift.tif", "4.5", "6", "snap.tif")
```

---

## Sort Coded Value Domain (Data Management)

## Summary

Sorts the code or description of a coded value domain in either ascending or descending order.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The geodatabase containing the domain to be sorted. | Workspace |
| Domain Name | The name of the coded value domain to be sorted. | String |
| Sort By | Specifies whether the code or description will be used to sort the domain. Code—Records are sorted based on the code value for the domain.Description—Records are sorted based on the description value for the domain. | String |
| Sort Order | Specifies the direction the records will be sorted. Ascending—Records are sorted from low value to high value.Descending—Records are sorted from high value to low value. | String |
| in_workspace | The geodatabase containing the domain to be sorted. | Workspace |
| domain_name | The name of the coded value domain to be sorted. | String |
| sort_by | Specifies whether the code or description will be used to sort the domain. CODE—Records are sorted based on the code value for the domain.DESCRIPTION—Records are sorted based on the description value for the domain. | String |
| sort_order | Specifies the direction the records will be sorted. ASCENDING—Records are sorted from low value to high value.DESCENDING—Records are sorted from high value to low value. | String |

## Code Samples

### Example 1

```python
arcpy.management.SortCodedValueDomain(in_workspace, domain_name, sort_by, sort_order)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.SortCodedValueDomain_management("montgomery.gdb", "material", "CODE", "ASCENDING")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.SortCodedValueDomain_management("montgomery.gdb", "material", "CODE", "ASCENDING")
```

---

## Sort (Data Management)

## Summary

Reorders records in a feature class or table, in ascending or descending order, based on one or multiple fields. The reordered result is written to a new dataset.

## Usage

- Feature classes can be spatially reordered, or sorted. The Shape field must be used as the sort field for spatial sorting. There are a number of spatial sort methods that arrange the features differently based on their location.
- If input records are selected, only the subset of selected records are sorted and written to the output.
- If more than one field is set as a sort field, rows are first sorted by the first field and within that order, sorted by the second field, and so on.
- Polygon features can be sorted by their area using the Shape_Area field of a geodatabase feature class. Similarly, polyline features can be sorted by their length using the Shape_Length field. To sort polygon features in a shapefile, add a new field, calculate the area into the new field using Calculate Field, and run Sort using the new field.
- To transfer the input dataset's subtypes, domains, and other advanced geodatabase field properties to the output dataset, use the Transfer Geodatabase Attribute Properties environment.
- To copy the input dataset's geodatabase attachments to the output dataset, use the Maintain Attachments environment.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input dataset with the records that will be reordered based on the field values in the sort field or fields. | Table View |
| Output Dataset | The output feature class or table. | Feature Class; Table |
| Field(s) | The field or fields whose values will be used to reorder the input records and the direction the records will be sorted. License:Sorting by the Shape field or by multiple fields is only available with an ArcGIS Pro Advanced license. Sorting by a single attribute field (excluding Shape) is available at all license levels.Ascending—Records will be sorted from low value to high value.Descending—Records will be sorted from high value to low value. | Value Table |
| Spatial Sort Method(Optional) | Specifies how features will be spatially sorted. The sort method is only enabled when the Shape field is designated as one of the sort fields.Upper right—Sorting will start at the upper right corner. This is the default.Upper left—Sorting will start at the upper left corner.Lower right—Sorting will start at the lower right corner.Lower left—Sorting will start at the lower left corner.Peano curve—A space filling curve algorithm, also known as a Peano curve, will be used to sort. | String |
| in_dataset | The input dataset with the records that will be reordered based on the field values in the sort field or fields. | Table View |
| out_dataset | The output feature class or table. | Feature Class; Table |
| sort_field[[sort_field, direction],...] | The field or fields whose values will be used to reorder the input records and the direction the records will be sorted. License:Sorting by the Shape field or by multiple fields is only available with an ArcGIS Pro Advanced license. Sorting by a single attribute field (excluding Shape) is available at all license levels.Ascending—Records will be sorted from low value to high value.Descending—Records will be sorted from high value to low value. | Value Table |
| spatial_sort_method(Optional) | Specifies how features will be spatially sorted. The sort method is only enabled when the Shape field is designated as one of the sort fields.UR—Sorting will start at the upper right corner. This is the default.UL—Sorting will start at the upper left corner.LR—Sorting will start at the lower right corner.LL—Sorting will start at the lower left corner.PEANO—A space filling curve algorithm, also known as a Peano curve, will be used to sort. | String |

## Code Samples

### Example 1

```python
arcpy.management.Sort(in_dataset, out_dataset, sort_field, {spatial_sort_method})
```

### Example 2

```python
import arcpy
from arcpy import env

env.workspace = "C:/data/city.gdb"

arcpy.Sort_management("crime", "crime_Sort", [["DATE_REP", "ASCENDING"]])
```

### Example 3

```python
import arcpy
from arcpy import env

env.workspace = "C:/data/city.gdb"

arcpy.Sort_management("crime", "crime_Sort", [["DATE_REP", "ASCENDING"]])
```

### Example 4

```python
# Name: Sort_example2.py
# Description: Sorts wells by location and well yield.

# Import system modules
import arcpy

# Set workspace environment
arcpy.env.workspace = "C:/data/newfoundland.gdb"

# set local variables
in_dataset = "wells"
out_dataset = "wells_Sort"

# Order features first by location (Shape) and then by WELL_YIELD
sort_fields = [["Shape", "ASCENDING"], ["WELL_YIELD", "DESCENDING"]]

# Use Peano algorithm
sort_method = "PEANO"

# execute the function
arcpy.Sort_management(in_dataset, out_dataset, sort_fields, sort_method)
```

### Example 5

```python
# Name: Sort_example2.py
# Description: Sorts wells by location and well yield.

# Import system modules
import arcpy

# Set workspace environment
arcpy.env.workspace = "C:/data/newfoundland.gdb"

# set local variables
in_dataset = "wells"
out_dataset = "wells_Sort"

# Order features first by location (Shape) and then by WELL_YIELD
sort_fields = [["Shape", "ASCENDING"], ["WELL_YIELD", "DESCENDING"]]

# Use Peano algorithm
sort_method = "PEANO"

# execute the function
arcpy.Sort_management(in_dataset, out_dataset, sort_fields, sort_method)
```

---

## Split Line at Point (Data Management)

## Summary

Splits line features based on intersection or proximity to point features.

## Usage

- The attributes of the input features will be maintained in the output feature class. The following fields will be added to the output feature class:ORIG_FID—Stores the feature IDs of the input featuresORIG_SEQ—Stores the sequence number for each output line following the order of the segments from the starting vertex of the input feature
- ORIG_FID—Stores the feature IDs of the input features
- ORIG_SEQ—Stores the sequence number for each output line following the order of the segments from the starting vertex of the input feature
- If the Search Radius parameter value is not specified, the nearest point will be used to split the line feature. This means that when multiple points coincide with the line, only one of the points will be used to split the line. If the Search Radius parameter value is specified, all points within the search radius will be used to split the line.
- To generate accurate results, use a projected coordinate system for the inputs. You can use the Project tool to project spatial data from a geographic coordinate system to a projected coordinate system before using the Split Line at Point tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input line features that will be split. | Feature Layer |
| Point Features | The input point features whose locations will be used to split the input lines. | Feature Layer |
| Output Feature Class | The output feature class that will contain the split lines. | Feature Class |
| Search Radius(Optional) | The distance that will be used to split lines by their proximity to point features. Points within the search distance to an input line will be used to split those lines at the nearest location to the point along the line segment. If this parameter is not specified, the single nearest point will be used to split the line feature. If a radius is specified, all points within the radius will be used to split the line. | Linear Unit |
| in_features | The input line features that will be split. | Feature Layer |
| point_features | The input point features whose locations will be used to split the input lines. | Feature Layer |
| out_feature_class | The output feature class that will contain the split lines. | Feature Class |
| search_radius(Optional) | The distance that will be used to split lines by their proximity to point features. Points within the search distance to an input line will be used to split those lines at the nearest location to the point along the line segment. If this parameter is not specified, the single nearest point will be used to split the line feature. If a radius is specified, all points within the radius will be used to split the line. | Linear Unit |

## Code Samples

### Example 1

```python
arcpy.management.SplitLineAtPoint(in_features, point_features, out_feature_class, {search_radius})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SplitLineAtPoint("streets.shp", "events.shp", 
                                  "splitline_out.shp", "20 Meters")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SplitLineAtPoint("streets.shp", "events.shp", 
                                  "splitline_out.shp", "20 Meters")
```

### Example 4

```python
# Name: SplitLineAtPoint_Example.py
# Description: Split line features based on near point features.

import arcpy

arcpy.env.workspace = "C:/data"
inFeatures = "streets.shp"
pointFeatures = "events.shp"
outFeatureclass = "splitline_out.shp"
searchRadius = "20 Meters"

arcpy.management.SplitLineAtPoint(inFeatures, pointFeatures, outFeatureclass, 
                                  searchRadius)
```

### Example 5

```python
# Name: SplitLineAtPoint_Example.py
# Description: Split line features based on near point features.

import arcpy

arcpy.env.workspace = "C:/data"
inFeatures = "streets.shp"
pointFeatures = "events.shp"
outFeatureclass = "splitline_out.shp"
searchRadius = "20 Meters"

arcpy.management.SplitLineAtPoint(inFeatures, pointFeatures, outFeatureclass, 
                                  searchRadius)
```

---

## Split Line At Vertices (Data Management)

## Summary

Creates a polyline feature class by splitting input lines or polygons at their vertices.

## Usage

- The attributes of the input features will be maintained in the output feature class. The following fields will be added to the output feature class:ORIG_FID—Stores the feature IDs of the input featuresORIG_SEQ—Stores the sequence number for each output line following the order of the segments from the starting vertex of the input feature
- ORIG_FID—Stores the feature IDs of the input features
- ORIG_SEQ—Stores the sequence number for each output line following the order of the segments from the starting vertex of the input feature
- If an input line has only two vertices, the line will be copied to the output as is. Otherwise, every segment between consecutive vertices will become a line feature in the output. The output feature class can be a much larger file, depending on the number of vertices the input features have.
- A parametric (true) curve line or segment will not be densified and will remain a true curve as an output line feature. This does not apply to shapefile data.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The input line or polygon features. | Feature Layer |
| Output Feature Class | The output line feature class. | Feature Class |
| in_features | The input line or polygon features. | Feature Layer |
| out_feature_class | The output line feature class. | Feature Class |

## Code Samples

### Example 1

```python
arcpy.management.SplitLine(in_features, out_feature_class)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SplitLine("roads.shp", "c:/output/output.gdb/roads_split")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.SplitLine("roads.shp", "c:/output/output.gdb/roads_split")
```

### Example 4

```python
# Name: SplitLine_Example2.py
# Description: Split a bus line feature at its vertices (bus stops)
#              and find a midpoint of each new line for further analysis.
 
# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFeatures = "buslines.shp"
outFeatureClass = "c:/output/output.gdb/buslines_segments"
midPtsFeatureClass = "c:/output/output.gdb/buslines_segments_midPts"

# Run SplitLine to get new lines, each of which is between two bus stops
arcpy.management.SplitLine(inFeatures, outFeatureClass)

# Run FeatureVerticesToPoints to find a midpoint for every new line
arcpy.management.FeatureVerticesToPoints(outFeatureClass,
                                         midPtsFeatureClass, "MID")

# Comments: You can add attribute information, such as driving time,
#           to the midpoint feature class and display the attributes 
#           as an alternative label for each line between two bus stops.
```

### Example 5

```python
# Name: SplitLine_Example2.py
# Description: Split a bus line feature at its vertices (bus stops)
#              and find a midpoint of each new line for further analysis.
 
# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"
 
# Set local variables
inFeatures = "buslines.shp"
outFeatureClass = "c:/output/output.gdb/buslines_segments"
midPtsFeatureClass = "c:/output/output.gdb/buslines_segments_midPts"

# Run SplitLine to get new lines, each of which is between two bus stops
arcpy.management.SplitLine(inFeatures, outFeatureClass)

# Run FeatureVerticesToPoints to find a midpoint for every new line
arcpy.management.FeatureVerticesToPoints(outFeatureClass,
                                         midPtsFeatureClass, "MID")

# Comments: You can add attribute information, such as driving time,
#           to the midpoint feature class and display the attributes 
#           as an alternative label for each line between two bus stops.
```

---

## Split Mosaic Dataset Items (Data Management)

## Summary

Splits mosaic dataset items that were merged together using Merge Mosaic Dataset Items.

## Usage

- If the selection or query does not contain any merged items, the tool will return an error message.
- When the tool is run, it removes the merged items from the mosaic dataset table then adds all the originally merged items as new rows.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset containing the items to split. | Mosaic Layer |
| Query Definition (Optional) | An SQL expression to select items to split.If the query does not contain any previously merged items, the tool will return an error. | SQL Expression |
| in_mosaic_dataset | The mosaic dataset containing the items to split. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select items to split.If the query does not contain any previously merged items, the tool will return an error. | SQL Expression |

## Code Samples

### Example 1

```python
arcpy.management.SplitMosaicDatasetItems(in_mosaic_dataset, {where_clause})
```

### Example 2

```python
import arcpy
arcpy.SplitMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md")
```

### Example 3

```python
import arcpy
arcpy.SplitMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md")
```

### Example 4

```python
#Split mosaic dataset items

import arcpy
    
arcpy.SplitMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md")
```

### Example 5

```python
#Split mosaic dataset items

import arcpy
    
arcpy.SplitMosaicDatasetItems_management("c:/data/merge_md_items.gdb/md")
```

---

## Split Raster (Data Management)

## Summary

Divides a raster dataset into smaller pieces, by tiles or features from a polygon.

## Usage

- The output files will share most of the properties of the input source raster, such as the spatial reference, source type, pixel type, pixel depth, and cell size.
- The tiling method determines which of the optional parameters are used to determine the dimensions and location of the output tiles. In both cases, NoData values are used to pad the tiles where there is no corresponding source data. The data format depends on the limitations of the individual format specifications and the source image data type. Invalid combinations result in an appropriate error message.
- If a tile already exists (if there is a file with the same name), it will not be overwritten.
- If a tile only contains NoData pixel values, it will not be created.
- Setting the Overlap parameter is recommended if you are working with elevation data, or raster data where you plan to apply focal functions, such as Slope, Aspect, Shaded Relief, and others.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster to split. | Mosaic Dataset; Mosaic Layer; Raster Layer |
| Output Folder | The destination for the new raster datasets. | Folder |
| Output Base Name | The prefix for each of the raster datasets you will create. A number will be appended to each prefix, starting with 0. | String |
| Split Method | Determines how to split the raster dataset.Size of tile—Specify the width and height of the tile.Number of tiles— Specify the number of raster tiles to create by breaking the dataset into a number of columns and rows.Polygon features— Use the individual polygon geometries in a feature class to split the raster. | String |
| Output Format | The format for the output raster datasets.Geotiff (*.tif)—Tagged Image File Format. This is the default.Bitmap (*.bmp)—Microsoft Bitmap.ENVI (*.dat)—ENVI DAT.Esri BIL (*.bil)—Esri Band Interleaved by Line.Esri BIP (*.bip)—Esri Band Interleaved by Pixel.Esri BSQ (*.bsq)—Esri Band Sequential.GIF (*.gif)—Graphic Interchange Format.Esri GRID—Esri Grid.ERDAS IMAGINE (*.img)—ERDAS IMAGINE.JPEG 2000 (*.jp2)—JPEG 2000.JPEG (*.jpeg)—Joint Photographic Experts Group.PNG (*.png)—Portable Network Graphics. | String |
| Resampling Technique(Optional) | Choose an appropriate technique based on the type of data you have.Nearest—The fastest resampling method, and it minimizes changes to pixel values. Suitable for discrete data, such as land cover.Bilinear—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.Cubic—Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |
| Number of Output Rasters(Optional) | The number of columns (x) and rows (y) to split the raster dataset into. The X coordinate is the number of columns and the Y coordinate is number of rows. | Point |
| Size of Output Rasters(Optional) | The x and y dimensions of the output tiles. The default unit of measurement is in pixels. You can change this with the Units for Output Raster Size and Overlap parameter. The X coordinate is the X (horizontal) dimension the output tiles and the Y coordinate is the Y (vertical) dimension of output tiles. | Point |
| Overlap(Optional) | The tiles do not have to line up perfectly; set the amount of overlap between tiles with this parameter. The default unit of measurement is in pixels. You can change this with the Units for Output Raster Size and Overlap parameter. | Double |
| Units for Output Raster Size and Overlap(Optional) | Set the units of measurement for the Size of Output Rasters parameter and the Overlap parameter.Pixels—The unit is in pixels. This is the default.Meters—The unit is in meters.Feet—The unit is in feet.Degrees—The unit is in decimal degrees.Miles—The unit is in miles.Kilometers—The unit is in kilometers. | String |
| Cell Size(Optional) | The spatial resolution of the output raster. If left blank, the output cell size will match the input raster. When you change the cell size values, the tile size is reset to the image size and the tile count is reset to 1. | Point |
| Lower left origin(Optional) | Change the coordinates for the lower left origin point, where the tiling scheme will begin. If left blank, the lower left origin would be the same as the input raster. | Point |
| Split Polygon Feature Class(Optional) | A feature class that will be used to split the raster dataset. | Feature Layer |
| Clip Type(Optional) | Limits the extent of your raster dataset before you split it.None— Use the full extent of the input raster dataset.Extent—Specify bounding box as your clipping boundary.Feature class—Specify a feature class to clip the extent. | String |
| Template Extent(Optional) | An extent or a dataset used to define the clipping boundary. The dataset can be a raster or feature class.Current Display Extent —The extent will be based on the active map or scene.Draw Extent —The extent will be based on a rectangle drawn on the map or scene.Extent of a Layer —The extent will be based on an active map layer. Choose an available layer or use the Extent of data in all layers option. Each map layer has the following options:All Features —The extent of all features.Selected Features —The extent of the selected features.Visible Features —The extent of visible features.Browse —The extent will be based on a dataset.Clipboard —The extent can be copied to and from the clipboard. Copy Extent —Copies the extent and coordinate system to the clipboard.Paste Extent —Pastes the extent and coordinate system from the clipboard. If the clipboard does not include a coordinate system, the extent will use the map’s coordinate system.Reset Extent —The extent will be reset to the default value.When coordinates are manually provided, the coordinates must be numeric values and in the active map's coordinate system. The map may use different display units than the provided coordinates. Use a negative value sign for south and west coordinates. | Extent |
| NoData Value (Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | String |
| in_raster | The raster to split. | Mosaic Dataset; Mosaic Layer; Raster Layer |
| out_folder | The destination for the new raster datasets. | Folder |
| out_base_name | The prefix for each of the raster datasets you will create. A number will be appended to each prefix, starting with 0. | String |
| split_method | Determines how to split the raster dataset.SIZE_OF_TILE—Specify the width and height of the tile.NUMBER_OF_TILES— Specify the number of raster tiles to create by breaking the dataset into a number of columns and rows.POLYGON_FEATURES— Use the individual polygon geometries in a feature class to split the raster. | String |
| format | The format for the output raster datasets.TIFF—Tagged Image File Format. This is the default.BMP—Microsoft Bitmap.ENVI—ENVI DAT.Esri BIL—Esri Band Interleaved by Line.Esri BIP—Esri Band Interleaved by Pixel.Esri BSQ—Esri Band Sequential.GIF—Graphic Interchange Format.GRID—Esri Grid.IMAGINE IMAGE—ERDAS IMAGINE.JP2—JPEG 2000.JPEG—Joint Photographic Experts Group.PNG—Portable Network Graphics. | String |
| resampling_type(Optional) | Choose an appropriate technique based on the type of data you have.NEAREST—The fastest resampling method, and it minimizes changes to pixel values. Suitable for discrete data, such as land cover.BILINEAR—Calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding 4 pixels. Suitable for continuous data.CUBIC—Calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. Produces the smoothest image, but can create values outside of the range found in the source data. Suitable for continuous data. | String |
| num_rasters(Optional) | The number of columns (x) and rows (y) to split the raster dataset into. This is a point whose X and Y coordinates define number of rows and columns. The X coordinate is the number of columns and the Y coordinate is the number of rows. | Point |
| tile_size(Optional) | The x and y dimensions of the output tiles. The default unit of measurement is in pixels. You can change this with the units parameter. This is a point whose X and Y coordinates define the dimensions of output tiles. The X coordinate is the horizontal dimension of the output and the Y coordinate is the vertical dimension of the output. | Point |
| overlap(Optional) | The tiles do not have to line up perfectly; set the amount of overlap between tiles with this parameter. The default unit of measurement is in pixels. You can change this with the units parameter. | Double |
| units(Optional) | Set the units of measurement for the tile_size and the overlap parameters.PIXELS—The unit is in pixels. This is the default.METERS—The unit is in meters.FEET—The unit is in feet.DEGREES—The unit is in decimal degrees.MILES—The unit is in miles.KILOMETERS—The unit is in kilometers. | String |
| cell_size(Optional) | The spatial resolution of the output raster. If left blank, the output cell size will match the input raster. When you change the cell size values, the tile size is reset to the image size and the tile count is reset to 1. | Point |
| origin(Optional) | Change the coordinates for the lower left origin point, where the tiling scheme will begin. If left blank, the lower left origin would be the same as the input raster. | Point |
| split_polygon_feature_class(Optional) | A feature class that will be used to split the raster dataset. | Feature Layer |
| clip_type(Optional) | Limits the extent of your raster dataset before you split it.NONE— Use the full extent of the input raster dataset.EXTENT—Specify bounding box as your clipping boundary.FEATURE_CLASS—Specify a feature class to clip the extent. | String |
| template_extent(Optional) | An extent or a dataset used to define the clipping boundary. The dataset can be a raster or feature class.MAXOF—The maximum extent of all inputs will be used.MINOF—The minimum area common to all inputs will be used.DISPLAY—The extent is equal to the visible display.Layer name—The extent of the specified layer will be used.Extent object—The extent of the specified object will be used. Space delimited string of coordinates—The extent of the specified string will be used. Coordinates are expressed in the order of x-min, y-min, x-max, y-max. | Extent |
| nodata_value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | String |

## Code Samples

### Example 1

```python
arcpy.management.SplitRaster(in_raster, out_folder, out_base_name, split_method, format, {resampling_type}, {num_rasters}, {tile_size}, {overlap}, {units}, {cell_size}, {origin}, {split_polygon_feature_class}, {clip_type}, {template_extent}, {nodata_value})
```

### Example 2

```python
import arcpy
arcpy.SplitRaster_management("c:/source/large.tif", "c:/output/splitras",
                             "ras", "NUMBER_OF_TILES", "TIFF", "NEAREST",
                             "2 2", "#", "10", "PIXELS", "#", "#")
```

### Example 3

```python
import arcpy
arcpy.SplitRaster_management("c:/source/large.tif", "c:/output/splitras",
                             "ras", "NUMBER_OF_TILES", "TIFF", "NEAREST",
                             "2 2", "#", "10", "PIXELS", "#", "#")
```

### Example 4

```python
##====================================
##Split Raster
##Usage: SplitRaster_management in_raster out_folder out_base_name SIZE_OF_TILE
##                              | NUMBER_OF_TILES | TIFF | BMP | ENVI | ESRI BIL |
##                              ESRI BIP | ESRI BSQ | GIF | GRID | IMAGINE IMAGE | 
##                              JP2 | JPG | PNG {NEAREST | BILINEAR | CUBIC | 
##                              MAJORITY} {num_rasters} {tile_size} {overlap} 
##                              {PIXELS | METERS | FEET | DEGREES | KILOMETERS | 
##                              MILES} {cell_size} {origin}
    
import arcpy
arcpy.env.workspace = r"\\myServer\PrjWorkspace\RasGP"

##Equally split a large TIFF image by number of images
arcpy.SplitRaster_management("large.tif", "splitras", "number", "NUMBER_OF_TILES",\
                             "TIFF", "NEAREST", "2 2", "#", "4", "PIXELS",\
                             "#", "#")

##Equally split a large TIFF image by size of images
arcpy.SplitRaster_management("large.tif", "splitras", "size2", "SIZE_OF_TILE",\
                             "TIFF", "BILINEAR", "#", "3500 3500", "4", "PIXELS",\
                             "#", "-50 60")
```

### Example 5

```python
##====================================
##Split Raster
##Usage: SplitRaster_management in_raster out_folder out_base_name SIZE_OF_TILE
##                              | NUMBER_OF_TILES | TIFF | BMP | ENVI | ESRI BIL |
##                              ESRI BIP | ESRI BSQ | GIF | GRID | IMAGINE IMAGE | 
##                              JP2 | JPG | PNG {NEAREST | BILINEAR | CUBIC | 
##                              MAJORITY} {num_rasters} {tile_size} {overlap} 
##                              {PIXELS | METERS | FEET | DEGREES | KILOMETERS | 
##                              MILES} {cell_size} {origin}
    
import arcpy
arcpy.env.workspace = r"\\myServer\PrjWorkspace\RasGP"

##Equally split a large TIFF image by number of images
arcpy.SplitRaster_management("large.tif", "splitras", "number", "NUMBER_OF_TILES",\
                             "TIFF", "NEAREST", "2 2", "#", "4", "PIXELS",\
                             "#", "#")

##Equally split a large TIFF image by size of images
arcpy.SplitRaster_management("large.tif", "splitras", "size2", "SIZE_OF_TILE",\
                             "TIFF", "BILINEAR", "#", "3500 3500", "4", "PIXELS",\
                             "#", "-50 60")
```

---

## Standardize Field (Data Management)

## Summary

Standardizes values in fields by converting them to values that follow a specified scale. Standardization methods include z-score, minimum-maximum, absolute maximum, and robust standardization.

## Usage

- The Z-Score method measures the difference between a value and the mean of all values in the field using standard deviations, otherwise known as the standard score. Potential application—Assess the significance of a value in relation to the distribution of values in a field. For example, a county's voter participation can be evaluated in the context of other counties across the country, helping identify typical voter participation patterns and counties with significantly high and low voter participation. Consideration—This method expects a normal distribution. Consequently, the method is not recommended if the distribution of the data is highly skewed.Equation—, where x' is the standardized value, x is the original value, x̄ is the mean (average), and σx is the standard deviation.
- Potential application—Assess the significance of a value in relation to the distribution of values in a field. For example, a county's voter participation can be evaluated in the context of other counties across the country, helping identify typical voter participation patterns and counties with significantly high and low voter participation.
- Consideration—This method expects a normal distribution. Consequently, the method is not recommended if the distribution of the data is highly skewed.
- Equation—, where x' is the standardized value, x is the original value, x̄ is the mean (average), and σx is the standard deviation.
- The Minimum-maximum method preserves the relationships among the original data values while converting the values to a scale between user-specified minimum and maximum values. Potential application—A real estate assessor may want to scale characteristics of homes, such as the number of rooms in a house or the age of the house in years to the same scale prior to using these characteristics in a model, such as the Forest-based Classification and Regression tool. Consideration—This approach is prone to influence by outliers, or extreme values, in the data.Equation—, where x' is the standardized value, x is the original value, min(x) is the minimum of the data, max(x) is the maximum of the data, a is the user-specified minimum, and b is the user-specified maximum.
- Potential application—A real estate assessor may want to scale characteristics of homes, such as the number of rooms in a house or the age of the house in years to the same scale prior to using these characteristics in a model, such as the Forest-based Classification and Regression tool.
- Consideration—This approach is prone to influence by outliers, or extreme values, in the data.
- Equation—, where x' is the standardized value, x is the original value, min(x) is the minimum of the data, max(x) is the maximum of the data, a is the user-specified minimum, and b is the user-specified maximum.
- The Absolute maximum method compares the difference between a value and the maximum absolute value in a distribution by dividing each value by the maximum absolute value in the field. Potential application—This method is useful when working with data that has a stable and logical maximum and you want to compare each value to this maximum. For example, the number of votes in a county cannot contain more votes than the number of voting-age people in the county. The county with the highest proportion of votes becomes this maximum, and all other counties are assessed in relation to the absolute maximum voter participation. Consideration—The output scale is between -1 and 1. Larger positive values correspond to values close to 1, and larger negative values correspond to values close to -1.Equation—, where x' is the standardized value, x is the original value, and max(|x|) is the maximum of the absolute values of the data.
- Potential application—This method is useful when working with data that has a stable and logical maximum and you want to compare each value to this maximum. For example, the number of votes in a county cannot contain more votes than the number of voting-age people in the county. The county with the highest proportion of votes becomes this maximum, and all other counties are assessed in relation to the absolute maximum voter participation.
- Consideration—The output scale is between -1 and 1. Larger positive values correspond to values close to 1, and larger negative values correspond to values close to -1.
- Equation—, where x' is the standardized value, x is the original value, and max(|x|) is the maximum of the absolute values of the data.
- The Robust standardization method standardizes the values in the specified fields using a robust variant of the z-score. This variant uses median and interquartile range in place of mean and standard deviation. Potential application—A real estate assessor is attempting to estimate home values in a city, and an exclusive neighborhood with extremely high home values results in outliers in the data. The assessor uses robust standardization to mitigate the impact of these outliers in the distribution of home values for the city. Consideration—With its use of median and interquartile range, this can be an effective method when attempting to mitigate the influence of outliers in the distribution.Equation—, where x' is the standardized value, x is the original value, median(x) is the median of the data, and IQR(x) is the interquartile range of the data.
- Potential application—A real estate assessor is attempting to estimate home values in a city, and an exclusive neighborhood with extremely high home values results in outliers in the data. The assessor uses robust standardization to mitigate the impact of these outliers in the distribution of home values for the city.
- Consideration—With its use of median and interquartile range, this can be an effective method when attempting to mitigate the influence of outliers in the distribution.
- Equation—, where x' is the standardized value, x is the original value, median(x) is the median of the data, and IQR(x) is the interquartile range of the data.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the field with the values to be standardized. | Table View; Raster Layer; Mosaic Layer |
| Field to Standardize | The fields containing the values to be standardized. For each field, an output field name can be specified. If an output field name is not provided, the tool will create an output field name using the field name and selected method. | Value Table |
| Standardization Method (Optional) | Specifies the method to use to standardize the values contained in the specified fields.Z-Score—The standard score, which is the number of standard deviations above or below the mean, is used. The calculation is the Z-Score formula, which calculates the difference between the value and the mean of the values in the column, divided by the standard deviation of the values in the column. This is the default.Minimum-maximum—The values are converted to a scale between the user-specified minimum and maximum values.Absolute maximum—Each value in the column is divided by the maximum absolute value in the column.Robust standardization— A robust variant of the Z-Score formula is used to standardize the values in the specified fields. This variant uses median and interquartile range in place of mean and standard deviation. | String |
| Minimum Value (Optional) | The value used by the Minimum-maximum method of the Standardization Method parameter to specify the minimum value in the scale of the provided output values. | Double |
| Maximum Value (Optional) | The value used by the Minimum-maximum method of the Standardization Method parameter to specify the maximum value in the scale of the provided output values. | Double |
| in_table | The table containing the field with the values to be standardized. | Table View; Raster Layer; Mosaic Layer |
| fields[[input_field, output_field],...] | The fields containing the values to be standardized. For each field, an output field name can be specified. If an output field name is not provided, the tool will create an output field name using the field name and selected method. | Value Table |
| method(Optional) | Specifies the method to use to standardize the values contained in the specified fields.Z-SCORE—The standard score, which is the number of standard deviations above or below the mean, is used. The calculation is the Z-Score formula, which calculates the difference between the value and the mean of the values in the column, divided by the standard deviation of the values in the column. This is the default.MIN-MAX—The values are converted to a scale between the user-specified minimum and maximum values.MAXABS—Each value in the column is divided by the maximum absolute value in the column.ROBUST— A robust variant of the Z-Score formula is used to standardize the values in the specified fields. This variant uses median and interquartile range in place of mean and standard deviation. | String |
| min_value(Optional) | The value used by the MIN-MAX method of the method parameter to specify the minimum value in the scale of the provided output values. | Double |
| max_value(Optional) | The value used by the MIN-MAX method of the method parameter to specify the maximum value in the scale of the provided output values. | Double |

## Code Samples

### Example 1

```python
arcpy.management.StandardizeField(in_table, fields, {method}, {min_value}, {max_value})
```

### Example 2

```python
arcpy.management.StandardizeField("County_VoterTurnout", 
       "voter_turnout voter_turnout_Z_SCORE", "Z-SCORE")
```

### Example 3

```python
arcpy.management.StandardizeField("County_VoterTurnout", 
       "voter_turnout voter_turnout_Z_SCORE", "Z-SCORE")
```

### Example 4

```python
# Import system modules
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Standardize\\MyData.gdb"
    inputFeatures = ”County_VoterTurnout”

    # Set the input fields that will be standardized
    fields = "votes_total;rawdiff_dem_vs_gop;pctdiff_dem_vs_gop"

    # Set the standardization method.
    method = "ROBUST"

    # Run the Standardize Field tool
    arcpy.management.StandardizeField(inputFeatures, fields, method)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

### Example 5

```python
# Import system modules
import arcpy

try:
    # Set the workspace and input features.
    arcpy.env.workspace = r"C:\\Standardize\\MyData.gdb"
    inputFeatures = ”County_VoterTurnout”

    # Set the input fields that will be standardized
    fields = "votes_total;rawdiff_dem_vs_gop;pctdiff_dem_vs_gop"

    # Set the standardization method.
    method = "ROBUST"

    # Run the Standardize Field tool
    arcpy.management.StandardizeField(inputFeatures, fields, method)

except arcpy.ExecuteError:
    # If an error occurred when running the tool, print the error message.
    print(arcpy.GetMessages())
```

---

## Subdivide Polygon (Data Management)

## Summary

Divides polygon features into a number of equal areas or parts.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The polygon features to be subdivided. | Feature Layer |
| Output Feature Class | The output feature class of subdivided polygons. | Feature Class |
| Subdivision Method | Specifies the method that will be used to divide the polygons.Number of equal parts— Polygons will be divided evenly into a number of parts. This is the default.Equal areas—Polygons will be divided into a specified number of parts of a certain area, and a remainder part. | String |
| Number of Areas(Optional) | The number of areas into which the polygon will be divided if the Number of equal parts subdivision method is specified. | Long |
| Target Area (Optional) | The area of the equal parts if the Equal areas subdivision method is specified. If the Target Area is larger than the area of the input polygon, the polygon will not be subdivided. | Areal Unit |
| RESERVED(Optional) | This parameter is not yet supported. | Linear Unit |
| Split Angle (Optional) | The angle that will be used to draw the lines that divide the polygon. The default is 0. | Double |
| Subdivision Type (Optional) | Specifies how the polygons will be divided.Strips— Polygons will be divided into strips. This is the default.Stacked blocks—Polygons will be divided into stacked blocks. | String |
| in_polygons | The polygon features to be subdivided. | Feature Layer |
| out_feature_class | The output feature class of subdivided polygons. | Feature Class |
| method | Specifies the method that will be used to divide the polygons.NUMBER_OF_EQUAL_PARTS— Polygons will be divided evenly into a number of parts. This is the default.EQUAL_AREAS—Polygons will be divided into a specified number of parts of a certain area, and a remainder part. | String |
| num_areas(Optional) | The number of areas into which the polygon will be divided if the NUMBER_OF_EQUAL_PARTS subdivision method is specified. | Long |
| target_area(Optional) | The area of the equal parts if the EQUAL_AREAS subdivision method is specified. If the target_area is larger than the area of the input polygon, the polygon will not be subdivided. | Areal Unit |
| target_width(Optional) | This parameter is not yet supported. | Linear Unit |
| split_angle(Optional) | The angle that will be used to draw the lines that divide the polygon. The default is 0. | Double |
| subdivision_type(Optional) | Specifies how the polygons will be divided.STRIPS— Polygons will be divided into strips. This is the default.STACKED_BLOCKS—Polygons will be divided into stacked blocks. | String |

## Code Samples

### Example 1

```python
arcpy.management.SubdividePolygon(in_polygons, out_feature_class, method, {num_areas}, {target_area}, {target_width}, {split_angle}, {subdivision_type})
```

### Example 2

```python
import arcpy

arcpy.env.workspace = r"C:/data/project.gdb"
arcpy.SubdividePolygon_management(
    "studyarea", "subdivisions", "NUMBER_OF_EQUAL_PARTS", 10, "", "", 0, 
    "STRIPS")
```

### Example 3

```python
import arcpy

arcpy.env.workspace = r"C:/data/project.gdb"
arcpy.SubdividePolygon_management(
    "studyarea", "subdivisions", "NUMBER_OF_EQUAL_PARTS", 10, "", "", 0, 
    "STRIPS")
```

---

## Subset Features (Data Management)

## Summary

Divides the records of a feature class or table into two subsets: one subset to be used as training data, and one subset to be used as test features to compare and validate the output surface.

## Usage

- In the Random number generator environment, only the Mersenne Twister option is supported. If other options are chosen, Mersenne twister will be used instead.
- Splitting a dataset into training and test features is common in interpolation, machine learning, and other analytical workflows that involve estimating and building models from data.
- If multipart features are used as input, the output will be a subset of multipart features, not individual features.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The features or table from which subsets will be created. | Table View |
| Output Training Feature Class | The subset of training features that will be created. | Feature Class; Table |
| Output Test Feature Class(Optional) | The subset of test features that will be created. | Feature Class; Table |
| Size of Training Feature Subset(Optional) | The size of the output training feature class, entered either as a percentage of the input features or as an absolute number of features. | Double |
| Subset Size Units(Optional) | Specifies whether the subset size value will be used as a percentage of the input features or as an absolute number of features.Percentage of input— The subset size will be used as a percentage of the input features that will be in the training dataset.Absolute value— The subset size will be used as the number of features that will be in the training dataset. | Boolean |
| in_features | The features or table from which subsets will be created. | Table View |
| out_training_feature_class | The subset of training features that will be created. | Feature Class; Table |
| out_test_feature_class(Optional) | The subset of test features that will be created. | Feature Class; Table |
| size_of_training_dataset(Optional) | The size of the output training feature class, entered either as a percentage of the input features or as an absolute number of features. | Double |
| subset_size_units(Optional) | Specifies whether the subset size value will be used as a percentage of the input features or as an absolute number of features.PERCENTAGE_OF_INPUT— The subset size will be used as a percentage of the input features that will be in the training dataset.ABSOLUTE_VALUE— The subset size will be used as the number of features that will be in the training dataset. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SubsetFeatures(in_features, out_training_feature_class, {out_test_feature_class}, {size_of_training_dataset}, {subset_size_units})
```

### Example 2

```python
import arcpy
arcpy.management.SubsetFeatures("ca_ozone_pts", "C:/gapyexamples/output/training", 
                                "", "", "PERCENTAGE_OF_INPUT")
```

### Example 3

```python
import arcpy
arcpy.management.SubsetFeatures("ca_ozone_pts", "C:/gapyexamples/output/training", 
                                "", "", "PERCENTAGE_OF_INPUT")
```

### Example 4

```python
# Description: Randomly split the features into two feature classes.

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/dmpyexamples/data.gdb/data"

# Set local variables
inPointFeatures = "ca_ozone_pts.shp"
outtrainPoints = "C:/dmpyexamples/output.gdb/training"
outtestPoints = "C:/dmpyexamples/output.gdb/training"
subsetSize = 50
subsizeUnits = "PERCENTAGE_OF_INPUT"

# Run SubsetFeatures
arcpy.management.SubsetFeatures(inPointFeatures, outtrainPoints, outtestPoints, 
                                subsetSize, subsizeUnits)
```

### Example 5

```python
# Description: Randomly split the features into two feature classes.

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/dmpyexamples/data.gdb/data"

# Set local variables
inPointFeatures = "ca_ozone_pts.shp"
outtrainPoints = "C:/dmpyexamples/output.gdb/training"
outtestPoints = "C:/dmpyexamples/output.gdb/training"
subsetSize = 50
subsizeUnits = "PERCENTAGE_OF_INPUT"

# Run SubsetFeatures
arcpy.management.SubsetFeatures(inPointFeatures, outtrainPoints, outtestPoints, 
                                subsetSize, subsizeUnits)
```

---

## Synchronize Changes (Data Management)

## Summary

Synchronizes updates between two replica geodatabases in a specified direction.

## Usage

- This tool is used when synchronizing replicas in connected mode.
- Two-way, one-way, and check-out replicas can be synchronized with this tool.
- The replica geodatabases can be local geodatabases or geodata services.
- Once synchronized, the changes (edits) will be reflected in the target geodatabase and viewable by all users.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Geodatabase 1 | The geodatabase hosting the replica to synchronize. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| Replica | A valid replica with a parent contained in one input geodatabase and a child in the other input geodatabase. | String |
| Geodatabase 2 | The geodatabase hosting the relative replica. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| Direction | Specifies the direction in which the changes will be synchronized: from geodatabase 1 to geodatabase 2, from geodatabase 2 to geodatabase 1, or in both directions. For check-out/check-in replicas or one-way replicas, there is only one appropriate direction. If the replica is two-way, all of the choices are available.Both directions—Changes will be synchronized in both directions. This is the default.From geodatabase 2 to geodatabase 1—Changes will be synchronized from geodatabase 2 to geodatabase 1.From geodatabase 1 to geodatabase 2—Changes will be synchronized from geodatabase 1 to geodatabase 2. | String |
| Conflict Resolution Policy | Specifies how conflicts will be resolved when they are encountered.Manually resolve conflicts—Conflicts will be resolved manually in the versioning reconcile environment.Resolve in favor of geodatabase 1—Conflicts will be resolved in favor of geodatabase 1. This is the default.Resolve in favor of geodatabase 2—Conflicts will be resolved in favor of geodatabase 2. | String |
| Conflict Definition | Specifies how conflicts will be defined.Conflicts defined by row—Changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default.Conflicts defined by column— Only changes to the same attribute (column) of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. | String |
| Reconcile with the Parent Version (Checkout only) | Specifies whether to automatically reconcile once data changes are sent to the parent replica if there are no conflicts present. This option is only available for check-out/check-in replicas.Unchecked—Do not reconcile with the parent version. This is the default.Checked—Reconcile with the parent version. | Boolean |
| geodatabase_1 | The geodatabase hosting the replica to synchronize. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| in_replica | A valid replica with a parent contained in one input geodatabase and a child in the other input geodatabase. | String |
| geodatabase_2 | The geodatabase hosting the relative replica. The geodatabase can be local or remote. | Workspace; GeoDataServer |
| in_direction | Specifies the direction in which the changes will be synchronized: from geodatabase 1 to geodatabase 2, from geodatabase 2 to geodatabase 1, or in both directions. For check-out/check-in replicas or one-way replicas, there is only one appropriate direction. If the replica is two-way, all of the choices are available.BOTH_DIRECTIONS—Changes will be synchronized in both directions. This is the default.FROM_GEODATABASE2_TO_1—Changes will be synchronized from geodatabase 2 to geodatabase 1.FROM_GEODATABASE1_TO_2—Changes will be synchronized from geodatabase 1 to geodatabase 2. | String |
| conflict_policy | Specifies how conflicts will be resolved when they are encountered.MANUAL—Conflicts will be resolved manually in the versioning reconcile environment.IN_FAVOR_OF_GDB1—Conflicts will be resolved in favor of geodatabase 1. This is the default.IN_FAVOR_OF_GDB2—Conflicts will be resolved in favor of geodatabase 2. | String |
| conflict_definition | Specifies how conflicts will be defined.BY_OBJECT—Changes to the same row or feature in the parent and child versions will conflict during reconcile. This is the default.BY_ATTRIBUTE— Only changes to the same attribute (column) of the same row or feature in the parent and child versions will be flagged as a conflict during reconcile. Changes to different attributes will not be considered a conflict during reconcile. | String |
| reconcile | Specifies whether to automatically reconcile once data changes are sent to the parent replica if there are no conflicts present. This option is only available for check-out/check-in replicas.DO_NOT_RECONCILE—Do not reconcile with the parent version. This is the default.RECONCILE—Reconcile with the parent version. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SynchronizeChanges(geodatabase_1, in_replica, geodatabase_2, in_direction, conflict_policy, conflict_definition, reconcile)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.SynchronizeChanges_management("MyData.sde", "My2wayReplica", 
                                    "MyData_child.sde", "BOTH_DIRECTIONS",
                                    "IN_FAVOR_OF_GDB1", "BY_ATTRIBUTE")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/Data"
arcpy.SynchronizeChanges_management("MyData.sde", "My2wayReplica", 
                                    "MyData_child.sde", "BOTH_DIRECTIONS",
                                    "IN_FAVOR_OF_GDB1", "BY_ATTRIBUTE")
```

### Example 4

```python
# Name: SynchronizeChanges_Example2.py
# Description: Synchronizes changes for a one way replica from the Parent to 
#              the child replica geodatabase. The parent is an enterprise 
#              geodatabase workspace, and the child is file geodatabase.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_gdb1 = "MyData.sde"
replica_gdb2 = "Counties_replica.gdb"
replica_name = "MyOneWayReplica"
sync_direction = "FROM_GEODATABASE1_TO_2"
conflict_policy = ""     # Not applicable for one way replicas, there is not conflict detection.
conflict_detection = ""  # Not applicable for one way replicas, there is not conflict detection.
reconcile = ""           # Only applicable for Checkout replicas

# Execute SynchronizeChanges
arcpy.SynchronizeChanges_management(replica_gdb1, replica_name, replica_gdb2, 
                                    sync_direction, conflict_policy, 
                                    conflict_detection, reconcile)
```

### Example 5

```python
# Name: SynchronizeChanges_Example2.py
# Description: Synchronizes changes for a one way replica from the Parent to 
#              the child replica geodatabase. The parent is an enterprise 
#              geodatabase workspace, and the child is file geodatabase.

# Import system modules
import arcpy

# Set workspace
arcpy.env.workspace = "C:/Data"

# Set local variables
replica_gdb1 = "MyData.sde"
replica_gdb2 = "Counties_replica.gdb"
replica_name = "MyOneWayReplica"
sync_direction = "FROM_GEODATABASE1_TO_2"
conflict_policy = ""     # Not applicable for one way replicas, there is not conflict detection.
conflict_detection = ""  # Not applicable for one way replicas, there is not conflict detection.
reconcile = ""           # Only applicable for Checkout replicas

# Execute SynchronizeChanges
arcpy.SynchronizeChanges_management(replica_gdb1, replica_name, replica_gdb2, 
                                    sync_direction, conflict_policy, 
                                    conflict_detection, reconcile)
```

---

## Synchronize Mosaic Dataset (Data Management)

## Summary

Synchronizes a mosaic dataset to keep it up to date. In addition to syncing data, you can update overviews if the underlying imagery has been changed, generate new overviews and cache, and restore the original configuration of mosaic dataset items. You can also remove paths to source data with this tool. To repair paths, use the Repair Mosaic Dataset Paths tool.

## Usage

- You can use a selection set with this tool to limit the raster items that are updated. When there is a selection or query, only those items will be processed.
- Synchronization can add new items, update existing items, or remove items.
- Stale items refer to source rasters that have been changed since the mosaic dataset was created or the last time the mosaic dataset was synchronized. For instance, the georeferencing may have been updated or the pyramids may have been built.
- Since the raster items will be reconstructed, any modifications made to them since the last time they were built will be lost, such as editing functions or content in the attribute table.
- If you remove items that have broken data sources, ensure that all network connections are working properly. This tool will remove any items that cannot be accessed.
- This tool can also build pyramids and calculate statistics on the source rasters as well as create thumbnails and raster cache for the raster items.
- This tool is particularly useful for keeping mosaic datasets up to date. If new raster datasets have been added to the workspaces that this mosaic dataset accesses, the new raster datasets can be added to the mosaic dataset. Mosaic datasets that are populated using tables that reside in an externally managed database can also be updated with this tool.
- To use the Refresh Aggregate Information parameter, uncheck the Update Existing Items parameter. For multidimensional mosaic datasets, the Refresh Aggregate Information parameter refreshes the multidimensional properties of the mosaic based on the content of the attribute table of the mosaic dataset.
- Database fragmentation and frequent data manipulation can significantly increase the size of a mosaic dataset. If the database size is large due to constant transactions, run the Compact tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Mosaic Dataset | The mosaic dataset that will be synchronized. | Mosaic Layer |
| Query Definition(Optional) | An SQL expression to select which mosaic dataset items will be synchronized. If an expression is not provided, all dataset items will be updated. | SQL Expression |
| Update With New Items (Optional) | Specifies whether new items will be included when synchronizing as well as the options to use under the Update With New Items Options submenu. If you use this option, the item's workspace will be searched for new data. When data is added to the mosaic dataset, it will use the same raster type as the other items in the same workspace.Unchecked—No new items will be added when synchronizing. This is the default.Checked—The mosaic dataset will be updated with new items in the workspaces. Optionally, the existing items can be modified by unchecking the Skip Existing Items parameter. | Boolean |
| Synchronize Stale Items Only(Optional) | Specifies whether mosaic dataset items will be updated only when the underlying raster datasets have been modified due to synchronizing. For example, building pyramids or updating the georeferencing of rasters will affect how the overviews are rendered.Checked—Only the items of the underlying raster datasets that have been modified will be updated. This is the default.Unchecked—All of the items in the mosaic dataset will be updated. | Boolean |
| Update Cell Size Ranges (Optional) | Specifies whether cell size ranges for the mosaic dataset will be recalculated. Checked—The cell size ranges for the entire mosaic dataset will be recalculated, but only for items that have an invalid visibility. This is the default.Unchecked—No cell size ranges will be recalculated. | Boolean |
| Update Boundary(Optional) | Specifies whether the boundary that shows the full extent of the mosaic dataset will be rebuilt. Check this parameter if syncing will change the extent of the mosaic dataset.Checked—The boundary will be rebuilt after the mosaic dataset is synchronized. This is the default.Unchecked—The boundary will not be rebuilt. | Boolean |
| Update Overviews(Optional) | Specifies whether obsolete overviews will be updated. The overview becomes obsolete if any underlying rasters have been modified due to synchronizing.Unchecked—The overviews will not be updated. This is the default.Checked—The affected overviews will be updated after the mosaic dataset is synchronized. | Boolean |
| Build Raster Pyramids (Optional) | Specifies whether pyramids will be built for the specified mosaic dataset items. Pyramids can be built for each raster item in the mosaic dataset. Pyramids can improve the speed at which each raster is displayed.Unchecked—Pyramids will not be built. This is the default.Checked—Pyramids will be built for all the mosaic raster items that were updated due to synchronization.Pyramids will not be built for items that were added due to synchronization. | Boolean |
| Calculate Statistics (Optional) | Specifies whether statistics will be calculated for the specified mosaic dataset items. Statistics are required for a mosaic dataset when performing certain tasks, such as applying a contrast stretch.Unchecked—Statistics will not be calculated. This is the default.Checked—Statistics will be calculated for the mosaic dataset items that were updated due to synchronization.Statistics will not be calculated for items that were added due to synchronization. | Boolean |
| Build Thumbnails (Optional) | Specifies whether thumbnails will be built for the specified mosaic dataset items. Thumbnails are small, highly resampled images that can be created for each raster item in the mosaic definition. Thumbnails can be accessed when the mosaic dataset is accessed as an image service and will display as part of the item description.Unchecked—Thumbnails will not be built or updated. This is the default.Checked—Thumbnails will be built or updated for all the raster items that were updated due to synchronization.Thumbnails will not be built for items that were added due to synchronization. | Boolean |
| Build Item Cache (Optional) | Choose whether to build a cache for the specified mosaic dataset items. A cache can be built when you've added data using the LAS, Terrain, or LAS dataset raster types. Unchecked—A cache will not be built or updated. This is the default.Checked—A cache will be built or updated for all the raster items specified.A cache will not be built for items that were added due to synchronization. | Boolean |
| Rebuild Raster From Data Source (Optional) | Specifies whether the raster items will be rebuilt from the data source using the original raster type.Checked—The rasters will be rebuilt from the source data. Any changes that you have performed on the mosaic dataset will be lost. This is the default.Unchecked—The rasters will not be rebuilt. Other primary fields will be reset if the Update Fields parameter is checked.This only affects items that will be synchronized. This parameter is not applicable if the Update With New Items parameter is checked. | Boolean |
| Update Fields (Optional) | Specifies whether the fields in the table will be updated. This only affects items that will be synchronized.Checked—The fields will be updated from the source files. This is the default.Unchecked—The fields in the table will not be updated from the source. If you update the fields, you can control which fields are updated using the Fields To Update parameter. If you made edits to some of the fields, you can deselect them using the Fields To Update parameter. | Boolean |
| Fields To Update (Optional) | The fields that will be updated.This parameter is only valid if the Update Fields parameter is checked.If you made edits to some of the fields, you can deselect them.The RASTER field can be refreshed, even if the Rebuild Raster From Data Source parameter is unchecked. However, if Rebuild Raster From Data Source is checked, the RASTER field will be rebuilt, even if the Fields To Update parameter is unchecked. | String |
| Update Existing Items (Optional) | Specifies whether existing items in the mosaic dataset will be updated. If you check this parameter, you must specify the options to update under the Update Existing Items Options submenu.Checked—The existing items will be updated with the options you chose to update. This is the default.Unchecked—The existing items will not be updated. | Boolean |
| Remove Items With Broken Data Source(Optional) | Specifies whether items with broken links will be removed.Ensure that all network connections are working properly. This tool will remove any items that cannot be accessed.Unchecked—Items with broken links will not be removed from the mosaic dataset. This is the default.Checked—Items with broken links will be removed from the mosaic dataset. | Boolean |
| Skip Existing Items (Optional) | Specifies whether existing mosaic dataset items will be skipped or updated with the modified files from disk. To use this parameter, the Update With New Items parameter must be checked.Checked—While adding new mosaic dataset items, existing mosaic dataset items will be skipped; they will not be updated. This is the default. Unchecked—While adding new mosaic dataset items, existing mosaic dataset items that correspond to modified files on disk will be updated. | Boolean |
| Refresh Aggregate Information (Optional) | Specifies whether data that may have been removed from the mosaic dataset will be included. To use this parameter, the Update Existing Items parameter must be unchecked.Unchecked—When synchronizing, rasters that may have been removed from the mosaic dataset will be excluded. This is the default. Checked—When synchronizing, rasters that may have been removed from the mosaic dataset will be included. | Boolean |
| Estimate Mosaic Dataset Statistics | Specifies whether statistics on the mosaic dataset will be estimated. Unchecked—When synchronizing, statistics on the mosaic dataset will not be estimated. This is the default. Checked—When synchronizing, statistics on the mosaic dataset will be estimated. | Boolean |
| in_mosaic_dataset | The mosaic dataset that will be synchronized. | Mosaic Layer |
| where_clause(Optional) | An SQL expression to select which mosaic dataset items will be synchronized. If an expression is not provided, all dataset items will be updated. | SQL Expression |
| new_items(Optional) | Specifies whether new items will be included when synchronizing as well as the options to use to update the new items.If you use this option, the item's workspace will be searched for new data. When data is added to the mosaic dataset, it will use the same raster type as the other items in the same workspace.NO_NEW_ITEMS—No new items will be added when synchronizing. This is the default.UPDATE_WITH_NEW_ITEMS—The mosaic dataset will be updated with new items in the workspaces. Optionally, the existing items can be modified by setting the skip_existing_items parameter to OVERWRITE_EXISTING_ITEMS. | Boolean |
| sync_only_stale(Optional) | Specifies whether mosaic dataset items will be updated only when the underlying raster datasets have been modified due to synchronizing. For example, building pyramids or updating the georeferencing of rasters will affect how the overviews are rendered. SYNC_STALE—Only the items of the underlying raster datasets that have been modified will be updated. This is the default.SYNC_ALL—All of the items in the mosaic dataset will be updated. | Boolean |
| update_cellsize_ranges(Optional) | Specifies whether cell size ranges for the mosaic dataset will be recalculated. UPDATE_CELL_SIZES—The cell size ranges for the entire mosaic dataset will be recalculated, but only for items that have an invalid visibility. This is the default.NO_CELL_SIZES—No cell size ranges will be recalculated. | Boolean |
| update_boundary(Optional) | Specifies whether the boundary that shows the full extent of the mosaic dataset will be rebuilt. Choose UPDATE_BOUNDARY if syncing will change the extent of the mosaic dataset.UPDATE_BOUNDARY— The boundary will be rebuilt after the mosaic dataset is synchronized. This is the default.NO_BOUNDARY— The boundary will not be rebuilt. | Boolean |
| update_overviews(Optional) | Specifies whether obsolete overviews will be updated. The overview becomes obsolete if any underlying rasters have been modified due to synchronizing.NO_OVERVIEWS— The overviews will not be updated. This is the default.UPDATE_OVERVIEWS— The affected overviews will be updated after the mosaic dataset is synchronized. | Boolean |
| build_pyramids(Optional) | Specifies whether pyramids will be built for the specified mosaic dataset items. Pyramids can be built for each raster item in the mosaic dataset. Pyramids can improve the speed at which each raster is displayed.NO_PYRAMIDS—Pyramids will not be built. This is the default.BUILD_PYRAMIDS— Pyramids will be built for all the mosaic raster items that were updated due to synchronization.Pyramids will not be built for items that were added due to synchronization. | Boolean |
| calculate_statistics(Optional) | Specifies whether statistics will be calculated for the specified mosaic dataset items. Statistics are required for a mosaic dataset when performing certain tasks, such as applying a contrast stretch.NO_STATISTICS—Statistics will not be calculated. This is the default.CALCULATE_STATISTICS— Statistics will be calculated for the mosaic dataset items that were updated due to synchronization.Statistics will not be calculated for items that were added due to synchronization. | Boolean |
| build_thumbnails(Optional) | Specifies whether thumbnails will be built for the specified mosaic dataset items. Thumbnails are small, highly resampled images that can be created for each raster item in the mosaic definition. Thumbnails can be accessed when the mosaic dataset is accessed as an image service and will display as part of the item description.NO_THUMBNAILS—Thumbnails will not be built or updated. This is the default.BUILD_THUMBNAILS—Thumbnails will be built or updated for all the raster items that were updated due to synchronization.Thumbnails will not be built for items that were added due to synchronization. | Boolean |
| build_item_cache(Optional) | Choose whether to build a cache for the specified mosaic dataset items. A cache can be built when you've added data using the LAS, Terrain, or LAS dataset raster types. NO_ITEM_CACHE—A cache will not be built or updated. This is the default.BUILD_ITEM_CACHE—A cache will be built or updated for all the raster items that were updated due to synchronization.A cache will not be built for items that were added due to synchronization. | Boolean |
| rebuild_raster(Optional) | Specifies whether the raster items will be rebuilt from the data source using the original raster type.REBUILD_RASTER—The rasters will be rebuilt from the source data. Any changes that you have performed on selected items in the mosaic dataset will be lost. This is the default.NO_RASTER—The rasters will not be rebuilt. Other primary fields will be reset if the update_fields parameter is set to UPDATE_FIELDS.This only affects items that will be synchronized. This parameter is not applicable if the new_items parameter is set to UPDATE_WITH_NEW_ITEMS. | Boolean |
| update_fields(Optional) | Specifies whether the fields in the table will be updated. This only affects items that will be synchronized.UPDATE_FIELDS—The fields will be updated from the source files. This is the default.NO_FIELDS—The fields in the table will not be updated from the source. If you update the fields, you can control which fields are updated using the fields_to_update parameter. If you made edits to some of the fields, you can remove them using the fields_to_update parameter. | Boolean |
| fields_to_update[field_to_update,...](Optional) | The fields that will be updated. This parameter is only valid if the update_fields parameter is set to UPDATE_FIELDS.If you made edits to some of the fields, make sure they are not listed.The RASTER field can be refreshed, even if REBUILD_RASTER is not specified. However, if REBUILD_RASTER is specified, the RASTER field will be rebuilt, even if the fields_to_update parameter value is not specified. | String |
| existing_items(Optional) | Specifies whether existing items in the mosaic dataset will be updated.If you use this parameter, choose which existing parameters to update: sync_only_stale, build_pyramids, calculate_statistics, build_thumbnails, build_item_cache, update_fields, or fields_to_update.UPDATE_EXISTING_ITEMS—The existing items will be updated with the parameters you chose to update. This is the default.IGNORE_EXISTING_ITEMS—The existing items will not be updated. | Boolean |
| broken_items(Optional) | Specifies whether items with broken links will be removed.Ensure that all network connections are working properly. This tool will remove any items that cannot be accessed.IGNORE_BROKEN_ITEMS—Items with broken links will not be removed from the mosaic dataset. This is the default.REMOVE_BROKEN_ITEMS—Items with broken links will be removed from the mosaic dataset. | Boolean |
| skip_existing_items(Optional) | Specifies whether existing mosaic dataset items will be skipped or updated with the modified files from disk. To use this parameter, the new_items parameter must be set to UPDATE_WITH_NEW_ITEMS.SKIP_EXISTING_ITEMS—While adding new mosaic dataset items, existing mosaic dataset items will be skipped; they will not be updated. This is the default.OVERWRITE_EXISTING_ITEMS—While adding new mosaic dataset items, existing mosaic dataset items that correspond to modified files on disk will be updated. | Boolean |
| refresh_aggregate_info(Optional) | Specifies whether data that may have been removed from the mosaic dataset will be included. To use this parameter, the existing_items parameter must be set to IGNORE_EXISTING_ITEMS.NO_REFRESH_INFO—When synchronizing, rasters that may have been removed from the mosaic dataset will be excluded. This is the default. REFRESH_INFO—When synchronizing, rasters that may have been removed from the mosaic dataset will be included. | Boolean |
| estimate_statistics | Specifies whether statistics on the mosaic dataset will be estimated. NO_STATISTICS—When synchronizing, statistics on the mosaic dataset will not be estimated. This is the default.ESTIMATE_STATISTICS—When synchronizing, statistics on the mosaic dataset will be estimated. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.SynchronizeMosaicDataset(in_mosaic_dataset, {where_clause}, {new_items}, {sync_only_stale}, {update_cellsize_ranges}, {update_boundary}, {update_overviews}, {build_pyramids}, {calculate_statistics}, {build_thumbnails}, {build_item_cache}, {rebuild_raster}, {update_fields}, {fields_to_update}, {existing_items}, {broken_items}, {skip_existing_items}, {refresh_aggregate_info}, estimate_statistics)
```

### Example 2

```python
import arcpy
arcpy.SynchronizeMosaicDataset_management(
    "c:/data/syncmd.gdb/md", "Year>1999", "NO_NEW_ITEMS", "SYNC_STALE",
    "#", "#", "#", "NO_PYRAMIDS", "NO_STATISTICS", "NO_THUMBNAILS", 
    "NO_ITEM_CACHE", "NO_RASTER", "NO_FIELDS", "#", "#")
```

### Example 3

```python
import arcpy
arcpy.SynchronizeMosaicDataset_management(
    "c:/data/syncmd.gdb/md", "Year>1999", "NO_NEW_ITEMS", "SYNC_STALE",
    "#", "#", "#", "NO_PYRAMIDS", "NO_STATISTICS", "NO_THUMBNAILS", 
    "NO_ITEM_CACHE", "NO_RASTER", "NO_FIELDS", "#", "#")
```

### Example 4

```python
# Synchronize source and add new data

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "syncmd.gdb/mdnew"
query = "#"
updatenew = "UPDATE_WITH_NEW_ITEMS"
syncstale = "SYNC_STALE"
updatecs = "#"
updatebnd = "#"
updateovr = "#"
buildpy = "NO_PYRAMIDS"
calcstats = "NO_STATISTICS"
buildthumb = "NO_THUMBNAILS"
buildcache = "NO_ITEM_CACHE"
updateras = "NO_RASTER"
updatefield = "NO_FIELDS"
fields = "#"

arcpy.SynchronizeMosaicDataset_management(
    mdname, query, updatenew, syncstale, updatecs, updatebnd, 
    updateovr, buildpy, calcstats, buildthumb, buildcache,
    updateras, updatefield, fields)
```

### Example 5

```python
# Synchronize source and add new data

import arcpy
arcpy.env.workspace = "C:/Workspace"

mdname = "syncmd.gdb/mdnew"
query = "#"
updatenew = "UPDATE_WITH_NEW_ITEMS"
syncstale = "SYNC_STALE"
updatecs = "#"
updatebnd = "#"
updateovr = "#"
buildpy = "NO_PYRAMIDS"
calcstats = "NO_STATISTICS"
buildthumb = "NO_THUMBNAILS"
buildcache = "NO_ITEM_CACHE"
updateras = "NO_RASTER"
updatefield = "NO_FIELDS"
fields = "#"

arcpy.SynchronizeMosaicDataset_management(
    mdname, query, updatenew, syncstale, updatecs, updatebnd, 
    updateovr, buildpy, calcstats, buildthumb, buildcache,
    updateras, updatefield, fields)
```

---

## Table Compare (Data Management)

## Summary

Compares two tables or table views and returns the comparison results.

## Usage

- This tool returns messages showing the comparison result. By default, it will stop executing after encountering the first miscompare. To report all differences, check on the Continue Comparison parameter.
- Table Compare can report differences and similarities with tabular values and field definitions.
- Multiple sort fields may be specified. Both the Input Base Table and Input Test Table are sorted based on the fields you specify. The first field is sorted, then the second field, and so on, in ascending order. Sorting by a common field in both the base and test table ensures that you are comparing the same row from each input dataset.
- By default the compare type is set to All. This means all properties of the tables being compared will be checked, including such things as field properties and attributes. However, you may choose a different compare type to check only specific properties of the tables being compared.
- The Ignore Options provide the flexibility to omit properties from the comparison. These properties include extension properties, subtypes, and relationship classes.
- When omitting fields that are not included in the field count comparison, the field definitions and tabular values for those fields are ignored.
- Attribute tolerances can only be specified for numeric field types.
- The Output Compare File will contain all similarities and differences between the Input Base Table and the Input Test Table. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different.
- When using this tool in Python, you can get the status of this tool using result.getOutput(1). The value will be 'true' when no differences are found and 'false' when differences are detected. Learn more about using tools in Python

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Base Table | The Input Base Table is compared with the Input Test Table. The Input Base Table refers to tabular data that you have declared valid. The base data has the correct field definitions and attribute values. | Table View; Raster Layer |
| Input Test Table | The Input Test Table is compared against the Input Base Table. The Input Test Table refers to data that you have made changes to by editing or compiling new fields, new records, or new attribute values. | Table View ; Raster Layer |
| Sort Field | The field or fields used to sort records in the Input Base Table and the Input Test Table. The records are sorted in ascending order. Sorting by a common field in both the Input Base Table and the Input Test Table ensures that you are comparing the same row from each input dataset. | Value Table |
| Compare Type(Optional) | The comparison type. ALL is the default. The default will compare all properties of the tables being compared.All—Compare all properties. This is the default.Attributes only—Only compare the attributes and their values.Schema only—Only compare the schema. | String |
| Ignore Options(Optional) | These properties will not be compared.Ignore extension properties—Do not compare extension properties.Ignore subtypes—Do not compare subtypes.Ignore relationship classes—Do not compare relationship classes.Ignore field alias—Do not compare field aliases. | String |
| Attribute Tolerance(Optional) | The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. | Value Table |
| Omit Fields(Optional) | The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. | String |
| Continue Comparison(Optional) | Indicates whether to compare all properties after encountering the first mismatch.Unchecked—Stop after encountering the first mismatch. This is the default.Checked—Compare other properties after encountering the first mismatch. | Boolean |
| Output Compare File(Optional) | This file will contain all similarities and differences between the Input Base Table and the Input Test Table. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. | File |
| in_base_table | The Input Base Table is compared with the Input Test Table. The Input Base Table refers to tabular data that you have declared valid. The base data has the correct field definitions and attribute values. | Table View; Raster Layer |
| in_test_table | The Input Test Table is compared against the Input Base Table. The Input Test Table refers to data that you have made changes to by editing or compiling new fields, new records, or new attribute values. | Table View ; Raster Layer |
| sort_field[sort_field,...] | The field or fields used to sort records in the Input Base Table and the Input Test Table. The records are sorted in ascending order. Sorting by a common field in both the Input Base Table and the Input Test Table ensures that you are comparing the same row from each input dataset. | Value Table |
| compare_type(Optional) | The comparison type. ALL is the default. The default will compare all properties of the tables being compared.ALL—Compare all properties. This is the default.ATTRIBUTES_ONLY—Only compare the attributes and their values.SCHEMA_ONLY—Only compare the schema. | String |
| ignore_options[ignore_options,...](Optional) | These properties will not be compared.IGNORE_EXTENSION_PROPERTIES—Do not compare extension properties.IGNORE_SUBTYPES—Do not compare subtypes.IGNORE_RELATIONSHIPCLASSES—Do not compare relationship classes.IGNORE_FIELDALIAS—Do not compare field aliases. | String |
| attribute_tolerances[[Field, {Tolerance}],...](Optional) | The numeric value that determines the range in which attribute values are considered equal. This only applies to numeric field types. | Value Table |
| omit_field[omit_field,...](Optional) | The field or fields that will be omitted during comparison. The field definitions and the tabular values for these fields will be ignored. | String |
| continue_compare(Optional) | Indicates whether to compare all properties after encountering the first mismatch.NO_CONTINUE_COMPARE—Stop after encountering the first mismatch. This is the default. CONTINUE_COMPARE—Compare other properties after encountering the first mismatch. | Boolean |
| out_compare_file(Optional) | This file will contain all similarities and differences between the in_base_table and the in_test_table. This file is a comma-delimited text file that can be viewed and used as a table in ArcGIS. | File |

## Code Samples

### Example 1

```python
arcpy.management.TableCompare(in_base_table, in_test_table, sort_field, {compare_type}, {ignore_options}, {attribute_tolerances}, {omit_field}, {continue_compare}, {out_compare_file})
```

### Example 2

```python
import arcpy
arcpy.management.TableCompare(
    r'c:\Workspace\wells.dbf', r'c:\Workspace\wells_new.dbf', 'WELL_ID', 
    'ALL', 'IGNORE_EXTENSION_PROPERTIES', 'WELL_DEPTH 0.001', '#', 
    'CONTINUE_COMPARE', r'C:\Workspace\well_compare.txt')
```

### Example 3

```python
import arcpy
arcpy.management.TableCompare(
    r'c:\Workspace\wells.dbf', r'c:\Workspace\wells_new.dbf', 'WELL_ID', 
    'ALL', 'IGNORE_EXTENSION_PROPERTIES', 'WELL_DEPTH 0.001', '#', 
    'CONTINUE_COMPARE', r'C:\Workspace\well_compare.txt')
```

### Example 4

```python
# Name: TableCompare.py
# Description: Compare two dBASE tables and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_table= "C:/Workspace/wells.dbf"
test_table = "C:/Workspace/wells_new.dbf"
sort_field = "WELL_ID"
compare_type = "ALL"
ignore_option = "IGNORE_EXTENSION_PROPERTIES"
attribute_tolerance = "WELL_DEPTH 0.001"
omit_field = "#"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/well_compare.txt"

# Process: FeatureCompare
compare_result = arcpy.management.TableCompare(
    base_table, test_table, sort_field, compare_type, ignore_option, 
    attribute_tolerance, omit_field, continue_compare, compare_file)
```

### Example 5

```python
# Name: TableCompare.py
# Description: Compare two dBASE tables and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_table= "C:/Workspace/wells.dbf"
test_table = "C:/Workspace/wells_new.dbf"
sort_field = "WELL_ID"
compare_type = "ALL"
ignore_option = "IGNORE_EXTENSION_PROPERTIES"
attribute_tolerance = "WELL_DEPTH 0.001"
omit_field = "#"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/well_compare.txt"

# Process: FeatureCompare
compare_result = arcpy.management.TableCompare(
    base_table, test_table, sort_field, compare_type, ignore_option, 
    attribute_tolerance, omit_field, continue_compare, compare_file)
```

---

## Table To Domain (Data Management)

## Summary

Creates or updates a coded value domain with values from a table.

## Usage

- A domain can also be created with the Create Domain tool.
- The standard delimiter for tabular text files with extensions .csv or .txt is a comma, and for files with a .tab extension, a tab. To use an input table with a nonstandard delimiter, you must first specify the correct delimiter used in the table using a schema.ini file.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The database table from which the domain values will be derived. | Table View |
| Code Field | The field in the database table from which the domain code values will be derived. | Field |
| Description Field | The field in the database table from which the domain description values will be derived. | Field |
| Input Workspace | The workspace that contains the domain that will be created or updated. | Workspace |
| Domain Name | The name of the domain that will be created or updated. | String |
| Domain Description(Optional) | The description of the domain that will be created or updated. Domain descriptions of existing domains are not updated. | String |
| Update Option(Optional) | Specifies how the domain will be updated when you're using an existing domain.Append the values—The values from the input table will be appended to the existing domain values. This is the default.Replace the values—The existing domain values will be replaced with the values from the input table. | String |
| in_table | The database table from which the domain values will be derived. | Table View |
| code_field | The field in the database table from which the domain code values will be derived. | Field |
| description_field | The field in the database table from which the domain description values will be derived. | Field |
| in_workspace | The workspace that contains the domain that will be created or updated. | Workspace |
| domain_name | The name of the domain that will be created or updated. | String |
| domain_description(Optional) | The description of the domain that will be created or updated. Domain descriptions of existing domains are not updated. | String |
| update_option(Optional) | Specifies how the domain will be updated when you're using an existing domain.APPEND—The values from the input table will be appended to the existing domain values. This is the default.REPLACE—The existing domain values will be replaced with the values from the input table. | String |

## Code Samples

### Example 1

```python
arcpy.management.TableToDomain(in_table, code_field, description_field, in_workspace, domain_name, {domain_description}, {update_option})
```

### Example 2

```python
import arcpy
from arcpy import env
env.workspace =  "C:/data"
arcpy.TableToDomain_management ("diameter.dbf",
                                "code",
                                "descript",
                                "montgomery.gdb",
                                "diameters",
                                "Valid pipe diameters")
```

### Example 3

```python
import arcpy
from arcpy import env
env.workspace =  "C:/data"
arcpy.TableToDomain_management ("diameter.dbf",
                                "code",
                                "descript",
                                "montgomery.gdb",
                                "diameters",
                                "Valid pipe diameters")
```

### Example 4

```python
# Name: TableToDomain.py
# Description: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"

#Set local parameters
domTable = "diameter.dbf"
codeField = "code"
descField = "descript"
dWorkspace = "Montgomery.gdb"
domName = "diameters"
domDesc = "Valid pipe diameters"

# Process: Create a domain from an existing table
arcpy.TableToDomain_management(domTable, codeField, descField, dWorkspace, domName, domDesc)
```

### Example 5

```python
# Name: TableToDomain.py
# Description: Update an attribute domain to constrain valid pipe material values

# Import system modules
import arcpy
 
# Set the workspace (to avoid having to type in the full path to the data every time)
arcpy.env.workspace = "C:/data"

#Set local parameters
domTable = "diameter.dbf"
codeField = "code"
descField = "descript"
dWorkspace = "Montgomery.gdb"
domName = "diameters"
domDesc = "Valid pipe diameters"

# Process: Create a domain from an existing table
arcpy.TableToDomain_management(domTable, codeField, descField, dWorkspace, domName, domDesc)
```

---

## Table To Ellipse (Data Management)

## Summary

Creates a feature class containing geodetic or planar ellipses from the values in an x-coordinate field, y-coordinate field, major axis and minor axis fields, and azimuth field of a table.

## Usage

- Output ellipses are constructed from field values. The field values include the following:The x- and y-coordinates of a center pointThe major and minor axis lengthsThe azimuth angleThe fields and their values will be included in the output.
- The x- and y-coordinates of a center point
- The major and minor axis lengths
- The azimuth angle
- When the output ellipsis are geodetic, the x- and y-coordinates and the lengths of the major and minor axes are measured on the surface of the earth, and the azimuth angle is measured from north. When the output ellipsis are planar, the x- and y-coordinates and the lengths of the major and minor axes are measured on the projected plane, and the azimuth angle is measured clockwise from grid north (vertical up on the map).
- A geodetic ellipse is a curve on the surface of the earth. However, a geodetic ellipse feature is not stored as a parametric (true) curve in the output; it is stored as a densified polyline representing the path of the geodetic ellipse.
- If you specify the same field for both the Major Field and Minor Field parameters, or if the values in both fields are equal, the output features represent geodetic circles.
- When the output is a feature class in a geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the Spatial Reference parameter, and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcGIS Pro Measure tool and choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement.
- By default, the output will be a line feature class. To create a polygon feature class, specify the Geometry Type parameter value as Polygon.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| Output Feature Class | The output feature class that will contain geodetic or planar ellipses. | Feature Class |
| X Field | A numerical field in the input table containing the x-coordinates (or longitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Y Field | A numerical field in the input table containing the y-coordinates (or latitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Major Field | A numerical field in the input table containing major axis lengths of the ellipses. | Field |
| Minor Field | A numerical field in the input table containing minor axis lengths of the ellipses. | Field |
| Distance Units | Specifies the units that will be used for the Major Field and Minor Field parameters.Meters—The units will be meters.Kilometers—The units will be kilometers.Miles—The units will be miles.Nautical miles—The units will be nautical miles.Feet—The units will be feet.U.S. survey feet—The units will be U.S. survey feet. | String |
| Azimuth Field (Optional) | A numerical field in the input table containing azimuth angle values for the major axis rotations of the output ellipses. The values are measured clockwise from north. | Field |
| Azimuth Units (Optional) | Specifies the units that will be used for the Azimuth Field parameter.Decimal degrees— The units will be decimal degrees. This is the default.Mils—The units will be mils.Radians—The units will be radians.Gradians—The units will be gradians. | String |
| ID (Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| Spatial Reference (Optional) | The spatial reference of the output feature class. The default is GCS_WGS_1984 or the input coordinate system if it is not Unknown. | Spatial Reference |
| Preserve attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.Unchecked—The remaining input fields will not be added to the output feature class. This is the default.Checked—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |
| Geometry Type(Optional) | Specifies the geometry type for the output feature class.Line—An output polyline feature class will be created. This is the default.Polygon—An output polygon feature class will be created. | String |
| Method(Optional) | Specifies whether the ellipse will be generated based on geodesic or planar measurements.Geodesic—A geodesic ellipse will be generated. The ellipse will accurately represent the shape on the surface of the earth. This is the default.Planar—A planar ellipse will be generated on the projected plane. It usually does not accurately represent the shape on the surface of the earth as a geodesic ellipse does. This option is not available for geographic coordinate systems. | String |
| in_table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| out_featureclass | The output feature class that will contain geodetic or planar ellipses. | Feature Class |
| x_field | A numerical field in the input table containing the x-coordinates (or longitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| y_field | A numerical field in the input table containing the y-coordinates (or latitudes) of the center points of ellipses to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| major_field | A numerical field in the input table containing major axis lengths of the ellipses. | Field |
| minor_field | A numerical field in the input table containing minor axis lengths of the ellipses. | Field |
| distance_units | Specifies the units that will be used for the major_field and minor_field parameters. METERS—The units will be meters.KILOMETERS—The units will be kilometers.MILES—The units will be miles.NAUTICAL_MILES—The units will be nautical miles.FEET—The units will be feet.US_SURVEY_FEET—The units will be U.S. survey feet. | String |
| azimuth_field(Optional) | A numerical field in the input table containing azimuth angle values for the major axis rotations of the output ellipses. The values are measured clockwise from north. | Field |
| azimuth_units(Optional) | Specifies the units that will be used for the azimuth_field parameter.DEGREES— The units will be decimal degrees. This is the default.MILS—The units will be mils.RADS—The units will be radians.GRADS—The units will be gradians. | String |
| id_field(Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| spatial_reference(Optional) | The spatial reference of the output feature class. A spatial reference can be specified as any of the following: The path to a .prj file, such as C:/workspace/watershed.prj The path to a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland A SpatialReference object, such as arcpy.SpatialReference("C:/data/Africa/Carthage.prj") | Spatial Reference |
| attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.NO_ATTRIBUTES—The remaining input fields will not be added to the output feature class. This is the default.ATTRIBUTES—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |
| geometry_type(Optional) | Specifies the geometry type for the output feature class.LINE—An output polyline feature class will be created. This is the default.POLYGON—An output polygon feature class will be created. | String |
| method(Optional) | Specifies whether the ellipse will be generated based on geodesic or planar measurements.GEODESIC—A geodesic ellipse will be generated. The ellipse will accurately represent the shape on the surface of the earth. This is the default.PLANAR—A planar ellipse will be generated on the projected plane. It usually does not accurately represent the shape on the surface of the earth as a geodesic ellipse does. This option is not available for geographic coordinate systems. | String |

## Code Samples

### Example 1

```python
arcpy.management.TableToEllipse(in_table, out_featureclass, x_field, y_field, major_field, minor_field, distance_units, {azimuth_field}, {azimuth_units}, {id_field}, {spatial_reference}, {attributes}, {geometry_type}, {method})
```

### Example 2

```python
# Import system modules
import arcpy

# Set local variables
input_table = r'c:\workspace\SGS\eltop.gdb\elret'
output_ellipse = r'c:\workspace\SGS\eltop.gdb\Eplyln_001'

# Run Table To Ellipse
arcpy.management.TableToEllipse(input_table, output_ellipse, 'lond', 'latd', 
                                'mjerr', 'mnerr', 'KILOMETERS', 'orient', 
                                'DEGREES', 'LinkID')
```

### Example 3

```python
# Import system modules
import arcpy

# Set local variables
input_table = r'c:\workspace\SGS\eltop.gdb\elret'
output_ellipse = r'c:\workspace\SGS\eltop.gdb\Eplyln_001'

# Run Table To Ellipse
arcpy.management.TableToEllipse(input_table, output_ellipse, 'lond', 'latd', 
                                'mjerr', 'mnerr', 'KILOMETERS', 'orient', 
                                'DEGREES', 'LinkID')
```

---

## Table To Relationship Class (Data Management)

## Summary

Creates an attributed relationship class from the origin, destination, and relationship tables.

## Usage

- This tool creates a table in the database containing the selected attribute fields of the relationship table. These fields store attributes of the relationship that are not attributed to either the origin or destination class. For example, in a parcel database, you may have a relationship class between parcels and owners in which owners own parcels and parcels are owned by owners. An attribute of that relationship may be percentage ownership.
- Simple or peer-to-peer relationships are relationships between two or more objects in the database that exist independently of each other. For example, in a railroad network, you may have railroad crossings that have one or more related signal lamps. However, a railroad crossing can exist without a signal lamp, and signal lamps exist on the railroad network where there are no railroad crossings. Simple relationships can have one-to-one, one-to-many, or many-to-many cardinality.
- A composite relationship is a relationship in which the lifetime of one object controls the lifetime of its related objects. For example, power poles support transformers and transformers are mounted on poles. Once a pole is deleted, a delete message is propagated to its related transformers, which are deleted from the transformers' feature class. Composite relationships are always one-to-many.
- Forward and backward path labels describe the relationship when navigating from one object to another. The forward path label describes the relationship navigated from the origin class to the destination class. In the pole-transformer example, a forward path label might be: Poles support transformers. The backward path label describes the relationship navigated from the destination to the origin class. In the pole-transformer example, a backward path label might be: Transformers are mounted on poles.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Origin Table | The table or feature class that will be associated to the destination table. | Table View |
| Destination Table | The table or feature class that will be associated to the origin table. | Table View |
| Output Relationship Class | The relationship class that will be created. | Relationship Class |
| Relationship Type | Specifies the type of association that will be created between the origin and destination tables.Simple—Each object will be independent of each other (a parent-to-parent relationship). This is the default. Composite—The lifetime of one object will control the lifetime of its related object (a parent-child relationship). | String |
| Forward Path Label | A label describing the relationship as it is traversed from the origin table or feature class to the destination table or feature class. | String |
| Backward Path Label | A label describing the relationship as it is traversed from the destination table or feature class to the origin table or feature class. | String |
| Message Direction | Specifies the direction messages that will be propagated between the objects in the relationship. For example, in a relationship between poles and transformers, when the pole is deleted, it sends a message to its related transformer objects informing them it was deleted.None (no messages propagated)—No messages will be propagated. This is the default. Forward (origin to destination)—Messages will be propagated from the origin to the destination. Backward (destination to origin)—Messages will be propagated from the destination to the origin. Both directions—Messages will be propagated from the origin to the destination and from the destination to the origin. | String |
| Cardinality | Specifies the cardinality of the relationship between the origin and destination.One to one (1:1)—Each object of the origin table or feature class can be related to zero or one object of the destination table or feature class. This is the default. One to many (1:M)—Each object of the origin table or feature class can be related to multiple objects in the destination table or feature class. Many to many (M:N)—Multiple objects of the origin table or feature class can be related to multiple objects in the destination table or feature class. | String |
| Relationship Table | The table containing attributes that will be added to the relationship class. | Table View |
| Attribute Fields | The names of the fields containing attribute values that will be added to the relationship class. The fields must be present in the Relationship Table parameter value. | Field |
| Origin Primary Key | The field in the origin table that will be used to create the relationship. | String |
| Origin Foreign Key | The name of the field in the relationship table that refers to the primary key field in the origin table or feature class. For table-based relationship classes, these values are used to populate the relationships in the relationship class so they cannot be null. | String |
| Destination primary key | The field in the destination table that will be used to create the relationship. | String |
| Destination Foreign Key | The field in the relationship table that refers to the primary key field in the destination table or feature class. For table-based relationship classes, these values are used to populate the relationships in the relationship class so they cannot be null. | String |
| origin_table | The table or feature class that will be associated to the destination table. | Table View |
| destination_table | The table or feature class that will be associated to the origin table. | Table View |
| out_relationship_class | The relationship class that will be created. | Relationship Class |
| relationship_type | Specifies the type of association that will be created between the origin and destination tables.SIMPLE—Each object will be independent of each other (a parent-to-parent relationship). This is the default. COMPOSITE—The lifetime of one object will control the lifetime of its related object (a parent-child relationship). | String |
| forward_label | A label describing the relationship as it is traversed from the origin table or feature class to the destination table or feature class. | String |
| backward_label | A label describing the relationship as it is traversed from the destination table or feature class to the origin table or feature class. | String |
| message_direction | Specifies the direction messages that will be propagated between the objects in the relationship. For example, in a relationship between poles and transformers, when the pole is deleted, it sends a message to its related transformer objects informing them it was deleted.NONE—No messages will be propagated. This is the default. FORWARD—Messages will be propagated from the origin to the destination. BACKWARD—Messages will be propagated from the destination to the origin. BOTH—Messages will be propagated from the origin to the destination and from the destination to the origin. | String |
| cardinality | Specifies the cardinality of the relationship between the origin and destination.ONE_TO_ONE—Each object of the origin table or feature class can be related to zero or one object of the destination table or feature class. This is the default. ONE_TO_MANY—Each object of the origin table or feature class can be related to multiple objects in the destination table or feature class. MANY_TO_MANY—Multiple objects of the origin table or feature class can be related to multiple objects in the destination table or feature class. | String |
| relationship_table | The table containing attributes that will be added to the relationship class. | Table View |
| attribute_fields[attribute_fields,...] | The names of the fields containing attribute values that will be added to the relationship class. The fields must be present in the relationship_table parameter value. | Field |
| origin_primary_key | The field in the origin table that will be used to create the relationship. | String |
| origin_foreign_key | The name of the field in the relationship table that refers to the primary key field in the origin table or feature class. For table-based relationship classes, these values are used to populate the relationships in the relationship class so they cannot be null. | String |
| destination_primary_key | The field in the destination table that will be used to create the relationship. | String |
| destination_foreign_key | The field in the relationship table that refers to the primary key field in the destination table or feature class. For table-based relationship classes, these values are used to populate the relationships in the relationship class so they cannot be null. | String |

## Code Samples

### Example 1

```python
arcpy.management.TableToRelationshipClass(origin_table, destination_table, out_relationship_class, relationship_type, forward_label, backward_label, message_direction, cardinality, relationship_table, attribute_fields, origin_primary_key, origin_foreign_key, destination_primary_key, destination_foreign_key)
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.TableToRelationshipClass_management("owners", "Parcels", "ownersParcels_RelClass",
                                          "SIMPLE", "Owns", "Is Owned By", "BACKWARD",
                                          "MANY_TO_MANY", "owners", ["OWNER_PERCENT", "DEED_DATE"],
                                          "OBJECTID", "owner_id", "OBJECTID", "parcel_id")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Montgomery.gdb"
arcpy.TableToRelationshipClass_management("owners", "Parcels", "ownersParcels_RelClass",
                                          "SIMPLE", "Owns", "Is Owned By", "BACKWARD",
                                          "MANY_TO_MANY", "owners", ["OWNER_PERCENT", "DEED_DATE"],
                                          "OBJECTID", "owner_id", "OBJECTID", "parcel_id")
```

### Example 4

```python
# Name: TableToRelationshipClass.py
# Description: Create an attributed relationship class between parcels
#              feature class and table with owner information
# Author: ESRI

# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy owners.dat to file gdb table, since both tables to be related
# must be in the same database
ownerDat = "owners.dat"
ownerTbl = "Montgomery.gdb/owners"
arcpy.CopyRows_management(ownerDat, ownerTbl)

# Create attributed relationship class between 'parcel' parcel layer
# and 'owner' table with additional parcel owner information
parcel = "Montgomery.gdb/Parcels"
relClass = "Montgomery.gdb/parcelowners_RelClass"
forLabel = "Owns"
backLabel = "Is Owned By"
attributeFields = ["OWNER_PERCENT", "DEED_DATE"]
originPK = "OBJECTID"
originFK = "owner_ID"
destinationPK = "OBJECTID"
destinationFK = "parcel_ID"
arcpy.TableToRelationshipClass_management(ownerTbl, parcel, relClass, "SIMPLE",
                                          forLabel, backLabel, "BACKWARD", "MANY_TO_MANY",
                                          ownerTbl, attributeFields, originPK, originFK,
                                          destinationPK, destinationFK)
```

### Example 5

```python
# Name: TableToRelationshipClass.py
# Description: Create an attributed relationship class between parcels
#              feature class and table with owner information
# Author: ESRI

# import system modules 
import arcpy
from arcpy import env

# Set environment settings
env.workspace = "C:/data"

# Copy owners.dat to file gdb table, since both tables to be related
# must be in the same database
ownerDat = "owners.dat"
ownerTbl = "Montgomery.gdb/owners"
arcpy.CopyRows_management(ownerDat, ownerTbl)

# Create attributed relationship class between 'parcel' parcel layer
# and 'owner' table with additional parcel owner information
parcel = "Montgomery.gdb/Parcels"
relClass = "Montgomery.gdb/parcelowners_RelClass"
forLabel = "Owns"
backLabel = "Is Owned By"
attributeFields = ["OWNER_PERCENT", "DEED_DATE"]
originPK = "OBJECTID"
originFK = "owner_ID"
destinationPK = "OBJECTID"
destinationFK = "parcel_ID"
arcpy.TableToRelationshipClass_management(ownerTbl, parcel, relClass, "SIMPLE",
                                          forLabel, backLabel, "BACKWARD", "MANY_TO_MANY",
                                          ownerTbl, attributeFields, originPK, originFK,
                                          destinationPK, destinationFK)
```

---

## TIN Compare (Data Management)

## Summary

Compares two TINs and returns the comparison results.

## Usage

- This tool returns messages showing the comparison result. By default, it will stop executing after encountering the first miscompare. To report all differences, check on the Continue Comparison parameter.
- TIN Compare can report differences with geometry, TIN node and triangle tags, and spatial reference.
- The Output Compare File will contain all similarities and differences between the Input Base TIN and the Input Test TIN. This file is a comma-delimited text file which can be viewed and used as a table in ArcGIS. For example, this table can be queried to obtain all the ObjectID values for all the rows that are different.
- When using this tool in Python, you can get the status of this tool using result.getOutput(1). The value will be 'true' when no differences are found and 'false' when differences are detected. Learn more about using tools in Python

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Base Tin | The Input Base Tin is compared with the Input Test Tin. Input Base Tin refers to data that you have declared valid. This base data has the correct geometry, tag values (if any), and spatial reference. | TIN Layer |
| Input Test Tin | The Input Test Tin is compared against the Input Base Tin. | TIN Layer |
| Compare Type(Optional) | The comparison type.All—This is the default. Properties only—Refers to both geometry and TIN tag values, if any, that are assigned to nodes and triangles. Spatial Reference only—Coordinate system information. | String |
| Continue Comparison(Optional) | Indicates whether to compare all properties after encountering the first mismatch.Unchecked—Stop after encountering the first mismatch. This is the default. Checked—Compare other properties after encountering the first mismatch. | Boolean |
| Output Compare File(Optional) | The name and path of the text file which will contain the comparison results. | File |
| in_base_tin | The Input Base Tin is compared with the Input Test Tin. Input Base Tin refers to data that you have declared valid. This base data has the correct geometry, tag values (if any), and spatial reference. | TIN Layer |
| in_test_tin | The Input Test Tin is compared against the Input Base Tin. | TIN Layer |
| compare_type(Optional) | The comparison type.ALL—This is the default. PROPERTIES_ONLY—Refers to both geometry and TIN tag values, if any, that are assigned to nodes and triangles. SPATIAL_REFERENCE_ONLY—Coordinate system information. | String |
| continue_compare(Optional) | Indicates whether to compare all properties after encountering the first mismatch.NO_CONTINUE_COMPARE—Stop after encountering the first mismatch. This is the default. CONTINUE_COMPARE—Compare other properties after encountering the first mismatch. | Boolean |
| out_compare_file(Optional) | The name and path of the text file which will contain the comparison results. | File |

## Code Samples

### Example 1

```python
arcpy.management.TINCompare(in_base_tin, in_test_tin, {compare_type}, {continue_compare}, {out_compare_file})
```

### Example 2

```python
import arcpy
arcpy.TINCompare_management(r'c:\Workspace\basetin', r'c:\Workspace\newtin', 
                            'ALL', 'CONTINUE_COMPARE', 
                            r'c:\Workspace\tincompare.txt')
```

### Example 3

```python
import arcpy
arcpy.TINCompare_management(r'c:\Workspace\basetin', r'c:\Workspace\newtin', 
                            'ALL', 'CONTINUE_COMPARE', 
                            r'c:\Workspace\tincompare.txt')
```

### Example 4

```python
# Description: Compare two TINs and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_tin= "C:/Workspace/basetin"
test_tin= "C:/Workspace/newtin"
compare_type = "ALL"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/tincompare.txt"

compare_result = arcpy.TINCompare_management(base_tin, test_tin, compare_type, 
                                             continue_compare, compare_file)
print(compare_result)
print(arcpy.GetMessages())
```

### Example 5

```python
# Description: Compare two TINs and return comparison result.

# import system modules 
import arcpy

# Set local variables
base_tin= "C:/Workspace/basetin"
test_tin= "C:/Workspace/newtin"
compare_type = "ALL"
continue_compare = "CONTINUE_COMPARE"
compare_file = "C:/Workspace/tincompare.txt"

compare_result = arcpy.TINCompare_management(base_tin, test_tin, compare_type, 
                                             continue_compare, compare_file)
print(compare_result)
print(arcpy.GetMessages())
```

---

## Transfer Files (Data Management)

## Summary

Transfers files between a file system and a cloud storage workspace.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Paths | The list of input files or folders that will be copied to the output folder. The path can be a file system path or cloud storage path where the .acs file can be used. | Raster Dataset; File; Folder |
| Output Folder | The output folder path where the files will be copied. | Folder |
| Filters (Optional) | A file pattern filter that will limit the number of files that need to be copied, such as .tif, .crf, and similar image file types. | String |
| input_paths[input_paths,...] | The list of input files or folders that will be copied to the output folder. The path can be a file system path or cloud storage path where the .acs file can be used. | Raster Dataset; File; Folder |
| output_folder | The output folder path where the files will be copied. | Folder |
| file_filter(Optional) | A file pattern filter that will limit the number of files that need to be copied, such as .tif, .crf, and similar image file types. | String |

## Code Samples

### Example 1

```python
arcpy.management.TransferFiles(input_paths, output_folder, {file_filter})
```

### Example 2

```python
import arcpy

#Transfer individual files
arcpy.management.TransferFiles(r"c:\test\raster.tif;c:\test\raster2.tif", r"c:\cloudstore\azurecloud.acsazfolder")
```

### Example 3

```python
import arcpy

#Transfer individual files
arcpy.management.TransferFiles(r"c:\test\raster.tif;c:\test\raster2.tif", r"c:\cloudstore\azurecloud.acsazfolder")
```

### Example 4

```python
#===========================
#Transfer files
'''Usage: TransferFiles_management(inputpaths;inputpaths..., outputfolder, {filefilter})'''

import arcpy

#Transfer folder of files with filter
input_folder = "c:\\test\\uploaddata"
output_foler = "c:\\clouconnection\\s3cloudstore.acs\\s3folder"
filter = "*.tif"

arcpy.management.TransferFiles(input_folder, output_foler, filter)
```

### Example 5

```python
#===========================
#Transfer files
'''Usage: TransferFiles_management(inputpaths;inputpaths..., outputfolder, {filefilter})'''

import arcpy

#Transfer folder of files with filter
input_folder = "c:\\test\\uploaddata"
output_foler = "c:\\clouconnection\\s3cloudstore.acs\\s3folder"
filter = "*.tif"

arcpy.management.TransferFiles(input_folder, output_foler, filter)
```

---

## Transform Field (Data Management)

## Summary

Transforms continuous values in one or more fields by applying mathematical functions to each value and changing the shape of the distribution. The transformation methods in the tool include log, square root, Box-Cox, multiplicative inverse, square, exponential, and inverse Box-Cox.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table or feature class containing the fields to be transformed. The newly transformed fields are added to the input table. | Table View; Raster Layer; Mosaic Layer |
| Field to Transform | The fields containing the values to be transformed. For each field, an output field name can be specified. If no output field name is provided, the tool creates an output field name using the field name and transformation method. | Value Table |
| Transformation Method (Optional) | Specifies the method that will be used to transform the values contained in the specified fields. Multiplicative Inverse—The multiplicative inverse (1/x) method will be applied to the original value (x) in the selected fields.Square root—The square root method will be applied to the original value in the selected fields.Log—The natural logarithmic function, log(x), will be applied to the original value (x) in the selected fields. Box-Cox—The Box-Cox power function will be applied to normally distribute the original values in the selected fields. This is the default.Inverse Box-Cox—The inverse of the Box-Cox transformation will be applied to the original values in the selected fields.Square (inverse square root)—The square method will be applied to the original values in the selected fields. This transformation is the inverse of square root.Exponential (inverse log)—The exponential function, exp(x), will be applied to the original value (x) in the selected fields. This transformation is the inverse of log. | String |
| Power (Optional) | The power parameter ( λ1) of the Box-Cox and inverse Box-Cox transformations. For the Box-Cox transformation, if no value is provided, an optimal value will be determined using maximum likelihood estimation (MLE). For the inverse Box-Cox transformation, a value must be provided. | Double |
| Shift (Optional) | The value that will be used to shift the data (a constant value is added). No shift is applied if 0 is specified. For log, Box-Cox, and square root transformations, a default shift value will be added before the transformation if there are negative or zero values. For exponential (inverse log), inverse Box-Cox, and square (inverse square root) transformations, no shift is applied by default. If a shift value is provided, the value is subtracted after the transformation is applied. This allows you to use the same shift value for transformations and their associated inverses. | Double |
| in_table | The input table or feature class containing the fields to be transformed. The newly transformed fields are added to the input table. | Table View; Raster Layer; Mosaic Layer |
| fields[[input_field, output_field_name],...] | The fields containing the values to be transformed. For each field, an output field name can be specified. If no output field name is provided, the tool creates an output field name using the field name and transformation method. | Value Table |
| method(Optional) | Specifies the method that will be used to transform the values contained in the specified fields. INVX—The multiplicative inverse (1/x) method will be applied to the original value (x) in the selected fields.SQRT—The square root method will be applied to the original value in the selected fields.LOG—The natural logarithmic function, log(x), will be applied to the original value (x) in the selected fields. BOX-COX—The Box-Cox power function will be applied to normally distribute the original values in the selected fields. This is the default.INV_BOX-COX—The inverse of the Box-Cox transformation will be applied to the original values in the selected fields.INV_SQRT—The square method will be applied to the original values in the selected fields. This transformation is the inverse of square root.INV_LOG—The exponential function, exp(x), will be applied to the original value (x) in the selected fields. This transformation is the inverse of log. | String |
| power(Optional) | The power parameter ( λ1) of the Box-Cox and inverse Box-Cox transformations. For the Box-Cox transformation, if no value is provided, an optimal value will be determined using maximum likelihood estimation (MLE). For the inverse Box-Cox transformation, a value must be provided. | Double |
| shift(Optional) | The value that will be used to shift the data (a constant value is added). No shift is applied if 0 is specified. For log, Box-Cox, and square root transformations, a default shift value will be added before the transformation if there are negative or zero values. For exponential (inverse log), inverse Box-Cox, and square (inverse square root) transformations, no shift is applied by default. If a shift value is provided, the value is subtracted after the transformation is applied. This allows you to use the same shift value for transformations and their associated inverses. | Double |

## Code Samples

### Example 1

```python
arcpy.management.TransformField(in_table, fields, {method}, {power}, {shift})
```

### Example 2

```python
import arcpy
arcpy.management.TransformField("County_Data", "Income", "LOG")
```

### Example 3

```python
import arcpy
arcpy.management.TransformField("County_Data", "Income", "LOG")
```

### Example 4

```python
# Import system modules. 
import arcpy 
 
try: 
    # Set the workspace and input features. 
    arcpy.env.workspace = r"C:\\Transform\\MyData.gdb" 
    inputFeatures = "County_Data" 
 
    # Set the input fields that will be standardized. 
    fields = "population_total;unemployment_rate;income" 
 
    # Set the standardization method. 
    method = "BOX-COX" 
 
    # Run the Transform Field tool. 
    arcpy.management.TransformField(inputFeatures, fields, method) 
 
except arcpy.ExecuteError: 
    # If an error occurred when running the tool, print the error message. 
    print(arcpy.GetMessages())
```

### Example 5

```python
# Import system modules. 
import arcpy 
 
try: 
    # Set the workspace and input features. 
    arcpy.env.workspace = r"C:\\Transform\\MyData.gdb" 
    inputFeatures = "County_Data" 
 
    # Set the input fields that will be standardized. 
    fields = "population_total;unemployment_rate;income" 
 
    # Set the standardization method. 
    method = "BOX-COX" 
 
    # Run the Transform Field tool. 
    arcpy.management.TransformField(inputFeatures, fields, method) 
 
except arcpy.ExecuteError: 
    # If an error occurred when running the tool, print the error message. 
    print(arcpy.GetMessages())
```

---

## Transpose Fields (Data Management)

## Summary

Switch data stored in fields or columns to rows in a new table or feature class.

## Usage

- By default, the output is a table. When the input is a feature class and you also want the output to be a feature class, you should add the Shape field in Attribute Fields.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input feature class or table containing data value fields to be transposed. | Table View |
| Fields To Transpose | The fields or columns containing data values in the input table that need to be transposed.Depending on your needs, you can select multiple fields to be transposed. The value here defines what the field name will be in the output. When not specified, the value is the same as the field name by default. However, you can also specify your own value. For example, if the field names to be transposed are Pop1991, Pop1992, and so on, by default, the values for these fields in the output will be the same (Pop1991, Pop1992, and so forth). However, you can choose to specify your own values such as 1991 and 1992. | Value Table |
| Output Table | The output feature class or table. The output will contain a transposed field, a value field, and any number of specified attribute fields that need to be inherited from the input table.By default the Output Table is a table. The output will be a feature class when the Input Table is a feature class and the Shape field is selected in the Attribute Fields parameter. | Table |
| Transposed Field | The name of the field that will be created to store field names of the transposed fields. Any valid field name can be used. | String |
| Value Field | The name of the field that will be created to store the corresponding values of the transposed fields. Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. | String |
| Attribute Fields(Optional) | Additional attribute fields from the input table to be included in the output table. If you want to output a feature class, add the Shape field. | Field |
| in_table | The input feature class or table containing data value fields to be transposed. | Table View |
| in_field[[field, {value}],...] | The fields or columns containing data values in the input table that need to be transposed.Depending on your needs, you can select multiple fields to be transposed. The value here defines what the field name will be in the output. When not specified, the value is the same as the field name by default. However, you can also specify your own value. For example, if the field names to be transposed are Pop1991, Pop1992, and so on, by default, the values for these fields in the output will be the same (Pop1991, Pop1992, and so forth). However, you can choose to specify your own values such as 1991 and 1992. | Value Table |
| out_table | The output feature class or table. The output will contain a transposed field, a value field, and any number of specified attribute fields that need to be inherited from the input table.By default the out_table is a table. The output will be a feature class when the in_table is a feature class and the Shape field is selected in the attribute_fields parameter. | Table |
| in_transposed_field_name | The name of the field that will be created to store field names of the transposed fields. Any valid field name can be used. | String |
| in_value_field_name | The name of the field that will be created to store the corresponding values of the transposed fields. Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. | String |
| attribute_fields[attribute_fields,...](Optional) | Additional attribute fields from the input table to be included in the output table. If you want to output a feature class, add the Shape field. | Field |

## Code Samples

### Example 1

```python
arcpy.management.TransposeFields(in_table, in_field, out_table, in_transposed_field_name, in_value_field_name, {attribute_fields})
```

### Example 2

```python
import arcpy
arcpy.TransposeFields_management("C:/Data/TemporalData.gdb/Input","Field1 newField1;Field2 newField2;Field3 newField3",
                                 "C:/Data/TemporalData.gdb/Output_Time","Transposed_Field", "Value","Shape;Type")
```

### Example 3

```python
import arcpy
arcpy.TransposeFields_management("C:/Data/TemporalData.gdb/Input","Field1 newField1;Field2 newField2;Field3 newField3",
                                 "C:/Data/TemporalData.gdb/Output_Time","Transposed_Field", "Value","Shape;Type")
```

### Example 4

```python
# Name: TransposeFields_Ex_02.py
# Description: Tranpose field names from column headers to values in one column
# Requirements: None

# Import system modules
import arcpy
from arcpy import env

# set workspace
arcpy.env.workspace = "C:/Data/TemporalData.gdb"

# Set local variables
inTable = "Input"
# Specify fields to transpose
fieldsToTranspose = "Field1 newField1;Field2 newField2;Field3 newField3"
# Set a variable to store output feature class or table
outTable = "Output_Time"
# Set a variable to store time field name
transposedFieldName = "Transposed_Field"
# Set a variable to store value field name
valueFieldName = "Value"
# Specify attribute fields to be included in the output
attrFields = "Shape;Type"

# Execute TransposeTimeFields
arcpy.TransposeFields_management(inTable, fieldsToTranspose, outTable, transposedFieldName, valueFieldName, attrFields)
```

### Example 5

```python
# Name: TransposeFields_Ex_02.py
# Description: Tranpose field names from column headers to values in one column
# Requirements: None

# Import system modules
import arcpy
from arcpy import env

# set workspace
arcpy.env.workspace = "C:/Data/TemporalData.gdb"

# Set local variables
inTable = "Input"
# Specify fields to transpose
fieldsToTranspose = "Field1 newField1;Field2 newField2;Field3 newField3"
# Set a variable to store output feature class or table
outTable = "Output_Time"
# Set a variable to store time field name
transposedFieldName = "Transposed_Field"
# Set a variable to store value field name
valueFieldName = "Value"
# Specify attribute fields to be included in the output
attrFields = "Shape;Type"

# Execute TransposeTimeFields
arcpy.TransposeFields_management(inTable, fieldsToTranspose, outTable, transposedFieldName, valueFieldName, attrFields)
```

---

## Transpose Time Fields (Data Management)

## Summary

Shifts fields from columns to rows in a table or feature class that have time as the field names. This tool is useful when your table or feature class stores time in field names (such as Pop1980, Pop1990, Pop2000, and so on), and you want to create time stamps for the feature class or table so that it can be animated through time.

## Usage

- If you want the output to be a table, you need to specify the input as a table.
- The output must be a geodatabase feature class. A shapefile is not a supported format for the output feature class.
- ObjectID (or OID, FID, and so on) and Shape fields should not be set as attribute fields.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Feature Class or Table | The input feature class or table for which time stamps will be created. | Table View |
| Fields to Transpose | The columns from the input table and the corresponding time values.Multiple strings can be entered, depending on how many fields you are transposing. Each string should be formatted as "Field_Name Time" (without the quotation marks). Each is a pair of substrings separated by a space. For example, the following string is a valid input: "POP1980 1980". In this example, POP1980 is the field name of a field containing population values for 1980. 1980 is the string that will be substituted for POP1980 and populated in the time field of the output table or feature class. | String |
| Output Feature Class or Table | The output feature class or table. The output table can be specified as a .dbf table, an info table, or a geodatabase table. The output feature class can only be stored in a geodatabase (shapefile is not available as a format for the output). The output feature class or table will contain a time field, a value field, and any number of attribute fields specified that need to be inherited from the input table. | Table |
| Time Field Name | The name of the time field that will be created to store time values. The default name is "Time". Any valid field name can be used. | String |
| Value Field Name | The name of the value field that will be created to store the values from the input table. The default name is "Value". Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. | String |
| Attribute Fields(Optional) | Attribute fields from the input table to be included in the output table. | Field |
| Input_Feature_Class_or_Table | The input feature class or table for which time stamps will be created. | Table View |
| Fields_to_Transpose[Fields_to_Transpose;Fields_to_Transpose...,...] | The columns from the input table and the corresponding time values.Multiple strings can be entered, depending on how many fields you are transposing. Each string should be formatted as "Field_Name Time" (without the quotation marks). Each is a pair of substrings separated by a space. For example, the following string is a valid input: "POP1980 1980". In this example, POP1980 is the field name of a field containing population values for 1980. 1980 is the string that will be substituted for POP1980 and populated in the time field of the output table or feature class. | String |
| Output_Feature_Class_or_Table | The output feature class or table. The output table can be specified as a .dbf table, an info table, or a geodatabase table. The output feature class can only be stored in a geodatabase (shapefile is not available as a format for the output). The output feature class or table will contain a time field, a value field, and any number of attribute fields specified that need to be inherited from the input table. | Table |
| Time_Field_Name | The name of the time field that will be created to store time values. The default name is "Time". Any valid field name can be used. | String |
| Value_Field_Name | The name of the value field that will be created to store the values from the input table. The default name is "Value". Any valid field name can be set, as long as it does not conflict with existing field names from the input table or feature class. | String |
| Attribute_Fields[Attribute_Fields;Attribute_Fields...,...](Optional) | Attribute fields from the input table to be included in the output table. | Field |

## Code Samples

### Example 1

```python
arcpy.management.TransposeTimeFields(Input_Feature_Class_or_Table, Fields_to_Transpose, Output_Feature_Class_or_Table, Time_Field_Name, Value_Field_Name, {Attribute_Fields})
```

### Example 2

```python
import arcpy
arcpy.TransposeTimeFields_management("c:/data/state_pop", "'Y1980 1980';'Y1981 1981';'Y1982 1982'",
                                     "c:/data/state_output", "Time", "Value", "STATE_NAME;AVG_ANUAL_")
```

### Example 3

```python
import arcpy
arcpy.TransposeTimeFields_management("c:/data/state_pop", "'Y1980 1980';'Y1981 1981';'Y1982 1982'",
                                     "c:/data/state_output", "Time", "Value", "STATE_NAME;AVG_ANUAL_")
```

### Example 4

```python
# Name: TransposeTimeFields_Ex_02.py
# Description: Tranpose time field names from column headers to time values in one column
# Requirements: None

# Import system modules
import arcpy
from arcpy import env

# set workspace
arcpy.env.workspace = "c:/data"

# Set local variables
inTable = "state_pop"
# Specify fields to transpose
fieldsToTranspose = "'Y1980 1980';'Y1981 1981';'Y1982 1982'"
# Set a variable to store output feature class or table
outTable = "state_output"
# Set a variable to store time field name
timeFieldName = "Time"
# Set a variable to store value field name
valueFieldName = "Value"
# Specify attribute fields to be included in the output
attrFields = "STATE_NAME;AVG_ANUAL_"

# Execute TransposeTimeFields
arcpy.TransposeTimeFields_management(inTable, fieldsToTranspose, outTable, timeFieldName, valueFieldName, attrFields)
```

### Example 5

```python
# Name: TransposeTimeFields_Ex_02.py
# Description: Tranpose time field names from column headers to time values in one column
# Requirements: None

# Import system modules
import arcpy
from arcpy import env

# set workspace
arcpy.env.workspace = "c:/data"

# Set local variables
inTable = "state_pop"
# Specify fields to transpose
fieldsToTranspose = "'Y1980 1980';'Y1981 1981';'Y1982 1982'"
# Set a variable to store output feature class or table
outTable = "state_output"
# Set a variable to store time field name
timeFieldName = "Time"
# Set a variable to store value field name
valueFieldName = "Value"
# Specify attribute fields to be included in the output
attrFields = "STATE_NAME;AVG_ANUAL_"

# Execute TransposeTimeFields
arcpy.TransposeTimeFields_management(inTable, fieldsToTranspose, outTable, timeFieldName, valueFieldName, attrFields)
```

---

## Trim Archive History (Data Management)

## Summary

Deletes retired archive records from nonversioned archive-enabled datasets.

## Usage

- IBM Db2
- Microsoft SQL Server
- Oracle
- PostgreSQL
- SAP HANA

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The nonversioned archive-enabled table with the archive history to be trimmed. | Table View |
| Trim Mode | Specifies the trim mode that will be used to trim the archive history.Delete—The archive records will be deleted.Note:At the current version of ArcGIS Pro, only the delete trim mode is available. | String |
| Trim Before Date (Optional) | Archive records older than this date and time will be deleted. The date and time must be in UTC. If no date is provided, all archive records will be deleted. | Date |
| in_table | The nonversioned archive-enabled table with the archive history to be trimmed. | Table View |
| trim_mode | Specifies the trim mode that will be used to trim the archive history.Note:At the current version of ArcGIS Pro, only the delete trim mode is available.DELETE—The archive records will be deleted. | String |
| trim_before_date(Optional) | Archive records older than this date and time will be deleted. The date and time must be in UTC. If no date is provided, all archive records will be deleted. | Date |

## Code Samples

### Example 1

```python
arcpy.management.TrimArchiveHistory(in_table, trim_mode, {trim_before_date})
```

### Example 2

```python
import arcpy
arcpy.management.TrimArchiveHistory("C:\\MyProject\\myGdb.sde\\mydatabase.user1.Parcels", 
                                    "DELETE", "2020/03/10 10:28:56 AM")
```

### Example 3

```python
import arcpy
arcpy.management.TrimArchiveHistory("C:\\MyProject\\myGdb.sde\\mydatabase.user1.Parcels", 
                                    "DELETE", "2020/03/10 10:28:56 AM")
```

---

## Truncate Table (Data Management)

## Summary

Removes all rows from a database table or feature class using truncate procedures in the database.

## Usage

- Supported data types are simple points, lines, or polygons stored in a database. Complex data types such as terrains, topologies, and network datasets are not supported as input.
- The input database table or feature class must be from a database connection established as the data owner.
- Versioned data is not supported as input. Data must be unregistered as versioned before the tool will run successfully.
- If the input table has archiving enabled, rows will still exist in the history table after running the tool. To remove these rows, you can disable archiving, run the tool, and enable archiving again.
- Truncate commands do not use database transactions and are unrecoverable. This improves performance over row-by-row deletion.
- Use this tool for workflows in which all rows will be removed from a table or feature class and there is no requirement to back up the transactions, such as the nightly reloading of data.
- This tool supports a feature service layer as input when connected as a user that can administer the service, and the supportsTruncate service property is true.
- This tool does not support feature classes with attachments. To delete rows from a feature class that has attachments, use the Delete Rows tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input database table or feature class that will be truncated. | Table View |
| in_table | The input database table or feature class that will be truncated. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.TruncateTable(in_table)
```

### Example 2

```python
import arcpy
arcpy.TruncateTable_management("neil/whistler.sde/function.junction.table")
```

### Example 3

```python
import arcpy
arcpy.TruncateTable_management("neil/whistler.sde/function.junction.table")
```

### Example 4

```python
# Name: TruncateTable_Example2.py
# Description: Truncates all tables in a file geodatabase.

# Import system modules
import arcpy

# Set the workspace.
arcpy.env.workspace = "C:/work/vancouver.gdb"

# Get a list of all the tables.
tableList = arcpy.ListTables()

# Loop through the list and run truncate
for table in tableList:
    arcpy.TruncateTable_management(table)
```

### Example 5

```python
# Name: TruncateTable_Example2.py
# Description: Truncates all tables in a file geodatabase.

# Import system modules
import arcpy

# Set the workspace.
arcpy.env.workspace = "C:/work/vancouver.gdb"

# Get a list of all the tables.
tableList = arcpy.ListTables()

# Loop through the list and run truncate
for table in tableList:
    arcpy.TruncateTable_management(table)
```

---

## Uncompress File Geodatabase Data (Data Management)

## Summary

Uncompresses all the contents in a geodatabase, all the contents in a feature dataset, or an individual stand-alone feature class or table.

## Usage

- When you uncompress a geodatabase, all feature classes and tables within it uncompress.
- When you uncompress a feature dataset, all its feature classes uncompress.
- You cannot individually compress or uncompress a feature class in a feature dataset to produce a mixed state where some feature classes are compressed and others are not. Compressed feature datasets allow you to add an uncompressed feature class through operations such as creating a new, empty feature class, copying and pasting, and importing. However, you cannot edit the uncompressed feature class if there are compressed feature classes in the same feature dataset. Once you've finished adding one or more uncompressed feature classes, you can recompress or uncompress the feature dataset so all its feature classes are either compressed or uncompressed.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input file geodatabase data | The geodatabase, feature dataset, feature class, or table to uncompress. | Workspace; Feature Dataset; Table View; Raster Layer; Geometric Network |
| Configuration keyword(Optional) | The configuration keyword defining how the data will store once uncompressed | String |
| in_data | The geodatabase, feature dataset, feature class, or table to uncompress. | Workspace; Feature Dataset; Table View; Raster Layer; Geometric Network |
| config_keyword(Optional) | The configuration keyword defining how the data will store once uncompressed | String |

## Code Samples

### Example 1

```python
arcpy.management.UncompressFileGeodatabaseData(in_data, {config_keyword})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.UncompressFileGeodatabaseData("london.gdb")
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data"
arcpy.management.UncompressFileGeodatabaseData("london.gdb")
```

### Example 4

```python
# Name: UncompressFileGeodatabaseData.py
# Description: Use the UncompressFileGeodatabaseData tool to uncompress a geodatabase

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"

# Process: Compress the data
arcpy.management.UncompressFileGeodatabaseData(geodatabase)
```

### Example 5

```python
# Name: UncompressFileGeodatabaseData.py
# Description: Use the UncompressFileGeodatabaseData tool to uncompress a geodatabase

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data"

# Set local variables
geodatabase = "london.gdb"

# Process: Compress the data
arcpy.management.UncompressFileGeodatabaseData(geodatabase)
```

---

## Unregister As Versioned (Data Management)

## Summary

Unregisters an enterprise geodatabase dataset as versioned.

## Usage

- The input dataset must be from a database connection established as the data owner.
- An exclusive lock is required on the dataset.
- Unregistering a branch versioned dataset without first posting all named versions to the default version may result in a loss of edits.
- Unregistering a traditional versioned dataset without first compressing the geodatabase may lead to loss of edited data.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The name of the dataset to be unregistered as versioned. | Table View; Feature Dataset |
| Do not run if there are versions with edits(Optional) | Specifies whether edits made to the versioned data will be maintained.Checked—When there are outstanding edits that could be lost, the tool will fail. Outstanding edits include edits in the delta tables (traditional versioned dataset) and edits in named versions (branch versioned dataset). This is the default. For traditional versioned datasets, do not use this option when compressing edits from the Default version in the Compress all edits in the Default version into the base table parameter.Unchecked—When there are outstanding edits, the tool will delete the edits. For traditional versioned datasets, use this option when compressing edits from the Default version in the Compress all edits in the Default version into the base table parameter. | Boolean |
| Compress all edits in the Default version into the base table(Optional) | Specifies whether edits will be compressed and unused data will be removed. This option is ignored if the Do not run if there are versions with edits parameter is checked.This parameter is only applicable for traditional versioned datasets.Checked—Edits in the Default version will be compressed to the base table.Unchecked—Any edits remaining in the delta tables will not be compressed. This is the default. | Boolean |
| in_dataset | The name of the dataset to be unregistered as versioned. | Table View; Feature Dataset |
| keep_edit(Optional) | Specifies whether edits made to the versioned data will be maintained.KEEP_EDIT—When there are outstanding edits that could be lost, the tool will fail. Outstanding edits include edits in the delta tables (traditional versioned dataset) and edits in named versions (branch versioned dataset). This is the default. For traditional versioned datasets, do not use this option when compressing edits from the Default version in the compress_default parameter.NO_KEEP_EDIT—When there are outstanding edits, the tool will delete the edits. For traditional versioned datasets, use this option when compressing edits from the Default version in the compress_default parameter. | Boolean |
| compress_default(Optional) | Specifies whether edits will be compressed and unused data will be removed. This option is ignored if the keep_edit parameter is set to KEEP_EDIT.This parameter is only applicable for traditional versioned datasets.COMPRESS_DEFAULT—Edits in the Default version will be compressed to the base table. NO_COMPRESS_DEFAULT—Any edits remaining in the delta tables will not be compressed. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.UnregisterAsVersioned(in_dataset, {keep_edit}, {compress_default})
```

### Example 2

```python
# Name: UnregisterAsVersioned_Example.py
# Description: Unregisters a dataset as versioned
# Author: ESRI

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/whistler@prod.sde/prod.GDB.ctgFuseFeature"

# Execute UnregisterAsVersioned
arcpy.UnregisterAsVersioned_management(datasetName,
                                       "NO_KEEP_EDIT",
                                       "COMPRESS_DEFAULT")
```

### Example 3

```python
# Name: UnregisterAsVersioned_Example.py
# Description: Unregisters a dataset as versioned
# Author: ESRI

# Import system modules
import arcpy

# Set local variables
datasetName = "c:/whistler@prod.sde/prod.GDB.ctgFuseFeature"

# Execute UnregisterAsVersioned
arcpy.UnregisterAsVersioned_management(datasetName,
                                       "NO_KEEP_EDIT",
                                       "COMPRESS_DEFAULT")
```

---

## Unregister Replica (Data Management)

## Summary

Unregisters a replica from an enterprise geodatabase.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Geodatabase | The enterprise geodatabase that contains the replica to unregister. | Workspace |
| Replica ID or Name | The name or ID of the replica that will be unregistered. If providing the replica name, it must be fully qualified, for example, myuser.myreplica. | String |
| in_geodatabase | The enterprise geodatabase that contains the replica to unregister. | Workspace |
| in_replica | The name or ID of the replica that will be unregistered. If providing the replica name, it must be fully qualified, for example, myuser.myreplica. | String |

## Code Samples

### Example 1

```python
arcpy.management.UnregisterReplica(in_geodatabase, in_replica)
```

### Example 2

```python
import arcpy
arcpy.management.UnregisterReplica("C:\\MyProject\\myConnection.sde", 
                                   "B6F7CAB6-B9E1-4B97-A67C-1499FF59CB7D")
```

### Example 3

```python
import arcpy
arcpy.management.UnregisterReplica("C:\\MyProject\\myConnection.sde", 
                                   "B6F7CAB6-B9E1-4B97-A67C-1499FF59CB7D")
```

### Example 4

```python
# Import modules
import arcpy

# Set local variables
sdeConnection = "C:\\MyProject\\myConnection.sde"

# Loop through all replicas and unregister each one
replicas = arcpy.da.ListReplicas(sdeConnection, True)
for replica in replicas:
    arcpy.management.UnregisterReplica(sdeConnection, replica.name)
```

### Example 5

```python
# Import modules
import arcpy

# Set local variables
sdeConnection = "C:\\MyProject\\myConnection.sde"

# Loop through all replicas and unregister each one
replicas = arcpy.da.ListReplicas(sdeConnection, True)
for replica in replicas:
    arcpy.management.UnregisterReplica(sdeConnection, replica.name)
```

---

## Unsplit Line (Data Management)

## Summary

Aggregates line features that have coincident endpoints and, optionally, common attribute values.

## Usage

- The attributes of the features that are aggregated by this tool can be summarized or described using a variety of statistics. The statistic used to summarize attributes is added to the output feature class as a single field with the naming standard of statistic type + underscore + input field name. For example, if the SUM statistic type is used on a field named POP, the output will include a field named SUM_POP.
- The availability of physical memory may limit the amount (and complexity) of input features that can be processed and aggregated into a single output line feature. This limitation may cause an error to occur, as the unsplit process may require more memory than is available. To prevent this, Unsplit Line may divide and process the input features using an adaptive tiling algorithm. To determine the features that have been tiled, run the Frequency tool on the result of this tool, specifying the same fields used in the Dissolve Field(s) parameter for the Frequency Field(s) parameter. Any record with a frequency value of 2 has been tiled. Tile boundaries are preserved in the output features to prevent the creation of features that are too large to be used by ArcGIS. Caution:Running Unsplit Line on the output of a previous unsplit process will rarely reduce the number of features in the output when the original processing divided and processed the inputs using adaptive tiling. The maximum size of any output feature is determined by the amount of available memory at run time; output containing tiles is an indicator that aggregating further with the available resources will cause an out-of-memory situation or result in a feature that is unusable. Additionally, running the Unsplit Line tool a second time on output that was created this way may result in slow performance for little to no gain and may cause an unexpected failure.
- Null values are excluded from all statistical calculations. For example, the average of 10, 5, and a null is 7.5 ((10 + 5) / 2). The count returns the number of values included in the statistical calculation, which in this case is 2.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Features | The line features to be aggregated. | Feature Layer |
| Output Feature Class | The feature class to be created that will contain the aggregated features. | Feature Class |
| Dissolve Fields(Optional) | The field or fields on which features will be aggregated. If no fields are specified, the tool will dissolve all features together. | Field |
| Statistics Fields(Optional) | Specifies the field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all calculations.By default, the tool will not calculate any statistics.Numeric attribute fields can be summarized using any statistic. Text attribute fields can be summarized using minimum, maximum, count, first, last, unique, concatenate, and mode statistics.Sum—The values for the specified field will be added together.Mean—The average for the specified field will be calculated.Minimum—The smallest value of the specified field will be identified.Maximum—The largest value of the specified field will be identified.Range—The range of values (maximum minus minimum) for the specified field will be calculated.Standard deviation—The standard deviation of values for the specified field will be calculated.Count—The number of values in the specified field will be identified.First—The specified field value of the first record in the input will be used.Last—The specified field value of the last record in the input will be used.Median—The median of the specified field will be calculated.Variance—The variance of the specified field will be calculated. Unique—The number of unique values of the specified field will be counted.Concatenate—The values for the specified field will be concatenated. The values can be separated using the Concatenation Separator parameter.Mode—The mode (the most common value) for the specified field will be identified. If more than one value is equally common, the lowest value will be returned. | Value Table |
| Concatenation Separator (Optional) | A character or characters that will be used to concatenate values when the Concatenation option is used for the Statistics Fields parameter. By default, the tool will concatenate values without a separator. | String |
| in_features | The line features to be aggregated. | Feature Layer |
| out_feature_class | The feature class to be created that will contain the aggregated features. | Feature Class |
| dissolve_field[dissolve_field,...](Optional) | The field or fields on which features will be aggregated. If no fields are specified, the tool will dissolve all features together. | Field |
| statistics_fields[[field, {statistic_type}],...](Optional) | Specifies the field or fields containing the attribute values that will be used to calculate the specified statistic. Multiple statistic and field combinations can be specified. Null values are excluded from all calculations.By default, the tool will not calculate any statistics.Numeric attribute fields can be summarized using any statistic. Text attribute fields can be summarized using minimum, maximum, count, first, last, unique, concatenate, and mode statistics.SUM—The values for the specified field will be added together.MEAN—The average for the specified field will be calculated.MIN—The smallest value of the specified field will be identified.MAX—The largest value of the specified field will be identified.RANGE—The range of values (maximum minus minimum) for the specified field will be calculated.STD—The standard deviation of values for the specified field will be calculated.COUNT—The number of values in the specified field will be identified.FIRST—The specified field value of the first record in the input will be used.LAST—The specified field value of the last record in the input will be used.MEDIAN—The median of the specified field will be calculated.VARIANCE—The variance of the specified field will be calculated. UNIQUE—The number of unique values of the specified field will be counted.CONCATENATE—The values for the specified field will be concatenated. The values can be separated using the concatenation_separator parameter.MODE—The mode (the most common value) for the specified field will be identified. If more than one value is equally common, the lowest value will be returned. | Value Table |
| concatenation_separator(Optional) | A character or characters that will be used to concatenate values when the CONCATENATION option is used for the statistics_fields parameter. By default, the tool will concatenate values without a separator. | String |

## Code Samples

### Example 1

```python
arcpy.management.UnsplitLine(in_features, out_feature_class, {dissolve_field}, {statistics_fields}, {concatenation_separator})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Streets"
arcpy.management.UnsplitLine("streets", "C:/output/output.gdb/streets_unsplit",
                             ["STREETNAME", "PREFIX"])
```

### Example 3

```python
import arcpy
arcpy.env.workspace = "C:/data/Portland.gdb/Streets"
arcpy.management.UnsplitLine("streets", "C:/output/output.gdb/streets_unsplit",
                             ["STREETNAME", "PREFIX"])
```

### Example 4

```python
# Name: UnsplitLine_Example2.py
# Description: Unsplit line features based on common attributes
 
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Portland.gdb/Streets"
 
# Set local variables
inFeatures = "streets"
outFeatureClass = "C:/output/output.gdb/streets_unsplit"
dissolveFields = ["STREETNAME", "PREFIX"]
 
# Run UnsplitLine using STREETNAME and PREFIX as Dissolve Fields
arcpy.management.UnsplitLine(inFeatures, outFeatureClass, dissolveFields)
```

### Example 5

```python
# Name: UnsplitLine_Example2.py
# Description: Unsplit line features based on common attributes
 
# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Portland.gdb/Streets"
 
# Set local variables
inFeatures = "streets"
outFeatureClass = "C:/output/output.gdb/streets_unsplit"
dissolveFields = ["STREETNAME", "PREFIX"]
 
# Run UnsplitLine using STREETNAME and PREFIX as Dissolve Fields
arcpy.management.UnsplitLine(inFeatures, outFeatureClass, dissolveFields)
```

---

## Update Data Loading Workspace Schema (Data Management)

## Summary

Creates a copy of the Data Loading Workspace and updates all the mapping and domain workbooks.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Data Reference Workbook | The path to Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |
| in_workbook | The path to Data Reference Workbook defining the data source, target, and mapping workbook paths. | File |

## Code Samples

### Example 1

```python
arcpy.management.UpdateDataLoadingWorkspace(in_workbook)
```

### Example 2

```python
import arcpy
arcpy.management.UpdateDataLoadingWorkspace("C:/data/DataLoadingWorkspace/DataReference.xlsx")
```

### Example 3

```python
import arcpy
arcpy.management.UpdateDataLoadingWorkspace("C:/data/DataLoadingWorkspace/DataReference.xlsx")
```

### Example 4

```python
# Name: UpdateDataLoadingWorkspace.py
# Description: Create a copy of a Data Loading Workspace and update all mapping
#              and domain workbooks.

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"

arcpy.management.UpdateDataLoadingWorkspace(in_workbook=workbook)
```

### Example 5

```python
# Name: UpdateDataLoadingWorkspace.py
# Description: Create a copy of a Data Loading Workspace and update all mapping
#              and domain workbooks.

# Import system modules
import arcpy

# Set local variables
workbook = "C:/data/DataLoadingWorkspace/DataReference.xlsx"

arcpy.management.UpdateDataLoadingWorkspace(in_workbook=workbook)
```

---

## Update Enterprise Geodatabase License (Data Management)

## Summary

Updates the ArcGIS Server license in an enterprise geodatabase.

## Usage

- The input workspace must be an enterprise geodatabase. This tool does not work with any other type of geodatabase.
- You must connect to the enterprise geodatabase as the geodatabase administrator to run this tool.
- To generate a license file for an enterprise geodatabases licensed with ArcGIS Enterprise on Kubernetes, use the exportGeodatabaseLicense REST operation.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Database Connection | A database connection (.sde file) to the enterprise geodatabase to authorize with a new ArcGIS Server enterprise authorization file.The database connection file must connect to the database as the geodatabase administrator. | Workspace |
| Authorization File | The path and file name of the keycodes file generated when ArcGIS Server enterprise was authorized. If necessary, copy the file from the ArcGIS Server machine to a directory that the tool can access. ArcGIS Server creates the keycodes file in the following location: \\Program Files\ESRI\License<release#>\sysgen (Microsoft Windows servers) or /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen (Linux servers). | File |
| input_database | A database connection (.sde file) to the enterprise geodatabase to authorize with a new ArcGIS Server enterprise authorization file.The database connection file must connect to the database as the geodatabase administrator. | Workspace |
| authorization_file | The path and file name of the keycodes file generated when ArcGIS Server enterprise was authorized. If necessary, copy the file from the ArcGIS Server machine to a directory that the tool can access. ArcGIS Server creates the keycodes file in the following location: \\Program Files\ESRI\License<release#>\sysgen (Microsoft Windows servers) or /arcgis/server/framework/runtime/.wine/drive_c/Program Files/ESRI/License<release#>/sysgen (Linux servers). | File |

## Code Samples

### Example 1

```python
arcpy.management.UpdateEnterpriseGeodatabaseLicense(input_database, authorization_file)
```

### Example 2

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
authorization_file = "C:\\temp\\keycodes"
arcpy.management.UpdateEnterpriseGeodatabaseLicense(ent_gdb, authorization_file)
```

### Example 3

```python
import arcpy
ent_gdb = "C:\\gdbs\\enterprisegdb.sde"
authorization_file = "C:\\temp\\keycodes"
arcpy.management.UpdateEnterpriseGeodatabaseLicense(ent_gdb, authorization_file)
```

### Example 4

```python
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "/usr/gdbs/enterprisegdb.sde"
authorization_file = "/usr/scratch/keycodes"

# Process: Import authorization information from a new keycodes file.
arcpy.management.UpdateEnterpriseGeodatabaseLicense(ent_gdb, authorization_file)
```

### Example 5

```python
# Import arcpy module
import arcpy

# Local variables:
ent_gdb = "/usr/gdbs/enterprisegdb.sde"
authorization_file = "/usr/scratch/keycodes"

# Process: Import authorization information from a new keycodes file.
arcpy.management.UpdateEnterpriseGeodatabaseLicense(ent_gdb, authorization_file)
```

---

## Update Geodatabase Connection Properties To Branch (Data Management)

## Summary

Updates an enterprise geodatabase connection to work with branch versioning.

## Usage

- To register a dataset as branch versioned, use this tool to update your existing database connection property, and then register the dataset as versioned. This is required when working with a utility network.
- The input connection file will remain updated to work with branch versioning unless the connection file is opened and modified. For example, if the connected user or password is altered, re-run the tool to update the connection file.
- The enterprise geodatabases that are supported by this tool are limited to the enterprise geodatabases that are supported by the utility network.
- Alternatively, you can use the Create Database Connection tool to create a new connection to a geodatabase and specify the branch version type.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Geodatabase Connection | The input enterprise geodatabase connection to update. | Workspace |
| input_database | The input enterprise geodatabase connection to update. | Workspace |

## Code Samples

### Example 1

```python
arcpy.management.UpdateGeodatabaseConnectionPropertiesToBranch(input_database)
```

### Example 2

```python
import arcpy
arcpy.UpdateGeodatabaseConnectionPropertiesToBranch_management("C:\\Projects\\MyUNProject\\UNOwnerConnection.sde")
```

### Example 3

```python
import arcpy
arcpy.UpdateGeodatabaseConnectionPropertiesToBranch_management("C:\\Projects\\MyUNProject\\UNOwnerConnection.sde")
```

---

## Update Interior Orientation (Data Management)

## Summary

Refines the interior orientation for each image in the mosaic dataset by constructing an affine transformation from a fiducial table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Mosaic Dataset | The mosaic dataset that is created from scanned aerial photos using the scanned raster type or frame camera raster type. | Mosaic Layer |
| Query Definition (Optional) | A query definition string that defines a subset of rasters for computing fiducials. | SQL Expression |
| Fiducial Table | The fiducial table created using the Compute Fiducials tool. | Table View |
| Film Coordinate System | Defines the film coordinate system of the scanned aerial photograph. It is used in computing fiducial information and affine transformation construction. No change—Maintain the coordinate system of the mosaic dataset. Do not change the film coordinate system of the scanned aerial photograph. Maintain the coordinate system of the mosaic dataset. X right, Y up—The origin of the scanned photo's coordinate system is the center, and positive X points right and positive Y points up.X up, Y left—The origin of the scanned photo's coordinate system is the center, and positive X points up and positive Y points left.X left, Y down—The origin of the scanned photo's coordinate system is the center, and positive X points left and positive Y points down. X down, Y right—The origin of the scanned photo's coordinate system is the center, and positive X points down and positive Y points right. | String |
| Update Footprints (Optional) | Generates or updates the footprints of the digital photos in the mosaic dataset.Checked—The footprints will be generated or updated.Unchecked—The footprints will not be generated or updated. This is the default | Boolean |
| in_mosaic_dataset | The mosaic dataset that is created from scanned aerial photos using the scanned raster type or frame camera raster type. | Mosaic Layer |
| where_clause(Optional) | A query definition string that defines a subset of rasters for computing fiducials. | SQL Expression |
| fiducial_table | The fiducial table created using the Compute Fiducials tool. | Table View |
| film_coordinate_system | Defines the film coordinate system of the scanned aerial photograph. It is used in computing fiducial information and affine transformation construction. NO_CHANGE—Maintain the coordinate system of the mosaic dataset. Do not change the film coordinate system of the scanned aerial photograph. Maintain the coordinate system of the mosaic dataset. X_RIGHT_Y_UP—The origin of the scanned photo's coordinate system is the center, and positive X points right and positive Y points up.X_UP_Y_LEFT—The origin of the scanned photo's coordinate system is the center, and positive X points up and positive Y points left.X_LEFT_Y_DOWN—The origin of the scanned photo's coordinate system is the center, and positive X points left and positive Y points down. X_DOWN_Y_RIGHT—The origin of the scanned photo's coordinate system is the center, and positive X points down and positive Y points right. | String |
| update_footprints(Optional) | Generates or updates the footprints of the digital photos in the mosaic dataset.UPDATE—The footprints will be generated or updated.NO_UPDATE—The footprints will not be generated or updated. This is the default | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.UpdateInteriorOrientation(in_mosaic_dataset, {where_clause}, fiducial_table, film_coordinate_system, {update_footprints})
```

### Example 2

```python
﻿import arcpy
arcpy.UpdateInteriorOrientation_management(
          "c:\\Test\\ortho.gdb\\orthoMD", "", "c:\\test\\fidducial.csv", 
          "X_DOWN_Y_RIGHT", "UPDATE")
```

### Example 3

```python
﻿import arcpy
arcpy.UpdateInteriorOrientation_management(
          "c:\\Test\\ortho.gdb\\orthoMD", "", "c:\\test\\fidducial.csv", 
          "X_DOWN_Y_RIGHT", "UPDATE")
```

### Example 4

```python
import arcpy

in_mosaic_dataset = "c:\\Test\\ortho.gdb\\orthoMD"
whereClause = ""
fiducialTable = "c:\\test\\fidducial.csv"
film_coordsys = "X_DOWN_Y_RIGHT"
update_footprints = "UPDATE"

arcpy.UpdateInteriorOrientation_management(in_mosaic_dataset, whereClause,
fiducialTable, film_coordsys, update_footprints)
```

### Example 5

```python
import arcpy

in_mosaic_dataset = "c:\\Test\\ortho.gdb\\orthoMD"
whereClause = ""
fiducialTable = "c:\\test\\fidducial.csv"
film_coordsys = "X_DOWN_Y_RIGHT"
update_footprints = "UPDATE"

arcpy.UpdateInteriorOrientation_management(in_mosaic_dataset, whereClause,
fiducialTable, film_coordsys, update_footprints)
```

---

## Update Portal Dataset Owner (Data Management)

## Summary

Updates the portal owner of a dataset to another user.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input dataset for which the portal owner will be updated. | Utility Network; Utility Network Layer; Trace Network; Trace Network Layer |
| Target Owner | The name of the portal user who will be the new portal owner of the dataset. | String |
| in_dataset | The input dataset for which the portal owner will be updated. | Utility Network; Utility Network Layer; Trace Network; Trace Network Layer |
| target_owner | The name of the portal user who will be the new portal owner of the dataset. | String |

## Code Samples

### Example 1

```python
arcpy.management.UpdatePortalDatasetOwner(in_dataset, target_owner)
```

### Example 2

```python
import arcpy
arcpy.UpdatePortalDatasetOwner_management(
    r"C:\MyProject\February\MyDatabase.sde\database.USER1.Electric\database.USER1.ElectricUN", 
    'gisadmin')
```

### Example 3

```python
import arcpy
arcpy.UpdatePortalDatasetOwner_management(
    r"C:\MyProject\February\MyDatabase.sde\database.USER1.Electric\database.USER1.ElectricUN", 
    'gisadmin')
```

---

## Upgrade Attachments (Data Management)

## Summary

Upgrades the attachments functionality on the data.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The feature class with attachments enabled. | Table View |
| in_dataset | The feature class with attachments enabled. | Table View |

## Code Samples

### Example 1

```python
arcpy.management.UpgradeAttachments(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.management.UpgradeAttachments("C:\\MyProject\\MyGDB.gdb\\MyFC")
```

### Example 3

```python
import arcpy
arcpy.management.UpgradeAttachments("C:\\MyProject\\MyGDB.gdb\\MyFC")
```

---

## Upgrade Dataset (Data Management)

## Summary

Upgrades the schema of a mosaic dataset, network dataset, annotation dataset, dimension dataset, parcel fabric, trace network, utility network, or 3D object feature class to the current ArcGIS release. Upgrading a dataset enables it to use new functionality in the current software release.

## Usage

- Before you upgrade a dataset, create a backup of it.
- Use the Upgrade Geodatabase tool to upgrade the geodatabase to the current release before running this tool to upgrade any dataset except a trace network or utility network.When upgrading ArcGIS Pro annotation and dimensions to ArcGIS Pro 3.0, the Upgrade Geodatabase tool does not need to be run.
- When upgrading ArcGIS Pro annotation and dimensions to ArcGIS Pro 3.0, the Upgrade Geodatabase tool does not need to be run.
- For utility networks, see Utility network dataset administration for additional requirements.
- For trace networks, see Trace network dataset administration for additional requirements.
- When upgrading a parcel fabric dataset, the following are required:An exclusive lock must be used, meaning that you must close active connections to the dataset, which may include stopping the parcel fabric layer service.The input parcel fabric must be from a database connection established as the database parcel fabric owner.
- An exclusive lock must be used, meaning that you must close active connections to the dataset, which may include stopping the parcel fabric layer service.
- The input parcel fabric must be from a database connection established as the database parcel fabric owner.
- Versioned annotation and dimension feature classes that are stored in an enterprise geodatabase must be unregistered as versioned prior to the upgrade.
- In an ArcGIS Desktop annotation feature class, graphics that are not text graphics are not supported by the upgrade process and will be deleted. A list of their OIDs will be created and included in the tool's messages.Learn more about moving the graphics out of the annotation feature class
- Once upgraded, an annotation dataset is no longer available for use in previous versions. The present version of an annotation feature class is displayed on the Source tab of the Feature Class Properties dialog box. The following table describes how annotation is supported:ArcGIS versionArcGIS Desktop annotationArcGIS Pro annotation 2.xArcGIS Pro annotation 3.xArcGIS Desktop 10.xFull read, modify, and write access.Not supported. Any feature class linked to it or in the same feature dataset will be read-only.Not supported. Any feature class linked to it or in the same feature dataset will be read-only.ArcGIS Pro1.xRead-only access.Not supported. Any feature class linked to it or in the same feature dataset will be read-only.Not supported. Any feature class linked to it or in the same feature dataset will be read-only.ArcGIS Pro2.xRead-only access.Full read, modify, and write access.Not supported. Any feature class linked to it or in the same feature dataset will be read-only.ArcGIS Pro 3.xRead-only access.Full read access. Partial modify and write access. It may be downgraded based on symbols and annotation properties used.Full read, modify, and write access. Consult the Companion ArcGIS Pro and ArcGIS Enterprise versions section in the Release notes when using annotation with ArcGIS Enterprise. The ArcGIS Enterprise version needs to match or be greater than the ArcGIS Pro version for use in GIS services. For instance, ArcGIS Pro 3.x annotation requires ArcGIS Enterprise 11.0 or higher.
- Once upgraded, a dimension dataset is no longer available for use in previous versions. The present version of a dimension feature class is displayed on the Source tab of the Feature Class Properties dialog box. The following table describes how annotation is supported:ArcGIS versionArcGIS Desktop dimensionsArcGIS Pro dimensions 2.xArcGIS Pro dimensions 3.xArcGIS Desktop 10.xFull read, modify, and write access.Not supported. Any feature class in the same feature dataset will be read-only.Not supported. Any feature class in the same feature dataset will be read-only.ArcGIS Pro1.xRead-only access.Not supported. Any feature class in the same feature dataset will be read-only.Not supported. Any feature class in the same feature dataset will be read-only.ArcGIS Pro2.xRead-only access.Full read, modify, and write access.Not supported. Any feature class in the same feature dataset will be read-only.ArcGIS Pro 3.xRead-only access.Full read access. Partial modify and write access. It may be downgraded based on symbols and dimension properties used.Full read, modify, and write access. Consult the Companion ArcGIS Pro and ArcGIS Enterprise versions section in the Release notes when using dimensions with ArcGIS Enterprise. The ArcGIS Enterprise version needs to match or be greater than the ArcGIS Pro version for use in GIS services. For instance, ArcGIS Pro 3.x dimensions require ArcGIS Enterprise 11.0 or higher.
- If a catalog dataset created in ArcGIS Pro 3.1 is upgraded in ArcGIS Pro 3.4 or later versions, the upgraded catalog dataset is no longer available for use in ArcGIS Pro 3.1.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Dataset to upgrade | The dataset that will be upgraded to the current ArcGIS client release. | Parcel Fabric Layer for ArcMap; Parcel Layer; Mosaic Layer; Network Dataset Layer; Trace Network Layer; Utility Network Layer; Annotation Layer; Dimension Layer; Catalog Layer; 3D Object Feature Layer |
| in_dataset | The dataset that will be upgraded to the current ArcGIS client release. | Parcel Fabric Layer for ArcMap; Parcel Layer; Mosaic Layer; Network Dataset Layer; Trace Network Layer; Utility Network Layer; Annotation Layer; Dimension Layer; Catalog Layer; 3D Object Feature Layer |

## Code Samples

### Example 1

```python
arcpy.management.UpgradeDataset(in_dataset)
```

### Example 2

```python
import arcpy
arcpy.UpgradeDataset_management("c:/Connections/city_data.sde/MontanaMD")
```

### Example 3

```python
import arcpy
arcpy.UpgradeDataset_management("c:/Connections/city_data.sde/MontanaMD")
```

---

## Upgrade Geodatabase (Data Management)

## Summary

Upgrades a geodatabase to the latest ArcGIS release to take advantage of new functionality.

## Usage

- This tool accepts a file geodatabase or an enterprise geodatabase connection file as input.
- You cannot upgrade enterprise geodatabases from ArcGIS Pro with a Basic license.
- Before you upgrade an enterprise geodatabase, you must perform the preparatory steps needed for the database management system you are using, including creating a backup of the database. For Oracle and for geodatabases in PostgreSQL that use the ST_Geometry type, you must download the DatabaseSupport folder from My Esri to obtain a new ST_Geometry library. For other preparatory steps, see the following topics:Upgrade a geodatabase in Db2Upgrade a geodatabase in OracleUpgrade a geodatabase in PostgreSQLUpgrade a geodatabase in SAP HANAUpgrade a geodatabase in SQL Server
- Upgrade a geodatabase in Db2
- Upgrade a geodatabase in Oracle
- Upgrade a geodatabase in PostgreSQL
- Upgrade a geodatabase in SAP HANA
- Upgrade a geodatabase in SQL Server
- You must check the Perform Pre-Requisite Check parameter, the Upgrade Geodatabase parameter, or both. You cannot run the tool until you check at least one of these parameters.
- Use the prerequisite check before upgrading the geodatabase to determine if it is ready for upgrading. If any prerequisites are not met, an error is logged in the geoprocessing history. This saves you from starting the upgrade only to have it fail due to one of these prerequisites not having been met. If any checks fail, you must correct the problem and restart the upgrade process.
- Upgrades from beta versions of the geodatabase are not supported.
- The Perform Pre-Requisite Check parameter runs different checks for each type of geodatabase. For file geodatabases, it determines if any of the following are true: The geodatabase is read-only. There are no other users connected to the geodatabase.The current connection is not editing data in the geodatabase. All the information in the current geodatabase system tables can be opened. For enterprise geodatabases, it determines if the following criteria are met: The connected user has the appropriate privileges to upgrade the geodatabase.The connected user is not editing data in the geodatabase. No other users are connected to the geodatabase. The database is enabled to support XML data types. All the information in the current geodatabase system tables can be opened. For geodatabases in Oracle and for geodatabases in PostgreSQL that use the ST_Geometry type, it detects whether the database can access the current version of the ST_Geometry library.
- The geodatabase is read-only.
- There are no other users connected to the geodatabase.
- The current connection is not editing data in the geodatabase.
- All the information in the current geodatabase system tables can be opened.
- For enterprise geodatabases, it determines if the following criteria are met: The connected user has the appropriate privileges to upgrade the geodatabase.The connected user is not editing data in the geodatabase. No other users are connected to the geodatabase. The database is enabled to support XML data types. All the information in the current geodatabase system tables can be opened. For geodatabases in Oracle and for geodatabases in PostgreSQL that use the ST_Geometry type, it detects whether the database can access the current version of the ST_Geometry library.
- The connected user has the appropriate privileges to upgrade the geodatabase.
- The connected user is not editing data in the geodatabase.
- No other users are connected to the geodatabase.
- The database is enabled to support XML data types.
- All the information in the current geodatabase system tables can be opened.
- For geodatabases in Oracle and for geodatabases in PostgreSQL that use the ST_Geometry type, it detects whether the database can access the current version of the ST_Geometry library.
- For enterprise geodatabases that contain branch versioned data, the upgrade process analyzes branch versions and branch versioned data for inconsistencies, and writes results of the analysis to a log file. See How Upgrade Geodatabase works for more information.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Geodatabase | The geodatabase that will be upgraded. When upgrading an enterprise geodatabase, specify a database connection file (.sde) that connects to the geodatabase as the geodatabase administrator. | Workspace |
| Perform Pre-Requisite Check | Specifies whether the prerequisite check will be run before upgrading the geodatabase.Unchecked—The prerequisite check will not be run. This is the default.Checked—The prerequisite check will be run before upgrading the geodatabase. | Boolean |
| Upgrade Geodatabase | Specifies whether the input geodatabase will be upgraded to match the release of the ArcGIS client that is running the tool.Unchecked—The geodatabase will not be upgraded. This is the default.Checked—The geodatabase will be upgraded. | Boolean |
| input_workspace | The geodatabase that will be upgraded. When upgrading an enterprise geodatabase, specify a database connection file (.sde) that connects to the geodatabase as the geodatabase administrator. | Workspace |
| input_prerequisite_check | Specifies whether the prerequisite check will be run before upgrading the geodatabase.NO_PREREQUISITE_CHECK—The prerequisite check will not be run. PREREQUISITE_CHECK—The prerequisite check will be run before upgrading the geodatabase. This is the default. | Boolean |
| input_upgradegdb_check | Specifies whether the input geodatabase will be upgraded.NO_UPGRADE—The geodatabase will not be upgraded.UPGRADE—The geodatabase will be upgraded. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.UpgradeGDB(input_workspace, input_prerequisite_check, input_upgradegdb_check)
```

### Example 2

```python
import arcpy
default_gdb = "C:/temp/Default.gdb"
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 3

```python
import arcpy
default_gdb = "C:/temp/Default.gdb"
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 4

```python
# Description: upgrade a geodatabase
 
# Import arcpy module
import arcpy

# Local variables:
default_gdb = "C:/temp/Default.gdb"

# Process: Upgrade Geodatabase
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 5

```python
# Description: upgrade a geodatabase
 
# Import arcpy module
import arcpy

# Local variables:
default_gdb = "C:/temp/Default.gdb"

# Process: Upgrade Geodatabase
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 6

```python
# Description: upgrade a geodatabase

# Import arcpy module
import arcpy

# Local variables:
default_gdb = "C:/connections/gdbconnection.sde"

# Process: Upgrade Geodatabase
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 7

```python
# Description: upgrade a geodatabase

# Import arcpy module
import arcpy

# Local variables:
default_gdb = "C:/connections/gdbconnection.sde"

# Process: Upgrade Geodatabase
arcpy.management.UpgradeGDB(default_gdb, "PREREQUISITE_CHECK", "UPGRADE")
```

### Example 8

```python
"""
Name: upgrade_gdb.py
Description: Provide connection information to an Enterprise geodatabase 
and upgrade the geodatabase
Type upgrade_gdb.py -h or upgrade_gdb.py --help for usage
"""

# Import system modules
import arcpy
import os
import optparse
import sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 2.0; valid for 10.1 only")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', 'DB2','INFORMIX','DB2ZOS',''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("--auth", dest="account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-u", dest="User", type="string", default="", help="Geodatabase administrator user name")
parser.add_option ("-p", dest="Password", type="string", default="", help="Geodatabase administrator password")
parser.add_option ("--upgrade", dest="Upgrade", type="choice", choices=['TRUE', 'FALSE'], default="FALSE", help="Upgrade Options (case-sensitive):  TRUE=Perform Pre-requisite check and upgrade geodatabase, FALSE=Perform Pre-requisite check only.  Default=FALSE")                   
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")


# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	
#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print("%s: error: %s\n" % (sys.argv[0], "No command options given"))
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection to upgrade
	account_authentication = options.account_authentication.upper()
	username = options.User.lower() 
	password = options.Password	
	do_upgrade = options.Upgrade
	database = options.Database.lower()
	database_type = options.Database_type.upper()
	instance = options.Instance
	
	if (database_type == ""):
		print("\nDatabase type must be specified!\n")
		parser.print_help()
		sys.exit(3)
	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level to upgrade
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + "  Enterprise geodatabase upgrade requires an ArcGIS Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS Server license.")
		sys.exit("Re-authorize ArcGIS before upgrading.")
	else:
		print("\n" + product_license + " license available!  Continuing to upgrade...")
		arcpy.AddMessage("+++++++++")
	
	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + username     
	
	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  
	
	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)
	
	print("\nCreating Database Connection File...\n")
	# Process: Create Database Connection File...
	# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, database, account_authentication, username, password, save_username_password(must be true)
	arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=username, password=password, save_user_pass="TRUE")
        for i in range(arcpy.GetMessageCount()):
		if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
			arcpy.AddReturnMessage(i)
			arcpy.AddMessage("\n+++++++++")
			arcpy.AddMessage("Exiting!!")
			arcpy.AddMessage("+++++++++\n")
			sys.exit(3)            
		else:
			arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
			
	# Check whether geodatabase needs upgrade
	isCurrent = arcpy.Describe(Connection_File_Name_full_path).currentRelease
	
	if isCurrent == True:
		print("The geodatabase is already at the current release and cannot be upgraded!")
		sys.exit("Upgrade did not run.")
	
	
	# Process: Upgrade geodatabase...
	try:
		if do_upgrade.lower() == "true":
			print("Upgrading Geodatabase...\n")
			arcpy.UpgradeGDB_management(input_workspace=Connection_File_Name_full_path, input_prerequisite_check="PREREQUISITE_CHECK", input_upgradegdb_check="UPGRADE")
			for i in range(arcpy.GetMessageCount()):
				arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
	
		else:
			print("Running Pre-Requisite Check...\n")
			arcpy.UpgradeGDB_management(input_workspace=Connection_File_Name_full_path, input_prerequisite_check="PREREQUISITE_CHECK", input_upgradegdb_check="NO_UPGRADE")
			for i in range(arcpy.GetMessageCount()):
				arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
		
	        
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)
	
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()
		parser.exit(2)
```

### Example 9

```python
"""
Name: upgrade_gdb.py
Description: Provide connection information to an Enterprise geodatabase 
and upgrade the geodatabase
Type upgrade_gdb.py -h or upgrade_gdb.py --help for usage
"""

# Import system modules
import arcpy
import os
import optparse
import sys


# Define usage and version
parser = optparse.OptionParser(usage = "usage: %prog [Options]", version="%prog 2.0; valid for 10.1 only")

#Define help and options
parser.add_option ("--DBMS", dest="Database_type", type="choice", choices=['SQLSERVER', 'ORACLE', 'POSTGRESQL', 'DB2','INFORMIX','DB2ZOS',''], default="", help="Type of enterprise DBMS:  SQLSERVER, ORACLE, or POSTGRESQL.")
parser.add_option ("-i", dest="Instance", type="string", default="", help="DBMS instance name")
parser.add_option ("--auth", dest="account_authentication", type ="choice", choices=['DATABASE_AUTH', 'OPERATING_SYSTEM_AUTH'], default='DATABASE_AUTH', help="Authentication type options (case-sensitive):  DATABASE_AUTH, OPERATING_SYSTEM_AUTH.  Default=DATABASE_AUTH")
parser.add_option ("-u", dest="User", type="string", default="", help="Geodatabase administrator user name")
parser.add_option ("-p", dest="Password", type="string", default="", help="Geodatabase administrator password")
parser.add_option ("--upgrade", dest="Upgrade", type="choice", choices=['TRUE', 'FALSE'], default="FALSE", help="Upgrade Options (case-sensitive):  TRUE=Perform Pre-requisite check and upgrade geodatabase, FALSE=Perform Pre-requisite check only.  Default=FALSE")                   
parser.add_option ("-D", dest="Database", type="string", default="none", help="Database name:  Not required for Oracle")


# Check if value entered for option
try:
	(options, args) = parser.parse_args()

	
#Check if no system arguments (options) entered
	if len(sys.argv) == 1:
		print("%s: error: %s\n" % (sys.argv[0], "No command options given"))
		parser.print_help()
		sys.exit(3)

	#Usage parameters for spatial database connection to upgrade
	account_authentication = options.account_authentication.upper()
	username = options.User.lower() 
	password = options.Password	
	do_upgrade = options.Upgrade
	database = options.Database.lower()
	database_type = options.Database_type.upper()
	instance = options.Instance
	
	if (database_type == ""):
		print("\nDatabase type must be specified!\n")
		parser.print_help()
		sys.exit(3)
	
	if (database_type == "SQLSERVER"):
		database_type = "SQL_SERVER"
	
	# Get the current product license
	product_license=arcpy.ProductInfo()
	
	# Checks required license level to upgrade
	if product_license.upper() == "ARCVIEW" or product_license.upper() == 'ENGINE':
		print("\n" + product_license + " license found!" + "  Enterprise geodatabase upgrade requires an ArcGIS Desktop Standard or Advanced, ArcGIS Engine with the Geodatabase Update extension, or ArcGIS Server license.")
		sys.exit("Re-authorize ArcGIS before upgrading.")
	else:
		print("\n" + product_license + " license available!  Continuing to upgrade...")
		arcpy.AddMessage("+++++++++")
	
	# Local variables
	instance_temp = instance.replace("\\","_")
	instance_temp = instance_temp.replace("/","_")
	instance_temp = instance_temp.replace(":","_")
	Conn_File_NameT = instance_temp + "_" + database + "_" + username     
	
	if os.environ.get("TEMP") == None:
		temp = "c:\\temp"	
	else:
		temp = os.environ.get("TEMP")
	
	if os.environ.get("TMP") == None:
		temp = "/usr/tmp"		
	else:
		temp = os.environ.get("TMP")  
	
	Connection_File_Name = Conn_File_NameT + ".sde"
	Connection_File_Name_full_path = temp + os.sep + Conn_File_NameT + ".sde"
	
	# Check for the .sde file and delete it if present
	arcpy.env.overwriteOutput=True
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)
	
	print("\nCreating Database Connection File...\n")
	# Process: Create Database Connection File...
	# Usage:  out_file_location, out_file_name, DBMS_TYPE, instnace, database, account_authentication, username, password, save_username_password(must be true)
	arcpy.CreateDatabaseConnection_management(out_folder_path=temp, out_name=Connection_File_Name, database_platform=database_type, instance=instance, database=database, account_authentication=account_authentication, username=username, password=password, save_user_pass="TRUE")
        for i in range(arcpy.GetMessageCount()):
		if "000565" in arcpy.GetMessage(i):   #Check if database connection was successful
			arcpy.AddReturnMessage(i)
			arcpy.AddMessage("\n+++++++++")
			arcpy.AddMessage("Exiting!!")
			arcpy.AddMessage("+++++++++\n")
			sys.exit(3)            
		else:
			arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
			
	# Check whether geodatabase needs upgrade
	isCurrent = arcpy.Describe(Connection_File_Name_full_path).currentRelease
	
	if isCurrent == True:
		print("The geodatabase is already at the current release and cannot be upgraded!")
		sys.exit("Upgrade did not run.")
	
	
	# Process: Upgrade geodatabase...
	try:
		if do_upgrade.lower() == "true":
			print("Upgrading Geodatabase...\n")
			arcpy.UpgradeGDB_management(input_workspace=Connection_File_Name_full_path, input_prerequisite_check="PREREQUISITE_CHECK", input_upgradegdb_check="UPGRADE")
			for i in range(arcpy.GetMessageCount()):
				arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
	
		else:
			print("Running Pre-Requisite Check...\n")
			arcpy.UpgradeGDB_management(input_workspace=Connection_File_Name_full_path, input_prerequisite_check="PREREQUISITE_CHECK", input_upgradegdb_check="NO_UPGRADE")
			for i in range(arcpy.GetMessageCount()):
				arcpy.AddReturnMessage(i)
			arcpy.AddMessage("+++++++++\n")
		
	        
	except:
		for i in range(arcpy.GetMessageCount()):
			arcpy.AddReturnMessage(i)
		
	if os.path.exists(Connection_File_Name_full_path):
		os.remove(Connection_File_Name_full_path)
	
#Check if no value entered for option	
except SystemExit as e:
	if e.code == 2:
		parser.usage = ""
		print("\n")
		parser.print_help()
		parser.exit(2)
```

---

## Upgrade Scene Layer (Data Management)

## Summary

Upgrades a scene layer package to the current I3S version in SLPK format or output to i3sREST format for use in ArcGIS Enterprise.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Dataset | The input scene layer package. | File |
| Output Folder | The location where the output scene layer package will be created or the cloud connection file (.acs) to output to i3sREST format. | Folder |
| Output Name | The name of the output scene layer. | String |
| Output Log File (Optional) | The output log file that will summarize the results of the evaluation. | File |
| Texture Optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the Desktop or None options. All—All texture formats, including JPEG, DXT, and KTX2, will be optimized for use in desktop, web, and mobile platforms.Desktop—Windows, Linux, and Mac supported textures, including JPEG and DXT, will be optimized for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.Mobile—Android and iOS supported textures, including JPEG and KTX2, will be optimized for use in ArcGIS Maps SDKs mobile applications.None—JPEG textures will be optimized for use in desktop and web platforms. | String |
| Date Format (Optional) | The format of the date values in the scene layers date fields. This parameter is hidden if no date fields are encountered. | String |
| in_dataset | The input scene layer package. | File |
| out_folder_path | The location where the output scene layer package will be created or the cloud connection file (.acs) to output to i3sREST format. | Folder |
| out_name | The name of the output scene layer. | String |
| out_log(Optional) | The output log file that will summarize the results of the evaluation. | File |
| texture_optimization(Optional) | Specifies the textures that will be optimized according to the target platform where the scene layer package is used.Caution:Optimizations that include KTX2 may take significant time to process. For fastest results, use the DESKTOP or NONE options. ALL—All texture formats, including JPEG, DXT, and KTX2, will be optimized for use in desktop, web, and mobile platforms.DESKTOP—Windows, Linux, and Mac supported textures, including JPEG and DXT, will be optimized for use in ArcGIS Pro clients on Windows and ArcGIS Maps SDKs desktop clients on Windows, Linux, and Mac. This is the default.MOBILE—Android and iOS supported textures, including JPEG and KTX2, will be optimized for use in ArcGIS Maps SDKs mobile applications.NONE—JPEG textures will be optimized for use in desktop and web platforms. | String |
| date_format(Optional) | The format of the date values in the scene layers date fields. This parameter is hidden if no date fields are encountered. | String |

## Code Samples

### Example 1

```python
arcpy.management.UpgradeSceneLayer(in_dataset, out_folder_path, out_name, {out_log}, {texture_optimization}, {date_format})
```

### Example 2

```python
import arcpy
arcpy.management.UpgradeSceneLayer(r"C:\temp\buildings.slpk, 
                                   r"C:\CloudConnections\AWS.acs", 
                                   "buildings_upgraded", 
                                   r"C:\temp\extracted\out.json", "DESKTOP")
```

### Example 3

```python
import arcpy
arcpy.management.UpgradeSceneLayer(r"C:\temp\buildings.slpk, 
                                   r"C:\CloudConnections\AWS.acs", 
                                   "buildings_upgraded", 
                                   r"C:\temp\extracted\out.json", "DESKTOP")
```

### Example 4

```python
import arcpy
arcpy.management.UpgradeSceneLayer(r"C:\temp\buildings.slpk, r"C:\packages", 
                                   "buildings_upgraded",  
                                   r"C:\temp\extracted\out.json", "NONE")
```

### Example 5

```python
import arcpy
arcpy.management.UpgradeSceneLayer(r"C:\temp\buildings.slpk, r"C:\packages", 
                                   "buildings_upgraded",  
                                   r"C:\temp\extracted\out.json", "NONE")
```

---

## Upload File To Portal (Data Management)

## Summary

Uploads a file to the active portal. Supported file types are .lyrx, .mapx, .pagx, .pdf, .rptt, .rptx, and .stylx.

## Usage

- Map (.mapx)
- Layer (.lyrx)
- Layout (.pagx)
- PDF (.pdf)
- Report (.rptx)
- Report template (.rptt)
- Style (.stylx)

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input File | The file that will be uploaded to the active portal. Supported file types are layer (.lyrx), layout (.pagx), map (.mapx), PDF (.pdf), report (.rptx), report template (.rptt), and style (.stylx). | File |
| Title | The title of the portal item. | String |
| Folder (Optional) | The name of an existing folder or a new folder on the portal. | String |
| Summary (Optional) | A short description of the item. | String |
| Tags (Optional) | The keywords or terms that describe the item. Separate individual tags with a comma. | String |
| Sharing Level (Optional) | Specifies the sharing level of the item.Owner—Only the owner of the item will have access.Organization—All members of the organization will have accessEveryone (public)—Everyone, including people outside the organization, will have access | String |
| Groups (Optional) | The groups with which the item will be shared. | String |
| in_file | The file that will be uploaded to the active portal. Supported file types are layer (.lyrx), layout (.pagx), map (.mapx), PDF (.pdf), report (.rptx), report template (.rptt), and style (.stylx). | File |
| title | The title of the portal item. | String |
| folder(Optional) | The name of an existing folder or a new folder on the portal. | String |
| summary(Optional) | A short description of the item. | String |
| tags(Optional) | The keywords or terms that describe the item. Separate individual tags with a comma. | String |
| sharing_level(Optional) | Specifies the sharing level of the item.OWNER—Only the owner of the item will have access.ORGANIZATION—All members of the organization will have accessEVERYONE—Everyone, including people outside the organization, will have access | String |
| groups[groups,...](Optional) | The groups with which the item will be shared. | String |

## Code Samples

### Example 1

```python
arcpy.management.UploadFileToPortal(in_file, title, {folder}, {summary}, {tags}, {sharing_level}, {groups})
```

### Example 2

```python
import arcpy
arcpy.management.UploadFileToPortal(r"C:\states.lyrx", "MyFile", "", 
                              "My Summary", "tag1, tag2", "EVERYONE", 
                              "MYGROUP")
```

### Example 3

```python
import arcpy
arcpy.management.UploadFileToPortal(r"C:\states.lyrx", "MyFile", "", 
                              "My Summary", "tag1, tag2", "EVERYONE", 
                              "MYGROUP")
```

---

## Validate Join (Data Management)

## Summary

Validates a join between two layers or tables to determine if the layers or tables have valid field names and Object ID fields, if the join produces matching records, if the join is a one-to-one or one-to-many join, and other properties of the join.

## Usage

- This tool will create messages to indicate characteristics of the validated join, including the following:The row count and match count of the specified join between the input layer or table view and the join table. If the join does not produce any matches, a warning message will be included. The row count and match count are also returned as derived outputs of the tool.If the join tables do not have Object IDs, a warning message will be included.The cardinality of the join (if it produces one-to-one results or one-to-many results).If the join fields have invalid starting characters or problematic characters anywhere in their names, a warning message will be included.If the join fields use reserved SQL keywords, a warning message will be included.The join fields are indexed or do not have an attribute index. Field indexes provide optimal performance for some data formats.The join tables are from the same workspace. When the input layer or table view and the join table are stored in the same workspace or database, the performance of the join will be considerably faster. Joins between tables in different databases is possible but performance will be reduced, as the database cannot be used to perform the join.
- The row count and match count of the specified join between the input layer or table view and the join table. If the join does not produce any matches, a warning message will be included. The row count and match count are also returned as derived outputs of the tool.
- If the join tables do not have Object IDs, a warning message will be included.
- The cardinality of the join (if it produces one-to-one results or one-to-many results).
- If the join fields have invalid starting characters or problematic characters anywhere in their names, a warning message will be included.
- If the join fields use reserved SQL keywords, a warning message will be included.
- The join fields are indexed or do not have an attribute index. Field indexes provide optimal performance for some data formats.
- The join tables are from the same workspace. When the input layer or table view and the join table are stored in the same workspace or database, the performance of the join will be considerably faster. Joins between tables in different databases is possible but performance will be reduced, as the database cannot be used to perform the join.
- This tool can produce an optional output table listing the problems found in the validated join. The output table will have the following fields:TYPE—A keyword used to indicate a specific characteristic or issue found in the validated join. Values include the following:KeywordDescription GPM_INVALID_CHARACTER_IN_NAMEThe field has an invalid character in its name. GPM_INVALID_FIRST_CHARACTERS_MSGThe field has an invalid first character. GPM_RESERVED_SQL_KEYWORDThe field includes reserved SQL keywords in its name. GPM_NO_MATCH_JOINThe join does not produce any matches. GPM_NO_OBJECTID_JOINThe layer or table view does not have an Object ID field. GPM_NOT_INDEX_FIELDThe field is not indexed. DIFFERENT_WORKSPACEThe tables are stored in different workspaces or databases.TABLE_NAME—The name of the table that produced the join validation messages or warning.FIELD_NAME—The name of the field in the input layer, table view, or join table that produced the join validation message or warning.DESC—Further description of the validation message or warning, including information about how to resolve a problem.
- TYPE—A keyword used to indicate a specific characteristic or issue found in the validated join. Values include the following:KeywordDescription GPM_INVALID_CHARACTER_IN_NAMEThe field has an invalid character in its name. GPM_INVALID_FIRST_CHARACTERS_MSGThe field has an invalid first character. GPM_RESERVED_SQL_KEYWORDThe field includes reserved SQL keywords in its name. GPM_NO_MATCH_JOINThe join does not produce any matches. GPM_NO_OBJECTID_JOINThe layer or table view does not have an Object ID field. GPM_NOT_INDEX_FIELDThe field is not indexed. DIFFERENT_WORKSPACEThe tables are stored in different workspaces or databases.
- TABLE_NAME—The name of the table that produced the join validation messages or warning.
- FIELD_NAME—The name of the field in the input layer, table view, or join table that produced the join validation message or warning.
- DESC—Further description of the validation message or warning, including information about how to resolve a problem.
- The cardinality of the join (whether it is one-to-one or one-to-many) will be validated by this tool. Records from the join table can be matched to more than one record in the input layer or table view. Likewise, multiple records from the join table can be matched to one record in the input layer or table view, producing a one-to-many join.
- When a one-to-many join is produced by the join, the result of the join can be viewed in the attribute table, where a warning message will indicate if the table has duplicate object IDs. Because many geoprocessing tools do not support data with duplicate object IDs and processing such data can produce unexpected results, it is recommended that you first copy the joined layer to a new feature class using the Export Features tool. Then use the new feature class as input to other geoprocessing tools.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Layer or Table View | The layer or table view with the join to the join table that will be validated. | Mosaic Layer; Raster Layer; Table View |
| Input Join Field | The field in the input layer or table view on which the join will be based. | Field |
| Join Table | The table or table view with the join to the input layer or table view that will be validated. | Mosaic Layer; Raster Layer; Table View |
| Join Table Field | The field in the join table that contains the values on which the join will be based. | Field |
| Output Table(Optional) | The output table containing the validation messages in a tabular form. | Table |
| in_layer_or_view | The layer or table view with the join to the join table that will be validated. | Mosaic Layer; Raster Layer; Table View |
| in_field | The field in the input layer or table view on which the join will be based. | Field |
| join_table | The table or table view with the join to the input layer or table view that will be validated. | Mosaic Layer; Raster Layer; Table View |
| join_field | The field in the join table that contains the values on which the join will be based. | Field |
| output_msg(Optional) | The output table containing the validation messages in a tabular form. | Table |

## Code Samples

### Example 1

```python
arcpy.management.ValidateJoin(in_layer_or_view, in_field, join_table, join_field, {output_msg})
```

### Example 2

```python
import arcpy
arcpy.management.ValidateJoin("vegetation", "HOLLAND95", "vegtable", "HOLLAND95")
arcpy.management.AddJoin("vegetation", "HOLLAND95", "vegtable", "HOLLAND95")
```

### Example 3

```python
import arcpy
arcpy.management.ValidateJoin("vegetation", "HOLLAND95", "vegtable", "HOLLAND95")
arcpy.management.AddJoin("vegetation", "HOLLAND95", "vegtable", "HOLLAND95")
```

### Example 4

```python
# Name: AttributeJoin.py
# Purpose: Join a table to a feature class and find one-to-many matches

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
arcpy.env.qualifiedFieldNames = False

# Set local variables
inFeatures = "vegtype"
joinTable = "vegtable"
joinField = "HOLLAND95"  # Both tables have HOLLAND95 field
outFeatures = "Vegtype_Joined"

# Join the feature layer to a table
val_res = arcpy.management.ValidateJoin(inFeatures, joinField, joinTable, joinField)
matched = int(val_res[0]) 
row_count = int(val_res[1])

print(arcpy.GetMessages())  # Tool messages about the Join

# Validate the join returns matched rows before proceeding
if matched >= 1:
    joined = arcpy.management.AddJoin(inFeatures, joinField, joinTable, joinField)

    # Copy the joined layer to a new permanent feature class
    arcpy.management.CopyFeatures(joined, outFeatures)

print(f"Output Features: {outFeatures} had matches {matched} and created {row_count} records")
```

### Example 5

```python
# Name: AttributeJoin.py
# Purpose: Join a table to a feature class and find one-to-many matches

# Import system modules
import arcpy

# Set environment settings
arcpy.env.workspace = "C:/data/Habitat_Analysis.gdb"
arcpy.env.qualifiedFieldNames = False

# Set local variables
inFeatures = "vegtype"
joinTable = "vegtable"
joinField = "HOLLAND95"  # Both tables have HOLLAND95 field
outFeatures = "Vegtype_Joined"

# Join the feature layer to a table
val_res = arcpy.management.ValidateJoin(inFeatures, joinField, joinTable, joinField)
matched = int(val_res[0]) 
row_count = int(val_res[1])

print(arcpy.GetMessages())  # Tool messages about the Join

# Validate the join returns matched rows before proceeding
if matched >= 1:
    joined = arcpy.management.AddJoin(inFeatures, joinField, joinTable, joinField)

    # Copy the joined layer to a new permanent feature class
    arcpy.management.CopyFeatures(joined, outFeatures)

print(f"Output Features: {outFeatures} had matches {matched} and created {row_count} records")
```

---

## Validate Scene Layer (Data Management)

## Summary

Evaluates a scene layer package (*.slpk or *.i3sREST) in a cloud store to determine its conformity to I3S specifications.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Scene Layer (Optional) | The scene layer package (*.slpk) that will be evaluated. | File |
| Output Log File | The output log file that will summarize the results of the evaluation. | File |
| Input Folder (Optional) | The scene layer content (*.i3sREST) in a cloud store that will be evaluated. | Folder |
| in_slpk(Optional) | The scene layer package (*.slpk) that will be evaluated. | File |
| out_report | The output log file that will summarize the results of the evaluation. | File |
| in_folder(Optional) | The scene layer content (*.i3sREST) in a cloud store that will be evaluated. | Folder |

## Code Samples

### Example 1

```python
arcpy.management.ValidateSceneLayerPackage({in_slpk}, out_report, {in_folder})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = 'C:/Data'
arcpy.management.ValidateSceneLayerPackage('street_furniture.slpk', 'validate_report.json')
```

### Example 3

```python
import arcpy
arcpy.env.workspace = 'C:/Data'
arcpy.management.ValidateSceneLayerPackage('street_furniture.slpk', 'validate_report.json')
```

### Example 4

```python
import arcpy
arcpy.env.workspace = 'C:/Data'
arcpy.management.ValidateSceneLayerPackage(None, 'validate_report.json',
                                           'C:/cloud_connections/AWS.acs/mySceneLayer.i3srest')
```

### Example 5

```python
import arcpy
arcpy.env.workspace = 'C:/Data'
arcpy.management.ValidateSceneLayerPackage(None, 'validate_report.json',
                                           'C:/cloud_connections/AWS.acs/mySceneLayer.i3srest')
```

---

## Validate Topology (Data Management)

## Summary

Validates a geodatabase topology.

## Usage

- Specific portions of a topology can be validated using the Extent environment and checking the Use extent environment parameter.
- This tool will only process dirty areas. To learn more about dirty areas, see Topology in ArcGIS.
- Starting at ArcGIS Pro 2.6, the input topology layer can be from a topology service if the service is published with ArcGIS Enterprise 10.8.1 or later. If the layer has been added to the Contents pane, you can drag the layer to the Input Topology parameter, or you can provide the URL of the topology layer feature service, for example, https://myserver.mydomain.com/server/rest/services/myTopoService/FeatureServer/0.
- When the input topology is from a feature service, this tool performs the validate process asynchronously.License:When working with branch versioning in an ArcGIS Enterprise 11.2 or later deployment, organization members must be assigned a license for the ArcGIS Advanced Editing user type extension to use this tool.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Topology | The geodatabase topology that will be validated. | Topology Layer |
| Use extent environment(Optional) | Specifies whether the Extent environment value for the topology or the full extent of the topology will be validated. Checked—The Extent environment value for the topology will be validated. If the environment has not been set, the full extent will be validated.Unchecked—The full extent of the topology will be validated. This is the default. | Boolean |
| in_topology | The geodatabase topology that will be validated. | Topology Layer |
| visible_extent(Optional) | Specifies whether the current visible extent of the map or the full extent of the topology will be validated. Visible_Extent— The current visible extent of the map will be validated.Full_Extent—The full extent of the topology will be validated. This is the default. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.ValidateTopology(in_topology, {visible_extent})
```

### Example 2

```python
import arcpy
arcpy.management.ValidateTopology("D:/Calgary/Trans.gdb/Streets/Street_Topo")
```

### Example 3

```python
import arcpy
arcpy.management.ValidateTopology("D:/Calgary/Trans.gdb/Streets/Street_Topo")
```

### Example 4

```python
# Name: ValidateTopology_Example.py
# Description: Validates a topology using the environment extent (visible map display)

import arcpy
processing_extent = arcpy.Extent(
    2357129.94288309, 1424105.53783632, 2573455.6694351, 1663482.36639798,
    spatial_reference=arcpy.SpatialReference(102758, vcs=5703)
)

# Use EnvManager to temporarily set the Extent environment for the duration of
# the with block.
with arcpy.EnvManager(extent=processing_extent):
    arcpy.management.ValidateTopology("D:/Calgary/Transport.gdb/Streets/Street_Topo")
```

### Example 5

```python
# Name: ValidateTopology_Example.py
# Description: Validates a topology using the environment extent (visible map display)

import arcpy
processing_extent = arcpy.Extent(
    2357129.94288309, 1424105.53783632, 2573455.6694351, 1663482.36639798,
    spatial_reference=arcpy.SpatialReference(102758, vcs=5703)
)

# Use EnvManager to temporarily set the Extent environment for the duration of
# the with block.
with arcpy.EnvManager(extent=processing_extent):
    arcpy.management.ValidateTopology("D:/Calgary/Transport.gdb/Streets/Street_Topo")
```

---

## Warp From File (Data Management)

## Summary

Transforms a raster dataset using an existing text file containing source and target control points.

## Usage

- Warp is useful when the raster requires a systematic geometric correction that can be modeled with a polynomial. A spatial transformation can invert or remove a distortion using polynomial transformation of the proper order. The higher the order, the more complex the distortion that can be corrected. The higher orders of polynomial will involve progressively more processing time.
- The default polynomial order will perform an affine transformation.
- To determine the minimum number of links necessary for a given order of polynomial, use the following formula:n = (p + 1) (p + 2) / 2where n is the minimum number of links required for a transformation of polynomial order p. It is suggested that you use more than the minimum number of links.
- This tool will determine the extent of the warped raster and will set the number of rows and columns to be about the same as in the input raster. Some minor differences may be due to the changed proportion between the output raster's sizes in the x and y directions. The default cell size used will be computed by dividing the extent by the previously determined number of rows and columns. The value of the cell size will be used by the resampling algorithm.If you choose to define an output cell size in the Environment settings, the number of rows and columns will be calculated as follows:
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- Each row in the input link file should have the following values, each delimited by a TAB:<From X> <From Y> <To X> <To Y>where each row represents the coordinates of a control point pair. There can be additional columns with residual values, but these are not required.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster to be transformed. | Mosaic Layer; Raster Layer |
| Output Raster Dataset | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality using environment settings..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| Link File | The text, CSV file, or TAB file containing the coordinates to warp the input raster. This can be generated from the Register Raster tool or from the Georeferencing tab. | Text File |
| Transformation Type (Optional) | Specifies the transformation method for shifting the raster dataset.Shift only— A zero-order polynomial will be used to shift the data. This is commonly used when the data is georeferenced, but a small shift will better line it up. Only one link is required to perform a zero-order polynomial shift.Affine transformation—A first-order polynomial (affine) will be used to fit a flat plane to the input points.Second-order polynomial transformation—A second-order polynomial will be used to fit a somewhat more complicated surface to the input points.Third-order polynomial transformation—A third-order polynomial will be used to fit a more complicated surface to the input points.Optimize for global and local accuracy— A polynomial transformation is combined with a triangulated irregular network (TIN) interpolation technique that will optimize both global and local accuracy.Spline transformation— The source control points will be transformed precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points will not.Projective transformation— Lines will be warped so that they remain straight. Lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.Similarity transformation— A first order transformation will be used that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit. | String |
| Resampling Technique (Optional) | The resampling algorithm to be used.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered.Nearest neighbor— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.Bilinear interpolation— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.Cubic convolution—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.Majority resampling—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |
| in_raster | The raster to be transformed. | Mosaic Layer; Raster Layer |
| out_raster | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, a JPEG 2000 file, a TIFF file, or a geodatabase, you can specify a compression type and compression quality using environment settings..bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| link_file | The text, CSV file, or TAB file containing the coordinates to warp the input raster. This can be generated from the Register Raster tool or from the Georeferencing tab. | Text File |
| transformation_type(Optional) | Specifies the transformation method for shifting the raster dataset.POLYORDER0— A zero-order polynomial will be used to shift the data. This is commonly used when the data is georeferenced, but a small shift will better line it up. Only one link is required to perform a zero-order polynomial shift.POLYSIMILARITY— A first order transformation will be used that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit.POLYORDER1—A first-order polynomial (affine) will be used to fit a flat plane to the input points.POLYORDER2—A second-order polynomial will be used to fit a somewhat more complicated surface to the input points.POLYORDER3—A third-order polynomial will be used to fit a more complicated surface to the input points.ADJUST— A polynomial transformation is combined with a triangulated irregular network (TIN) interpolation technique that will optimize both global and local accuracy.SPLINE— The source control points will be transformed precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points will not.PROJECTIVE— Lines will be warped so that they remain straight. Lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products. | String |
| resampling_type(Optional) | The resampling algorithm to be used.NEAREST— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.BILINEAR— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.CUBIC—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.MAJORITY—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered. | String |

## Code Samples

### Example 1

```python
n = (p + 1) (p + 2) / 2
```

### Example 2

```python
n = (p + 1) (p + 2) / 2
```

### Example 3

```python
arcpy.management.WarpFromFile(in_raster, out_raster, link_file, {transformation_type}, {resampling_type})
```

### Example 4

```python
import arcpy
arcpy.WarpFromFile_management(
     "\\cpu\data\raster.img", "\\cpu\data\warp_out.tif",
     "\\cpu\data\gcpfile.txt", "POLYORDER2", "BILINEAR")
```

### Example 5

```python
import arcpy
arcpy.WarpFromFile_management(
     "\\cpu\data\raster.img", "\\cpu\data\warp_out.tif",
     "\\cpu\data\gcpfile.txt", "POLYORDER2", "BILINEAR")
```

### Example 6

```python
##Warp image with signiture file

import arcpy
arcpy.env.workspace = r"C:/Workspace"
    
    
arcpy.WarpFromFile_management("raster.img", "warp_output.tif", "gcpfile.txt", 
                      "POLYORDER2", "BILINEAR")
```

### Example 7

```python
##Warp image with signiture file

import arcpy
arcpy.env.workspace = r"C:/Workspace"
    
    
arcpy.WarpFromFile_management("raster.img", "warp_output.tif", "gcpfile.txt", 
                      "POLYORDER2", "BILINEAR")
```

---

## Warp (Data Management)

## Summary

Transforms a raster dataset using source and target control points. This is similar to georeferencing.

## Usage

- You must specify the source and target coordinates. The transformation type (polynomial order) from which to choose is dependent on the number of control points entered.
- The default polynomial order will perform an affine transformation.
- Warp is useful when the raster requires a systematic geometric correction that can be modeled with a polynomial. A spatial transformation can invert or remove a distortion using polynomial transformation of the proper order. The higher the order, the more complex the distortion that can be corrected. The higher orders of polynomial will involve progressively more processing time.
- To determine the minimum number of links necessary for a given order of polynomial, use the following formula:n = (p + 1) (p + 2) / 2 where n is the minimum number of links required for a transformation of polynomial order p. It is recommended that you use more than the minimum number of links.
- This tool will determine the extent of the warped raster and will set the number of rows and columns to be about the same as in the input raster. Some minor differences may be due to the changed proportion between the output raster's sizes in the x and y directions. The default cell size used will be computed by dividing the extent by the previously determined number of rows and columns. The value of the cell size will be used by the resampling algorithm.
- If you choose to define an output cell size in the Environment settings, the number of rows and columns will be calculated as follows:columns = (xmax - xmin) / cell size rows = (ymax - ymin) / cell size
- You can save the output to BIL, BIP, BMP, BSQ, DAT, Esri Grid, GIF, IMG, JPEG, JPEG 2000, PNG, TIFF, MRF, or CRF format, or any geodatabase raster dataset.
- When storing a raster dataset to a JPEG format file, a JPEG 2000 format file, or a geodatabase, you can specify a Compression Type value and a Compression Quality value in the geoprocessing environments.
- This tool supports multidimensional raster data. To run the tool on each slice in the multidimensional raster and generate a multidimensional raster output, be sure to save the output to CRF.Supported input multidimensional dataset types include multidimensional raster layer, mosaic dataset, image service, and CRF.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Raster | The raster to be transformed. | Mosaic Layer; Raster Layer |
| Source Control Points | The coordinates of the raster to be warped. | Point |
| Target Control Points | The coordinates to which the source raster will be warped. | Point |
| Output Raster Dataset | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, JPEG 2000 file, TIFF file, or geodatabase, you can specify a compression type and compression quality.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| Transformation Type(Optional) | Specifies the transformation method for shifting the raster dataset.Shift only— A zero-order polynomial will be used to shift the data. This is commonly used when the data is georeferenced, but a small shift will better line it up. Only one link is required to perform a zero-order polynomial shift.Affine transformation—A first-order polynomial (affine) will be used to fit a flat plane to the input points.Second-order polynomial transformation—A second-order polynomial will be used to fit a somewhat more complicated surface to the input points.Third-order polynomial transformation—A third-order polynomial will be used to fit a more complicated surface to the input points.Optimize for global and local accuracy— A polynomial transformation is combined with a triangulated irregular network (TIN) interpolation technique that will optimize both global and local accuracy.Spline transformation— The source control points will be transformed precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points will not.Projective transformation— Lines will be warped so that they remain straight. Lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products.Similarity transformation— A first order transformation will be used that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit. | String |
| Resampling Technique(Optional) | Specifies the resampling technique that will be used. The default is Nearest.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered.Nearest neighbor— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.Bilinear interpolation— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.Cubic convolution—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.Majority resampling—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data. | String |
| in_raster | The raster to be transformed. | Mosaic Layer; Raster Layer |
| source_control_points[source_control_point,...] | The coordinates of the raster to be warped. | Point |
| target_control_points[target_control_point,...] | The coordinates to which the source raster will be warped. | Point |
| out_raster | The name, location, and format for the dataset you are creating. When storing a raster dataset in a geodatabase, do not add a file extension to the name of the raster dataset. When storing your raster dataset to a JPEG file, JPEG 2000 file, TIFF file, or geodatabase, you can specify a compression type and compression quality.When storing the raster dataset in a file format, specify the file extension as follows:.bil—Esri BIL.bip—Esri BIP.bmp—BMP.bsq—Esri BSQ.dat—ENVI DAT.gif—GIF.img—ERDAS IMAGINE.jpg—JPEG.jp2—JPEG 2000.png—PNG.tif—TIFF.mrf—MRF.crf—CRFNo extension for Esri Grid | Raster Dataset |
| transformation_type(Optional) | Specifies the transformation method for shifting the raster dataset.POLYORDER0— A zero-order polynomial will be used to shift the data. This is commonly used when the data is georeferenced, but a small shift will better line it up. Only one link is required to perform a zero-order polynomial shift.POLYSIMILARITY— A first order transformation will be used that attempts to preserve the shape of the original raster. The RMS error tends to be higher than other polynomial transformations because the preservation of shape is more important than the best fit.POLYORDER1—A first-order polynomial (affine) will be used to fit a flat plane to the input points.POLYORDER2—A second-order polynomial will be used to fit a somewhat more complicated surface to the input points.POLYORDER3—A third-order polynomial will be used to fit a more complicated surface to the input points.ADJUST— A polynomial transformation is combined with a triangulated irregular network (TIN) interpolation technique that will optimize both global and local accuracy.SPLINE— The source control points will be transformed precisely to the target control points. In the output, the control points will be accurate, but the raster pixels between the control points will not.PROJECTIVE— Lines will be warped so that they remain straight. Lines that were once parallel may no longer remain parallel. The projective transformation is especially useful for oblique imagery, scanned maps, and for some imagery products. | String |
| resampling_type(Optional) | Specifies the resampling technique that will be used. The default is Nearest.NEAREST— The nearest neighbor technique will be used. It minimizes changes to pixel values since no new values are created and is the fastest resampling technique. It is suitable for discrete data, such as land cover.BILINEAR— The bilinear interpolation technique will be used. It calculates the value of each pixel by averaging (weighted for distance) the values of the surrounding four pixels. It is suitable for continuous data.CUBIC—The cubic convolution technique will be used. It calculates the value of each pixel by fitting a smooth curve based on the surrounding 16 pixels. This produces the smoothest image but can create values outside of the range found in the source data. It is suitable for continuous data.MAJORITY—The majority resampling technique will be used. It determines the value of each pixel based on the most popular value in a 4 by 4 window. It is suitable for discrete data.The Nearest and Majority options are used for categorical data, such as a land-use classification. The Nearest option is the default. It is the quickest and does not change the pixel values. Do not use either of these options for continuous data, such as elevation surfaces.The Bilinear and Cubic options are most appropriate for continuous data. It is recommended that you do not use either of these options with categorical data because the pixel values may be altered. | String |

## Code Samples

### Example 1

```python
n = (p + 1) (p + 2) / 2
```

### Example 2

```python
n = (p + 1) (p + 2) / 2
```

### Example 3

```python
columns = (xmax - xmin) / cell size
rows = (ymax - ymin) / cell size
```

### Example 4

```python
columns = (xmax - xmin) / cell size
rows = (ymax - ymin) / cell size
```

### Example 5

```python
arcpy.management.Warp(in_raster, source_control_points, target_control_points, out_raster, {transformation_type}, {resampling_type})
```

### Example 6

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
source_pnt = "'234718 3804287';'241037 3804297';'244193 3801275'"
target_pnt = "'246207 3820084';'270620 3824967';'302634 3816147'"
arcpy.Warp_management("raster.img", source_pnt, target_pnt, "warp.tif", "POLYORDER1",\
                          "BILINEAR")
```

### Example 7

```python
import arcpy
from arcpy import env
env.workspace = "c:/data"
source_pnt = "'234718 3804287';'241037 3804297';'244193 3801275'"
target_pnt = "'246207 3820084';'270620 3824967';'302634 3816147'"
arcpy.Warp_management("raster.img", source_pnt, target_pnt, "warp.tif", "POLYORDER1",\
                          "BILINEAR")
```

### Example 8

```python
##====================================
##Warp
##Usage: Warp_management in_raster source_control_points;source_control_points... 
##                       target_control_points;target_control_points... out_raster
##                       {POLYORDER_ZERO | POLYORDER1 | POLYORDER2 | POLYORDER3 | 
##                       ADJUST | SPLINE | PROJECTIVE} {NEAREST | BILINEAR | 
##                       CUBIC | MAJORITY}
    

import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Warp a TIFF raster dataset with control points
##Define source control points
source_pnt = "'234718 3804287';'241037 3804297';'244193 3801275'"

##Define target control points
target_pnt = "'246207 3820084';'270620 3824967';'302634 3816147'"

arcpy.Warp_management("raster.img", source_pnt, target_pnt, "warp.tif", "POLYORDER2",\
                      "BILINEAR")
```

### Example 9

```python
##====================================
##Warp
##Usage: Warp_management in_raster source_control_points;source_control_points... 
##                       target_control_points;target_control_points... out_raster
##                       {POLYORDER_ZERO | POLYORDER1 | POLYORDER2 | POLYORDER3 | 
##                       ADJUST | SPLINE | PROJECTIVE} {NEAREST | BILINEAR | 
##                       CUBIC | MAJORITY}
    

import arcpy

arcpy.env.workspace = r"C:/Workspace"

##Warp a TIFF raster dataset with control points
##Define source control points
source_pnt = "'234718 3804287';'241037 3804297';'244193 3801275'"

##Define target control points
target_pnt = "'246207 3820084';'270620 3824967';'302634 3816147'"

arcpy.Warp_management("raster.img", source_pnt, target_pnt, "warp.tif", "POLYORDER2",\
                      "BILINEAR")
```

---

## Workspace To Raster Dataset (Data Management)

## Summary

Merges all of the raster datasets in a folder into one raster dataset.

## Usage

- The target raster dataset must already exist for the tool to run.
- If a target raster dataset does not already exist, use the Create Raster Dataset tool to create a new raster dataset.
- Since mosaicking will take place, you will need to specify the mosaic method and color map mode to use.
- If the target raster dataset is an empty raster dataset, the cell size and spatial reference of the first input raster dataset will be applied to the mosaic.
- Whenever possible use the Last Mosaic Operator to mosaic raster datasets to an existing raster dataset in a file geodatabase or an enterprise geodatabase; it is by far the most effective way to mosaic.
- For mosaicking of discrete data, the First, Minimum, or Maximum option in Mosaic Operator will provide the most meaningful results. The Blend and Mean options are best suited for continuous data.
- The color matching method drop-down arrow allows you to choose an algorithm to color match the datasets in your mosaic.
- For file-based rasters, Ignore Background Value must be set to the same value as NoData for the background value to be ignored. Geodatabase rasters will work without this extra step.
- For floating-point input raster datasets of different resolutions or when cells are not aligned, it is recommended that you resample all the data using bilinear interpolation or cubic convolution before running Mosaic. Otherwise, Mosaic will automatically resample the raster datasets using nearest neighbor resampling, which is not appropriate for continuous data types.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Workspace | The folder containing the raster datasets to merge. | Workspace |
| Target Raster Dataset | An existing raster dataset in which to merge all of the raster datasets from the input workspace. | Raster Dataset |
| Include Sub-directories(Optional) | Specifies whether subdirectories will be included.Unchecked—Subdirectories will not be included. This is the default.Checked—All raster datasets in the subdirectories will be included when loading. | Boolean |
| Mosaic Operator(Optional) | Specifies the method that will be used to mosaic overlapping areas.First—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.Last—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.Blend—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.Mean—The output cell value of the overlapping areas will be the average value of the overlapping cells.Minimum—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.Maximum—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.Sum—The output cell value of the overlapping areas will be the total sum of the overlapping cells. | String |
| Mosaic Colormap Mode(Optional) | Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.First—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.Last—The color map from the last raster dataset in the list will be applied to the output raster mosaic.Match—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.Reject—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |
| Ignore Background Value(Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. Enterprise and file geodatabase rasters will work without this extra step. | Double |
| NoData Value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | Double |
| Convert 1 bit data to 8 bit(Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.Unchecked—No conversion will occur. This is the default.Checked—The input raster will be converted. | Boolean |
| Mosaicking Tolerance(Optional) | When mosaicking occurs, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, you need to decide whether to resample or shift the data. The mosaicking tolerance controls whether resampling of the pixels will occur or the pixels will be shifted.If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will occur. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not occur and a shift will be performed.The unit of tolerance is a pixel with a valid value range of 0 to 0.5. A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero guarantees resampling will occur if there is a misalignment in pixels.For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, resampling will occur since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, the pixels will be shifted. | Double |
| Color Matching Method (Optional) | The color matching method to apply to the rasters.None—This option will not use the color matching operation when mosaicking your raster datasets.Match statistics—This method will use descriptive statistics from the overlapping areas; the transformation will then be applied to the entire target dataset.Match histogram—This method will match the histogram from the reference overlap area to the source overlap area; the transformation will then be applied to the entire target dataset.Linear correlation—This method will match overlapping pixels and then interpolated the rest of the source dataset; pixels without a one-to-one relationship will use a weighted average. | String |
| Colormap to RGB (Optional) | Specifies whether the input raster dataset will be converted to a three-band output raster dataset if the input raster dataset includes a color map. This is useful when mosaicking rasters with different color maps.Unchecked—No conversion will occur. This is the default.Checked—The input dataset will be converted. | Boolean |
| in_workspace | The folder containing the raster datasets to merge. | Workspace |
| in_raster_dataset | An existing raster dataset in which to merge all of the raster datasets from the input workspace. | Raster Dataset |
| include_subdirectories(Optional) | Specifies whether subdirectories will be included.NONE—Subdirectories will not be included. This is the default.INCLUDE_SUBDIRECTORIES—All raster datasets in the subdirectories will be included when loading. | Boolean |
| mosaic_type(Optional) | Specifies the method that will be used to mosaic overlapping areas.FIRST—The output cell value of the overlapping areas will be the value from the first raster dataset mosaicked into that location.LAST—The output cell value of the overlapping areas will be the value from the last raster dataset mosaicked into that location. This is the default.BLEND—The output cell value of the overlapping areas will be a horizontally weighted calculation of the values of the cells in the overlapping area.MEAN—The output cell value of the overlapping areas will be the average value of the overlapping cells.MINIMUM—The output cell value of the overlapping areas will be the minimum value of the overlapping cells.MAXIMUM—The output cell value of the overlapping areas will be the maximum value of the overlapping cells.SUM—The output cell value of the overlapping areas will be the total sum of the overlapping cells. | String |
| colormap(Optional) | Specifies the method that will be used to choose which color map from the input rasters will be applied to the mosaic output.FIRST—The color map from the first raster dataset in the list will be applied to the output raster mosaic. This is the default.LAST—The color map from the last raster dataset in the list will be applied to the output raster mosaic.MATCH—All the color maps will be considered when mosaicking. If all possible values are already used (for the bit depth), the tool will match the value with the closest available color.REJECT—Only the raster datasets that do not have a color map associated with them will be mosaicked. | String |
| background_value(Optional) | Remove the unwanted values created around the raster data. The value specified will be distinguished from other valuable data in the raster dataset. For example, a value of zero along the raster dataset's borders will be distinguished from zero values in the raster dataset.The pixel value specified will be set to NoData in the output raster dataset. For file-based rasters the Ignore Background Value must be set to the same value as NoData in order for the background value to be ignored. Enterprise and file geodatabase rasters will work without this extra step. | Double |
| nodata_value(Optional) | All the pixels with the specified value will be set to NoData in the output raster dataset. | Double |
| onebit_to_eightbit(Optional) | Specifies whether the input 1-bit raster dataset will be converted to an 8-bit raster dataset. In this conversion, the value 1 in the input raster dataset will be changed to 255 in the output raster dataset. This is useful when importing a 1-bit raster dataset to a geodatabase. One-bit raster datasets have 8-bit pyramid layers when stored in a file system, but in a geodatabase, 1-bit raster datasets can only have 1-bit pyramid layers, which results in a lower-quality display. By converting the data to 8 bit in a geodatabase, the pyramid layers are built as 8 bit instead of 1 bit, resulting in a proper raster dataset in the display.NONE—No conversion will occur. This is the default.OneBitTo8Bit—The input raster will be converted. | Boolean |
| mosaicking_tolerance(Optional) | When mosaicking occurs, the target and the source pixels do not always line up exactly. When there is a misalignment of pixels, you need to decide whether to resample or shift the data. The mosaicking tolerance controls whether resampling of the pixels will occur or the pixels will be shifted.If the difference in pixel alignment (of the incoming dataset and the target dataset) is greater than the tolerance, resampling will occur. If the difference in pixel alignment (of the incoming dataset and the target dataset) is less than the tolerance, resampling will not occur and a shift will be performed.The unit of tolerance is a pixel with a valid value range of 0 to 0.5. A tolerance of 0.5 will guarantee a shift occurs. A tolerance of zero guarantees resampling will occur if there is a misalignment in pixels.For example, the source and target pixels have a misalignment of 0.25. If the mosaicking tolerance is set to 0.2, resampling will occur since the pixel misalignment is greater than the tolerance. If the mosaicking tolerance is set to 0.3, the pixels will be shifted. | Double |
| MatchingMethod(Optional) | The color matching method to apply to the rasters.NONE—This option will not use the color matching operation when mosaicking your raster datasets.STATISTIC_MATCHING—This method will use descriptive statistics from the overlapping areas; the transformation will then be applied to the entire target dataset.HISTOGRAM_MATCHING—This method will match the histogram from the reference overlap area to the source overlap area; the transformation will then be applied to the entire target dataset.LINEARCORRELATION_MATCHING—This method will match overlapping pixels and then interpolated the rest of the source dataset; pixels without a one-to-one relationship will use a weighted average. | String |
| colormap_to_RGB(Optional) | Specifies whether the input raster dataset will be converted to a three-band output raster dataset if the input raster dataset includes a color map. This is useful when mosaicking rasters with different color maps.NONE—No conversion will occur. This is the default.ColormapToRGB—The input dataset will be converted. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.WorkspaceToRasterDataset(in_workspace, in_raster_dataset, {include_subdirectories}, {mosaic_type}, {colormap}, {background_value}, {nodata_value}, {onebit_to_eightbit}, {mosaicking_tolerance}, {MatchingMethod}, {colormap_to_RGB})
```

### Example 2

```python
import arcpy
arcpy.WorkspaceToRasterDataset_management("c:/data/WS2RD", "c:/fgdb.gdb/outdats",
                                          "INCLUDE_SUBDIRECTORIES", "LAST",
                                          "FIRST", "0", "9", "", "",
                                          "HISTOGRAM_MATCHING", "")
```

### Example 3

```python
import arcpy
arcpy.WorkspaceToRasterDataset_management("c:/data/WS2RD", "c:/fgdb.gdb/outdats",
                                          "INCLUDE_SUBDIRECTORIES", "LAST",
                                          "FIRST", "0", "9", "", "",
                                          "HISTOGRAM_MATCHING", "")
```

### Example 4

```python
##==================================
##Workspace To Raster Dataset
##Usage: WorkspaceToRasterDataset_management in_workspace in_raster_dataset {NONE | INCLUDE_SUBDIRECTORIES} 
##                                           {LAST | FIRST | BLEND | MEAN | MINIMUM | MAXIMUM} {FIRST | REJECT
##                                           | LAST | MATCH} {background_value} {nodata_value} {NONE | OneBitTo8Bit} 
##                                           {mosaicking_tolerance}  {NONE | STATISTIC_MATCHING | HISTOGRAM_MATCHING
##                                           | LINEARCORRELATION_MATCHING} {NONE | ColormapToRGB}

import arcpy
arcpy.env.workspace = r"\\MyMachine\PrjWorkspace\RasGP"
##Mosaic images to File Geodatabase Raster Dataset with Background and Nodata setting and Color Correction
arcpy.WorkspaceToRasterDataset_management("WS2RD", "fgdb.gdb\\dataset", "INCLUDE_SUBDIRECTORIES", "LAST", \
                                          "FIRST", "0", "9", "", "", "HISTOGRAM_MATCHING", "")

##Mosaic Colormap image to RGB image
arcpy.WorkspaceToRasterDataset_management("WS2RD_clr","fgdb.gdb\\dataset2", "INCLUDE_SUBDIRECTORIES", "LAST",\
                                          "FIRST", "", "", "", "0.3", "", "ColormapToRGB")
```

### Example 5

```python
##==================================
##Workspace To Raster Dataset
##Usage: WorkspaceToRasterDataset_management in_workspace in_raster_dataset {NONE | INCLUDE_SUBDIRECTORIES} 
##                                           {LAST | FIRST | BLEND | MEAN | MINIMUM | MAXIMUM} {FIRST | REJECT
##                                           | LAST | MATCH} {background_value} {nodata_value} {NONE | OneBitTo8Bit} 
##                                           {mosaicking_tolerance}  {NONE | STATISTIC_MATCHING | HISTOGRAM_MATCHING
##                                           | LINEARCORRELATION_MATCHING} {NONE | ColormapToRGB}

import arcpy
arcpy.env.workspace = r"\\MyMachine\PrjWorkspace\RasGP"
##Mosaic images to File Geodatabase Raster Dataset with Background and Nodata setting and Color Correction
arcpy.WorkspaceToRasterDataset_management("WS2RD", "fgdb.gdb\\dataset", "INCLUDE_SUBDIRECTORIES", "LAST", \
                                          "FIRST", "0", "9", "", "", "HISTOGRAM_MATCHING", "")

##Mosaic Colormap image to RGB image
arcpy.WorkspaceToRasterDataset_management("WS2RD_clr","fgdb.gdb\\dataset2", "INCLUDE_SUBDIRECTORIES", "LAST",\
                                          "FIRST", "", "", "", "0.3", "", "ColormapToRGB")
```

---

## XY Table To Point (Data Management)

## Summary

Creates a point feature class based on x-, y-, and z-coordinates from a table.

## Usage


## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The table containing the x- and y-coordinates that define the locations of the point features that will be created. | Table View |
| Output Feature Class | The feature class containing the output point features. | Feature Class |
| X Field | The field in the input table that contains the x-coordinates (longitude). | Field |
| Y Field | The field in the input table that contains the y-coordinates (latitude). | Field |
| Z Field (Optional) | The field in the input table that contains the z-coordinates. | Field |
| Coordinate System (Optional) | The coordinate system of the x- and y-coordinates. This will be the coordinate system of the output feature class. | Spatial Reference |
| in_table | The table containing the x- and y-coordinates that define the locations of the point features that will be created. | Table View |
| out_feature_class | The feature class containing the output point features. | Feature Class |
| x_field | The field in the input table that contains the x-coordinates (longitude). | Field |
| y_field | The field in the input table that contains the y-coordinates (latitude). | Field |
| z_field(Optional) | The field in the input table that contains the z-coordinates. | Field |
| coordinate_system(Optional) | The coordinate system of the x- and y-coordinates. This will be the coordinate system of the output feature class. | Spatial Reference |

## Code Samples

### Example 1

```python
arcpy.management.XYTableToPoint(in_table, out_feature_class, x_field, y_field, {z_field}, {coordinate_system})
```

### Example 2

```python
import arcpy
arcpy.env.workspace = r"c:\output.gdb"
arcpy.management.XYTableToPoint(r"c:\data\tree.csv", "tree_points",
                                "longitude", "latitude", "elevation",
                                arcpy.SpatialReference(4759, 115700))
```

### Example 3

```python
import arcpy
arcpy.env.workspace = r"c:\output.gdb"
arcpy.management.XYTableToPoint(r"c:\data\tree.csv", "tree_points",
                                "longitude", "latitude", "elevation",
                                arcpy.SpatialReference(4759, 115700))
```

### Example 4

```python
# XYTableToPoint.py
# Description: Creates a point feature class from input table

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = r"c:\output.gdb"

# Set the local variables
in_table = r"c:\data\tree.csv"
out_feature_class = "tree_points"
x_coords = "longitude"
y_coords = "latitude"
z_coords = "elevation"

# Make the XY event layer...
arcpy.management.XYTableToPoint(in_table, out_feature_class,
                                x_coords, y_coords, z_coords,
                                arcpy.SpatialReference(4759, 115700))

# Print the total rows
print(arcpy.management.GetCount(out_feature_class))
```

### Example 5

```python
# XYTableToPoint.py
# Description: Creates a point feature class from input table

# import system modules 
import arcpy

# Set environment settings
arcpy.env.workspace = r"c:\output.gdb"

# Set the local variables
in_table = r"c:\data\tree.csv"
out_feature_class = "tree_points"
x_coords = "longitude"
y_coords = "latitude"
z_coords = "elevation"

# Make the XY event layer...
arcpy.management.XYTableToPoint(in_table, out_feature_class,
                                x_coords, y_coords, z_coords,
                                arcpy.SpatialReference(4759, 115700))

# Print the total rows
print(arcpy.management.GetCount(out_feature_class))
```

---

## XY To Line (Data Management)

## Summary

Creates a feature class containing geodetic or planar line features from the values in a start x-coordinate field, start y-coordinate field, end x-coordinate field, and end y-coordinate field of a table.

## Usage

- Output lines are constructed from field values. The field values include the following:The x- and y-coordinates of a starting pointThe x- and y-coordinates of an ending pointThe fields and their values will be included in the output.
- The x- and y-coordinates of a starting point
- The x- and y-coordinates of an ending point
- For a geodetic line, the x- and y-coordinates are locations on the surface of the earth. For a planar line, the coordinates are locations on the projected plane.
- A geodetic line is a curve on the surface of the earth. However, a geodetic line feature is not stored as a parametric (true) curve in the output; rather, it is stored as a densified polyline representing the path of the geodetic line. If the length of a geodetic line is relatively short, it may be represented by a straight line in the output. As the length of the line increases, more vertices are used to represent the path.
- When the output is a feature class in a geodatabase, the values in the Shape_Length field are always in the units of the output coordinate system specified by the Spatial Reference parameter, and they are the planar lengths of the polylines. To measure a geodesic length or distance, use the ArcGIS Pro Measure tool and choose the Geodesic, Loxodrome, or Great Elliptic option accordingly before taking a measurement.

## Parameters

| Parameter | Description | Data Type |
|-----------|-------------|-----------|
| Input Table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| Output Feature Class | The output feature class containing geodetic or planar lines. | Feature Class |
| Start X Field | A numerical field in the input table containing the x-coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Start Y Field | A numerical field in the input table containing the y-coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| End X Field | A numerical field in the input table containing the x-coordinates (or longitudes) of the ending points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| End Y Field | A numerical field in the input table containing the y-coordinates (or latitudes) of the ending points of lines to be positioned in the output coordinate system specified by the Spatial Reference parameter. | Field |
| Line Type (Optional) | Specifies the type of line that will be constructed.Geodesic— A type of geodetic line that most accurately represents the shortest distance between any two points on the surface of the earth will be constructed. This is the default.Great circle—A type of geodetic line that represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth will be constructed. If the Spatial Reference parameter value is a spheroid-based coordinate system, the line is a great elliptic. If the Spatial Reference parameter value is a sphere-based coordinate system, the line is uniquely called a great circle—a circle of the largest radius on the spherical surface.Rhumb line—A type of geodetic line, also known as a loxodrome line, that represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole will be constructed. A rhumb line is shown as a straight line in the Mercator projection.Normal section—A type of geodetic line that represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points will be constructed. The normal section line from point A to point B is different from the line from point B to point A.Planar line—A straight line in the projected plane will be used. A planar line usually does not accurately represent the shortest distance on the surface of the earth as a geodesic line does. This option is not available for geographic coordinate systems. | String |
| ID (Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| Spatial Reference (Optional) | The spatial reference of the output feature class. The default is GCS_WGS_1984 or the input coordinate system if it is not Unknown. | Spatial Reference |
| Preserve attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.Unchecked—The remaining input fields will not be added to the output feature class. This is the default.Checked—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |
| in_table | The input table. It can be a text file, CSV file, Excel file, dBASE table, or geodatabase table. | Table View |
| out_featureclass | The output feature class containing geodetic or planar lines. | Feature Class |
| startx_field | A numerical field in the input table containing the x-coordinates (or longitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| starty_field | A numerical field in the input table containing the y-coordinates (or latitudes) of the starting points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| endx_field | A numerical field in the input table containing the x-coordinates (or longitudes) of the ending points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| endy_field | A numerical field in the input table containing the y-coordinates (or latitudes) of the ending points of lines to be positioned in the output coordinate system specified by the spatial_reference parameter. | Field |
| line_type(Optional) | Specifies the type of line that will be constructed.GEODESIC— A type of geodetic line that most accurately represents the shortest distance between any two points on the surface of the earth will be constructed. This is the default.GREAT_CIRCLE—A type of geodetic line that represents the path between any two points along the intersection of the surface of the earth and a plane that passes through the center of the earth will be constructed. If the Spatial Reference parameter value is a spheroid-based coordinate system, the line is a great elliptic. If the Spatial Reference parameter value is a sphere-based coordinate system, the line is uniquely called a great circle—a circle of the largest radius on the spherical surface.RHUMB_LINE—A type of geodetic line, also known as a loxodrome line, that represents a path between any two points on the surface of a spheroid defined by a constant azimuth from a pole will be constructed. A rhumb line is shown as a straight line in the Mercator projection.NORMAL_SECTION—A type of geodetic line that represents a path between any two points on the surface of a spheroid defined by the intersection of the spheroid surface and a plane that passes through the two points and is normal (perpendicular) to the spheroid surface at the starting point of the two points will be constructed. The normal section line from point A to point B is different from the line from point B to point A.PLANAR—A straight line in the projected plane will be used. A planar line usually does not accurately represent the shortest distance on the surface of the earth as a geodesic line does. This option is not available for geographic coordinate systems. | String |
| id_field(Optional) | A field in the input table. This field and the values are included in the output and can be used to join the output features with the records in the input table. | Field |
| spatial_reference(Optional) | The spatial reference of the output feature class. A spatial reference can be specified as any of the following: The path to a .prj file, such as C:/workspace/watershed.prj The path to a feature class or feature dataset whose spatial reference you want to apply, such as C:/workspace/myproject.gdb/landuse/grassland A SpatialReference object, such as arcpy.SpatialReference("C:/data/Africa/Carthage.prj") | Spatial Reference |
| attributes(Optional) | Specifies whether the remaining input fields will be added to the output feature class.NO_ATTRIBUTES—The remaining input fields will not be added to the output feature class. This is the default.ATTRIBUTES—The remaining input fields will be added to the output feature class. A new field, ORIG_FID, will also be added to the output feature class to store the input feature ID values. | Boolean |

## Code Samples

### Example 1

```python
arcpy.management.XYToLine(in_table, out_featureclass, startx_field, starty_field, endx_field, endy_field, {line_type}, {id_field}, {spatial_reference}, {attributes})
```

### Example 2

```python
# Import system modules
import arcpy

# Set local variables
input_table = r"c:\workspace\city2city.dbf"
out_lines = r"c:\workspace\flt4421.gdb\routing001"

# XY To Line
arcpy.XYToLine_management(input_table, out_lines, "LOND1", "LATD1", "LOND2",
                          "LATD2", "GEODESIC", "idnum")
```

### Example 3

```python
# Import system modules
import arcpy

# Set local variables
input_table = r"c:\workspace\city2city.dbf"
out_lines = r"c:\workspace\flt4421.gdb\routing001"

# XY To Line
arcpy.XYToLine_management(input_table, out_lines, "LOND1", "LATD1", "LOND2",
                          "LATD2", "GEODESIC", "idnum")
```

---
